GitDiffStart: f012c2d44d4b667ae932da60aab6fe6ba216f86e | Tue Jan 4 12:28:10 2011 +0000
diff --git a/lucene/contrib/CHANGES.txt b/lucene/contrib/CHANGES.txt
index fb58977..4d9ae07 100644
--- a/lucene/contrib/CHANGES.txt
+++ b/lucene/contrib/CHANGES.txt
@@ -7,6 +7,8 @@ Build
  * LUCENE-2413: Moved the demo out of lucene core and into contrib/demo.
    (Robert Muir)
 
+ * LUCENE-2845: Moved contrib/benchmark to modules.
+
 New Features
 
   * LUCENE-2604: Added RegexpQuery support to contrib/queryparser.
diff --git a/lucene/contrib/benchmark/.rsync-filter b/lucene/contrib/benchmark/.rsync-filter
deleted file mode 100644
index 48f861a..0000000
--- a/lucene/contrib/benchmark/.rsync-filter
+++ /dev/null
@@ -1,2 +0,0 @@
-- /work
-- /temp
diff --git a/lucene/contrib/benchmark/CHANGES.txt b/lucene/contrib/benchmark/CHANGES.txt
deleted file mode 100644
index 58f0f70..0000000
--- a/lucene/contrib/benchmark/CHANGES.txt
+++ /dev/null
@@ -1,359 +0,0 @@
-Lucene Benchmark Contrib Change Log
-
-The Benchmark contrib package contains code for benchmarking Lucene in a variety of ways.
-
-10/10/2010
-  The locally built patched version of the Xerces-J jar introduced
-  as part of LUCENE-1591 is no longer required, because Xerces
-  2.10.0, which contains a fix for XERCESJ-1257 (see
-  http://svn.apache.org/viewvc?view=revision&revision=554069),
-  was released earlier this year.  Upgraded
-  xerces-2.9.1-patched-XERCESJ-1257.jar and xml-apis-2.9.0.jar
-  to xercesImpl-2.10.0.jar and xml-apis-2.10.0.jar. (Steven Rowe)
-
-8/2/2010
-  LUCENE-2582: You can now specify the default codec to use for
-  writing new segments by adding default.codec = Pulsing (for
-  example), in the alg file.  (Mike McCandless)
-
-4/27/2010: WriteLineDocTask now supports multi-threading. Also, 
-  StringBufferReader was renamed to StringBuilderReader and works on 
-  StringBuilder now. In addition, LongToEnglishCountentSource starts from 0
-  (instead of Long.MIN_VAL+10) and wraps around to MIN_VAL (if you ever hit 
-  Long.MAX_VAL). (Shai Erera)
-
-4/07/2010
-  LUCENE-2377: Enable the use of NoMergePolicy and NoMergeScheduler by 
-  CreateIndexTask. (Shai Erera)
-  
-3/28/2010
-  LUCENE-2353: Fixed bug in Config where Windows absolute path property values 
-  were incorrectly handled (Shai Erera)
-  
-3/24/2010
-  LUCENE-2343: Added support for benchmarking collectors. (Grant Ingersoll, Shai Erera)
-
-2/21/2010
-  LUCENE-2254: Add support to the quality package for running
-  experiments with any combination of Title, Description, and Narrative.
-  (Robert Muir)
-
-1/28/2010
-  LUCENE-2223: Add a benchmark for ShingleFilter. You can wrap any
-  analyzer with ShingleAnalyzerWrapper and specify shingle parameters
-  with the NewShingleAnalyzer task.  (Steven Rowe via Robert Muir)
-
-1/14/2010
-  LUCENE-2210: TrecTopicsReader now properly reads descriptions and
-  narratives from trec topics files.  (Robert Muir)
-
-1/11/2010
-  LUCENE-2181: Add a benchmark for collation. This adds NewLocaleTask,
-  which sets a Locale in the run data for collation to use, and can be
-  used in the future for benchmarking localized range queries and sorts.
-  Also add NewCollationAnalyzerTask, which works with both JDK and ICU
-  Collator implementations. Fix ReadTokensTask to not tokenize fields
-  unless they should be tokenized according to DocMaker config. The 
-  easiest way to run the benchmark is to run 'ant collation'
-  (Steven Rowe via Robert Muir)
-
-12/22/2009
-  LUCENE-2178: Allow multiple locations to add to the class path with
-  -Dbenchmark.ext.classpath=... when running "ant run-task" (Steven
-  Rowe via Mike McCandless)
-
-12/17/2009
-  LUCENE-2168: Allow negative relative thread priority for BG tasks
-  (Mike McCandless)
-
-12/07/2009
-  LUCENE-2106: ReadTask does not close its Reader when 
-  OpenReader/CloseReader are not used. (Mark Miller)
-
-11/17/2009
-  LUCENE-2079: Allow specifying delta thread priority after the "&";
-  added log.time.step.msec to print per-time-period counts; fixed
-  NearRealTimeTask to print reopen times (in msec) of each reopen, at
-  the end.  (Mike McCandless)
-
-11/13/2009
-  LUCENE-2050: Added ability to run tasks within a serial sequence in
-  the background, by appending "&".  The tasks are stopped & joined at
-  the end of the sequence.  Also added Wait and RollbackIndex tasks.
-  Genericized NearRealTimeReaderTask to only reopen the reader
-  (previously it spawned its own thread, and also did searching).
-  Also changed the API of PerfRunData.getIndexReader: it now returns a
-  reference, and it's your job to decRef the reader when you're done
-  using it.  (Mike McCandless)
-
-11/12/2009
-  LUCENE-2059: allow TrecContentSource not to change the docname.
-  Previously, it would always append the iteration # to the docname.
-  With the new option content.source.excludeIteration, you can disable this.
-  The resulting index can then be used with the quality package to measure
-  relevance. (Robert Muir)
-  
-11/12/2009
-  LUCENE-2058: specify trec_eval submission output from the command line.
-  Previously, 4 arguments were required, but the third was unused. The 
-  third argument is now the desired location of submission.txt  (Robert Muir)
-
-11/08/2009
-  LUCENE-2044: Added delete.percent.rand.seed to seed the Random instance
-  used by DeleteByPercentTask.  (Mike McCandless)
-
-11/07/2009
-  LUCENE-2043: Fix CommitIndexTask to also commit pending IndexReader
-  changes (Mike McCandless)
-
-11/07/2009
-  LUCENE-2042: Added print.hits.field, to print each hit from the
-  Search* tasks.  (Mike McCandless)
-
-11/04/2009
-  LUCENE-2029: Added doc.body.stored and doc.body.tokenized; each
-  falls back to the non-body variant as its default.  (Mike McCandless)
-
-10/28/2009
-  LUCENE-1994: Fix thread safety of EnwikiContentSource and DocMaker
-  when doc.reuse.fields is false.  Also made docs.reuse.fields=true
-  thread safe.  (Mark Miller, Shai Erera, Mike McCandless)
-
-8/4/2009
-  LUCENE-1770: Add EnwikiQueryMaker (Mark Miller)
-
-8/04/2009
-  LUCENE-1773: Add FastVectorHighlighter tasks.  This change is a
-  non-backwards compatible change in how subclasses of ReadTask define
-  a highlighter.  The methods doHighlight, isMergeContiguousFragments,
-  maxNumFragments and getHighlighter are no longer used and have been
-  mark deprecated and package protected private so there's a compile
-  time error.  Instead, the new getBenchmarkHighlighter method should
-  return an appropriate highlighter for the task. The configuration of
-  the highlighter tasks (maxFrags, mergeContiguous, etc.) is now
-  accepted as params to the task.  (Koji Sekiguchi via Mike McCandless)
-
-8/03/2009
-  LUCENE-1778: Add support for log.step setting per task type. Perviously, if
-  you included a log.step line in the .alg file, it had been applied to all
-  tasks. Now, you can include a log.step.AddDoc, or log.step.DeleteDoc (for 
-  example) to control logging for just these tasks. If you want to ommit logging
-  for any other task, include log.step=-1. The syntax is "log.step." together
-  with the Task's 'short' name (i.e., without the 'Task' part).
-  (Shai Erera via Mark Miller)
-
-7/24/2009
-  LUCENE-1595: Deprecate LineDocMaker and EnwikiDocMaker in favor of
-  using DocMaker directly, with content.source = LineDocSource or
-  EnwikiContentSource.  NOTE: with this change, the "id" field from
-  the Wikipedia XML export is now indexed as the "docname" field
-  (previously it was indexed as "docid").  Additionaly, the
-  SearchWithSort task now accepts all types that SortField can accept
-  and no longer falls back to SortField.AUTO, which has been
-  deprecated. (Mike McCandless)
-
-7/20/2009
-  LUCENE-1755: Fix WriteLineDocTask to output a document if it contains either 
-  a title or body (or both).  (Shai Erera via Mark Miller)
-
-7/14/2009
-  LUCENE-1725: Fix the example Sort algorithm - auto is now deprecated and no longer works
-  with Benchmark. Benchmark will now throw an exception if you specify sort fields without
-  a type. The example sort algorithm is now typed.  (Mark Miller)
-
-7/6/2009
-  LUCENE-1730: Fix TrecContentSource to use ISO-8859-1 when reading the TREC files, 
-  unless a different encoding is specified. Additionally, ContentSource now supports 
-  a content.source.encoding parameter in the configuration file. 
-  (Shai Erera via Mark Miller)
-
-6/26/2009
-  LUCENE-1716: Added the following support: 
-  doc.tokenized.norms: specifies whether to store norms
-  doc.body.tokenized.norms: special attribute for the body field
-  doc.index.props: specifies whether DocMaker should index the properties set on
-  DocData
-  writer.info.stream: specifies the info stream to set on IndexWriter (supported
-  values are: SystemOut, SystemErr and a file name). (Shai Erera via Mike McCandless)
-  
-6/23/09
-  LUCENE-1714: WriteLineDocTask incorrectly  normalized text, by replacing only 
-  occurrences of "\t" with a space. It now replaces "\r\n" in addition to that, 
-  so that LineDocMaker won't fail. (Shai Erera via Michael McCandless)
-  
-6/17/09 
-  LUCENE-1595: This issue breaks previous external algorithms. DocMaker has been 
-  replaced with a concrete class which accepts a ContentSource for iterating over 
-  a content source's documents. Most of the old DocMakers were changed to a 
-  ContentSource implementation, and DocMaker is now a default document creation impl
-  that provides an easy way for reusing fields. When [doc.maker] is not defined in 
-  an algorithm, the new DocMaker is the default. If you have .alg files which 
-  specify a DocMaker (like ReutersDocMaker), you should change the [doc.maker] line to: 
-  [content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource]
-  
-  i.e.
-  doc.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersDocMaker
-  becomes
-  content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-  
-  doc.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleDocMaker
-  becomes
-  content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
- 	
-  Also, PerfTask now logs a message in tearDown() rather than each Task doing its
-  own logging. A new setting called [log.step] is consulted to determine how often 
-  to log. [doc.add.log.step] is no longer a valid setting. For easy migration of 
-  current .alg files, rename [doc.add.log.step] to [log.step] and [doc.delete.log.step] 
-  to [delete.log.step]. 
-  
-  Additionally, [doc.maker.forever] should be changed to [content.source.forever].
-  (Shai Erera via Mark Miller)
-
-6/12/09 
-  LUCENE-1539: Added DeleteByPercentTask which enables deleting a
-  percentage of documents and searching on them.  Changed CommitIndex
-  to optionally accept a label (recorded as userData=<label> in the
-  commit point).  Added FlushReaderTask, and modified OpenReaderTask
-  to also optionally take a label referencing a commit point to open.
-  Also changed default autoCommit (when IndexWriter is opened) to
-  true. (Jason Rutherglen via Mike McCandless)
-
-12/20/08
-  LUCENE-1495: Allow task sequence to run for specfied number of seconds by adding ": 2.7s" (for example).
-
-12/16/08
-  LUCENE-1493: Stop using deprecated Hits API for searching; add new
-  param search.num.hits to set top N docs to collect.
-
-12/16/08
-  LUCENE-1492: Added optional readOnly param (default true) to OpenReader task.
-
-9/9/08
- LUCENE-1243: Added new sorting benchmark capabilities.  Also Reopen and commit tasks.  (Mark Miller via Grant Ingersoll)
-
-5/10/08
-  LUCENE-1090: remove relative paths assumptions from benchmark code.
-  Only build.xml was modified: work-dir definition must remain so  
-  benchmark tests can run from both trunk-home and benchmark-home.  
-  
-3/9/08
-  LUCENE-1209: Fixed DocMaker settings by round. Prior to this fix, DocMaker settings of 
-  first round were used in all rounds.  (E.g. term vectors.)
-  (Mark Miller via Doron Cohen) 
-
-1/30/08
-  LUCENE-1156: Fixed redirect problem in EnwikiDocMaker.  Refactored ExtractWikipedia to use EnwikiDocMaker.  Added property to EnwikiDocMaker to allow
-  for skipping image only documents.
-
-1/24/2008
-  LUCENE-1136: add ability to not count sub-task doLogic increment
-  
-1/23/2008
-  LUCENE-1129: ReadTask properly uses the traversalSize value
-  LUCENE-1128: Added support for benchmarking the highlighter
-
-01/20/08
-  LUCENE-1139: various fixes
-  - add merge.scheduler, merge.policy config properties
-  - refactor Open/CreateIndexTask to share setting config on IndexWriter
-  - added doc.reuse.fields=true|false for LineDocMaker
-  - OptimizeTask now takes int param to call optimize(int maxNumSegments)
-  - CloseIndexTask now takes bool param to call close(false) (abort running merges)
-
-
-01/03/08
-  LUCENE-1116: quality package improvements:
-  - add MRR computation; 
-  - allow control of max #queries to run;
-  - verify log & report are flushed.
-  - add TREC query reader for the 1MQ track.  
-      
-12/31/07
-  LUCENE-1102: EnwikiDocMaker now indexes the docid field, so results might not be comparable with results prior to this change, although
-  it is doubted that this one small field makes much difference.
-  
-12/13/07
-  LUCENE-1086: DocMakers setup for the "docs.dir" property
-  fixed to properly handle absolute paths. (Shai Erera via Doron Cohen)
-  
-9/18/07
-  LUCENE-941: infinite loop for alg: {[AddDoc(4000)]: 4} : *
-  ResetInputsTask fixed to work also after exhaustion.
-  All Reset Tasks now subclas ResetInputsTask.
-
-8/9/07
-  LUCENE-971: Change enwiki tasks to a doc maker (extending
-  LineDocMaker) that directly processes the Wikipedia XML and produces
-  documents.  Intermediate files (one per document) are no longer
-  created.
-
-8/1/07
-  LUCENE-967: Add "ReadTokensTask" to allow for benchmarking just tokenization.
-
-7/27/07
-  LUCENE-836: Add support for search quality benchmarking, running 
-  a set of queries against a searcher, and, optionally produce a submission
-  report, and, if query judgements are available, compute quality measures:
-  recall, precision_at_N, average_precision, MAP. TREC specific Judge (based 
-  on TREC QRels) and TREC Topics reader are included in o.a.l.benchmark.quality.trec
-  but any other format of queries and judgements can be implemented and used.
-  
-7/24/07
-  LUCENE-947: Add support for creating and index "one document per
-  line" from a large text file, which reduces per-document overhead of
-  opening a single file for each document.
-
-6/30/07
-  LUCENE-848: Added support for Wikipedia benchmarking.
-
-6/25/07
-- LUCENE-940: Multi-threaded issues fixed: SimpleDateFormat; logging for addDoc/deleteDoc tasks.
-- LUCENE-945: tests fail to find data dirs. Added sys-prop benchmark.work.dir and cfg-prop work.dir.
-(Doron Cohen)
-
-4/17/07
-- LUCENE-863: Deprecated StandardBenchmarker in favour of byTask code.
-  (Otis Gospodnetic)
-
-4/13/07
-
-Better error handling and javadocs around "exhaustive" doc making.
-
-3/25/07
-
-LUCENE-849: 
-1. which HTML Parser is used is configurable with html.parser property.
-2. External classes added to classpath with -Dbenchmark.ext.classpath=path.
-3. '*' as repeating number now means "exhaust doc maker - no repetitions".
-
-3/22/07
-
--Moved withRetrieve() call out of the loop in ReadTask
--Added SearchTravRetLoadFieldSelectorTask to help benchmark some of the FieldSelector capabilities
--Added options to store content bytes on the Reuters Doc (and others, but Reuters is the only one w/ it enabled)
-
-3/21/07
-
-Tests (for benchmarking code correctness) were added - LUCENE-840.
-To be invoked by "ant test" from contrib/benchmark. (Doron Cohen)
-
-3/19/07
-
-1. Introduced an AbstractQueryMaker to hold common QueryMaker code. (GSI)
-2. Added traversalSize parameter to SearchTravRetTask and SearchTravTask.  Changed SearchTravRetTask to extend SearchTravTask. (GSI)
-3. Added FileBasedQueryMaker to run queries from a File or resource. (GSI)
-4. Modified query-maker generation for read related tasks to make further read tasks addition simpler and safer. (DC)
-5. Changed Taks' setParams() to throw UnsupportedOperationException if that task does not suppot command line param. (DC)
-6. Improved javadoc to specify all properties command line params currently supported. (DC)
-7. Refactored ReportTasks so that it is easy/possible now to create new report tasks. (DC)
-
-01/09/07
-
-1. Committed Doron Cohen's benchmarking contribution, which provides an easily expandable task based approach to benchmarking.  See the javadocs for information. (Doron Cohen via Grant Ingersoll)
-
-2. Added this file.
-
-3. 2/11/07: LUCENE-790 and 788:  Fixed Locale issue with date formatter. Fixed some minor issues with benchmarking by task.  Added a dependency
- on the Lucene demo to the build classpath.  (Doron Cohen, Grant Ingersoll)
-
-4. 2/13/07: LUCENE-801: build.xml now builds Lucene core and Demo first and has classpath dependencies on the output of that build.  (Doron Cohen, Grant Ingersoll)
diff --git a/lucene/contrib/benchmark/README.enwiki b/lucene/contrib/benchmark/README.enwiki
deleted file mode 100644
index f9d4930..0000000
--- a/lucene/contrib/benchmark/README.enwiki
+++ /dev/null
@@ -1,22 +0,0 @@
-Support exists for downloading, parsing, and loading the English
-version of wikipedia (enwiki).
-
-The build file can automatically try to download the most current
-enwiki dataset (pages-articles.xml.bz2) from the "latest" directory,
-http://download.wikimedia.org/enwiki/latest/. However, this file
-doesn't always exist, depending on where wikipedia is in the dump
-process and whether prior dumps have succeeded. If this file doesn't
-exist, you can sometimes find an older or in progress version by
-looking in the dated directories under
-http://download.wikimedia.org/enwiki/. For example, as of this
-writing, there is a page file in
-http://download.wikimedia.org/enwiki/20070402/. You can download this
-file manually and put it in temp. Note that the file you download will
-probably have the date in the name, e.g.,
-http://download.wikimedia.org/enwiki/20070402/enwiki-20070402-pages-articles.xml.bz2. When
-you put it in temp, rename it to enwiki-latest-pages-articles.xml.bz2.
-
-After that, ant enwiki should process the data set and run a load
-test. Ant targets get-enwiki, expand-enwiki, and extract-enwiki can
-also be used to download, decompress, and extract (to individual files
-in work/enwiki) the dataset, respectively.
diff --git a/lucene/contrib/benchmark/build.xml b/lucene/contrib/benchmark/build.xml
deleted file mode 100644
index cbc6d1d..0000000
--- a/lucene/contrib/benchmark/build.xml
+++ /dev/null
@@ -1,260 +0,0 @@
-<?xml version="1.0"?>
-<project name="benchmark" default="default">
-
-    <description>
-        Lucene Benchmarking Contributions
-    </description>
-
-    <import file="../contrib-build.xml"/>
-    <property name="working.dir" location="work"/>
-
-    <!-- the tests have some parallel problems -->
-    <property name="tests.threadspercpu" value="0"/>
-
-    <contrib-uptodate name="highlighter" property="highlighter.uptodate" classpath.property="highlighter.jar"/>
-    <module-uptodate name="analysis/icu" jarfile="${common.dir}/../modules/analysis/build/icu/lucene-analyzers-icu-${version}.jar"
-      property="analyzers-icu.uptodate" classpath.property="analyzers-icu.jar"/>
-    <!-- analyzers common needs a hack for the jar file: -->
-    <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
-      property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
-    <contrib-uptodate name="memory" property="memory.uptodate" classpath.property="memory.jar"/>
-    <contrib-uptodate name="demo" property="demo.uptodate" classpath.property="demo.jar"/>
-
-    <target name="check-files">
-        <available file="temp/news20.tar.gz" property="news20.exists"/>
-
-        <available file="${working.dir}/20_newsgroup" property="news20.expanded"/>
-
-        <available file="temp/reuters21578.tar.gz" property="reuters.exists"/>
-        <available file="${working.dir}/reuters" property="reuters.expanded"/>
-        <available file="${working.dir}/reuters-out" property="reuters.extracted"/>
-        <available file="temp/20news-18828.tar.gz" property="20news-18828.exists"/>
-        <available file="${working.dir}/20news-18828" property="20news-18828.expanded"/>
-        <available file="${working.dir}/mini_newsgroups" property="mini.expanded"/>
-        
-        <available file="temp/enwiki-20070527-pages-articles.xml.bz2" property="enwiki.exists"/>
-        <available file="temp/enwiki-20070527-pages-articles.xml" property="enwiki.expanded"/>
-        <available file="${working.dir}/enwiki.txt" property="enwiki.extracted"/>
-    	<available file="temp/${top.100k.words.archive.filename}"
-                   property="top.100k.words.archive.present"/>
-    	<available file="${working.dir}/top100k-out" 
-                   property="top.100k.word.files.expanded"/>
-    </target>
-
-    <target name="enwiki-files" depends="check-files">
-        <mkdir dir="temp"/>
-        <antcall target="get-enwiki"/>
-        <antcall target="expand-enwiki"/>
-    </target>
-
-    <target name="get-enwiki" unless="enwiki.exists">
-        <get src="http://people.apache.org/~gsingers/wikipedia/enwiki-20070527-pages-articles.xml.bz2"
-             dest="temp/enwiki-20070527-pages-articles.xml.bz2"/>
-    </target>
-
-    <target name="expand-enwiki"  unless="enwiki.expanded">
-        <bunzip2 src="temp/enwiki-20070527-pages-articles.xml.bz2" dest="temp"/>
-    </target>
-
-    <target name="get-news-20" unless="20news-18828.exists">
-        <get src="http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz"
-             dest="temp/news20.tar.gz"/>
-
-    </target>
-    <target name="get-reuters" unless="reuters.exists">
-
-        <get src="http://www.daviddlewis.com/resources/testcollections/reuters21578/reuters21578.tar.gz"
-            dest="temp/reuters21578.tar.gz"/>
-    </target>
-
-    <target name="expand-news-20"  unless="news20.expanded">
-        <gunzip src="temp/news20.tar.gz" dest="temp"/>
-        <untar src="temp/news20.tar" dest="${working.dir}"/>
-    </target>
-    <target name="expand-reuters" unless="reuters.expanded">
-        <gunzip src="temp/reuters21578.tar.gz" dest="temp"/>
-        <mkdir dir="${working.dir}/reuters"/>
-        <untar src="temp/reuters21578.tar" dest="${working.dir}/reuters"/>
-        <delete >
-            <fileset dir="${working.dir}/reuters">
-                <include name="*.txt"/>
-            </fileset>
-        </delete>
-
-    </target>
-    <target name="extract-reuters" depends="check-files" unless="reuters.extracted">
-        <mkdir dir="${working.dir}/reuters-out"/>
-        <java classname="org.apache.lucene.benchmark.utils.ExtractReuters" maxmemory="1024M" fork="true">
-            <classpath refid="run.classpath"/>
-            <arg file="${working.dir}/reuters"/>
-            <arg file="${working.dir}/reuters-out"/>
-        </java>
-    </target>
-    <target name="get-20news-18828" unless="20news-18828.exists">
-        <get src="http://people.csail.mit.edu/u/j/jrennie/public_html/20Newsgroups/20news-18828.tar.gz"
-             dest="temp/20news-18828.tar.gz"/>
-
-    </target>
-    <target name="expand-20news-18828" unless="20news-18828.expanded">
-        <gunzip src="temp/20news-18828.tar.gz" dest="temp"/>
-        <untar src="temp/20news-18828.tar" dest="${working.dir}"/>
-    </target>
-    <target name="get-mini-news" unless="mini.exists">
-        <get src="http://kdd.ics.uci.edu/databases/20newsgroups/mini_newsgroups.tar.gz"
-             dest="temp/mini_newsgroups.tar.gz"/>
-    </target>
-    <target name="expand-mini-news" unless="mini.expanded">
-        <gunzip src="temp/mini_newsgroups.tar.gz" dest="temp"/>
-        <untar src="temp/mini_newsgroups.tar" dest="${working.dir}"/>
-    </target>
-
-	<property name="top.100k.words.archive.filename" 
-	          value="top.100k.words.de.en.fr.uk.wikipedia.2009-11.tar.bz2"/>
-	<property name="top.100k.words.archive.base.url"
-	          value="http://people.apache.org/~rmuir/wikipedia"/>
-	<target name="get-top-100k-words-archive" unless="top.100k.words.archive.present">
-		<mkdir dir="temp"/>
-	    <get src="${top.100k.words.archive.base.url}/${top.100k.words.archive.filename}"
-	         dest="temp/${top.100k.words.archive.filename}"/>
-	</target>
-	<target name="expand-top-100k-word-files" unless="top.100k.word.files.expanded">
-		<mkdir dir="${working.dir}/top100k-out"/>
-	    <untar src="temp/${top.100k.words.archive.filename}"
-	           overwrite="true" compression="bzip2" dest="${working.dir}/top100k-out"/>
-	</target>
-	
-	<target name="top-100k-wiki-word-files" depends="check-files">
-	  <mkdir dir="${working.dir}"/>
-	  <antcall target="get-top-100k-words-archive"/>
-	  <antcall target="expand-top-100k-word-files"/>
-	</target>
-	
-    <target name="get-files" depends="check-files">
-        <mkdir dir="temp"/>
-        <antcall target="get-reuters"/>
-        <antcall target="expand-reuters"/>
-        <antcall target="extract-reuters"/>
-    </target>
-
-    <path id="classpath">
-      <pathelement path="${memory.jar}"/>
-      <pathelement path="${highlighter.jar}"/>
-      <pathelement path="${analyzers-common.jar}"/>
-      <pathelement path="${demo.jar}"/>
-      <path refid="base.classpath"/>
-    	<fileset dir="lib">
-    		<include name="**/*.jar"/>
-    	</fileset>
-    </path>
-    <path id="run.classpath">
-        <path refid="classpath"/>
-        <pathelement location="${build.dir}/classes/java"/>
-        <pathelement path="${benchmark.ext.classpath}"/>
-    </path>
-
-    <property name="task.alg" location="conf/micro-standard.alg"/>
-    <property name="task.mem" value="140M"/>
-
-    <target name="run-task" depends="compile,check-files,get-files" 
-     description="Run compound penalty perf test (optional: -Dtask.alg=your-algorithm-file -Dtask.mem=java-max-mem)">
-        <echo>Working Directory: ${working.dir}</echo>
-        <java classname="org.apache.lucene.benchmark.byTask.Benchmark" maxmemory="${task.mem}" fork="true">
-            <classpath refid="run.classpath"/>
-            <arg file="${task.alg}"/>
-        </java>
-    </target>
-
-    <target name="enwiki" depends="compile,check-files,enwiki-files">
-        <echo>Working Directory: ${working.dir}</echo>
-        <java classname="org.apache.lucene.benchmark.byTask.Benchmark" maxmemory="1024M" fork="true">
-            <assertions>
-              <enable/>
-            </assertions>
-            <classpath refid="run.classpath"/>
-            <arg file="conf/extractWikipedia.alg"/>
-        </java>
-    </target>
-
-	<property name="collation.alg.file" location="conf/collation.alg"/>
-	<property name="collation.output.file" 
-	          value="${working.dir}/collation.benchmark.output.txt"/>
-	<property name="collation.jira.output.file" 
-	          value="${working.dir}/collation.bm2jira.output.txt"/>
-	
-	<path id="collation.runtime.classpath">
-	  <path refid="run.classpath"/>
-    <pathelement path="${analyzers-icu.jar}"/>
-    <fileset dir="${common.dir}/../modules/analysis/icu/lib" includes="icu4j*.jar"/>
-	</path>
-	
-	<target name="collation" depends="compile,compile-analyzers-icu,top-100k-wiki-word-files">
-	    <echo>Running contrib/benchmark with alg file: ${collation.alg.file}</echo>
-	    <java fork="true" classname="org.apache.lucene.benchmark.byTask.Benchmark" 
-	          maxmemory="${task.mem}" output="${collation.output.file}">
-	      <classpath refid="collation.runtime.classpath"/>
-	      <arg file="${collation.alg.file}"/>
-	    </java>
-	    <echo>Benchmark output is in file: ${collation.output.file}</echo>
-	    <echo>Converting to JIRA table format...</echo>
-	    <exec executable="perl" output="${collation.jira.output.file}" failonerror="true">
-	      <arg value="scripts/collation.bm2jira.pl"/>
-	      <arg value="${collation.output.file}"/>
-	    </exec>
-	    <echo>Benchmark output in JIRA table format is in file: ${collation.jira.output.file}</echo>
-	</target>
-	
-    <property name="shingle.alg.file" location="conf/shingle.alg"/>
-    <property name="shingle.output.file" 
-              value="${working.dir}/shingle.benchmark.output.txt"/>
-    <property name="shingle.jira.output.file" 
-              value="${working.dir}/shingle.bm2jira.output.txt"/>
-	
-    <path id="shingle.runtime.classpath">
-      <path refid="run.classpath"/>
-    </path>
-	
-    <target name="shingle" depends="compile,get-files">
-      <echo>Running contrib/benchmark with alg file: ${shingle.alg.file}</echo>
-      <java fork="true" classname="org.apache.lucene.benchmark.byTask.Benchmark" 
-            maxmemory="${task.mem}" output="${shingle.output.file}">
-        <classpath refid="run.classpath"/>
-        <arg file="${shingle.alg.file}"/>
-      </java>
-      <echo>Benchmark output is in file: ${shingle.output.file}</echo>
-      <echo>Converting to JIRA table format...</echo>
-      <exec executable="perl" output="${shingle.jira.output.file}" failonerror="true">
-        <arg value="scripts/shingle.bm2jira.pl"/>
-        <arg value="${shingle.output.file}"/>
-      </exec>
-      <echo>Benchmark output in JIRA table format is in file: ${shingle.jira.output.file}</echo>
-    </target>
-
-    <target name="compile-demo" unless="demo.uptodate">
-      <subant target="default">
-         <fileset dir="${common.dir}/contrib/demo" includes="build.xml"/>
-      </subant>
-    </target>
-    <target name="compile-highlighter" unless="highlighter.uptodate">
-      <subant target="default">
-         <fileset dir="${common.dir}/contrib/highlighter" includes="build.xml"/>
-      </subant>
-    </target>
-    <target name="compile-analyzers-icu" unless="analyzers-icu.uptodate">
-      <subant target="default">
-         <fileset dir="${common.dir}/../modules/analysis/icu" includes="build.xml"/>
-      </subant>
-    </target>
-    <target name="compile-analyzers-common" unless="analyzers-common.uptodate">
-      <subant target="default">
-        <fileset dir="${common.dir}/../modules/analysis/common" includes="build.xml"/>
-      </subant>
-    </target>
-    <target name="compile-memory" unless="memory.uptodate">
-      <subant target="default">
-         <fileset dir="${common.dir}/contrib/memory" includes="build.xml"/>
-      </subant>
-    </target>
-
-    <target name="init" depends="contrib-build.init,compile-demo,compile-memory,compile-highlighter,compile-analyzers-common"/>
-    
-</project>
diff --git a/lucene/contrib/benchmark/conf/analyzer.alg b/lucene/contrib/benchmark/conf/analyzer.alg
deleted file mode 100644
index 1a1ec4c..0000000
--- a/lucene/contrib/benchmark/conf/analyzer.alg
+++ /dev/null
@@ -1,79 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-merge.factor=mrg:10
-#:100:10:100
-max.buffered=buf:10
-#:10:100:100
-compound=true
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-#directory=RamDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=500
-
-docs.dir=reuters-out
-#docs.dir=reuters-111
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-
-{ "Rounds"
-
-    ResetSystemErase
-#If the analyzer is in o.a.l.analysis, then just the classname can be used, otherwise the FQN must be used
-#Standard Analyzer can be shortened to standard.StandardAnalyzer
-    {"NewAnalyzer" NewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer) > 
-    { "Populate"
-        CreateIndex
-        { "MAddDocs" AddDoc > : 2000
-        Optimize
-        CloseIndex
-    }
-
-    OpenReader  
-    { "SearchSameRdr" Search > : 5000
-    CloseReader 
-                
-    { "WarmNewRdr" Warm > : 50
-                
-    { "SrchNewRdr" Search > : 500
-                
-    { "SrchTrvNewRdr" SearchTrav(1000) > : 300
-                
-    { "SrchTrvRetNewRdr" SearchTravRet(2000) > : 100
-                
-    NewRound
-
-} : 4
-
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/lucene/contrib/benchmark/conf/basicNRT.alg b/lucene/contrib/benchmark/conf/basicNRT.alg
deleted file mode 100644
index 259b613..0000000
--- a/lucene/contrib/benchmark/conf/basicNRT.alg
+++ /dev/null
@@ -1,80 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-#
-# based on micro-standard
-#
-# modified to use wikipedia sources and index entire docs
-# currently just used to measure ingest rate
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-
-work.dir = /x/lucene/wiki.5M
-
-doc.stored=true
-doc.body.stored=false
-doc.tokenized=false
-doc.body.tokenized=true
-doc.term.vector=false
-log.step.AddDoc = 10000
-log.step.Search = 10000
-compound = false
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
-content.source.forever = false
-file.query.maker.file = queries.txt
-
-query.maker=org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker
-docs.file = /x/lucene/enwiki-20090306-lines-1k-fixed.txt
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-
-# -------------------------------------------------------------------------------------
-
-# Open a writer
-OpenIndex
-{
-  # Get a new near-real-time reader, once per second:
-  NearRealtimeReader(1.0) &
-
-  # Warm
-  Search
-
-  # Index with 2 threads, each adding 100 docs per sec
-  [ "Indexing" { AddDoc > : * : 100/sec ] : 2 &
-
-  # Redline search (from queries.txt) with 4 threads
-  [ "Searching" { Search > : * ] : 4 &
-
-  # Wait 60 sec, then wrap up
-  Wait(5.0)
-}
-CloseReader
-
-# Don't keep any changes, so we can re-test on the same index again
-RollbackIndex
-
-RepSumByPref Indexing
-RepSumByPref Searching
-RepSumByPref NearRealtimeReader
-
-
diff --git a/lucene/contrib/benchmark/conf/collation.alg b/lucene/contrib/benchmark/conf/collation.alg
deleted file mode 100644
index 798befc..0000000
--- a/lucene/contrib/benchmark/conf/collation.alg
+++ /dev/null
@@ -1,97 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-# 
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
-content.source.encoding=UTF-8
-doc.tokenized=false
-doc.body.tokenized=true
-docs.file=work/top100k-out/top.fr.wikipedia.words.txt
-content.source.forever=false
-log.step=100000
-
-{ "Rounds"
-    -NewAnalyzer(KeywordAnalyzer)
-    -SetProp(docs.file,work/top100k-out/top.fr.wikipedia.words.txt)
-    ResetInputs
-    { "FrenchKeyword" { ReadTokens > : * ResetInputs } : 10
-
-    -NewAnalyzer(KeywordAnalyzer)
-    -SetProp(docs.file,work/top100k-out/top.de.wikipedia.words.txt)
-    ResetInputs
-    { "GermanKeyword" { ReadTokens > : * ResetInputs } : 10
-
-    -NewAnalyzer(KeywordAnalyzer)
-    -SetProp(docs.file,work/top100k-out/top.uk.wikipedia.words.txt)
-    ResetInputs
-    { "UkrainianKeyword" { ReadTokens > : * ResetInputs } : 10
- 
-    -NewAnalyzer(KeywordAnalyzer)
-    -SetProp(docs.file,work/top100k-out/top.en.wikipedia.words.txt)
-    ResetInputs
-    { "EnglishKeyword" { ReadTokens > : * ResetInputs } : 10
- 
-    -NewLocale(fr)
-    -NewCollationAnalyzer
-    -SetProp(docs.file,work/top100k-out/top.fr.wikipedia.words.txt)
-    ResetInputs
-    { "FrenchJDK" { ReadTokens > : * ResetInputs } : 10
-
-    -NewLocale(de)
-    -NewCollationAnalyzer
-    -SetProp(docs.file,work/top100k-out/top.de.wikipedia.words.txt)
-    ResetInputs
-    { "GermanJDK" { ReadTokens > : * ResetInputs } : 10
-
-    -NewLocale(uk)
-    -NewCollationAnalyzer
-    -SetProp(docs.file,work/top100k-out/top.uk.wikipedia.words.txt)
-    ResetInputs
-    { "UkrainianJDK" { ReadTokens > : * ResetInputs } : 10
-
-    -NewLocale(en)
-    -NewCollationAnalyzer
-    -SetProp(docs.file,work/top100k-out/top.en.wikipedia.words.txt)
-    ResetInputs
-    { "EnglishJDK" { ReadTokens > : * ResetInputs } : 10
-
-    -NewLocale(fr)
-    -NewCollationAnalyzer(impl:icu)
-    -SetProp(docs.file,work/top100k-out/top.fr.wikipedia.words.txt)
-    ResetInputs
-    { "FrenchICU" { ReadTokens > : * ResetInputs } : 10
-
-    -NewLocale(de)
-    -NewCollationAnalyzer(impl:icu)
-    -SetProp(docs.file,work/top100k-out/top.de.wikipedia.words.txt)
-    ResetInputs
-    { "GermanICU" { ReadTokens > : * ResetInputs } : 10
-
-    -NewLocale(uk)
-    -NewCollationAnalyzer(impl:icu)
-    -SetProp(docs.file,work/top100k-out/top.uk.wikipedia.words.txt)
-    ResetInputs
-    { "UkrainianICU" { ReadTokens > : * ResetInputs } : 10
-
-    -NewLocale(en)
-    -NewCollationAnalyzer(impl:icu)
-    -SetProp(docs.file,work/top100k-out/top.en.wikipedia.words.txt)
-    ResetInputs
-    { "EnglishICU" { ReadTokens > : * ResetInputs } : 10
-
-    NewRound
-
-} : 5
-
-RepSumByNameRound
diff --git a/lucene/contrib/benchmark/conf/collector-small.alg b/lucene/contrib/benchmark/conf/collector-small.alg
deleted file mode 100644
index c67cab9..0000000
--- a/lucene/contrib/benchmark/conf/collector-small.alg
+++ /dev/null
@@ -1,91 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-# collector.class can be:
-#    Fully Qualified Class Name of a Collector with a empty constructor
-#    topScoreDocOrdered - Creates a TopScoreDocCollector that requires in order docs
-#    topScoreDocUnordered - Like above, but allows out of order
-collector.class=coll:topScoreDocOrdered:topScoreDocUnordered:topScoreDocOrdered:topScoreDocUnordered
-
-analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer
-directory=FSDirectory
-#directory=RamDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=100000
-
-search.num.hits=100000
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.LongToEnglishContentSource
-
-
-query.maker=org.apache.lucene.benchmark.byTask.feeds.LongToEnglishQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-
-{ "Rounds"
-
-    ResetSystemErase
-
-    { "Populate"
-        CreateIndex
-        { "MAddDocs" AddDoc } : 200000
-        Optimize
-        CloseIndex
-    }
-
-    OpenReader
-    { "topDocs" SearchWithCollector > : 10
-    CloseReader
-
-#    OpenReader
-#uses an array of search.num.hits size, but can also take in a parameter
-#    { "psc" SearchWithPostSortCollector > : 10
-#    { "psc100" SearchWithPostSortCollector(100) > : 10
-#    { "psc1000" SearchWithPostSortCollector(1000) > : 10
-#    { "psc10000" SearchWithPostSortCollector(10000) > : 10
-#    { "psc50000" SearchWithPostSortCollector(50000) > : 10
-#    CloseReader
-
-    RepSumByPref topDocs
-#    RepSumByPref psc
-#    RepSumByPref psc100
-#    RepSumByPref psc1000
-#    RepSumByPref psc10000
-#    RepSumByPref psc50000
-
-    NewRound
-
-} : 4
-
-#RepSumByNameRound
-#RepSumByName
-#RepSumByPrefRound topDocs
-#RepSumByPrefRound psc
-#RepSumByPrefRound psc100
-#RepSumByPrefRound psc1000
-#RepSumByPrefRound psc10000
-#RepSumByPrefRound psc50000
-
diff --git a/lucene/contrib/benchmark/conf/collector.alg b/lucene/contrib/benchmark/conf/collector.alg
deleted file mode 100644
index a80d564..0000000
--- a/lucene/contrib/benchmark/conf/collector.alg
+++ /dev/null
@@ -1,91 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-# collector.class can be:
-#    Fully Qualified Class Name of a Collector with a empty constructor
-#    topScoreDocOrdered - Creates a TopScoreDocCollector that requires in order docs
-#    topScoreDocUnordered - Like above, but allows out of order
-collector.class=coll:topScoreDocOrdered:topScoreDocUnordered:topScoreDocOrdered:topScoreDocUnordered
-
-analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer
-directory=FSDirectory
-#directory=RamDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=100000
-
-search.num.hits=1000000
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.LongToEnglishContentSource
-
-
-query.maker=org.apache.lucene.benchmark.byTask.feeds.LongToEnglishQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-
-{ "Rounds"
-
-    ResetSystemErase
-
-    { "Populate"
-        CreateIndex
-        { "MAddDocs" AddDoc } : 2000000
-        Optimize
-        CloseIndex
-    }
-
-    OpenReader
-    { "topDocs" SearchWithCollector > : 10
-    CloseReader
-
-#    OpenReader
-#uses an array of search.num.hits size, but can also take in a parameter
-#    { "psc" SearchWithPostSortCollector > : 10
-#    { "psc100" SearchWithPostSortCollector(100) > : 10
-#    { "psc1000" SearchWithPostSortCollector(1000) > : 10
-#    { "psc10000" SearchWithPostSortCollector(10000) > : 10
-#    { "psc50000" SearchWithPostSortCollector(50000) > : 10
-#    CloseReader
-
-    RepSumByPref topDocs
-#    RepSumByPref psc
-#    RepSumByPref psc100
-#    RepSumByPref psc1000
-#    RepSumByPref psc10000
-#    RepSumByPref psc50000
-
-    NewRound
-
-} : 4
-
-#RepSumByNameRound
-#RepSumByName
-#RepSumByPrefRound topDocs
-#RepSumByPrefRound psc
-#RepSumByPrefRound psc100
-#RepSumByPrefRound psc1000
-#RepSumByPrefRound psc10000
-#RepSumByPrefRound psc50000
-
diff --git a/lucene/contrib/benchmark/conf/compound-penalty.alg b/lucene/contrib/benchmark/conf/compound-penalty.alg
deleted file mode 100644
index 1291198..0000000
--- a/lucene/contrib/benchmark/conf/compound-penalty.alg
+++ /dev/null
@@ -1,92 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-
-# --------------------------------------------------------
-# Compound: what is the cost of compound format in indexing?
-# It does twice as much IO, is it twice slower? (no)
-# --------------------------------------------------------
-
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-merge.factor=mrg:10
-max.buffered=buf:10
-compound=compnd:true:false
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-#directory=RamDirectory
-
-doc.stored=stored:true:true:false:false
-doc.tokenized=true
-doc.term.vector=vector:true:true:false:false
-log.step=500
-log.step.DeleteDoc=100
-
-docs.dir=reuters-out
-#docs.dir=reuters-111
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=1
-
-log.queries=false
-# -------------------------------------------------------------------------------------
-
-ResetSystemErase
-
-{ "Round"
-  CreateIndex
-  { "AddDocs" AddDoc > : 10000
-  CloseIndex
-
-  OpenReader  
-  { "SearchSameRdr" Search > : 500
-  CloseReader 
-              
-  { "WarmNewRdr" Warm > : 50
-              
-  { "SrchNewRdr" Search > : 500
-              
-  { "SrchTrvNewRdr" SearchTrav > : 300
-              
-  { "SrchTrvRetNewRdr" SearchTravRet > : 100
-
-  [ "WarmNewRdr" Warm > : 50
-              
-  [ "SrchNewRdr" Search > : 500
-              
-  [ "SrchTrvNewRdr" SearchTrav > : 300
-              
-  [ "SrchTrvRetNewRdr" SearchTravRet > : 100
-
-  ResetInputs
-  RepSumByName
-  NewRound
-} : 4
-            
-RepSumByName
-RepSumByNameRound
-RepSumByPrefRound AddDocs
-RepSumByPrefRound SearchSameRdr
-RepSumByPrefRound WarmNewRdr
-RepSumByPrefRound SrchTrvNewRdr
-RepSumByPrefRound SrchTrvRetNewRdr
diff --git a/lucene/contrib/benchmark/conf/createLineFile.alg b/lucene/contrib/benchmark/conf/createLineFile.alg
deleted file mode 100644
index 969f307..0000000
--- a/lucene/contrib/benchmark/conf/createLineFile.alg
+++ /dev/null
@@ -1,43 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-
-#
-# This alg will process the Reuters documents feed to produce a
-# single file that contains all documents, one per line.
-#
-# To use this, first cd to contrib/benchmark and then run:
-#
-#   ant run-task -Dtask.alg=conf/createLineFile.alg
-#
-# Then, to index the documents in the line file, see
-# indexLineFile.alg.
-#
-
-# Where to get documents from:
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-# Where to write the line file output:
-line.file.out=work/reuters.lines.txt
-
-# Stop after processing the document feed once:
-content.source.forever=false
-
-# -------------------------------------------------------------------------------------
-
-# Process all documents, appending each one to the line file:
-{WriteLineDoc()}: * 
diff --git a/lucene/contrib/benchmark/conf/deletepercent.alg b/lucene/contrib/benchmark/conf/deletepercent.alg
deleted file mode 100644
index 59d1672..0000000
--- a/lucene/contrib/benchmark/conf/deletepercent.alg
+++ /dev/null
@@ -1,105 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-#directory=RamDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=500
-
-docs.dir=reuters-out
-#docs.dir=reuters-111
-
-#doc.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleDocMaker
-#doc.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersDocMaker
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-deletion.policy=org.apache.lucene.index.NoDeletionPolicy
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-
-{ "Rounds"
-
-    ResetSystemErase
-
-    { "Populate"
-        -CreateIndex
-        { "MAddDocs" AddDoc > : 1000
-        CommitIndex(original)
-        CloseIndex
-    }
-
-    OpenReader(false,original)
-    DeleteByPercent(5)
-    { "SearchSameRdr5" Search > : 500
-    FlushReader(5%)
-    CloseReader 
-    PrintReader(5%)
-
-    OpenReader(false,5%)
-    DeleteByPercent(10)
-    { "SearchSameRdr10" Search > : 500
-    FlushReader(10%)
-    CloseReader 
-    PrintReader(10%)
-
-    OpenReader(false,10%)
-    DeleteByPercent(20)
-    { "SearchSameRdr20" Search > : 500
-    FlushReader(20%)
-    CloseReader 
-    PrintReader(20%)
-    
-    OpenReader(false,20%)
-    DeleteByPercent(60)
-    { "SearchSameRdr60" Search > : 500
-    FlushReader(60%)
-    CloseReader 
-    PrintReader(60%)
-    
-    OpenReader(false,60%)
-    DeleteByPercent(75)
-    { "SearchSameRdr75" Search > : 500
-    FlushReader(75%)
-    CloseReader 
-    PrintReader(75%)
-
-    # Test lower percentage of deletes (so undeleteAll is used)
-    OpenReader(false,75%)
-    DeleteByPercent(7)
-    { "SearchSameRdr7" Search > : 500
-    FlushReader(7%)
-    CloseReader 
-    PrintReader(7%)
-
-    NewRound
-
-} : 1
-
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/lucene/contrib/benchmark/conf/deletes.alg b/lucene/contrib/benchmark/conf/deletes.alg
deleted file mode 100644
index a54d4f8..0000000
--- a/lucene/contrib/benchmark/conf/deletes.alg
+++ /dev/null
@@ -1,70 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# --------------------------------------------------------
-# Deletes: what is the cost of deleting documents?
-# --------------------------------------------------------
-
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-merge.factor=mrg:10
-max.buffered=buf:100
-compound=true
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-#directory=RamDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=10000
-log.step.DeleteDoc=100
-
-docs.dir=reuters-out
-#docs.dir=reuters-111
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
-#content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
-#query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=1
-
-log.queries=false
-# -------------------------------------------------------------------------------------
-
-ResetSystemErase
-
-CreateIndex
-CloseIndex
-
-{ "Populate"
-    OpenIndex
-    { AddDoc(10) > : 200000
-    Optimize
-    CloseIndex
-> 
-
-{ "Deletions"
-   OpenReader(false)  DeleteDoc   CloseReader
-} : 4000
-
-RepSumByName
-
diff --git a/lucene/contrib/benchmark/conf/extractWikipedia.alg b/lucene/contrib/benchmark/conf/extractWikipedia.alg
deleted file mode 100644
index f0df54d..0000000
--- a/lucene/contrib/benchmark/conf/extractWikipedia.alg
+++ /dev/null
@@ -1,44 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-
-#
-# This alg will process the Wikipedia documents feed to produce a
-# single file that contains all documents, one per line.
-#
-# To use this, first cd to contrib/benchmark and then run:
-#
-#   ant run-task -Dtask.alg=conf/extractWikipedia.alg
-#
-# Then, to index the documents in the line file, see
-# indexLineFile.alg.
-#
-
-# Where to get documents from:
-content.source=org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource
-docs.file=temp/enwiki-20070527-pages-articles.xml
-
-# Where to write the line file output:
-line.file.out=work/enwiki.txt
-
-# Stop after processing the document feed once:
-content.source.forever=false
-
-# -------------------------------------------------------------------------------------
-
-# Process all documents, appending each one to the line file:
-{WriteLineDoc() > : *
diff --git a/lucene/contrib/benchmark/conf/highlight-profile.alg b/lucene/contrib/benchmark/conf/highlight-profile.alg
deleted file mode 100644
index 234ebb1..0000000
--- a/lucene/contrib/benchmark/conf/highlight-profile.alg
+++ /dev/null
@@ -1,68 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-ram.flush.mb=flush:32:32
-compound=cmpnd:true:false
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=true
-doc.term.vector.offsets=true
-doc.term.vector.positions=true
-log.step=2000
-
-docs.dir=reuters-out
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-{ "Populate"
-        CreateIndex
-        { "MAddDocs" AddDoc } : 20000
-        Optimize
-        CloseIndex
-    }
-{ "Rounds"
-
-    ResetSystemSoft
-
-
-    OpenReader
-      { "SearchHlgtSameRdr" SearchTravRetHighlight(maxFrags[10],fields[body]) > : 1000
-
-    CloseReader
-
-    RepSumByPref MAddDocs
-
-    NewRound
-
-} : 4
-
-RepSumByNameRound
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/lucene/contrib/benchmark/conf/highlight-vs-vector-highlight.alg b/lucene/contrib/benchmark/conf/highlight-vs-vector-highlight.alg
deleted file mode 100644
index a98e321..0000000
--- a/lucene/contrib/benchmark/conf/highlight-vs-vector-highlight.alg
+++ /dev/null
@@ -1,80 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-
-ram.flush.mb=flush:32:32
-compound=cmpnd:true:false
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=true
-doc.term.vector.offsets=true
-doc.term.vector.positions=true
-log.step=2000
-
-docs.dir=reuters-out
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource
-docs.file=temp/enwiki-20070527-pages-articles.xml
-
-query.maker=org.apache.lucene.benchmark.byTask.feeds.EnwikiQueryMaker
-enwikiQueryMaker.disableSpanQueries=true
-
-max.field.length=2147483647
-highlighter.maxDocCharsToAnalyze=2147483647
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-{ "Populate"
-        CreateIndex
-        { "MAddDocs" AddDoc } : 20000
-        Optimize
-        CloseIndex
-}
-{
-        OpenReader
-          { "WarmTV" SearchTravRetVectorHighlight(maxFrags[3],fields[body]) > : 100
-        CloseReader
-}
-{
-	"Rounds"
-
-        ResetSystemSoft
-
-        OpenReader
-          { "SearchVecHlgtSameRdr" SearchTravRetVectorHighlight(maxFrags[3],fields[body]) > : 200
-        CloseReader
-
-        ResetSystemSoft
-
-        OpenReader
-          { "SearchHlgtSameRdr" SearchTravRetHighlight(maxFrags[3],fields[body]) > : 200
-        CloseReader
-
-        RepSumByPref Search
-
-        NewRound
-} : 4
-
-RepSumByNameRound
-RepSumByName
diff --git a/lucene/contrib/benchmark/conf/indexLineFile.alg b/lucene/contrib/benchmark/conf/indexLineFile.alg
deleted file mode 100644
index bcb9922..0000000
--- a/lucene/contrib/benchmark/conf/indexLineFile.alg
+++ /dev/null
@@ -1,53 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-
-#
-# This file indexes documents contained in a single text file, one per
-# line.  See createLineFile.alg for how to create this file.  The
-# benefit of this is it removes the IO cost of opening one file per
-# document to let you more accurately measure time spent analyzing and
-# indexing your documents vs time spent creating the documents.
-#
-# To use this, you must first run the createLineFile.alg, then cd to
-# contrib/benchmark and then run:
-#
-#   ant run-task -Dtask.alg=conf/indexLineFile.alg
-#
-
-analyzer=org.apache.lucene.analysis.core.SimpleAnalyzer
-
-# Feed that knows how to process the line file format:
-content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
-
-# File that contains one document per line:
-docs.file=work/reuters.lines.txt
-
-# Process documents only once:
-content.source.forever=false
-
-# -------------------------------------------------------------------------------------
-
-# Reset the system, create a new index, index all docs from the line
-# file, close the index, produce a report.
-
-ResetSystemErase
-CreateIndex
-{AddDoc}: *
-CloseIndex
-
-RepSumByPref AddDoc 
diff --git a/lucene/contrib/benchmark/conf/indexing-flush-by-RAM-multithreaded.alg b/lucene/contrib/benchmark/conf/indexing-flush-by-RAM-multithreaded.alg
deleted file mode 100644
index 58028f9..0000000
--- a/lucene/contrib/benchmark/conf/indexing-flush-by-RAM-multithreaded.alg
+++ /dev/null
@@ -1,70 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-writer.version=LUCENE_40
-#merge.factor=mrg:10:100:10:100:10:100:10:100
-#max.buffered=buf:10:10:100:100:10:10:100:100
-ram.flush.mb=flush:32:40:48:56:32:40:48:56
-compound=cmpnd:true:true:true:true:false:false:false:false
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-#directory=RamDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=2000
-
-docs.dir=reuters-out
-#docs.dir=reuters-111
-
-#content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-
-{ "Rounds"
-
-    ResetSystemErase
-
-    { "Populate"
-        CreateIndex
-        [{ "MAddDocs" AddDoc } : 5000] : 4
-        Optimize
-        CloseIndex
-    }
-
-
-    RepSumByPref MAddDocs
-
-    NewRound
-
-} : 8
-
-RepSumByNameRound
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/lucene/contrib/benchmark/conf/indexing-flush-by-RAM.alg b/lucene/contrib/benchmark/conf/indexing-flush-by-RAM.alg
deleted file mode 100644
index be88a1f..0000000
--- a/lucene/contrib/benchmark/conf/indexing-flush-by-RAM.alg
+++ /dev/null
@@ -1,70 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-writer.version=LUCENE_40
-#merge.factor=mrg:10:100:10:100:10:100:10:100
-#max.buffered=buf:10:10:100:100:10:10:100:100
-ram.flush.mb=flush:32:40:48:56:32:40:48:56
-compound=cmpnd:true:true:true:true:false:false:false:false
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-#directory=RamDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=2000
-
-docs.dir=reuters-out
-#docs.dir=reuters-111
-
-#content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-
-{ "Rounds"
-
-    ResetSystemErase
-
-    { "Populate"
-        CreateIndex
-        { "MAddDocs" AddDoc } : 20000
-        Optimize
-        CloseIndex
-    }
-
-
-    RepSumByPref MAddDocs
-
-    NewRound
-
-} : 8
-
-RepSumByNameRound
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/lucene/contrib/benchmark/conf/indexing-multithreaded.alg b/lucene/contrib/benchmark/conf/indexing-multithreaded.alg
deleted file mode 100644
index 261cdb3..0000000
--- a/lucene/contrib/benchmark/conf/indexing-multithreaded.alg
+++ /dev/null
@@ -1,71 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-writer.version=LUCENE_40
-merge.factor=mrg:10:100:10:100:10:100:10:100
-max.buffered=buf:10:10:100:100:10:10:100:100
-#ram.flush.mb=flush:32:40:48:56:32:40:48:56
-compound=cmpnd:true:true:true:true:false:false:false:false
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-#directory=RamDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=2000
-
-docs.dir=reuters-out
-#docs.dir=reuters-111
-
-#content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-
-{ "Rounds"
-
-    ResetSystemErase
-
-    { "Populate"
-        CreateIndex
-        [{ "MAddDocs" AddDoc } : 5000] : 4
-        Optimize
-        CommitIndex(commit1)
-        CloseIndex
-    }
-
-
-    RepSumByPref MAddDocs
-
-    NewRound
-
-} : 8
-
-RepSumByNameRound
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/lucene/contrib/benchmark/conf/indexing.alg b/lucene/contrib/benchmark/conf/indexing.alg
deleted file mode 100644
index 7c8673b..0000000
--- a/lucene/contrib/benchmark/conf/indexing.alg
+++ /dev/null
@@ -1,70 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-writer.version=LUCENE_40
-merge.factor=mrg:10:100:10:100:10:100:10:100
-max.buffered=buf:10:10:100:100:10:10:100:100
-#ram.flush.mb=flush:32:40:48:56:32:40:48:56
-compound=cmpnd:true:true:true:true:false:false:false:false
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-#directory=RamDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=2000
-
-docs.dir=reuters-out
-#docs.dir=reuters-111
-
-#content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-
-{ "Rounds"
-
-    ResetSystemErase
-
-    { "Populate"
-        CreateIndex
-        { "MAddDocs" AddDoc } : 20000
-        Optimize
-        CloseIndex
-    }
-
-
-    RepSumByPref MAddDocs
-
-    NewRound
-
-} : 8
-
-RepSumByNameRound
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/lucene/contrib/benchmark/conf/micro-standard-flush-by-ram.alg b/lucene/contrib/benchmark/conf/micro-standard-flush-by-ram.alg
deleted file mode 100644
index 0d2c685..0000000
--- a/lucene/contrib/benchmark/conf/micro-standard-flush-by-ram.alg
+++ /dev/null
@@ -1,77 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-#merge.factor=mrg:10:100:10:100
-#max.buffered=buf:10:10:100:100
-ram.flush.mb=flush:32:40:48:56
-compound=true
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-#directory=RamDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=500
-
-docs.dir=reuters-out
-#docs.dir=reuters-111
-
-#content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-
-{ "Rounds"
-
-    ResetSystemErase
-
-    { "Populate"
-        CreateIndex
-        { "MAddDocs" AddDoc > : 2000
-        Optimize
-        CloseIndex
-    }
-
-    OpenReader  
-    { "SearchSameRdr" Search > : 5000
-    CloseReader 
-                
-    { "WarmNewRdr" Warm > : 50
-                
-    { "SrchNewRdr" Search > : 500
-                
-    { "SrchTrvNewRdr" SearchTrav(1000) > : 300
-                
-    { "SrchTrvRetNewRdr" SearchTravRet(2000) > : 100
-                
-    NewRound
-
-} : 4
-
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/lucene/contrib/benchmark/conf/micro-standard.alg b/lucene/contrib/benchmark/conf/micro-standard.alg
deleted file mode 100644
index e0a554a..0000000
--- a/lucene/contrib/benchmark/conf/micro-standard.alg
+++ /dev/null
@@ -1,76 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-merge.factor=mrg:10:100:10:100
-max.buffered=buf:10:10:100:100
-compound=true
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-#directory=RamDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=500
-
-docs.dir=reuters-out
-#docs.dir=reuters-111
-
-#content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-
-{ "Rounds"
-
-    ResetSystemErase
-
-    { "Populate"
-        -CreateIndex
-        { "MAddDocs" AddDoc > : 2000
-        -Optimize
-        -CloseIndex
-    }
-
-    OpenReader  
-    { "SearchSameRdr" Search > : 5000
-    CloseReader 
-                
-    { "WarmNewRdr" Warm > : 50
-                
-    { "SrchNewRdr" Search > : 500
-                
-    { "SrchTrvNewRdr" SearchTrav(1000) > : 300
-                
-    { "SrchTrvRetNewRdr" SearchTravRet(2000) > : 100
-                
-    NewRound
-
-} : 4
-
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/lucene/contrib/benchmark/conf/readContentSource.alg b/lucene/contrib/benchmark/conf/readContentSource.alg
deleted file mode 100644
index 9923af0..0000000
--- a/lucene/contrib/benchmark/conf/readContentSource.alg
+++ /dev/null
@@ -1,45 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-
-#
-# This alg reads the information from a ContentSoruce. It is useful for 
-# measuring the performance of a particular ContentSource implementation, or 
-# gather baselines for operations like indexing (if reading from the content 
-# source takes 'X' time, we cannot index faster).
-#
-# To use this, first cd to contrib/benchmark and then run:
-#
-#   ant run-task -Dtask.alg=conf/readContentSource.alg
-#
-
-# Where to get documents from:
-content.source=org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource
-docs.file=temp/enwiki-20070527-pages-articles.xml.bz2
-
-# Stop after processing the document feed once:
-content.source.forever=false
-
-# Log messages every:
-log.step=100000
-
-# -------------------------------------------------------------------------------------
-
-# Process all documents, appending each one to the line file:
-{ ConsumeContentSource } : *
-
-RepSumByPref ConsumeContentSource
diff --git a/lucene/contrib/benchmark/conf/sample.alg b/lucene/contrib/benchmark/conf/sample.alg
deleted file mode 100644
index c7b9f25..0000000
--- a/lucene/contrib/benchmark/conf/sample.alg
+++ /dev/null
@@ -1,85 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# --------------------------------------------------------
-# 
-# Sample: what is the effect of doc size on indexing time?
-#
-# There are two parts in this test:
-# - PopulateShort adds 2N documents of length  L
-# - PopulateLong  adds  N documents of length 2L
-# Which one would be faster?
-# The comparison is done twice.
-#
-# --------------------------------------------------------
-
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-merge.factor=mrg:10:20
-max.buffered=buf:100:1000
-compound=true
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-#directory=RamDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=500
-
-docs.dir=reuters-out
-#docs.dir=reuters-111
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
-#content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
-#query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=false
-# -------------------------------------------------------------------------------------
-
-{
-
-    { "PopulateShort"
-        CreateIndex
-        { AddDoc(4000) > : 20000
-        Optimize
-        CloseIndex
-    >
-
-    ResetSystemErase
-    
-    { "PopulateLong"
-        CreateIndex
-        { AddDoc(8000) > : 10000
-        Optimize
-        CloseIndex
-    >
-
-    ResetSystemErase
-
-    NewRound
-
-} : 2
-
-RepSumByName
-RepSelectByPref Populate
diff --git a/lucene/contrib/benchmark/conf/shingle.alg b/lucene/contrib/benchmark/conf/shingle.alg
deleted file mode 100644
index 5fb6876..0000000
--- a/lucene/contrib/benchmark/conf/shingle.alg
+++ /dev/null
@@ -1,48 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-# 
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-doc.tokenized=false
-doc.body.tokenized=true
-docs.dir=reuters-out
-log.step=1000
-
-{ "Rounds"
-
-    -NewShingleAnalyzer(maxShingleSize:2,outputUnigrams:true)
-    -ResetInputs
-    { "BigramsAndUnigrams" { ReadTokens > : 10000 }
-
-    -NewShingleAnalyzer(maxShingleSize:2,outputUnigrams:false)
-    -ResetInputs
-    { "BigramsOnly" { ReadTokens > : 10000 }
-
-    -NewShingleAnalyzer(maxShingleSize:4,outputUnigrams:true)
-    -ResetInputs
-    { "FourgramsAndUnigrams" { ReadTokens > : 10000 }
-
-    -NewShingleAnalyzer(maxShingleSize:4,outputUnigrams:false)
-    -ResetInputs
-    { "FourgramsOnly" { ReadTokens > : 10000 }
-
-    -NewAnalyzer(standard.StandardAnalyzer)
-    -ResetInputs
-    { "UnigramsOnly" { ReadTokens > : 10000 }
-
-    NewRound
-
-} : 5
-
-RepSumByNameRound
diff --git a/lucene/contrib/benchmark/conf/sloppy-phrase.alg b/lucene/contrib/benchmark/conf/sloppy-phrase.alg
deleted file mode 100644
index f0caad7..0000000
--- a/lucene/contrib/benchmark/conf/sloppy-phrase.alg
+++ /dev/null
@@ -1,74 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-max.buffered=100
-merge.factor=10
-compound=true
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-#directory=RamDirectory
-
-doc.stored=false
-doc.tokenized=true
-doc.term.vector=false
-log.step=500
-
-docs.dir=reuters-out
-#docs.dir=reuters-111
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
-#content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleSloppyPhraseQueryMaker
-#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
-#query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=1
-
-log.queries=false
-# -------------------------------------------------------------------------------------
-
-
-ResetSystemErase
-
-{ "Populate"
-    CreateIndex
-    { "MAddDocs" AddDoc(2000) > : 20000     
-    Optimize
-    CloseIndex
-}
-
-
-{ "Round"
-
-  OpenReader  
-  { "SearchSameRdr" Search > : 6000
-  CloseReader 
-
-  ResetInputs
-  RepSumByName
-  NewRound
-} : 4
-            
-RepSumByPrefRound MAddDocs
-
-RepSumByName
-RepSumByPrefRound Search
diff --git a/lucene/contrib/benchmark/conf/sort-standard.alg b/lucene/contrib/benchmark/conf/sort-standard.alg
deleted file mode 100644
index c7413fc..0000000
--- a/lucene/contrib/benchmark/conf/sort-standard.alg
+++ /dev/null
@@ -1,71 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-merge.factor=mrg:50
-compound=false
-
-sort.rng=20000:10000:20000:10000
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-#directory=RamDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=100000
-
-docs.dir=reuters-out
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.SortableSingleDocSource
-
-query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-
-{ "Rounds"
-	{ "Run"
-      ResetSystemErase
-
-      { "Populate"
-        -CreateIndex
-        { "MAddDocs" AddDoc(100) > : 500000
-        -Optimize
-        -CloseIndex
-      }
-    
-      { "TestSortSpeed"
-        OpenReader  
-        { "LoadFieldCacheAndSearch" SearchWithSort(sort_field:int) > : 1 
-        { "SearchWithSort" SearchWithSort(sort_field:int) > : 5000
-        CloseReader 
-      
-      }
-    
-      NewRound
-     } : 4
-
-} 
-
-RepSumByName
-
diff --git a/lucene/contrib/benchmark/conf/standard-flush-by-RAM.alg b/lucene/contrib/benchmark/conf/standard-flush-by-RAM.alg
deleted file mode 100644
index ba60ac8..0000000
--- a/lucene/contrib/benchmark/conf/standard-flush-by-RAM.alg
+++ /dev/null
@@ -1,92 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-#merge.factor=mrg:10:100:10:100:10:100:10:100
-#max.buffered=buf:10:10:100:100:10:10:100:100
-ram.flush.mb=flush:32:40:48:56:32:40:48:56
-compound=cmpnd:true:true:true:true:false:false:false:false
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-#directory=RamDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=2000
-
-docs.dir=reuters-out
-#docs.dir=reuters-111
-
-#content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-
-{ "Rounds"
-
-    ResetSystemErase
-
-    { "Populate"
-        CreateIndex
-        { "MAddDocs" AddDoc } : 20000
-        Optimize
-        CloseIndex
-    }
-
-    OpenReader  
-    { "SearchSameRdr" Search > : 5000
-    CloseReader 
-                
-    { "WarmNewRdr" Warm > : 50
-                
-    { "SrchNewRdr" Search > : 500
-                
-    { "SrchTrvNewRdr" SearchTrav > : 300
-                
-    { "SrchTrvRetNewRdr" SearchTravRet > : 100
-                
-    OpenReader  
-    [ "SearchSameRdr" Search > : 5000 : 2500
-    CloseReader 
-                
-    [ "WarmNewRdr" Warm > : 50 : 25
-                
-    [ "SrchNewRdr" Search > : 50 : 25
-                
-    [ "SrchTrvNewRdr" SearchTrav > : 300 : 150
-                
-    [ "SrchTrvRetNewRdr" SearchTravRet > : 100 : 50
-
-    RepSumByPref MAddDocs
-
-    NewRound
-
-} : 8
-
-RepSumByNameRound
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/lucene/contrib/benchmark/conf/standard-highlights-notv.alg b/lucene/contrib/benchmark/conf/standard-highlights-notv.alg
deleted file mode 100644
index 889f5d7..0000000
--- a/lucene/contrib/benchmark/conf/standard-highlights-notv.alg
+++ /dev/null
@@ -1,69 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-ram.flush.mb=flush:32:32
-compound=cmpnd:true:false
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-doc.term.vector.offsets=false
-doc.term.vector.positions=false
-log.step=2000
-
-docs.dir=reuters-out
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-{ "Populate"
-        CreateIndex
-        { "MAddDocs" AddDoc } : 20000
-        Optimize
-        CloseIndex
-}
-{ "Rounds"
-
-    ResetSystemSoft
-    OpenReader
-      { "SrchTrvRetNewRdr" SearchTravRet(10) > : 1000
-    CloseReader
-    OpenReader
-      { "SearchHlgtSameRdr" SearchTravRetHighlight(size[10],highlight[10],mergeContiguous[true],maxFrags[3],fields[body]) > : 1000
-
-    CloseReader
-
-    RepSumByPref SearchHlgtSameRdr
-
-    NewRound
-
-} : 2
-
-RepSumByNameRound
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/lucene/contrib/benchmark/conf/standard-highlights-tv.alg b/lucene/contrib/benchmark/conf/standard-highlights-tv.alg
deleted file mode 100644
index 8c7f533..0000000
--- a/lucene/contrib/benchmark/conf/standard-highlights-tv.alg
+++ /dev/null
@@ -1,69 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-ram.flush.mb=flush:32:32
-compound=cmpnd:true:false
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=true
-doc.term.vector.offsets=true
-doc.term.vector.positions=true
-log.step=2000
-
-docs.dir=reuters-out
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-{ "Populate"
-        CreateIndex
-        { "MAddDocs" AddDoc } : 20000
-        Optimize
-        CloseIndex
-}
-{ "Rounds"
-
-    ResetSystemSoft
-    OpenReader
-      { "SrchTrvRetNewRdr" SearchTravRet(10) > : 1000
-    CloseReader
-    OpenReader
-      { "SearchHlgtSameRdr" SearchTravRetHighlight(size[10],highlight[10],mergeContiguous[true],maxFrags[3],fields[body]) > : 1000
-
-    CloseReader
-
-    RepSumByPref SearchHlgtSameRdr
-
-    NewRound
-
-} : 2
-
-RepSumByNameRound
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/lucene/contrib/benchmark/conf/standard.alg b/lucene/contrib/benchmark/conf/standard.alg
deleted file mode 100644
index 66e66ef..0000000
--- a/lucene/contrib/benchmark/conf/standard.alg
+++ /dev/null
@@ -1,92 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-writer.version=LUCENE_40
-merge.factor=mrg:10:100:10:100:10:100:10:100
-max.buffered=buf:10:10:100:100:10:10:100:100
-compound=cmpnd:true:true:true:true:false:false:false:false
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-#directory=RamDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=2000
-
-docs.dir=reuters-out
-#docs.dir=reuters-111
-
-#content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-
-{ "Rounds"
-
-    ResetSystemErase
-
-    { "Populate"
-        CreateIndex
-        { "MAddDocs" AddDoc } : 20000
-        Optimize
-        CloseIndex
-    }
-
-    OpenReader  
-    { "SearchSameRdr" Search > : 5000
-    CloseReader 
-                
-    { "WarmNewRdr" Warm > : 50
-                
-    { "SrchNewRdr" Search > : 500
-                
-    { "SrchTrvNewRdr" SearchTrav > : 300
-                
-    { "SrchTrvRetNewRdr" SearchTravRet > : 100
-                
-    OpenReader  
-    [ "SearchSameRdr" Search > : 5000 : 2500
-    CloseReader 
-                
-    [ "WarmNewRdr" Warm > : 50 : 25
-                
-    [ "SrchNewRdr" Search > : 50 : 25
-                
-    [ "SrchTrvNewRdr" SearchTrav > : 300 : 150
-                
-    [ "SrchTrvRetNewRdr" SearchTravRet > : 100 : 50
-
-    RepSumByPref MAddDocs
-
-    NewRound
-
-} : 8
-
-RepSumByNameRound
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/lucene/contrib/benchmark/conf/tokenize.alg b/lucene/contrib/benchmark/conf/tokenize.alg
deleted file mode 100644
index 57951ff..0000000
--- a/lucene/contrib/benchmark/conf/tokenize.alg
+++ /dev/null
@@ -1,36 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-
-#
-# This alg reads all tokens out of a document but does not index them.
-# This is useful for benchmarking tokenizers.
-#
-# To use this, cd to contrib/benchmark and then run:
-#
-#   ant run-task -Dtask.alg=conf/tokenize.alg
-#
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-content.source.forever=false
-
-
-#
--------------------------------------------------------------------------------------
-
-{ReadTokens > : *
-RepSumByName
diff --git a/lucene/contrib/benchmark/conf/vector-highlight-profile.alg b/lucene/contrib/benchmark/conf/vector-highlight-profile.alg
deleted file mode 100644
index 6b456df..0000000
--- a/lucene/contrib/benchmark/conf/vector-highlight-profile.alg
+++ /dev/null
@@ -1,68 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-
-ram.flush.mb=flush:32:32
-compound=cmpnd:true:false
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=true
-doc.term.vector.offsets=true
-doc.term.vector.positions=true
-log.step=2000
-
-docs.dir=reuters-out
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
-
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=true
-# -------------------------------------------------------------------------------------
-{ "Populate"
-        CreateIndex
-        { "MAddDocs" AddDoc } : 20000
-        Optimize
-        CloseIndex
-    }
-{ "Rounds"
-
-    ResetSystemSoft
-
-
-    OpenReader
-      { "SearchVecHlgtSameRdr" SearchTravRetVectorHighlight(maxFrags[10],fields[body]) > : 1000
-
-    CloseReader
-
-    RepSumByPref MAddDocs
-
-    NewRound
-
-} : 4
-
-RepSumByNameRound
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/lucene/contrib/benchmark/conf/wikipedia-flush-by-RAM.alg b/lucene/contrib/benchmark/conf/wikipedia-flush-by-RAM.alg
deleted file mode 100644
index 5bedfb3..0000000
--- a/lucene/contrib/benchmark/conf/wikipedia-flush-by-RAM.alg
+++ /dev/null
@@ -1,69 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-#
-# based on micro-standard
-#
-# modified to use wikipedia sources and index entire docs
-# currently just used to measure ingest rate
-
-#merge.factor=mrg:10:100:10:100
-#max.buffered=buf:10:10:100:100
-ram.flush.mb=ram:32:40:48:56
-
-max.field.length=2147483647
-
-
-compound=true
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=5000
-
-docs.file=temp/enwiki-20070527-pages-articles.xml
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource
-
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=false
-# -------------------------------------------------------------------------------------
-
-{ "Rounds"
-
-    ResetSystemErase
-
-    { "Populate"
-        CreateIndex
-        { "MAddDocs" AddDoc > : 200000
-        CloseIndex
-    }
-
-    NewRound
-
-} : 4
-
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/lucene/contrib/benchmark/conf/wikipedia.alg b/lucene/contrib/benchmark/conf/wikipedia.alg
deleted file mode 100644
index 1417f24..0000000
--- a/lucene/contrib/benchmark/conf/wikipedia.alg
+++ /dev/null
@@ -1,65 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-#
-# based on micro-standard
-#
-# modified to use wikipedia sources and index entire docs
-# currently just used to measure ingest rate
-
-merge.factor=mrg:10:100:10:100
-max.field.length=2147483647
-max.buffered=buf:10:10:100:100
-compound=true
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=5000
-
-docs.file=temp/enwiki-20070527-pages-articles.xml
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource
-
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=false
-# -------------------------------------------------------------------------------------
-
-{ "Rounds"
-
-    ResetSystemErase
-
-    { "Populate"
-        CreateIndex
-        { "MAddDocs" AddDoc > : 200000
-        CloseIndex
-    }
-
-    NewRound
-
-} : 4
-
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/lucene/contrib/benchmark/conf/wikipediaOneRound.alg b/lucene/contrib/benchmark/conf/wikipediaOneRound.alg
deleted file mode 100644
index e2aaa74..0000000
--- a/lucene/contrib/benchmark/conf/wikipediaOneRound.alg
+++ /dev/null
@@ -1,65 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-#
-# based on micro-standard
-#
-# modified to use wikipedia sources and index entire docs
-# currently just used to measure ingest rate
-
-merge.factor=mrg:10:100:10:100
-max.field.length=2147483647
-max.buffered=buf:10:10:100:100
-compound=true
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=5000
-
-docs.file=temp/enwiki-20070527-pages-articles.xml
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource
-
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=false
-# -------------------------------------------------------------------------------------
-
-{ "Rounds"
-
-    ResetSystemErase
-
-    { "Populate"
-        CreateIndex
-        { "MAddDocs" AddDoc > : 200000
-        CloseIndex
-    }
-
-    NewRound
-
-} : 1
-
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/lucene/contrib/benchmark/lib/commons-beanutils-1.7.0.jar b/lucene/contrib/benchmark/lib/commons-beanutils-1.7.0.jar
deleted file mode 100644
index e211356..0000000
--- a/lucene/contrib/benchmark/lib/commons-beanutils-1.7.0.jar
+++ /dev/null
@@ -1,2 +0,0 @@
-AnyObjectId[b1b89c9c921f16af22a88db3ff28975a8e40d886] was removed in git history.
-Apache SVN contains full history.
\ No newline at end of file
diff --git a/lucene/contrib/benchmark/lib/commons-collections-3.1.jar b/lucene/contrib/benchmark/lib/commons-collections-3.1.jar
deleted file mode 100644
index 6e5f877..0000000
--- a/lucene/contrib/benchmark/lib/commons-collections-3.1.jar
+++ /dev/null
@@ -1,2 +0,0 @@
-AnyObjectId[41e230feeaa53618b6ac5f8d11792c2eecf4d4fd] was removed in git history.
-Apache SVN contains full history.
\ No newline at end of file
diff --git a/lucene/contrib/benchmark/lib/commons-compress-1.0.jar b/lucene/contrib/benchmark/lib/commons-compress-1.0.jar
deleted file mode 100644
index 473e2bf..0000000
--- a/lucene/contrib/benchmark/lib/commons-compress-1.0.jar
+++ /dev/null
@@ -1,2 +0,0 @@
-AnyObjectId[78d832c11c42023d4bc12077a1d9b7b5025217bc] was removed in git history.
-Apache SVN contains full history.
\ No newline at end of file
diff --git a/lucene/contrib/benchmark/lib/commons-digester-1.7.jar b/lucene/contrib/benchmark/lib/commons-digester-1.7.jar
deleted file mode 100644
index 97d5d05..0000000
--- a/lucene/contrib/benchmark/lib/commons-digester-1.7.jar
+++ /dev/null
@@ -1,2 +0,0 @@
-AnyObjectId[1783dbea232ced6db122268f8faa5ce773c7ea42] was removed in git history.
-Apache SVN contains full history.
\ No newline at end of file
diff --git a/lucene/contrib/benchmark/lib/commons-logging-1.0.4.jar b/lucene/contrib/benchmark/lib/commons-logging-1.0.4.jar
deleted file mode 100644
index f330fde..0000000
--- a/lucene/contrib/benchmark/lib/commons-logging-1.0.4.jar
+++ /dev/null
@@ -1,2 +0,0 @@
-AnyObjectId[b73a80fab641131e6fbe3ae833549efb3c540d17] was removed in git history.
-Apache SVN contains full history.
\ No newline at end of file
diff --git a/lucene/contrib/benchmark/lib/xercesImpl-2.10.0.jar b/lucene/contrib/benchmark/lib/xercesImpl-2.10.0.jar
deleted file mode 100644
index 11b416c..0000000
--- a/lucene/contrib/benchmark/lib/xercesImpl-2.10.0.jar
+++ /dev/null
@@ -1,2 +0,0 @@
-AnyObjectId[9dcd8c38196b24e51f78d8e1b0a42d1ffef60acb] was removed in git history.
-Apache SVN contains full history.
\ No newline at end of file
diff --git a/lucene/contrib/benchmark/lib/xml-apis-2.10.0.jar b/lucene/contrib/benchmark/lib/xml-apis-2.10.0.jar
deleted file mode 100644
index c59f0f1..0000000
--- a/lucene/contrib/benchmark/lib/xml-apis-2.10.0.jar
+++ /dev/null
@@ -1,2 +0,0 @@
-AnyObjectId[46733464fc746776c331ecc51061f3a05e662fd1] was removed in git history.
-Apache SVN contains full history.
\ No newline at end of file
diff --git a/lucene/contrib/benchmark/pom.xml.template b/lucene/contrib/benchmark/pom.xml.template
deleted file mode 100644
index 9e6a1ec..0000000
--- a/lucene/contrib/benchmark/pom.xml.template
+++ /dev/null
@@ -1,67 +0,0 @@
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
-
-  <!--
-    Licensed to the Apache Software Foundation (ASF) under one
-    or more contributor license agreements.  See the NOTICE file
-    distributed with this work for additional information
-    regarding copyright ownership.  The ASF licenses this file
-    to you under the Apache License, Version 2.0 (the
-    "License"); you may not use this file except in compliance
-    with the License.  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing,
-    software distributed under the License is distributed on an
-    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-    KIND, either express or implied.  See the License for the
-    specific language governing permissions and limitations
-    under the License.
-  -->
-  <modelVersion>4.0.0</modelVersion>
-  <parent>
-    <groupId>org.apache.lucene</groupId>
-    <artifactId>lucene-contrib</artifactId>
-    <version>@version@</version>
-  </parent>
-  <groupId>org.apache.lucene</groupId>
-  <artifactId>lucene-benchmark</artifactId>
-  <name>Lucene Benchmark</name>
-  <version>@version@</version>
-  <description>Lucene Benchmarking Contributions</description>
-  <packaging>jar</packaging>
-  <dependencies>
-    <dependency>
-      <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-demos</artifactId>
-      <version>@version@</version>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-highlighter</artifactId>
-      <version>@version@</version>
-    </dependency>
-    <dependency>
-      <groupId>commons-beanutils</groupId>
-      <artifactId>commons-beanutils</artifactId>
-      <version>${commons-beanutils-version}</version>
-    </dependency>
-    <dependency>
-      <groupId>commons-collections</groupId>
-      <artifactId>commons-collections</artifactId>
-      <version>${commons-collections-version}</version>
-    </dependency>
-    <dependency>
-      <groupId>commons-digester</groupId>
-      <artifactId>commons-digester</artifactId>
-      <version>${commons-digester-version}</version>
-    </dependency>
-    <dependency>
-      <groupId>commons-logging</groupId>
-      <artifactId>commons-logging</artifactId>
-      <version>${commons-logging-version}</version>
-    </dependency>
-  </dependencies>
-</project>
diff --git a/lucene/contrib/benchmark/scripts/collation.bm2jira.pl b/lucene/contrib/benchmark/scripts/collation.bm2jira.pl
deleted file mode 100644
index b423f75..0000000
--- a/lucene/contrib/benchmark/scripts/collation.bm2jira.pl
+++ /dev/null
@@ -1,63 +0,0 @@
-#!/usr/bin/perl
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-# 
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# ----------
-# bm2jira.pl
-#
-# Converts Lucene contrib-benchmark output produced using the 
-# benchmark.collation.alg file into a JIRA-formatted table.
-#
-
-use strict;
-use warnings;
-
-my %min_elapsed = ();
-
-while (<>) {
-  if (/(\S+)(Keyword|JDK|ICU)_\d+\s*([^\s{].*)/) {
-    my $lang = $1;
-    my $analyzer = $2;
-    my $stats = $3;
-    my ($elapsed) = $stats =~ /(?:[\d,.]+[-\s]*){4}([.\d]+)/;
-    $min_elapsed{$analyzer}{$lang} = $elapsed
-      unless (defined($min_elapsed{$analyzer}{$lang})
-              && $elapsed >= $min_elapsed{$analyzer}{$lang});
-  }
-}
-
-# Print out platform info
-print "JAVA:\n", `java -version 2>&1`, "\nOS:\n";
-if ($^O =~ /win/i) {
-  print "$^O\n";
-  eval {
-    require Win32;
-    print Win32::GetOSName(), "\n", Win32::GetOSVersion(), "\n";
-  };
-  die "Error loading Win32: $@" if ($@);
-} else {
-  print `uname -a 2>&1`;
-}
-
-print "\n||Language||java.text||ICU4J||KeywordAnalyzer||ICU4J Improvement||\n";
-
-for my $lang (sort keys %{$min_elapsed{ICU}}) {
-  my $ICU = $min_elapsed{ICU}{$lang};
-  my $JDK = $min_elapsed{JDK}{$lang};
-  my $keyword = $min_elapsed{Keyword}{$lang};
-  my $improved = int(100 * ($JDK - $ICU) / ($ICU - $keyword) + 0.5);
-  printf "|$lang|${JDK}s|${ICU}s|${keyword}s|\%d%%|\n", $improved;
-}
diff --git a/lucene/contrib/benchmark/scripts/compare.collation.benchmark.tables.pl b/lucene/contrib/benchmark/scripts/compare.collation.benchmark.tables.pl
deleted file mode 100644
index bd94176..0000000
--- a/lucene/contrib/benchmark/scripts/compare.collation.benchmark.tables.pl
+++ /dev/null
@@ -1,91 +0,0 @@
-#!/usr/bin/perl
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-# 
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# ------------------------------------------
-# compare.collation.benchmark.jira.tables.pl
-#
-# Takes as cmdline parameters two JIRA-formatted benchmark results, as produced
-# by bm2jira.pl (located in the same directory as this script), and outputs a
-# third JIRA-formatted comparison table, showing the differences between two
-# benchmarking runs' java.text and ICU4J columns, after accounting for the
-# KeywordAnalyzer column; the "ICU4J Improvement" column is ignored.
-#
-# The difference is calculated as a percentage:
-#
-#   100 * (patched-rate - unpatched-rate / unpatched-rate)
-#
-# where the (un)patched-rate is:
-#
-#   1 / ( elapsed-(un)patched-time - elapsed-KeywordAnalyzer-time)
-#
-
-use strict;
-use warnings;
-
-my $usage = "Usage: $0 <unpatched-file> <patched-file>\n";
-
-die $usage unless ($#ARGV == 1 && -f $ARGV[0] && -f $ARGV[1]);
-
-my %stats = ();
-
-open UNPATCHED, "<$ARGV[0]" || die "ERROR opening '$ARGV[0]': $!";
-while (<UNPATCHED>) {
-  # ||Language||java.text||ICU4J||KeywordAnalyzer||ICU4J Improvement||
-  # |English|4.51s|2.47s|1.47s|204%|
-  next unless (/^\|([^|]+)\|([^|s]+)s\|([^|s]+)s\|([^|s]+)s/);
-  my ($lang, $jdk_elapsed, $icu_elapsed, $keyword_analyzer_elapsed)
-    = ($1, $2, $3, $4);
-  $stats{unpatched}{$lang}{jdk} = $jdk_elapsed;
-  $stats{unpatched}{$lang}{icu} = $icu_elapsed;
-  $stats{unpatched}{$lang}{keyword_analyzer} = $keyword_analyzer_elapsed;
-}
-close UNPATCHED;
-
-open PATCHED, "<$ARGV[1]" || die "ERROR opening '$ARGV[1]': $!";
-while (<PATCHED>) {
-  # ||Language||java.text||ICU4J||KeywordAnalyzer||ICU4J Improvement||
-  # |English|4.51s|2.47s|1.47s|204%|
-  next unless (/^\|([^|]+)\|([^|s]+)s\|([^|s]+)s\|([^|s]+)s/);
-  my ($lang, $jdk_elapsed, $icu_elapsed, $keyword_analyzer_elapsed)
-    = ($1, $2, $3, $4);
-  $stats{patched}{$lang}{jdk} = $jdk_elapsed;
-  $stats{patched}{$lang}{icu} = $icu_elapsed;
-  $stats{patched}{$lang}{keyword_analyzer} = $keyword_analyzer_elapsed;
-}
-close PATCHED;
-
-print "||Language||java.text improvement||ICU4J improvement||\n";
-for my $lang (sort keys %{$stats{unpatched}}) {
-  my $keyword_analyzer1 = $stats{unpatched}{$lang}{keyword_analyzer};
-  my $jdk1 = $stats{unpatched}{$lang}{jdk};
-  my $jdk_diff1 = $jdk1 - $keyword_analyzer1;
-  my $icu1 = $stats{unpatched}{$lang}{icu};
-  my $icu_diff1 = $icu1 - $keyword_analyzer1;
-
-  my $keyword_analyzer2 = $stats{patched}{$lang}{keyword_analyzer};
-  my $jdk2 = $stats{patched}{$lang}{jdk};
-  my $jdk_diff2 = $jdk2 - $keyword_analyzer2;
-  my $icu2 = $stats{patched}{$lang}{icu};
-  my $icu_diff2 = $icu2 - $keyword_analyzer2;
-
-  my $jdk_impr 
-    = int((1./$jdk_diff2 - 1./$jdk_diff1) / (1./$jdk_diff1) * 1000 + 5) / 10;
-  my $icu_impr
-    = int((1./$icu_diff2 - 1./$icu_diff1) / (1./$icu_diff1) * 1000 + 5) / 10;
-
-  printf "|$lang|%2.1f%%|%2.1f%%|\n", $jdk_impr, $icu_impr;
-}
diff --git a/lucene/contrib/benchmark/scripts/compare.shingle.benchmark.tables.pl b/lucene/contrib/benchmark/scripts/compare.shingle.benchmark.tables.pl
deleted file mode 100644
index 3af2c78..0000000
--- a/lucene/contrib/benchmark/scripts/compare.shingle.benchmark.tables.pl
+++ /dev/null
@@ -1,116 +0,0 @@
-#!/usr/bin/perl
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-# 
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# ------------------------------------------
-# compare.shingle.benchmark.jira.tables.pl
-#
-# Takes as cmdline parameters two JIRA-formatted benchmark results, as produced
-# by shingle.bm2jira.pl (located in the same directory as this script), and
-# outputs a third JIRA-formatted comparison table.
-#
-# The difference is calculated as a percentage:
-#
-#   100 * (unpatched-elapsed - patched-elapsed / patched-elapsed)
-#
-# where (un)patched-elapsed values have had the no-shingle-filter 
-# (StandardAnalyzer) elapsed time subtracted from them.
-#
-#
-# Example shingle.bm2jira.pl output:
-# ----------------------------------
-# JAVA:
-# java version "1.5.0_15"
-# Java(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_15-b04)
-# Java HotSpot(TM) 64-Bit Server VM (build 1.5.0_15-b04, mixed mode)
-#
-# OS:
-# cygwin
-# WinVistaService Pack 2
-# Service Pack 26060022202561
-#
-# ||Max Shingle Size||Unigrams?||Elapsed||
-# |1 (Unigrams)|yes|2.19s|
-# |2|no|4.74s|
-# |2|yes|4.90s|
-# |4|no|5.82s|
-# |4|yes|5.97s|
-
-use strict;
-use warnings;
-
-my $usage = "Usage: $0 <unpatched-file> <patched-file>\n";
-
-die $usage unless ($#ARGV == 1 && -f $ARGV[0] && -f $ARGV[1]);
-
-my %stats = ();
-
-open UNPATCHED, "<$ARGV[0]" || die "ERROR opening '$ARGV[0]': $!";
-my $table_encountered = 0;
-my $standard_analyzer_elapsed = 0;
-my %unpatched_stats = ();
-my %patched_stats = ();
-while (<UNPATCHED>) {
-  unless ($table_encountered) {
-    if (/\Q||Max Shingle Size||Unigrams?||Elapsed||\E/) {
-      $table_encountered = 1;
-    } else {
-      print;
-    }
-  } elsif (/\|([^|]+)\|([^|]+)\|([\d.]+)s\|/) {
-    my $max_shingle_size = $1;
-    my $output_unigrams = $2;
-    my $elapsed = $3;
-    if ($max_shingle_size =~ /Unigrams/) {
-      $standard_analyzer_elapsed = $elapsed;
-    } else {
-      $unpatched_stats{$max_shingle_size}{$output_unigrams} = $elapsed;
-    }
-  }
-}
-close UNPATCHED;
-
-open PATCHED, "<$ARGV[1]" || die "ERROR opening '$ARGV[1]': $!";
-while (<PATCHED>) {
-  if (/\|([^|]+)\|([^|]+)\|([\d.]+)s\|/) {
-    my $max_shingle_size = $1;
-    my $output_unigrams = $2;
-    my $elapsed = $3;
-    if ($max_shingle_size =~ /Unigrams/) {
-      $standard_analyzer_elapsed = $elapsed
-         if ($elapsed < $standard_analyzer_elapsed);
-    } else {
-      $patched_stats{$max_shingle_size}{$output_unigrams} = $elapsed;
-    }
-  }
-}
-close PATCHED;
-
-print "||Max Shingle Size||Unigrams?||Unpatched||Patched||StandardAnalyzer||Improvement||\n";
-for my $max_shingle_size (sort { $a <=> $b } keys %unpatched_stats) {
-  for my $output_unigrams (sort keys %{$unpatched_stats{$max_shingle_size}}) {
-    my $improvement 
-      = ( $unpatched_stats{$max_shingle_size}{$output_unigrams}
-        - $patched_stats{$max_shingle_size}{$output_unigrams})
-      / ( $patched_stats{$max_shingle_size}{$output_unigrams}
-        - $standard_analyzer_elapsed);
-    $improvement = int($improvement * 1000 + .5) / 10; # Round and truncate
-    printf "|$max_shingle_size|$output_unigrams"
-          ."|$unpatched_stats{$max_shingle_size}{$output_unigrams}s"
-          ."|$patched_stats{$max_shingle_size}{$output_unigrams}s"
-          ."|${standard_analyzer_elapsed}s|%2.1f%%|\n", $improvement;
-  }
-}
diff --git a/lucene/contrib/benchmark/scripts/shingle.bm2jira.pl b/lucene/contrib/benchmark/scripts/shingle.bm2jira.pl
deleted file mode 100644
index ce6d193..0000000
--- a/lucene/contrib/benchmark/scripts/shingle.bm2jira.pl
+++ /dev/null
@@ -1,73 +0,0 @@
-#!/usr/bin/perl
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-# 
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# ----------
-# shingle.bm2jira.pl
-#
-# Converts Lucene contrib-benchmark output produced using the 
-# conf/shingle.alg file into a JIRA-formatted table.
-#
-
-use strict;
-use warnings;
-
-my %min_elapsed = ();
-
-#Operation           round  runCnt  recsPerRun  rec/s      elapsedSec  avgUsedMem  avgTotalMem
-#BigramsAndUnigrams  0      1       255691      21,147.22  12.09       15,501,840  35,061,760
-#BigramsOnly   -  -  0 -  - 1 -  -  127383   -  16,871.92  7.55    -   31,725,312  41,746,432
-#FourgramsAndUnigrams
-#FourgramsOnly
-#UnigramsOnly
-
-while (<>) {
-  if (/^((?:Uni|Bi|Four)grams\S+)[-\s]*([^\s{].*)/) {
-    my $operation = $1;
-    my $stats = $2;
-    my $max_shingle_size 
-    = ($operation =~ /^Bigrams/ ? 2 : $operation =~ /^Unigrams/ ? 1 : 4);
-    my $output_unigrams 
-      = ($operation =~ /(?:AndUnigrams|UnigramsOnly)$/ ? 'yes' : 'no'); 
-    my ($elapsed) = $stats =~ /(?:[\d,.]+[-\s]*){4}([.\d]+)/;
-    $min_elapsed{$max_shingle_size}{$output_unigrams} = $elapsed
-      unless (defined($min_elapsed{$max_shingle_size}{$output_unigrams})
-              && $elapsed >= $min_elapsed{$max_shingle_size}{$output_unigrams});
-  }
-}
-
-# Print out platform info
-print "JAVA:\n", `java -version 2>&1`, "\nOS:\n";
-if ($^O =~ /win/i) {
-  print "$^O\n";
-  eval {
-    require Win32;
-    print Win32::GetOSName(), "\n", Win32::GetOSVersion(), "\n";
-  };
-  die "Error loading Win32: $@" if ($@);
-} else {
-  print `uname -a 2>&1`;
-}
-
-print "\n||Max Shingle Size||Unigrams?||Elapsed||\n";
-
-for my $max_shingle_size (sort { $a <=> $b } keys %min_elapsed) {
-  for my $output_unigrams (sort keys %{$min_elapsed{$max_shingle_size}}) {
-    my $size = (1 == $max_shingle_size ? '1 (Unigrams)' : $max_shingle_size);   
-    printf "|$size|$output_unigrams|\%2.2fs|\n",
-           $min_elapsed{$max_shingle_size}{$output_unigrams};
-  }
-}
diff --git a/lucene/contrib/benchmark/sortBench.py b/lucene/contrib/benchmark/sortBench.py
deleted file mode 100644
index f027bf2..0000000
--- a/lucene/contrib/benchmark/sortBench.py
+++ /dev/null
@@ -1,553 +0,0 @@
-import types
-import re
-import time
-import os
-import shutil
-import sys
-import cPickle
-import datetime
-
-# TODO
-#   - build wiki/random index as needed (balanced or not, varying # segs, docs)
-#   - verify step
-#   - run searches
-#   - get all docs query in here
-
-if sys.platform.lower().find('darwin') != -1:
-  osName = 'osx'
-elif sys.platform.lower().find('win') != -1:
-  osName = 'windows'
-elif sys.platform.lower().find('linux') != -1:
-  osName = 'linux'
-else:
-  osName = 'unix'
-
-TRUNK_DIR = '/lucene/clean'
-FLEX_DIR = '/lucene/flex.branch'
-
-DEBUG = False
-
-# let shell find it:
-JAVA_COMMAND = 'java -Xms2048M -Xmx2048M -Xbatch -server'
-#JAVA_COMMAND = 'java -Xms1024M -Xmx1024M -Xbatch -server -XX:+AggressiveOpts -XX:CompileThreshold=100 -XX:+UseFastAccessorMethods'
-
-INDEX_NUM_THREADS = 1
-
-INDEX_NUM_DOCS = 5000000
-
-LOG_DIR = 'logs'
-
-DO_BALANCED = False
-
-if osName == 'osx':
-  WIKI_FILE = '/x/lucene/enwiki-20090724-pages-articles.xml.bz2'
-  INDEX_DIR_BASE = '/lucene'
-else:
-  WIKI_FILE = '/x/lucene/enwiki-20090724-pages-articles.xml.bz2'
-  INDEX_DIR_BASE = '/x/lucene'
-
-if DEBUG:
-  NUM_ROUND = 0
-else:
-  NUM_ROUND = 7
-
-if 0:
-  print 'compile...'
-  if '-nocompile' not in sys.argv:
-    if os.system('ant compile > compile.log 2>&1') != 0:
-      raise RuntimeError('compile failed (see compile.log)')
-
-BASE_SEARCH_ALG = '''
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-work.dir = $INDEX$
-search.num.hits = $NUM_HITS$
-query.maker=org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker
-file.query.maker.file = queries.txt
-print.hits.field = $PRINT_FIELD$
-log.queries=true
-log.step=100000
-
-$OPENREADER$
-{"XSearchWarm" $SEARCH$}
-
-# Turn off printing, after warming:
-SetProp(print.hits.field,)
-
-$ROUNDS$
-CloseReader 
-RepSumByPrefRound XSearch
-'''
-
-BASE_INDEX_ALG = '''
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-
-$OTHER$
-deletion.policy = org.apache.lucene.index.NoDeletionPolicy
-doc.tokenized = false
-doc.body.tokenized = true
-doc.stored = true
-doc.body.stored = false
-doc.term.vector = false
-log.step.AddDoc=10000
-
-directory=FSDirectory
-autocommit=false
-compound=false
-
-work.dir=$WORKDIR$
-
-{ "BuildIndex"
-  - CreateIndex
-  $INDEX_LINE$
-  - CommitIndex(dp0)
-  - CloseIndex
-  $DELETIONS$
-}
-
-RepSumByPrefRound BuildIndex
-'''
-
-class RunAlgs:
-
-  def __init__(self, resultsPrefix):
-    self.counter = 0
-    self.results = []
-    self.fOut = open('%s.txt' % resultsPrefix, 'wb')
-    
-  def makeIndex(self, label, dir, source, numDocs, balancedNumSegs=None, deletePcts=None):
-
-    if source not in ('wiki', 'random'):
-      raise RuntimeError('source must be wiki or random')
-
-    if dir is not None:
-      fullDir = '%s/contrib/benchmark' % dir
-      if DEBUG:
-        print '  chdir %s' % fullDir
-      os.chdir(fullDir)
-
-    indexName = '%s.%s.nd%gM' % (source, label, numDocs/1000000.0)
-    if balancedNumSegs is not None:
-      indexName += '_balanced%d' % balancedNumSegs
-    fullIndexPath = '%s/%s' % (INDEX_DIR_BASE, indexName)
-    
-    if os.path.exists(fullIndexPath):
-      print 'Index %s already exists...' % fullIndexPath
-      return indexName
-
-    print 'Now create index %s...' % fullIndexPath
-
-    s = BASE_INDEX_ALG
-
-    if source == 'wiki':
-      other = '''doc.index.props = true
-content.source=org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource
-docs.file=%s
-''' % WIKI_FILE
-      #addDoc = 'AddDoc(1024)'
-      addDoc = 'AddDoc'
-    else:
-      other = '''doc.index.props = true
-content.source=org.apache.lucene.benchmark.byTask.feeds.SortableSingleDocSource
-'''
-      addDoc = 'AddDoc'
-    if INDEX_NUM_THREADS > 1:
-      #other += 'doc.reuse.fields=false\n'
-      s = s.replace('$INDEX_LINE$', '[ { "AddDocs" %s > : %s } : %s' % \
-                    (addDoc, numDocs/INDEX_NUM_THREADS, INDEX_NUM_THREADS))
-    else:
-      s = s.replace('$INDEX_LINE$', '{ "AddDocs" %s > : %s' % \
-                    (addDoc, numDocs))
-
-    s = s.replace('$WORKDIR$', fullIndexPath)
-
-    if deletePcts is not None:
-      dp = '# Do deletions\n'
-      dp += 'OpenReader(false)\n'
-      for pct in deletePcts:
-        if pct != 0:
-          dp += 'DeleteByPercent(%g)\n' % pct
-          dp += 'CommitIndex(dp%g)\n' % pct
-      dp += 'CloseReader()\n'
-    else:
-      dp = ''
-
-    s = s.replace('$DELETIONS$', dp)
-
-    if balancedNumSegs is not None:
-      other += '''  merge.factor=1000
-  max.buffered=%d
-  ram.flush.mb=2000
-  ''' % (numDocs/balancedNumSegs)
-    else:
-      if source == 'random':
-        other += 'ram.flush.mb=1.0\n'
-      else:
-        other += 'ram.flush.mb=32.0\n'
-
-    s = s.replace('$OTHER$', other)
-
-    try:
-      self.runOne(dir, s, 'index_%s' % indexName, isIndex=True)
-    except:
-      if os.path.exists(fullIndexPath):
-        shutil.rmtree(fullIndexPath)
-      raise
-    return indexName
-    
-  def getLogPrefix(self, **dArgs):
-    l = dArgs.items()
-    l.sort()
-    s = '_'.join(['%s=%s' % tup for tup in l])
-    s = s.replace(' ', '_')
-    s = s.replace('"', '_')
-    return s
-             
-  def runOne(self, dir, alg, logFileName, expectedMaxDocs=None, expectedNumDocs=None, queries=None, verify=False, isIndex=False):
-
-    fullDir = '%s/contrib/benchmark' % dir
-    if DEBUG:
-      print '  chdir %s' % fullDir
-    os.chdir(fullDir)
-               
-    if queries is not None:
-      if type(queries) in types.StringTypes:
-        queries = [queries]
-      open('queries.txt', 'wb').write('\n'.join(queries))
-
-    if DEBUG:
-      algFile = 'tmp.alg'
-    else:
-      algFile = 'tmp.%s.alg' % os.getpid()
-    open(algFile, 'wb').write(alg)
-
-    fullLogFileName = '%s/contrib/benchmark/%s/%s' % (dir, LOG_DIR, logFileName)
-    print '  log: %s' % fullLogFileName
-    if not os.path.exists(LOG_DIR):
-      print '  mkdir %s' % LOG_DIR
-      os.makedirs(LOG_DIR)
-
-    command = '%s -classpath ../../build/classes/java:../../build/classes/demo:../../build/contrib/highlighter/classes/java:lib/commons-digester-1.7.jar:lib/commons-collections-3.1.jar:lib/commons-compress-1.0.jar:lib/commons-logging-1.0.4.jar:lib/commons-beanutils-1.7.0.jar:lib/xerces-2.10.0.jar:lib/xml-apis-2.10.0.jar:../../build/contrib/benchmark/classes/java org.apache.lucene.benchmark.byTask.Benchmark %s > "%s" 2>&1' % (JAVA_COMMAND, algFile, fullLogFileName)
-
-    if DEBUG:
-      print 'command=%s' % command
-      
-    try:
-      t0 = time.time()
-      if os.system(command) != 0:
-        raise RuntimeError('FAILED')
-      t1 = time.time()
-    finally:
-      if not DEBUG:
-        os.remove(algFile)
-
-    if isIndex:
-      s = open(fullLogFileName, 'rb').read()
-      if s.find('Exception in thread "') != -1 or s.find('at org.apache.lucene') != -1:
-        raise RuntimeError('alg hit exceptions')
-      return
-
-    else:
-
-      # Parse results:
-      bestQPS = None
-      count = 0
-      nhits = None
-      numDocs = None
-      maxDocs = None
-      warmTime = None
-      r = re.compile('^  ([0-9]+): (.*)$')
-      topN = []
-
-      for line in open(fullLogFileName, 'rb').readlines():
-        m = r.match(line.rstrip())
-        if m is not None:
-          topN.append(m.group(2))
-        if line.startswith('totalHits = '):
-          nhits = int(line[12:].strip())
-        if line.startswith('maxDoc()  = '):
-          maxDocs = int(line[12:].strip())
-        if line.startswith('numDocs() = '):
-          numDocs = int(line[12:].strip())
-        if line.startswith('XSearchWarm'):
-          v = line.strip().split()
-          warmTime = float(v[5])
-        if line.startswith('XSearchReal'):
-          v = line.strip().split()
-          # print len(v), v
-          upto = 0
-          i = 0
-          qps = None
-          while i < len(v):
-            if v[i] == '-':
-              i += 1
-              continue
-            else:
-              upto += 1
-              i += 1
-              if upto == 5:
-                qps = float(v[i-1].replace(',', ''))
-                break
-
-          if qps is None:
-            raise RuntimeError('did not find qps')
-
-          count += 1
-          if bestQPS is None or qps > bestQPS:
-            bestQPS = qps
-
-      if not verify:
-        if count != NUM_ROUND:
-          raise RuntimeError('did not find %s rounds (got %s)' % (NUM_ROUND, count))
-        if warmTime is None:
-          raise RuntimeError('did not find warm time')
-      else:
-        bestQPS = 1.0
-        warmTime = None
-
-      if nhits is None:
-        raise RuntimeError('did not see "totalHits = XXX"')
-
-      if maxDocs is None:
-        raise RuntimeError('did not see "maxDoc() = XXX"')
-
-      if maxDocs != expectedMaxDocs:
-        raise RuntimeError('maxDocs() mismatch: expected %s but got %s' % (expectedMaxDocs, maxDocs))
-
-      if numDocs is None:
-        raise RuntimeError('did not see "numDocs() = XXX"')
-
-      if numDocs != expectedNumDocs:
-        raise RuntimeError('numDocs() mismatch: expected %s but got %s' % (expectedNumDocs, numDocs))
-      
-      return nhits, warmTime, bestQPS, topN
-
-  def getAlg(self, indexPath, searchTask, numHits, deletes=None, verify=False, printField=''):
-
-    s = BASE_SEARCH_ALG
-    s = s.replace('$PRINT_FIELD$', 'doctitle')
-
-    if not verify:
-      s = s.replace('$ROUNDS$',
-  '''                
-  { "Rounds"
-    { "Run"
-      { "TestSearchSpeed"
-        { "XSearchReal" $SEARCH$ > : 3.0s
-      }
-      NewRound
-    } : %d
-  } 
-  ''' % NUM_ROUND)
-    else:
-      s = s.replace('$ROUNDS$', '')
-
-    if deletes is None:
-      s = s.replace('$OPENREADER$', 'OpenReader')
-    else:
-      s = s.replace('$OPENREADER$', 'OpenReader(true,dp%g)' % deletes)
-    s = s.replace('$INDEX$', indexPath)
-    s = s.replace('$SEARCH$', searchTask)
-    s = s.replace('$NUM_HITS$', str(numHits))
-    
-    return s
-
-  def compare(self, baseline, new, *params):
-
-    if new[0] != baseline[0]:
-      raise RuntimeError('baseline found %d hits but new found %d hits' % (baseline[0], new[0]))
-
-    qpsOld = baseline[2]
-    qpsNew = new[2]
-    pct = 100.0*(qpsNew-qpsOld)/qpsOld
-    print '  diff: %.1f%%' % pct
-    self.results.append((qpsOld, qpsNew, params))
-
-    self.fOut.write('|%s|%.2f|%.2f|%.1f%%|\n' % \
-                    ('|'.join(str(x) for x in params),
-                     qpsOld, qpsNew, pct))
-    self.fOut.flush()
-
-  def save(self, name):
-    f = open('%s.pk' % name, 'wb')
-    cPickle.dump(self.results, f)
-    f.close()
-
-def verify(r1, r2):
-  if r1[0] != r2[0]:
-    raise RuntimeError('different total hits: %s vs %s' % (r1[0], r2[0]))
-                       
-  h1 = r1[3]
-  h2 = r2[3]
-  if len(h1) != len(h2):
-    raise RuntimeError('different number of results')
-  else:
-    for i in range(len(h1)):
-      s1 = h1[i].replace('score=NaN', 'score=na').replace('score=0.0', 'score=na')
-      s2 = h2[i].replace('score=NaN', 'score=na').replace('score=0.0', 'score=na')
-      if s1 != s2:
-        raise RuntimeError('hit %s differs: %s vs %s' % (i, s1 ,s2))
-
-def usage():
-  print
-  print 'Usage: python -u %s -run <name> | -report <name>' % sys.argv[0]
-  print
-  print '  -run <name> runs all tests, saving results to file <name>.pk'
-  print '  -report <name> opens <name>.pk and prints Jira table'
-  print '  -verify confirm old & new produce identical results'
-  print
-  sys.exit(1)
-
-def main():
-
-  if not os.path.exists(LOG_DIR):
-    os.makedirs(LOG_DIR)
-
-  if '-run' in sys.argv:
-    i = sys.argv.index('-run')
-    mode = 'run'
-    if i < len(sys.argv)-1:
-      name = sys.argv[1+i]
-    else:
-      usage()
-  elif '-report' in sys.argv:
-    i = sys.argv.index('-report')
-    mode = 'report'
-    if i < len(sys.argv)-1:
-      name = sys.argv[1+i]
-    else:
-      usage()
-  elif '-verify' in sys.argv:
-    mode = 'verify'
-    name = None
-  else:
-    usage()
-
-  if mode in ('run', 'verify'):
-    run(mode, name)
-  else:
-    report(name)
-
-def report(name):
-
-  print '||Query||Deletes %||Tot hits||QPS old||QPS new||Pct change||'
-
-  results = cPickle.load(open('%s.pk' % name))
-  for qpsOld, qpsNew, params in results:
-    pct = 100.0*(qpsNew-qpsOld)/qpsOld
-    if pct < 0.0:
-      c = 'red'
-    else:
-      c = 'green'
-
-    params = list(params)
-
-    query = params[0]
-    if query == '*:*':
-      query = '<all>'
-    params[0] = query
-    
-    pct = '{color:%s}%.1f%%{color}' % (c, pct)
-    print '|%s|%.2f|%.2f|%s|' % \
-          ('|'.join(str(x) for x in params),
-           qpsOld, qpsNew, pct)
-
-def run(mode, name):
-
-  for dir in (TRUNK_DIR, FLEX_DIR):
-    dir = '%s/contrib/benchmark' % dir
-    print '"ant compile" in %s...' % dir
-    os.chdir(dir)
-    if os.system('ant compile') != 0:
-      raise RuntimeError('ant compile failed')
-  
-  r = RunAlgs(name)
-
-  if not os.path.exists(WIKI_FILE):
-    print
-    print 'ERROR: wiki source file "%s" does not exist' % WIKI_FILE
-    print
-    sys.exit(1)
-
-  print
-  print 'JAVA:\n%s' % os.popen('java -version 2>&1').read()
-    
-  print
-  if osName != 'windows':
-    print 'OS:\n%s' % os.popen('uname -a 2>&1').read()
-  else:
-    print 'OS:\n%s' % sys.platform
-
-  deletePcts = (0.0, 0.1, 1.0, 10)
-
-  indexes = {}
-  for rev in ('baseline', 'flex'):
-    if rev == 'baseline':
-      dir = TRUNK_DIR
-    else:
-      dir = FLEX_DIR
-    source = 'wiki'
-    indexes[rev] = r.makeIndex(rev, dir, source, INDEX_NUM_DOCS, deletePcts=deletePcts)
-
-  doVerify = mode == 'verify'
-  source = 'wiki'
-  numHits = 10
-
-  queries = (
-    'body:[tec TO tet]',
-    'real*',
-    '1',
-    '2',
-    '+1 +2',
-    '+1 -2',
-    '1 2 3 -4',
-    '"world economy"')
-
-  for query in queries:
-
-    for deletePct in deletePcts:
-
-      print '\nRUN: query=%s deletes=%g%% nhits=%d' % \
-            (query, deletePct, numHits)
-
-      maxDocs = INDEX_NUM_DOCS
-      numDocs = int(INDEX_NUM_DOCS * (1.0-deletePct/100.))
-
-      prefix = r.getLogPrefix(query=query, deletePct=deletePct)
-      indexPath = '%s/%s' % (INDEX_DIR_BASE, indexes['baseline'])
-
-      # baseline (trunk)
-      s = r.getAlg(indexPath,
-                   'Search',
-                   numHits,
-                   deletes=deletePct,
-                   verify=doVerify,
-                   printField='doctitle')
-      baseline = r.runOne(TRUNK_DIR, s, 'baseline_%s' % prefix, maxDocs, numDocs, query, verify=doVerify)
-
-      # flex
-      indexPath = '%s/%s' % (INDEX_DIR_BASE, indexes['flex'])
-      s = r.getAlg(indexPath,
-                   'Search',
-                   numHits,
-                   deletes=deletePct,
-                   verify=doVerify,
-                   printField='doctitle')
-      flex = r.runOne(FLEX_DIR, s, 'flex_%s' % prefix, maxDocs, numDocs, query, verify=doVerify)
-
-      print '  %d hits' % flex[0]
-
-      verify(baseline, flex)
-
-      if mode == 'run' and not DEBUG:
-        r.compare(baseline, flex,
-                  query, deletePct, baseline[0])
-        r.save(name)
-
-def cleanScores(l):
-  for i in range(len(l)):
-    pos = l[i].find(' score=')
-    l[i] = l[i][:pos].strip()
-
-if __name__ == '__main__':
-  main()
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/Constants.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/Constants.java
deleted file mode 100644
index 4292632..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/Constants.java
+++ /dev/null
@@ -1,33 +0,0 @@
-package org.apache.lucene.benchmark;
-/**
- * Copyright 2005 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-
-/**
- *
- *
- **/
-public class Constants
-{
-    public static final int DEFAULT_RUN_COUNT = 5;
-    public static final int DEFAULT_SCALE_UP = 5;
-    public static final int DEFAULT_LOG_STEP = 1000;
-
-    public static Boolean[] BOOLEANS = new Boolean[] { Boolean.FALSE, Boolean.TRUE };
-
-    public static final int DEFAULT_MAXIMUM_DOCUMENTS = Integer.MAX_VALUE;
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/Benchmark.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/Benchmark.java
deleted file mode 100644
index 9b85743..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/Benchmark.java
+++ /dev/null
@@ -1,135 +0,0 @@
-package org.apache.lucene.benchmark.byTask;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.io.FileReader;
-import java.io.Reader;
-
-import org.apache.lucene.benchmark.byTask.utils.Algorithm;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-
-
-/**
- * Run the benchmark algorithm.
- * <p>Usage: java Benchmark  algorithm-file
- * <ol>
- * <li>Read algorithm.</li>
- * <li> Run the algorithm.</li>
- * </ol>
- * Things to be added/fixed in "Benchmarking by tasks":
- * <ol>
- * <li>TODO - report into Excel and/or graphed view.</li>
- * <li>TODO - perf comparison between Lucene releases over the years.</li>
- * <li>TODO - perf report adequate to include in Lucene nightly build site? (so we can easily track performance changes.)</li>
- * <li>TODO - add overall time control for repeated execution (vs. current by-count only).</li>
- * <li>TODO - query maker that is based on index statistics.</li>
- * </ol>
- */
-public class Benchmark {
-
-  private PerfRunData runData;
-  private Algorithm algorithm;
-  private boolean executed;
-  
-  public Benchmark (Reader algReader) throws Exception {
-    // prepare run data
-    try {
-      runData = new PerfRunData(new Config(algReader));
-    } catch (Exception e) {
-      e.printStackTrace();
-      throw new Exception("Error: cannot init PerfRunData!",e);
-    }
-    
-    // parse algorithm
-    try {
-      algorithm = new Algorithm(runData);
-    } catch (Exception e) {
-      throw new Exception("Error: cannot understand algorithm!",e);
-    }
-  }
-  
-  public synchronized void  execute() throws Exception {
-    if (executed) {
-      throw new IllegalStateException("Benchmark was already executed");
-    }
-    executed = true;
-    runData.setStartTimeMillis();
-    algorithm.execute();
-  }
-  
-  /**
-   * Run the benchmark algorithm.
-   * @param args benchmark config and algorithm files
-   */
-  public static void main(String[] args) {
-    // verify command line args
-    if (args.length < 1) {
-      System.err.println("Usage: java Benchmark <algorithm file>");
-      System.exit(1);
-    }
-    
-    // verify input files 
-    File algFile = new File(args[0]);
-    if (!algFile.exists() || !algFile.isFile() || !algFile.canRead()) {
-      System.err.println("cannot find/read algorithm file: "+algFile.getAbsolutePath()); 
-      System.exit(1);
-    }
-    
-    System.out.println("Running algorithm from: "+algFile.getAbsolutePath());
-    
-    Benchmark benchmark = null;
-    try {
-      benchmark = new Benchmark(new FileReader(algFile));
-    } catch (Exception e) {
-      e.printStackTrace();
-      System.exit(1);
-    }
-
-    System.out.println("------------> algorithm:");
-    System.out.println(benchmark.getAlgorithm().toString());
-
-    // execute
-    try {
-      benchmark.execute();
-    } catch (Exception e) {
-      System.err.println("Error: cannot execute the algorithm! "+e.getMessage());
-      e.printStackTrace();
-    }
-
-    System.out.println("####################");
-    System.out.println("###  D O N E !!! ###");
-    System.out.println("####################");
-
-  }
-
-  /**
-   * @return Returns the algorithm.
-   */
-  public Algorithm getAlgorithm() {
-    return algorithm;
-  }
-
-  /**
-   * @return Returns the runData.
-   */
-  public PerfRunData getRunData() {
-    return runData;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/PerfRunData.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/PerfRunData.java
deleted file mode 100644
index 64f5731..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/PerfRunData.java
+++ /dev/null
@@ -1,297 +0,0 @@
-package org.apache.lucene.benchmark.byTask;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Locale;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
-import org.apache.lucene.benchmark.byTask.feeds.QueryMaker;
-import org.apache.lucene.benchmark.byTask.stats.Points;
-import org.apache.lucene.benchmark.byTask.tasks.ReadTask;
-import org.apache.lucene.benchmark.byTask.tasks.SearchTask;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.benchmark.byTask.utils.FileUtils;
-import org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.store.RAMDirectory;
-
-/**
- * Data maintained by a performance test run.
- * <p>
- * Data includes:
- * <ul>
- *  <li>Configuration.
- *  <li>Directory, Writer, Reader.
- *  <li>Docmaker and a few instances of QueryMaker.
- *  <li>Analyzer.
- *  <li>Statistics data which updated during the run.
- * </ul>
- * Config properties: work.dir=&lt;path to root of docs and index dirs| Default: work&gt;
- * </ul>
- */
-public class PerfRunData {
-
-  private Points points;
-  
-  // objects used during performance test run
-  // directory, analyzer, docMaker - created at startup.
-  // reader, writer, searcher - maintained by basic tasks. 
-  private Directory directory;
-  private Analyzer analyzer;
-  private DocMaker docMaker;
-  private Locale locale;
-  
-  // we use separate (identical) instances for each "read" task type, so each can iterate the quries separately.
-  private HashMap<Class<? extends ReadTask>,QueryMaker> readTaskQueryMaker;
-  private Class<? extends QueryMaker> qmkrClass;
-
-  private IndexReader indexReader;
-  private IndexSearcher indexSearcher;
-  private IndexWriter indexWriter;
-  private Config config;
-  private long startTimeMillis;
-  
-  // constructor
-  public PerfRunData (Config config) throws Exception {
-    this.config = config;
-    // analyzer (default is standard analyzer)
-    analyzer = NewAnalyzerTask.createAnalyzer(config.get("analyzer",
-        "org.apache.lucene.analysis.standard.StandardAnalyzer"));
-    // doc maker
-    docMaker = Class.forName(config.get("doc.maker",
-        "org.apache.lucene.benchmark.byTask.feeds.DocMaker")).asSubclass(DocMaker.class).newInstance();
-    docMaker.setConfig(config);
-    // query makers
-    readTaskQueryMaker = new HashMap<Class<? extends ReadTask>,QueryMaker>();
-    qmkrClass = Class.forName(config.get("query.maker","org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker")).asSubclass(QueryMaker.class);
-
-    // index stuff
-    reinit(false);
-    
-    // statistic points
-    points = new Points(config);
-    
-    if (Boolean.valueOf(config.get("log.queries","false")).booleanValue()) {
-      System.out.println("------------> queries:");
-      System.out.println(getQueryMaker(new SearchTask(this)).printQueries());
-    }
-  }
-
-  // clean old stuff, reopen 
-  public void reinit(boolean eraseIndex) throws Exception {
-
-    // cleanup index
-    if (indexWriter!=null) {
-      indexWriter.close();
-      indexWriter = null;
-    }
-    if (indexReader!=null) {
-      indexReader.close();
-      indexReader = null;
-    }
-    if (directory!=null) {
-      directory.close();
-    }
-    
-    // directory (default is ram-dir).
-    if ("FSDirectory".equals(config.get("directory","RAMDirectory"))) {
-      File workDir = new File(config.get("work.dir","work"));
-      File indexDir = new File(workDir,"index");
-      if (eraseIndex && indexDir.exists()) {
-        FileUtils.fullyDelete(indexDir);
-      }
-      indexDir.mkdirs();
-      directory = FSDirectory.open(indexDir);
-    } else {
-      directory = new RAMDirectory();
-    }
-
-    // inputs
-    resetInputs();
-    
-    // release unused stuff
-    System.runFinalization();
-    System.gc();
-
-    // Re-init clock
-    setStartTimeMillis();
-  }
-  
-  public long setStartTimeMillis() {
-    startTimeMillis = System.currentTimeMillis();
-    return startTimeMillis;
-  }
-
-  /**
-   * @return Start time in milliseconds
-   */
-  public long getStartTimeMillis() {
-    return startTimeMillis;
-  }
-
-  /**
-   * @return Returns the points.
-   */
-  public Points getPoints() {
-    return points;
-  }
-
-  /**
-   * @return Returns the directory.
-   */
-  public Directory getDirectory() {
-    return directory;
-  }
-
-  /**
-   * @param directory The directory to set.
-   */
-  public void setDirectory(Directory directory) {
-    this.directory = directory;
-  }
-
-  /**
-   * @return Returns the indexReader.  NOTE: this returns a
-   * reference.  You must call IndexReader.decRef() when
-   * you're done.
-   */
-  public synchronized IndexReader getIndexReader() {
-    if (indexReader != null) {
-      indexReader.incRef();
-    }
-    return indexReader;
-  }
-
-  /**
-   * @return Returns the indexSearcher.  NOTE: this returns
-   * a reference to the underlying IndexReader.  You must
-   * call IndexReader.decRef() when you're done.
-   */
-  public synchronized IndexSearcher getIndexSearcher() {
-    if (indexReader != null) {
-      indexReader.incRef();
-    }
-    return indexSearcher;
-  }
-
-  /**
-   * @param indexReader The indexReader to set.
-   */
-  public synchronized void setIndexReader(IndexReader indexReader) throws IOException {
-    if (this.indexReader != null) {
-      // Release current IR
-      this.indexReader.decRef();
-    }
-    this.indexReader = indexReader;
-    if (indexReader != null) {
-      // Hold reference to new IR
-      indexReader.incRef();
-      indexSearcher = new IndexSearcher(indexReader);
-    } else {
-      indexSearcher = null;
-    }
-  }
-
-  /**
-   * @return Returns the indexWriter.
-   */
-  public IndexWriter getIndexWriter() {
-    return indexWriter;
-  }
-
-  /**
-   * @param indexWriter The indexWriter to set.
-   */
-  public void setIndexWriter(IndexWriter indexWriter) {
-    this.indexWriter = indexWriter;
-  }
-
-  /**
-   * @return Returns the anlyzer.
-   */
-  public Analyzer getAnalyzer() {
-    return analyzer;
-  }
-
-
-  public void setAnalyzer(Analyzer analyzer) {
-    this.analyzer = analyzer;
-  }
-
-  /** Returns the docMaker. */
-  public DocMaker getDocMaker() {
-    return docMaker;
-  }
-
-  /**
-   * @return the locale
-   */
-  public Locale getLocale() {
-    return locale;
-  }
-
-  /**
-   * @param locale the locale to set
-   */
-  public void setLocale(Locale locale) {
-    this.locale = locale;
-  }
-
-  /**
-   * @return Returns the config.
-   */
-  public Config getConfig() {
-    return config;
-  }
-
-  public void resetInputs() throws IOException {
-    docMaker.resetInputs();
-    for (final QueryMaker queryMaker : readTaskQueryMaker.values()) {
-      queryMaker.resetInputs();
-    }
-  }
-
-  /**
-   * @return Returns the queryMaker by read task type (class)
-   */
-  synchronized public QueryMaker getQueryMaker(ReadTask readTask) {
-    // mapping the query maker by task class allows extending/adding new search/read tasks
-    // without needing to modify this class.
-    Class<? extends ReadTask> readTaskClass = readTask.getClass();
-    QueryMaker qm = readTaskQueryMaker.get(readTaskClass);
-    if (qm == null) {
-      try {
-        qm = qmkrClass.newInstance();
-        qm.setConfig(config);
-      } catch (Exception e) {
-        throw new RuntimeException(e);
-      }
-      readTaskQueryMaker.put(readTaskClass,qm);
-    }
-    return qm;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/AbstractQueryMaker.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/AbstractQueryMaker.java
deleted file mode 100644
index a9c1c0d..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/AbstractQueryMaker.java
+++ /dev/null
@@ -1,72 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-/**
- * Copyright 2005 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-
-/**
- * Abstract base query maker. 
- * Each query maker should just implement the {@link #prepareQueries()} method.
- **/
-public abstract class AbstractQueryMaker implements QueryMaker {
-
-  protected int qnum = 0;
-  protected Query[] queries;
-  protected Config config;
-
-  public void resetInputs() {
-    qnum = 0;
-  }
-
-  protected abstract Query[] prepareQueries() throws Exception;
-
-  public void setConfig(Config config) throws Exception {
-    this.config = config;
-    queries = prepareQueries();
-  }
-
-  public String printQueries() {
-    String newline = System.getProperty("line.separator");
-    StringBuilder sb = new StringBuilder();
-    if (queries != null) {
-      for (int i = 0; i < queries.length; i++) {
-        sb.append(i+". "+ queries[i].getClass().getSimpleName()+" - "+queries[i].toString());
-        sb.append(newline);
-      }
-    }
-    return sb.toString();
-  }
-
-  public Query makeQuery() throws Exception {
-    return queries[nextQnum()];
-  }
-  
-  // return next qnum
-  protected synchronized int nextQnum() {
-    int res = qnum;
-    qnum = (qnum+1) % queries.length;
-    return res;
-  }
-
-  /*
-  *  (non-Javadoc)
-  * @see org.apache.lucene.benchmark.byTask.feeds.QueryMaker#makeQuery(int)
-  */
-  public Query makeQuery(int size) throws Exception {
-    throw new Exception(this+".makeQuery(int size) is not supported!");
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ContentSource.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ContentSource.java
deleted file mode 100644
index 817e57d..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ContentSource.java
+++ /dev/null
@@ -1,206 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.BufferedInputStream;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.commons.compress.compressors.CompressorException;
-import org.apache.commons.compress.compressors.CompressorStreamFactory;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-
-/**
- * Represents content from a specified source, such as TREC, Reuters etc. A
- * {@link ContentSource} is responsible for creating {@link DocData} objects for
- * its documents to be consumed by {@link DocMaker}. It also keeps track
- * of various statistics, such as how many documents were generated, size in
- * bytes etc.
- * <p>
- * Supports the following configuration parameters:
- * <ul>
- * <li><b>content.source.forever</b> - specifies whether to generate documents
- * forever (<b>default=true</b>).
- * <li><b>content.source.verbose</b> - specifies whether messages should be
- * output by the content source (<b>default=false</b>).
- * <li><b>content.source.encoding</b> - specifies which encoding to use when
- * reading the files of that content source. Certain implementations may define
- * a default value if this parameter is not specified. (<b>default=null</b>).
- * <li><b>content.source.log.step</b> - specifies for how many documents a
- * message should be logged. If set to 0 it means no logging should occur.
- * <b>NOTE:</b> if verbose is set to false, logging should not occur even if
- * logStep is not 0 (<b>default=0</b>).
- * </ul>
- */
-public abstract class ContentSource {
-  
-  private static final int BZIP = 0;
-  private static final int OTHER = 1;
-  private static final Map<String,Integer> extensionToType = new HashMap<String,Integer>();
-  static {
-    extensionToType.put(".bz2", Integer.valueOf(BZIP));
-    extensionToType.put(".bzip", Integer.valueOf(BZIP));
-  }
-  
-  protected static final int BUFFER_SIZE = 1 << 16; // 64K
-
-  private long bytesCount;
-  private long totalBytesCount;
-  private int docsCount;
-  private int totalDocsCount;
-  private Config config;
-
-  protected boolean forever;
-  protected int logStep;
-  protected boolean verbose;
-  protected String encoding;
-  
-  private CompressorStreamFactory csFactory = new CompressorStreamFactory();
-
-  protected final synchronized void addBytes(long numBytes) {
-    bytesCount += numBytes;
-    totalBytesCount += numBytes;
-  }
-  
-  protected final synchronized void addDoc() {
-    ++docsCount;
-    ++totalDocsCount;
-  }
-
-  /**
-   * A convenience method for collecting all the files of a content source from
-   * a given directory. The collected {@link File} instances are stored in the
-   * given <code>files</code>.
-   */
-  protected final void collectFiles(File dir, ArrayList<File> files) {
-    if (!dir.canRead()) {
-      return;
-    }
-    
-    File[] dirFiles = dir.listFiles();
-    Arrays.sort(dirFiles);
-    for (int i = 0; i < dirFiles.length; i++) {
-      File file = dirFiles[i];
-      if (file.isDirectory()) {
-        collectFiles(file, files);
-      } else if (file.canRead()) {
-        files.add(file);
-      }
-    }
-  }
-
-  /**
-   * Returns an {@link InputStream} over the requested file. This method
-   * attempts to identify the appropriate {@link InputStream} instance to return
-   * based on the file name (e.g., if it ends with .bz2 or .bzip, return a
-   * 'bzip' {@link InputStream}).
-   */
-  protected InputStream getInputStream(File file) throws IOException {
-    // First, create a FileInputStream, as this will be required by all types.
-    // Wrap with BufferedInputStream for better performance
-    InputStream is = new BufferedInputStream(new FileInputStream(file), BUFFER_SIZE);
-    
-    String fileName = file.getName();
-    int idx = fileName.lastIndexOf('.');
-    int type = OTHER;
-    if (idx != -1) {
-      Integer typeInt = extensionToType.get(fileName.substring(idx));
-      if (typeInt != null) {
-        type = typeInt.intValue();
-      }
-    }
-    switch (type) {
-      case BZIP:
-        try {
-          // According to BZip2CompressorInputStream's code, it reads the first 
-          // two file header chars ('B' and 'Z'). It is important to wrap the
-          // underlying input stream with a buffered one since
-          // Bzip2CompressorInputStream uses the read() method exclusively.
-          is = csFactory.createCompressorInputStream("bzip2", is);
-        } catch (CompressorException e) {
-          IOException ioe = new IOException(e.getMessage());
-          ioe.initCause(e);
-          throw ioe;
-        }
-        break;
-      default: // Do nothing, stay with FileInputStream
-    }
-    
-    return is;
-  }
-  
-  /**
-   * Returns true whether it's time to log a message (depending on verbose and
-   * the number of documents generated).
-   */
-  protected final boolean shouldLog() {
-    return verbose && logStep > 0 && docsCount % logStep == 0;
-  }
-
-  /** Called when reading from this content source is no longer required. */
-  public abstract void close() throws IOException;
-  
-  /** Returns the number of bytes generated since last reset. */
-  public final long getBytesCount() { return bytesCount; }
-
-  /** Returns the number of generated documents since last reset. */
-  public final int getDocsCount() { return docsCount; }
-  
-  public final Config getConfig() { return config; }
-
-  /** Returns the next {@link DocData} from the content source. */
-  public abstract DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException;
-
-  /** Returns the total number of bytes that were generated by this source. */ 
-  public final long getTotalBytesCount() { return totalBytesCount; }
-
-  /** Returns the total number of generated documents. */
-  public final int getTotalDocsCount() { return totalDocsCount; }
-
-  /**
-   * Resets the input for this content source, so that the test would behave as
-   * if it was just started, input-wise.
-   * <p>
-   * <b>NOTE:</b> the default implementation resets the number of bytes and
-   * documents generated since the last reset, so it's important to call
-   * super.resetInputs in case you override this method.
-   */
-  public void resetInputs() throws IOException {
-    bytesCount = 0;
-    docsCount = 0;
-  }
-
-  /**
-   * Sets the {@link Config} for this content source. If you override this
-   * method, you must call super.setConfig.
-   */
-  public void setConfig(Config config) {
-    this.config = config;
-    forever = config.get("content.source.forever", true);
-    logStep = config.get("content.source.log.step", 0);
-    verbose = config.get("content.source.verbose", false);
-    encoding = config.get("content.source.encoding", null);
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DemoHTMLParser.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DemoHTMLParser.java
deleted file mode 100755
index d57777a..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DemoHTMLParser.java
+++ /dev/null
@@ -1,69 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.Reader;
-import java.text.DateFormat;
-import java.text.ParseException;
-import java.util.Date;
-import java.util.Properties;
-
-/**
- * HTML Parser that is based on Lucene's demo HTML parser.
- */
-public class DemoHTMLParser implements org.apache.lucene.benchmark.byTask.feeds.HTMLParser {
-
-  public DocData parse(DocData docData, String name, Date date, Reader reader, DateFormat dateFormat) throws IOException, InterruptedException {
-    org.apache.lucene.demo.html.HTMLParser p = new org.apache.lucene.demo.html.HTMLParser(reader);
-    
-    // title
-    String title = p.getTitle();
-    // properties 
-    Properties props = p.getMetaTags(); 
-    // body
-    Reader r = p.getReader();
-    char c[] = new char[1024];
-    StringBuilder bodyBuf = new StringBuilder();
-    int n;
-    while ((n = r.read(c)) >= 0) {
-      if (n>0) {
-        bodyBuf.append(c,0,n);
-      }
-    }
-    r.close();
-    if (date == null && props.getProperty("date")!=null) {
-      try {
-        date = dateFormat.parse(props.getProperty("date").trim());
-      } catch (ParseException e) {
-        // do not fail test just because a date could not be parsed
-        System.out.println("ignoring date parse exception (assigning 'now') for: "+props.getProperty("date"));
-        date = new Date(); // now 
-      }
-    }
-    
-    docData.clear();
-    docData.setName(name);
-    docData.setBody(bodyBuf.toString());
-    docData.setTitle(title);
-    docData.setProps(props);
-    docData.setDate(date);
-    return docData;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DirContentSource.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DirContentSource.java
deleted file mode 100644
index 66dcac8..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DirContentSource.java
+++ /dev/null
@@ -1,247 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.utils.Config;
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileFilter;
-import java.io.FileReader;
-import java.io.IOException;
-import java.text.DateFormat;
-import java.text.ParsePosition;
-import java.text.SimpleDateFormat;
-import java.util.Arrays;
-import java.util.Date;
-import java.util.Locale;
-import java.util.Stack;
-
-/**
- * A {@link ContentSource} using the Dir collection for its input. Supports
- * the following configuration parameters (on top of {@link ContentSource}):
- * <ul>
- * <li><b>work.dir</b> - specifies the working directory. Required if "docs.dir"
- * denotes a relative path (<b>default=work</b>).
- * <li><b>docs.dir</b> - specifies the directory the Dir collection. Can be set
- * to a relative path if "work.dir" is also specified (<b>default=dir-out</b>).
- * </ul>
- */
-public class DirContentSource extends ContentSource {
-
-  private static final class DateFormatInfo {
-    DateFormat df;
-    ParsePosition pos;
-  }
-  
-  public static class Iterator implements java.util.Iterator<File> {
-
-    static class Comparator implements java.util.Comparator<File> {
-      public int compare(File _a, File _b) {
-        String a = _a.toString();
-        String b = _b.toString();
-        int diff = a.length() - b.length();
-
-        if (diff > 0) {
-          while (diff-- > 0) {
-            b = "0" + b;
-          }
-        } else if (diff < 0) {
-          diff = -diff;
-          while (diff-- > 0) {
-            a = "0" + a;
-          }
-        }
-
-        /* note it's reversed because we're going to push,
-           which reverses again */
-        return b.compareTo(a);
-      }
-    }
-
-    int count = 0;
-
-    Stack<File> stack = new Stack<File>();
-
-    /* this seems silly ... there must be a better way ...
-       not that this is good, but can it matter? */
-
-    Comparator c = new Comparator();
-
-    public Iterator(File f) {
-      push(f);
-    }
-
-    void find() {
-      if (stack.empty()) {
-        return;
-      }
-      if (!(stack.peek()).isDirectory()) {
-        return;
-      }
-      File f = stack.pop();
-      push(f);
-    }
-
-    void push(File f) {
-      push(f.listFiles(new FileFilter() {
-
-        public boolean accept(File file) {
-          return file.isDirectory();
-        }
-      }));
-      push(f.listFiles(new FileFilter() {
-
-        public boolean accept(File file) {
-          return file.getName().endsWith(".txt");
-        }
-      }));
-      find();
-    }
-
-    void push(File[] files) {
-      Arrays.sort(files, c);
-      for(int i = 0; i < files.length; i++) {
-        // System.err.println("push " + files[i]);
-        stack.push(files[i]);
-      }
-    }
-
-    public int getCount(){
-      return count;
-    }
-
-    public boolean hasNext() {
-      return stack.size() > 0;
-    }
-    
-    public File next() {
-      assert hasNext();
-      count++;
-      File object = stack.pop();
-      // System.err.println("pop " + object);
-      find();
-      return object;
-    }
-
-    public void remove() {
-      throw new RuntimeException("cannot");
-    }
-
-  }
-  
-  private ThreadLocal<DateFormatInfo> dateFormat = new ThreadLocal<DateFormatInfo>();
-  private File dataDir = null;
-  private int iteration = 0;
-  private Iterator inputFiles = null;
-
-  // get/initiate a thread-local simple date format (must do so 
-  // because SimpleDateFormat is not thread-safe).
-  private DateFormatInfo getDateFormatInfo() {
-    DateFormatInfo dfi = dateFormat.get();
-    if (dfi == null) {
-      dfi = new DateFormatInfo();
-      dfi.pos = new ParsePosition(0);
-      // date format: 30-MAR-1987 14:22:36.87
-      dfi.df = new SimpleDateFormat("dd-MMM-yyyy kk:mm:ss.SSS", Locale.US);
-      dfi.df.setLenient(true);
-      dateFormat.set(dfi);
-    }
-    return dfi;
-  }
-  
-  private Date parseDate(String dateStr) {
-    DateFormatInfo dfi = getDateFormatInfo();
-    dfi.pos.setIndex(0);
-    dfi.pos.setErrorIndex(-1);
-    return dfi.df.parse(dateStr.trim(), dfi.pos);
-  }
-
-  @Override
-  public void close() throws IOException {
-    inputFiles = null;
-  }
-  
-  @Override
-  public DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException {
-    File f = null;
-    String name = null;
-    synchronized (this) {
-      if (!inputFiles.hasNext()) { 
-        // exhausted files, start a new round, unless forever set to false.
-        if (!forever) {
-          throw new NoMoreDataException();
-        }
-        inputFiles = new Iterator(dataDir);
-        iteration++;
-      }
-      f = inputFiles.next();
-      // System.err.println(f);
-      name = f.getCanonicalPath()+"_"+iteration;
-    }
-    
-    BufferedReader reader = new BufferedReader(new FileReader(f));
-    String line = null;
-    //First line is the date, 3rd is the title, rest is body
-    String dateStr = reader.readLine();
-    reader.readLine();//skip an empty line
-    String title = reader.readLine();
-    reader.readLine();//skip an empty line
-    StringBuilder bodyBuf = new StringBuilder(1024);
-    while ((line = reader.readLine()) != null) {
-      bodyBuf.append(line).append(' ');
-    }
-    reader.close();
-    addBytes(f.length());
-    
-    Date date = parseDate(dateStr);
-    
-    docData.clear();
-    docData.setName(name);
-    docData.setBody(bodyBuf.toString());
-    docData.setTitle(title);
-    docData.setDate(date);
-    return docData;
-  }
-  
-  @Override
-  public synchronized void resetInputs() throws IOException {
-    super.resetInputs();
-    inputFiles = new Iterator(dataDir);
-    iteration = 0;
-  }
-
-  @Override
-  public void setConfig(Config config) {
-    super.setConfig(config);
-    
-    File workDir = new File(config.get("work.dir", "work"));
-    String d = config.get("docs.dir", "dir-out");
-    dataDir = new File(d);
-    if (!dataDir.isAbsolute()) {
-      dataDir = new File(workDir, d);
-    }
-
-    inputFiles = new Iterator(dataDir);
-
-    if (inputFiles == null) {
-      throw new RuntimeException("No txt files in dataDir: " + dataDir.getAbsolutePath());
-    }
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocData.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocData.java
deleted file mode 100755
index 7415211..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocData.java
+++ /dev/null
@@ -1,106 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Date;
-import java.util.Properties;
-
-import org.apache.lucene.document.DateTools;
-
-/** Output of parsing (e.g. HTML parsing) of an input document. */
-public class DocData {
-  
-  private String name;
-  private String body;
-  private String title;
-  private String date;
-  private int id;
-  private Properties props;
-  
-  public void clear() {
-    name = null;
-    body = null;
-    title = null;
-    date = null;
-    props = null;
-    id = -1;
-  }
-  
-  public String getBody() {
-    return body;
-  }
-
-  /**
-   * @return the date. If the ctor with Date was called, then the String
-   *         returned is the output of
-   *         {@link DateTools#dateToString(Date, org.apache.lucene.document.DateTools.Resolution)}
-   *         . Otherwise it's the String passed to the other ctor.
-   */
-  public String getDate() {
-    return date;
-  }
-
-  public String getName() {
-    return name;
-  }
-
-  public int getID() {
-    return id;
-  }
-
-  public Properties getProps() {
-    return props;
-  }
-
-  public String getTitle() {
-    return title;
-  }
-
-  public void setBody(String body) {
-    this.body = body;
-  }
-
-  public void setDate(Date date) {
-    if (date != null) {
-      setDate(DateTools.dateToString(date, DateTools.Resolution.SECOND));
-    } else {
-      this.date = null;
-    }
-  }
-
-  public void setDate(String date) {
-    this.date = date;
-  }
-
-  public void setName(String name) {
-    this.name = name;
-  }
-
-  public void setID(int id) {
-    this.id = id;
-  }
-
-  public void setProps(Properties props) {
-    this.props = props;
-  }
-
-  public void setTitle(String title) {
-    this.title = title;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocMaker.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocMaker.java
deleted file mode 100644
index 142e408..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocMaker.java
+++ /dev/null
@@ -1,502 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.UnsupportedEncodingException;
-import java.util.HashMap;
-import java.util.Calendar;
-import java.util.Map;
-import java.util.Properties;
-import java.util.Locale;
-import java.util.Random;
-import java.util.Date;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.text.SimpleDateFormat;
-import java.text.ParsePosition;
-
-import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.benchmark.byTask.utils.Format;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.NumericField;
-import org.apache.lucene.document.Field.Index;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.Field.TermVector;
-
-/**
- * Creates {@link Document} objects. Uses a {@link ContentSource} to generate
- * {@link DocData} objects. Supports the following parameters:
- * <ul>
- * <li><b>content.source</b> - specifies the {@link ContentSource} class to use
- * (default <b>SingleDocSource</b>).
- * <li><b>doc.stored</b> - specifies whether fields should be stored (default
- * <b>false</b>).
- * <li><b>doc.body.stored</b> - specifies whether the body field should be stored (default
- * = <b>doc.stored</b>).
- * <li><b>doc.tokenized</b> - specifies whether fields should be tokenized
- * (default <b>true</b>).
- * <li><b>doc.body.tokenized</b> - specifies whether the
- * body field should be tokenized (default = <b>doc.tokenized</b>).
- * <li><b>doc.tokenized.norms</b> - specifies whether norms should be stored in
- * the index or not. (default <b>false</b>).
- * <li><b>doc.body.tokenized.norms</b> - specifies whether norms should be
- * stored in the index for the body field. This can be set to true, while
- * <code>doc.tokenized.norms</code> is set to false, to allow norms storing just
- * for the body field. (default <b>true</b>).
- * <li><b>doc.term.vector</b> - specifies whether term vectors should be stored
- * for fields (default <b>false</b>).
- * <li><b>doc.term.vector.positions</b> - specifies whether term vectors should
- * be stored with positions (default <b>false</b>).
- * <li><b>doc.term.vector.offsets</b> - specifies whether term vectors should be
- * stored with offsets (default <b>false</b>).
- * <li><b>doc.store.body.bytes</b> - specifies whether to store the raw bytes of
- * the document's content in the document (default <b>false</b>).
- * <li><b>doc.reuse.fields</b> - specifies whether Field and Document objects
- * should be reused (default <b>true</b>).
- * <li><b>doc.index.props</b> - specifies whether the properties returned by
- * <li><b>doc.random.id.limit</b> - if specified, docs will be assigned random
- * IDs from 0 to this limit.  This is useful with UpdateDoc
- * for testing performance of IndexWriter.updateDocument.
- * {@link DocData#getProps()} will be indexed. (default <b>false</b>).
- * </ul>
- */
-public class DocMaker {
-
-  private static class LeftOver {
-    private DocData docdata;
-    private int cnt;
-  }
-
-  private Random r;
-  private int updateDocIDLimit;
-
-  static class DocState {
-    
-    private final Map<String,Field> fields;
-    private final Map<String,NumericField> numericFields;
-    private final boolean reuseFields;
-    final Document doc;
-    DocData docData = new DocData();
-    
-    public DocState(boolean reuseFields, Store store, Store bodyStore, Index index, Index bodyIndex, TermVector termVector) {
-
-      this.reuseFields = reuseFields;
-      
-      if (reuseFields) {
-        fields =  new HashMap<String,Field>();
-        numericFields = new HashMap<String,NumericField>();
-        
-        // Initialize the map with the default fields.
-        fields.put(BODY_FIELD, new Field(BODY_FIELD, "", bodyStore, bodyIndex, termVector));
-        fields.put(TITLE_FIELD, new Field(TITLE_FIELD, "", store, index, termVector));
-        fields.put(DATE_FIELD, new Field(DATE_FIELD, "", store, index, termVector));
-        fields.put(ID_FIELD, new Field(ID_FIELD, "", Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
-        fields.put(NAME_FIELD, new Field(NAME_FIELD, "", store, index, termVector));
-
-        numericFields.put(DATE_MSEC_FIELD, new NumericField(DATE_MSEC_FIELD));
-        numericFields.put(TIME_SEC_FIELD, new NumericField(TIME_SEC_FIELD));
-        
-        doc = new Document();
-      } else {
-        numericFields = null;
-        fields = null;
-        doc = null;
-      }
-    }
-
-    /**
-     * Returns a field corresponding to the field name. If
-     * <code>reuseFields</code> was set to true, then it attempts to reuse a
-     * Field instance. If such a field does not exist, it creates a new one.
-     */
-    Field getField(String name, Store store, Index index, TermVector termVector) {
-      if (!reuseFields) {
-        return new Field(name, "", store, index, termVector);
-      }
-      
-      Field f = fields.get(name);
-      if (f == null) {
-        f = new Field(name, "", store, index, termVector);
-        fields.put(name, f);
-      }
-      return f;
-    }
-
-    NumericField getNumericField(String name) {
-      if (!reuseFields) {
-        return new NumericField(name);
-      }
-
-      NumericField f = numericFields.get(name);
-      if (f == null) {
-        f = new NumericField(name);
-        numericFields.put(name, f);
-      }
-      return f;
-    }
-  }
-  
-  private boolean storeBytes = false;
-
-  private static class DateUtil {
-    public SimpleDateFormat parser = new SimpleDateFormat("dd-MMM-yyyy HH:mm:ss", Locale.US);
-    public Calendar cal = Calendar.getInstance();
-    public ParsePosition pos = new ParsePosition(0);
-    public DateUtil() {
-      parser.setLenient(true);
-    }
-  }
-
-  // leftovers are thread local, because it is unsafe to share residues between threads
-  private ThreadLocal<LeftOver> leftovr = new ThreadLocal<LeftOver>();
-  private ThreadLocal<DocState> docState = new ThreadLocal<DocState>();
-  private ThreadLocal<DateUtil> dateParsers = new ThreadLocal<DateUtil>();
-
-  public static final String BODY_FIELD = "body";
-  public static final String TITLE_FIELD = "doctitle";
-  public static final String DATE_FIELD = "docdate";
-  public static final String DATE_MSEC_FIELD = "docdatenum";
-  public static final String TIME_SEC_FIELD = "doctimesecnum";
-  public static final String ID_FIELD = "docid";
-  public static final String BYTES_FIELD = "bytes";
-  public static final String NAME_FIELD = "docname";
-
-  protected Config config;
-
-  protected Store storeVal = Store.NO;
-  protected Store bodyStoreVal = Store.NO;
-  protected Index indexVal = Index.ANALYZED_NO_NORMS;
-  protected Index bodyIndexVal = Index.ANALYZED;
-  protected TermVector termVecVal = TermVector.NO;
-  
-  protected ContentSource source;
-  protected boolean reuseFields;
-  protected boolean indexProperties;
-  
-  private int lastPrintedNumUniqueTexts = 0;
-
-  private long lastPrintedNumUniqueBytes = 0;
-  private final AtomicInteger numDocsCreated = new AtomicInteger();
-
-  private int printNum = 0;
-
-  // create a doc
-  // use only part of the body, modify it to keep the rest (or use all if size==0).
-  // reset the docdata properties so they are not added more than once.
-  private Document createDocument(DocData docData, int size, int cnt) throws UnsupportedEncodingException {
-
-    final DocState ds = getDocState();
-    final Document doc = reuseFields ? ds.doc : new Document();
-    doc.getFields().clear();
-    
-    // Set ID_FIELD
-    Field idField = ds.getField(ID_FIELD, storeVal, Index.NOT_ANALYZED_NO_NORMS, termVecVal);
-    int id;
-    if (r != null) {
-      id = r.nextInt(updateDocIDLimit);
-    } else {
-      id = docData.getID();
-      if (id == -1) {
-        id = numDocsCreated.getAndIncrement();
-      }
-    }
-    idField.setValue(Integer.toString(id));
-    doc.add(idField);
-    
-    // Set NAME_FIELD
-    String name = docData.getName();
-    if (name == null) name = "";
-    name = cnt < 0 ? name : name + "_" + cnt;
-    Field nameField = ds.getField(NAME_FIELD, storeVal, indexVal, termVecVal);
-    nameField.setValue(name);
-    doc.add(nameField);
-    
-    // Set DATE_FIELD
-    DateUtil util = dateParsers.get();
-    if (util == null) {
-      util = new DateUtil();
-      dateParsers.set(util);
-    }
-    Date date = null;
-    String dateString = docData.getDate();
-    if (dateString != null) {
-      util.pos.setIndex(0);
-      date = util.parser.parse(dateString, util.pos);
-      //System.out.println(dateString + " parsed to " + date);
-    } else {
-      dateString = "";
-    }
-    Field dateStringField = ds.getField(DATE_FIELD, storeVal, indexVal, termVecVal);
-    dateStringField.setValue(dateString);
-    doc.add(dateStringField);
-
-    if (date == null) {
-      // just set to right now
-      date = new Date();
-    }
-
-    NumericField dateField = ds.getNumericField(DATE_MSEC_FIELD);
-    dateField.setLongValue(date.getTime());
-    doc.add(dateField);
-
-    util.cal.setTime(date);
-    final int sec = util.cal.get(Calendar.HOUR_OF_DAY)*3600 + util.cal.get(Calendar.MINUTE)*60 + util.cal.get(Calendar.SECOND);
-
-    NumericField timeSecField = ds.getNumericField(TIME_SEC_FIELD);
-    timeSecField.setIntValue(sec);
-    doc.add(timeSecField);
-    
-    // Set TITLE_FIELD
-    String title = docData.getTitle();
-    Field titleField = ds.getField(TITLE_FIELD, storeVal, indexVal, termVecVal);
-    titleField.setValue(title == null ? "" : title);
-    doc.add(titleField);
-    
-    String body = docData.getBody();
-    if (body != null && body.length() > 0) {
-      String bdy;
-      if (size <= 0 || size >= body.length()) {
-        bdy = body; // use all
-        docData.setBody(""); // nothing left
-      } else {
-        // attempt not to break words - if whitespace found within next 20 chars...
-        for (int n = size - 1; n < size + 20 && n < body.length(); n++) {
-          if (Character.isWhitespace(body.charAt(n))) {
-            size = n;
-            break;
-          }
-        }
-        bdy = body.substring(0, size); // use part
-        docData.setBody(body.substring(size)); // some left
-      }
-      Field bodyField = ds.getField(BODY_FIELD, bodyStoreVal, bodyIndexVal, termVecVal);
-      bodyField.setValue(bdy);
-      doc.add(bodyField);
-      
-      if (storeBytes) {
-        Field bytesField = ds.getField(BYTES_FIELD, Store.YES, Index.NOT_ANALYZED_NO_NORMS, TermVector.NO);
-        bytesField.setValue(bdy.getBytes("UTF-8"));
-        doc.add(bytesField);
-      }
-    }
-
-    if (indexProperties) {
-      Properties props = docData.getProps();
-      if (props != null) {
-        for (final Map.Entry<Object,Object> entry : props.entrySet()) {
-          Field f = ds.getField((String) entry.getKey(), storeVal, indexVal, termVecVal);
-          f.setValue((String) entry.getValue());
-          doc.add(f);
-        }
-        docData.setProps(null);
-      }
-    }
-    
-    //System.out.println("============== Created doc "+numDocsCreated+" :\n"+doc+"\n==========");
-    return doc;
-  }
-
-  private void resetLeftovers() {
-    leftovr.set(null);
-  }
-
-  protected DocState getDocState() {
-    DocState ds = docState.get();
-    if (ds == null) {
-      ds = new DocState(reuseFields, storeVal, bodyStoreVal, indexVal, bodyIndexVal, termVecVal);
-      docState.set(ds);
-    }
-    return ds;
-  }
-
-  /**
-   * Closes the {@link DocMaker}. The base implementation closes the
-   * {@link ContentSource}, and it can be overridden to do more work (but make
-   * sure to call super.close()).
-   */
-  public void close() throws IOException {
-    source.close();
-  }
-  
-  /**
-   * Returns the number of bytes generated by the content source since last
-   * reset.
-   */
-  public synchronized long getBytesCount() {
-    return source.getBytesCount();
-  }
-
-  /**
-   * Returns the total number of bytes that were generated by the content source
-   * defined to that doc maker.
-   */ 
-  public long getTotalBytesCount() {
-    return source.getTotalBytesCount();
-  }
-
-  /**
-   * Creates a {@link Document} object ready for indexing. This method uses the
-   * {@link ContentSource} to get the next document from the source, and creates
-   * a {@link Document} object from the returned fields. If
-   * <code>reuseFields</code> was set to true, it will reuse {@link Document}
-   * and {@link Field} instances.
-   */
-  public Document makeDocument() throws Exception {
-    resetLeftovers();
-    DocData docData = source.getNextDocData(getDocState().docData);
-    Document doc = createDocument(docData, 0, -1);
-    return doc;
-  }
-
-  /**
-   * Same as {@link #makeDocument()}, only this method creates a document of the
-   * given size input by <code>size</code>.
-   */
-  public Document makeDocument(int size) throws Exception {
-    LeftOver lvr = leftovr.get();
-    if (lvr == null || lvr.docdata == null || lvr.docdata.getBody() == null
-        || lvr.docdata.getBody().length() == 0) {
-      resetLeftovers();
-    }
-    DocData docData = getDocState().docData;
-    DocData dd = (lvr == null ? source.getNextDocData(docData) : lvr.docdata);
-    int cnt = (lvr == null ? 0 : lvr.cnt);
-    while (dd.getBody() == null || dd.getBody().length() < size) {
-      DocData dd2 = dd;
-      dd = source.getNextDocData(new DocData());
-      cnt = 0;
-      dd.setBody(dd2.getBody() + dd.getBody());
-    }
-    Document doc = createDocument(dd, size, cnt);
-    if (dd.getBody() == null || dd.getBody().length() == 0) {
-      resetLeftovers();
-    } else {
-      if (lvr == null) {
-        lvr = new LeftOver();
-        leftovr.set(lvr);
-      }
-      lvr.docdata = dd;
-      lvr.cnt = ++cnt;
-    }
-    return doc;
-  }
-  
-  public void printDocStatistics() {
-    boolean print = false;
-    String col = "                  ";
-    StringBuilder sb = new StringBuilder();
-    String newline = System.getProperty("line.separator");
-    sb.append("------------> ").append(getClass().getSimpleName()).append(" statistics (").append(printNum).append("): ").append(newline);
-    int nut = source.getTotalDocsCount();
-    if (nut > lastPrintedNumUniqueTexts) {
-      print = true;
-      sb.append("total count of unique texts: ").append(Format.format(0,nut,col)).append(newline);
-      lastPrintedNumUniqueTexts = nut;
-    }
-    long nub = getTotalBytesCount();
-    if (nub > lastPrintedNumUniqueBytes) {
-      print = true;
-      sb.append("total bytes of unique texts: ").append(Format.format(0,nub,col)).append(newline);
-      lastPrintedNumUniqueBytes = nub;
-    }
-    if (source.getDocsCount() > 0) {
-      print = true;
-      sb.append("num docs added since last inputs reset:   ").append(Format.format(0,source.getDocsCount(),col)).append(newline);
-      sb.append("total bytes added since last inputs reset: ").append(Format.format(0,getBytesCount(),col)).append(newline);
-    }
-    if (print) {
-      System.out.println(sb.append(newline).toString());
-      printNum++;
-    }
-  }
-  
-  /** Reset inputs so that the test run would behave, input wise, as if it just started. */
-  public synchronized void resetInputs() throws IOException {
-    printDocStatistics();
-    // re-initiate since properties by round may have changed.
-    setConfig(config);
-    source.resetInputs();
-    numDocsCreated.set(0);
-    resetLeftovers();
-  }
-  
-  /** Set the configuration parameters of this doc maker. */
-  public void setConfig(Config config) {
-    this.config = config;
-    try {
-      String sourceClass = config.get("content.source", "org.apache.lucene.benchmark.byTask.feeds.SingleDocSource");
-      source = Class.forName(sourceClass).asSubclass(ContentSource.class).newInstance();
-      source.setConfig(config);
-    } catch (Exception e) {
-      // Should not get here. Throw runtime exception.
-      throw new RuntimeException(e);
-    }
-
-    boolean stored = config.get("doc.stored", false);
-    boolean bodyStored = config.get("doc.body.stored", stored);
-    boolean tokenized = config.get("doc.tokenized", true);
-    boolean bodyTokenized = config.get("doc.body.tokenized", tokenized);
-    boolean norms = config.get("doc.tokenized.norms", false);
-    boolean bodyNorms = config.get("doc.body.tokenized.norms", true);
-    boolean termVec = config.get("doc.term.vector", false);
-    storeVal = (stored ? Field.Store.YES : Field.Store.NO);
-    bodyStoreVal = (bodyStored ? Field.Store.YES : Field.Store.NO);
-    if (tokenized) {
-      indexVal = norms ? Index.ANALYZED : Index.ANALYZED_NO_NORMS;
-    } else {
-      indexVal = norms ? Index.NOT_ANALYZED : Index.NOT_ANALYZED_NO_NORMS;
-    }
-
-    if (bodyTokenized) {
-      bodyIndexVal = bodyNorms ? Index.ANALYZED : Index.ANALYZED_NO_NORMS;
-    } else {
-      bodyIndexVal = bodyNorms ? Index.NOT_ANALYZED : Index.NOT_ANALYZED_NO_NORMS;
-    }
-
-    boolean termVecPositions = config.get("doc.term.vector.positions", false);
-    boolean termVecOffsets = config.get("doc.term.vector.offsets", false);
-    if (termVecPositions && termVecOffsets) {
-      termVecVal = TermVector.WITH_POSITIONS_OFFSETS;
-    } else if (termVecPositions) {
-      termVecVal = TermVector.WITH_POSITIONS;
-    } else if (termVecOffsets) {
-      termVecVal = TermVector.WITH_OFFSETS;
-    } else if (termVec) {
-      termVecVal = TermVector.YES;
-    } else {
-      termVecVal = TermVector.NO;
-    }
-    storeBytes = config.get("doc.store.body.bytes", false);
-    
-    reuseFields = config.get("doc.reuse.fields", true);
-
-    // In a multi-rounds run, it is important to reset DocState since settings
-    // of fields may change between rounds, and this is the only way to reset
-    // the cache of all threads.
-    docState = new ThreadLocal<DocState>();
-    
-    indexProperties = config.get("doc.index.props", false);
-
-    updateDocIDLimit = config.get("doc.random.id.limit", -1);
-    if (updateDocIDLimit != -1) {
-      r = new Random(179);
-    }
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiContentSource.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiContentSource.java
deleted file mode 100644
index 5c71c5a..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiContentSource.java
+++ /dev/null
@@ -1,307 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.util.ThreadInterruptedException;
-import org.xml.sax.Attributes;
-import org.xml.sax.InputSource;
-import org.xml.sax.SAXException;
-import org.xml.sax.XMLReader;
-import org.xml.sax.helpers.DefaultHandler;
-import org.xml.sax.helpers.XMLReaderFactory;
-
-/**
- * A {@link ContentSource} which reads the English Wikipedia dump. You can read
- * the .bz2 file directly (it will be decompressed on the fly). Config
- * properties:
- * <ul>
- * <li>keep.image.only.docs=false|true (default <b>true</b>).
- * <li>docs.file=&lt;path to the file&gt;
- * </ul>
- */
-public class EnwikiContentSource extends ContentSource {
-
-  private class Parser extends DefaultHandler implements Runnable {
-    private Thread t;
-    private boolean threadDone;
-    private String[] tuple;
-    private NoMoreDataException nmde;
-    private StringBuilder contents = new StringBuilder();
-    private String title;
-    private String body;
-    private String time;
-    private String id;
-    
-    String[] next() throws NoMoreDataException {
-      if (t == null) {
-        threadDone = false;
-        t = new Thread(this);
-        t.setDaemon(true);
-        t.start();
-      }
-      String[] result;
-      synchronized(this){
-        while(tuple == null && nmde == null && !threadDone) {
-          try {
-            wait();
-          } catch (InterruptedException ie) {
-            throw new ThreadInterruptedException(ie);
-          }
-        }
-        if (nmde != null) {
-          // Set to null so we will re-start thread in case
-          // we are re-used:
-          t = null;
-          throw nmde;
-        }
-        if (t != null && threadDone) {
-          // The thread has exited yet did not hit end of
-          // data, so this means it hit an exception.  We
-          // throw NoMorDataException here to force
-          // benchmark to stop the current alg:
-          throw new NoMoreDataException();
-        }
-        result = tuple;
-        tuple = null;
-        notify();
-      }
-      return result;
-    }
-    
-    String time(String original) {
-      StringBuilder buffer = new StringBuilder();
-
-      buffer.append(original.substring(8, 10));
-      buffer.append('-');
-      buffer.append(months[Integer.valueOf(original.substring(5, 7)).intValue() - 1]);
-      buffer.append('-');
-      buffer.append(original.substring(0, 4));
-      buffer.append(' ');
-      buffer.append(original.substring(11, 19));
-      buffer.append(".000");
-
-      return buffer.toString();
-    }
-    
-    @Override
-    public void characters(char[] ch, int start, int length) {
-      contents.append(ch, start, length);
-    }
-
-    @Override
-    public void endElement(String namespace, String simple, String qualified)
-      throws SAXException {
-      int elemType = getElementType(qualified);
-      switch (elemType) {
-        case PAGE:
-          // the body must be null and we either are keeping image docs or the
-          // title does not start with Image:
-          if (body != null && (keepImages || !title.startsWith("Image:"))) {
-            String[] tmpTuple = new String[LENGTH];
-            tmpTuple[TITLE] = title.replace('\t', ' ');
-            tmpTuple[DATE] = time.replace('\t', ' ');
-            tmpTuple[BODY] = body.replaceAll("[\t\n]", " ");
-            tmpTuple[ID] = id;
-            synchronized(this) {
-              while (tuple != null) {
-                try {
-                  wait();
-                } catch (InterruptedException ie) {
-                  throw new ThreadInterruptedException(ie);
-                }
-              }
-              tuple = tmpTuple;
-              notify();
-            }
-          }
-          break;
-        case BODY:
-          body = contents.toString();
-          //workaround that startswith doesn't have an ignore case option, get at least 20 chars.
-          String startsWith = body.substring(0, Math.min(10, contents.length())).toLowerCase();
-          if (startsWith.startsWith("#redirect")) {
-            body = null;
-          }
-          break;
-        case DATE:
-          time = time(contents.toString());
-          break;
-        case TITLE:
-          title = contents.toString();
-          break;
-        case ID:
-          //the doc id is the first one in the page.  All other ids after that one can be ignored according to the schema
-          if (id == null) {
-            id = contents.toString();
-          }
-          break;
-        default:
-          // this element should be discarded.
-      }
-    }
-
-    public void run() {
-
-      try {
-        XMLReader reader = XMLReaderFactory.createXMLReader();
-        reader.setContentHandler(this);
-        reader.setErrorHandler(this);
-        while(true){
-          final InputStream localFileIS = is;
-          try {
-            reader.parse(new InputSource(localFileIS));
-          } catch (IOException ioe) {
-            synchronized(EnwikiContentSource.this) {
-              if (localFileIS != is) {
-                // fileIS was closed on us, so, just fall
-                // through
-              } else
-                // Exception is real
-                throw ioe;
-            }
-          }
-          synchronized(this) {
-            if (!forever) {
-              nmde = new NoMoreDataException();
-              notify();
-              return;
-            } else if (localFileIS == is) {
-              // If file is not already re-opened then re-open it now
-              is = getInputStream(file);
-            }
-          }
-        }
-      } catch (SAXException sae) {
-        throw new RuntimeException(sae);
-      } catch (IOException ioe) {
-        throw new RuntimeException(ioe);
-      } finally {
-        synchronized(this) {
-          threadDone = true;
-          notify();
-        }
-      }
-    }
-
-    @Override
-    public void startElement(String namespace, String simple, String qualified,
-                             Attributes attributes) {
-      int elemType = getElementType(qualified);
-      switch (elemType) {
-        case PAGE:
-          title = null;
-          body = null;
-          time = null;
-          id = null;
-          break;
-        // intentional fall-through.
-        case BODY:
-        case DATE:
-        case TITLE:
-        case ID:
-          contents.setLength(0);
-          break;
-        default:
-          // this element should be discarded.
-      }
-    }
-  }
-
-  private static final Map<String,Integer> ELEMENTS = new HashMap<String,Integer>();
-  private static final int TITLE = 0;
-  private static final int DATE = TITLE + 1;
-  private static final int BODY = DATE + 1;
-  private static final int ID = BODY + 1;
-  private static final int LENGTH = ID + 1;
-  // LENGTH is used as the size of the tuple, so whatever constants we need that
-  // should not be part of the tuple, we should define them after LENGTH.
-  private static final int PAGE = LENGTH + 1;
-
-  private static final String[] months = {"JAN", "FEB", "MAR", "APR",
-                                  "MAY", "JUN", "JUL", "AUG",
-                                  "SEP", "OCT", "NOV", "DEC"};
-
-  static {
-    ELEMENTS.put("page", Integer.valueOf(PAGE));
-    ELEMENTS.put("text", Integer.valueOf(BODY));
-    ELEMENTS.put("timestamp", Integer.valueOf(DATE));
-    ELEMENTS.put("title", Integer.valueOf(TITLE));
-    ELEMENTS.put("id", Integer.valueOf(ID));
-  }
-  
-  /**
-   * Returns the type of the element if defined, otherwise returns -1. This
-   * method is useful in startElement and endElement, by not needing to compare
-   * the element qualified name over and over.
-   */
-  private final static int getElementType(String elem) {
-    Integer val = ELEMENTS.get(elem);
-    return val == null ? -1 : val.intValue();
-  }
-  
-  private File file;
-  private boolean keepImages = true;
-  private InputStream is;
-  private Parser parser = new Parser();
-  
-  @Override
-  public void close() throws IOException {
-    synchronized (EnwikiContentSource.this) {
-      if (is != null) {
-        is.close();
-        is = null;
-      }
-    }
-  }
-  
-  @Override
-  public synchronized DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException {
-    String[] tuple = parser.next();
-    docData.clear();
-    docData.setName(tuple[ID]);
-    docData.setBody(tuple[BODY]);
-    docData.setDate(tuple[DATE]);
-    docData.setTitle(tuple[TITLE]);
-    return docData;
-  }
-
-  @Override
-  public void resetInputs() throws IOException {
-    super.resetInputs();
-    is = getInputStream(file);
-  }
-  
-  @Override
-  public void setConfig(Config config) {
-    super.setConfig(config);
-    keepImages = config.get("keep.image.only.docs", true);
-    String fileName = config.get("docs.file", null);
-    if (fileName == null) {
-      throw new IllegalArgumentException("docs.file must be set");
-    }
-    file = new File(fileName).getAbsoluteFile();
-  }
-  
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiQueryMaker.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiQueryMaker.java
deleted file mode 100644
index 106c6c3..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiQueryMaker.java
+++ /dev/null
@@ -1,137 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.WildcardQuery;
-import org.apache.lucene.search.spans.SpanFirstQuery;
-import org.apache.lucene.search.spans.SpanNearQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.apache.lucene.search.spans.SpanTermQuery;
-import org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask;
-import org.apache.lucene.util.Version;
-
-/**
- * A QueryMaker that uses common and uncommon actual Wikipedia queries for
- * searching the English Wikipedia collection. 90 queries total.
- */
-public class EnwikiQueryMaker extends AbstractQueryMaker implements
-    QueryMaker {
-
-  // common and a few uncommon queries from wikipedia search logs
-  private static String[] STANDARD_QUERIES = { "Images catbox gif",
-      "Imunisasi haram", "Favicon ico", "Michael jackson", "Unknown artist",
-      "Lily Thai", "Neda", "The Last Song", "Metallica", "Nicola Tesla",
-      "Max B", "Skil Corporation", "\"The 100 Greatest Artists of All Time\"",
-      "\"Top 100 Global Universities\"", "Pink floyd", "Bolton Sullivan",
-      "Frank Lucas Jr", "Drake Woods", "Radiohead", "George Freeman",
-      "Oksana Grigorieva", "The Elder Scrolls V", "Deadpool", "Green day",
-      "\"Red hot chili peppers\"", "Jennifer Bini Taylor",
-      "The Paradiso Girls", "Queen", "3Me4Ph", "Paloma Jimenez", "AUDI A4",
-      "Edith Bouvier Beale: A Life In Pictures", "\"Skylar James Deleon\"",
-      "Simple Explanation", "Juxtaposition", "The Woody Show", "London WITHER",
-      "In A Dark Place", "George Freeman", "LuAnn de Lesseps", "Muhammad.",
-      "U2", "List of countries by GDP", "Dean Martin Discography", "Web 3.0",
-      "List of American actors", "The Expendables",
-      "\"100 Greatest Guitarists of All Time\"", "Vince Offer.",
-      "\"List of ZIP Codes in the United States\"", "Blood type diet",
-      "Jennifer Gimenez", "List of hobbies", "The beatles", "Acdc",
-      "Nightwish", "Iron maiden", "Murder Was the Case", "Pelvic hernia",
-      "Naruto Shippuuden", "campaign", "Enthesopathy of hip region",
-      "operating system", "mouse",
-      "List of Xbox 360 games without region encoding", "Shakepearian sonnet",
-      "\"The Monday Night Miracle\"", "India", "Dad's Army",
-      "Solanum melanocerasum", "\"List of PlayStation Portable Wi-Fi games\"",
-      "Little Pixie Geldof", "Planes, Trains & Automobiles", "Freddy Ingalls",
-      "The Return of Chef", "Nehalem", "Turtle", "Calculus", "Superman-Prime",
-      "\"The Losers\"", "pen-pal", "Audio stream input output", "lifehouse",
-      "50 greatest gunners", "Polyfecalia", "freeloader", "The Filthy Youth" };
-
-  private static Query[] getPrebuiltQueries(String field) {
-    WildcardQuery wcq = new WildcardQuery(new Term(field, "fo*"));
-    wcq .setRewriteMethod(MultiTermQuery.CONSTANT_SCORE_FILTER_REWRITE);
-    // be wary of unanalyzed text
-    return new Query[] {
-        new SpanFirstQuery(new SpanTermQuery(new Term(field, "ford")), 5),
-        new SpanNearQuery(new SpanQuery[] {
-            new SpanTermQuery(new Term(field, "night")),
-            new SpanTermQuery(new Term(field, "trading")) }, 4, false),
-        new SpanNearQuery(new SpanQuery[] {
-            new SpanFirstQuery(new SpanTermQuery(new Term(field, "ford")), 10),
-            new SpanTermQuery(new Term(field, "credit")) }, 10, false), wcq, };
-  }
-
-  /**
-   * Parse the strings containing Lucene queries.
-   * 
-   * @param qs array of strings containing query expressions
-   * @param a analyzer to use when parsing queries
-   * @return array of Lucene queries
-   */
-  private static Query[] createQueries(List<Object> qs, Analyzer a) {
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, DocMaker.BODY_FIELD, a);
-    List<Object> queries = new ArrayList<Object>();
-    for (int i = 0; i < qs.size(); i++) {
-      try {
-
-        Object query = qs.get(i);
-        Query q = null;
-        if (query instanceof String) {
-          q = qp.parse((String) query);
-
-        } else if (query instanceof Query) {
-          q = (Query) query;
-
-        } else {
-          System.err.println("Unsupported Query Type: " + query);
-        }
-
-        if (q != null) {
-          queries.add(q);
-        }
-
-      } catch (Exception e) {
-        e.printStackTrace();
-      }
-    }
-
-    return queries.toArray(new Query[0]);
-  }
-
-  @Override
-  protected Query[] prepareQueries() throws Exception {
-    // analyzer (default is standard analyzer)
-    Analyzer anlzr = NewAnalyzerTask.createAnalyzer(config.get("analyzer", StandardAnalyzer.class.getName()));
-
-    List<Object> queryList = new ArrayList<Object>(20);
-    queryList.addAll(Arrays.asList(STANDARD_QUERIES));
-    if(!config.get("enwikiQueryMaker.disableSpanQueries", false))
-      queryList.addAll(Arrays.asList(getPrebuiltQueries(DocMaker.BODY_FIELD)));
-    return createQueries(queryList, anlzr);
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java
deleted file mode 100644
index dbfc731..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java
+++ /dev/null
@@ -1,97 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask;
-import org.apache.lucene.util.Version;
-
-import java.io.*;
-import java.util.ArrayList;
-import java.util.List;
-
-/**
- * Copyright 2004 The Apache Software Foundation
- * <p/>
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Create queries from a FileReader.  One per line, pass them through the
- * QueryParser.  Lines beginning with # are treated as comments
- *
- * File can be specified as a absolute, relative or resource.
- * Two properties can be set:
- * file.query.maker.file=&lt;Full path to file containing queries&gt;
- * <br/>
- * file.query.maker.default.field=&lt;Name of default field - Default value is "body"&gt;
- *
- * Example:
- * file.query.maker.file=c:/myqueries.txt
- * file.query.maker.default.field=body
- */
-public class FileBasedQueryMaker extends AbstractQueryMaker implements QueryMaker{
-
-
-  @Override
-  protected Query[] prepareQueries() throws Exception {
-
-    Analyzer anlzr = NewAnalyzerTask.createAnalyzer(config.get("analyzer",
-            "org.apache.lucene.analysis.standard.StandardAnalyzer"));
-    String defaultField = config.get("file.query.maker.default.field", DocMaker.BODY_FIELD);
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, defaultField, anlzr);
-    qp.setAllowLeadingWildcard(true);
-
-    List<Query> qq = new ArrayList<Query>();
-    String fileName = config.get("file.query.maker.file", null);
-    if (fileName != null)
-    {
-      File file = new File(fileName);
-      Reader reader = null;
-      if (file.exists()) {
-        reader = new FileReader(file);
-      } else {
-        //see if we can find it as a resource
-        InputStream asStream = FileBasedQueryMaker.class.getClassLoader().getResourceAsStream(fileName);
-        if (asStream != null) {
-          reader = new InputStreamReader(asStream);
-        }
-      }
-      if (reader != null) {
-        try {
-          BufferedReader buffered = new BufferedReader(reader);
-          String line = null;
-          int lineNum = 0;
-          while ((line = buffered.readLine()) != null) {
-            line = line.trim();
-            if (line.length() != 0 && !line.startsWith("#")) {
-              try {
-                qq.add(qp.parse(line));
-              } catch (ParseException e) {
-                System.err.println("Exception: " + e.getMessage() + " occurred while parsing line: " + lineNum + " Text: " + line);
-              }
-            }
-            lineNum++;
-          }
-        } finally {
-          reader.close();
-        }
-      } else {
-        System.err.println("No Reader available for: " + fileName);
-      }
-      
-    }
-    return qq.toArray(new Query[qq.size()]) ;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/HTMLParser.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/HTMLParser.java
deleted file mode 100755
index 6c8b9fa..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/HTMLParser.java
+++ /dev/null
@@ -1,44 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.Reader;
-import java.text.DateFormat;
-import java.util.Date;
-
-/**
- * HTML Parsing Interface for test purposes
- */
-public interface HTMLParser {
-
-  /**
-   * Parse the input Reader and return DocData. 
-   * A provided name or date is used for the result, otherwise an attempt is 
-   * made to set them from the parsed data.
-   * @param dateFormat date formatter to use for extracting the date.   
-   * @param name name of the result doc data. If null, attempt to set by parsed data.
-   * @param date date of the result doc data. If null, attempt to set by parsed data.
-   * @param reader of html text to parse.
-   * @return Parsed doc data.
-   * @throws IOException
-   * @throws InterruptedException
-   */
-  public DocData parse(DocData docData, String name, Date date, Reader reader, DateFormat dateFormat) throws IOException, InterruptedException;
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LineDocSource.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LineDocSource.java
deleted file mode 100644
index 9ab6527..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LineDocSource.java
+++ /dev/null
@@ -1,129 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-
-import org.apache.lucene.benchmark.byTask.tasks.WriteLineDocTask;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-
-/**
- * A {@link ContentSource} reading one line at a time as a
- * {@link org.apache.lucene.document.Document} from a single file. This saves IO
- * cost (over DirContentSource) of recursing through a directory and opening a
- * new file for every document.<br>
- * The expected format of each line is (arguments are separated by &lt;TAB&gt;):
- * <i>title, date, body</i>. If a line is read in a different format, a
- * {@link RuntimeException} will be thrown. In general, you should use this
- * content source for files that were created with {@link WriteLineDocTask}.<br>
- * <br>
- * Config properties:
- * <ul>
- * <li>docs.file=&lt;path to the file&gt;
- * <li>content.source.encoding - default to UTF-8.
- * </ul>
- */
-public class LineDocSource extends ContentSource {
-
-  private final static char SEP = WriteLineDocTask.SEP;
-
-  private File file;
-  private BufferedReader reader;
-  private int readCount;
-
-  private synchronized void openFile() {
-    try {
-      if (reader != null) {
-        reader.close();
-      }
-      InputStream is = getInputStream(file);
-      reader = new BufferedReader(new InputStreamReader(is, encoding), BUFFER_SIZE);
-    } catch (IOException e) {
-      throw new RuntimeException(e);
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (reader != null) {
-      reader.close();
-      reader = null;
-    }
-  }
-  
-  @Override
-  public DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException {
-    final String line;
-    final int myID;
-    
-    synchronized(this) {
-      line = reader.readLine();
-      myID = readCount++;
-      if (line == null) {
-        if (!forever) {
-          throw new NoMoreDataException();
-        }
-        // Reset the file
-        openFile();
-        return getNextDocData(docData);
-      }
-    }
-    
-    // A line must be in the following format. If it's not, fail !
-    // title <TAB> date <TAB> body <NEWLINE>
-    int spot = line.indexOf(SEP);
-    if (spot == -1) {
-      throw new RuntimeException("line: [" + line + "] is in an invalid format !");
-    }
-    int spot2 = line.indexOf(SEP, 1 + spot);
-    if (spot2 == -1) {
-      throw new RuntimeException("line: [" + line + "] is in an invalid format !");
-    }
-    // The date String was written in the format of DateTools.dateToString.
-    docData.clear();
-    docData.setID(myID);
-    docData.setBody(line.substring(1 + spot2, line.length()));
-    docData.setTitle(line.substring(0, spot));
-    docData.setDate(line.substring(1 + spot, spot2));
-    return docData;
-  }
-
-  @Override
-  public void resetInputs() throws IOException {
-    super.resetInputs();
-    openFile();
-  }
-  
-  @Override
-  public void setConfig(Config config) {
-    super.setConfig(config);
-    String fileName = config.get("docs.file", null);
-    if (fileName == null) {
-      throw new IllegalArgumentException("docs.file must be set");
-    }
-    file = new File(fileName).getAbsoluteFile();
-    if (encoding == null) {
-      encoding = "UTF-8";
-    }
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LongToEnglishContentSource.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LongToEnglishContentSource.java
deleted file mode 100644
index 4d20e91..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LongToEnglishContentSource.java
+++ /dev/null
@@ -1,61 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.English;
-
-import java.io.IOException;
-import java.util.Date;
-
-/**
- * Creates documents whose content is a <code>long</code> number starting from
- * <code>{@link Long#MIN_VALUE} + 10</code>.
- */
-public class LongToEnglishContentSource extends ContentSource{
-  private long counter = 0;
-
-  @Override
-  public void close() throws IOException {
-  }
-  
-  @Override
-  public synchronized DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException {
-    docData.clear();
-    // store the current counter to avoid synchronization later on
-    long curCounter;
-    synchronized (this) {
-      curCounter = counter;
-      if (counter == Long.MAX_VALUE){
-        counter = Long.MIN_VALUE;//loop around
-      } else {
-        ++counter;
-      }
-    }    
-    docData.setBody(English.longToEnglish(curCounter));
-    docData.setName("doc_" + String.valueOf(curCounter));
-    docData.setTitle("title_" + String.valueOf(curCounter));
-    docData.setDate(new Date());
-    return docData;
-  }
-
-  @Override
-  public void resetInputs() throws IOException {
-    counter = Long.MIN_VALUE + 10;
-  }
-  
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LongToEnglishQueryMaker.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LongToEnglishQueryMaker.java
deleted file mode 100644
index 6abe9fc..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LongToEnglishQueryMaker.java
+++ /dev/null
@@ -1,49 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.util.English;
-import org.apache.lucene.util.Version;
-
-
-/**
- *
- *
- **/
-public class LongToEnglishQueryMaker implements QueryMaker {
-  long counter = Long.MIN_VALUE + 10;
-  protected QueryParser parser;
-
-  public Query makeQuery(int size) throws Exception {
-    throw new UnsupportedOperationException();
-  }
-
-  public synchronized Query makeQuery() throws Exception {
-
-    return parser.parse("" + English.longToEnglish(getNextCounter()) + "");
-  }
-
-  private synchronized long getNextCounter() {
-    if (counter == Long.MAX_VALUE){
-      counter = Long.MIN_VALUE + 10;
-    }
-    return counter++;
-  }
-
-  public void setConfig(Config config) throws Exception {
-    Analyzer anlzr = NewAnalyzerTask.createAnalyzer(config.get("analyzer", StandardAnalyzer.class.getName()));
-    parser = new QueryParser(Version.LUCENE_CURRENT, DocMaker.BODY_FIELD, anlzr);
-  }
-
-  public void resetInputs() {
-    counter = Long.MIN_VALUE + 10;
-  }
-
-  public String printQueries() {
-    return "LongToEnglish: [" + Long.MIN_VALUE + " TO " + counter + "]";
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/NoMoreDataException.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/NoMoreDataException.java
deleted file mode 100755
index a4e9ed8..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/NoMoreDataException.java
+++ /dev/null
@@ -1,27 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Exception indicating there is no more data.
- * Thrown by Docs Makers if doc.maker.forever is false and docs sources of that maker where exhausted.
- * This is useful for iterating all document of a source, in case we don't know in advance how many docs there are.
- */
-public class NoMoreDataException extends Exception {
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/QueryMaker.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/QueryMaker.java
deleted file mode 100644
index 4a409c6..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/QueryMaker.java
+++ /dev/null
@@ -1,49 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-
-
-/**
- * Create queries for the test.
- */
-public interface QueryMaker {
-
-  /** 
-   * Create the next query, of the given size.
-   * @param size the size of the query - number of terms, etc.
-   * @exception Exception if cannot make the query, or if size>0 was specified but this feature is not supported.
-   */ 
-  public Query makeQuery (int size) throws Exception;
-
-  /** Create the next query */ 
-  public Query makeQuery () throws Exception;
-
-  /** Set the properties 
-   * @throws Exception */
-  public void setConfig (Config config) throws Exception;
-  
-  /** Reset inputs so that the test run would behave, input wise, as if it just started. */
-  public void resetInputs();
-  
-  /** Print the queries */
-  public String printQueries();
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersContentSource.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersContentSource.java
deleted file mode 100644
index 11265bd..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersContentSource.java
+++ /dev/null
@@ -1,151 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileReader;
-import java.io.IOException;
-import java.text.DateFormat;
-import java.text.ParsePosition;
-import java.text.SimpleDateFormat;
-import java.util.ArrayList;
-import java.util.Date;
-import java.util.Locale;
-
-import org.apache.lucene.benchmark.byTask.utils.Config;
-
-/**
- * A {@link ContentSource} reading from the Reuters collection.
- * <p>
- * Config properties:
- * <ul>
- * <li><b>work.dir</b> - path to the root of docs and indexes dirs (default
- * <b>work</b>).
- * <li><b>docs.dir</b> - path to the docs dir (default <b>reuters-out</b>).
- * </ul>
- */
-public class ReutersContentSource extends ContentSource {
-
-  private static final class DateFormatInfo {
-    DateFormat df;
-    ParsePosition pos;
-  }
-
-  private ThreadLocal<DateFormatInfo> dateFormat = new ThreadLocal<DateFormatInfo>();
-  private File dataDir = null;
-  private ArrayList<File> inputFiles = new ArrayList<File>();
-  private int nextFile = 0;
-  private int iteration = 0;
-  
-  @Override
-  public void setConfig(Config config) {
-    super.setConfig(config);
-    File workDir = new File(config.get("work.dir", "work"));
-    String d = config.get("docs.dir", "reuters-out");
-    dataDir = new File(d);
-    if (!dataDir.isAbsolute()) {
-      dataDir = new File(workDir, d);
-    }
-    inputFiles.clear();
-    collectFiles(dataDir, inputFiles);
-    if (inputFiles.size() == 0) {
-      throw new RuntimeException("No txt files in dataDir: "+dataDir.getAbsolutePath());
-    }
-  }
-
-  private synchronized DateFormatInfo getDateFormatInfo() {
-    DateFormatInfo dfi = dateFormat.get();
-    if (dfi == null) {
-      dfi = new DateFormatInfo();
-      // date format: 30-MAR-1987 14:22:36.87
-      dfi.df = new SimpleDateFormat("dd-MMM-yyyy kk:mm:ss.SSS",Locale.US);
-      dfi.df.setLenient(true);
-      dfi.pos = new ParsePosition(0);
-      dateFormat.set(dfi);
-    }
-    return dfi;
-  }
-  
-  private Date parseDate(String dateStr) {
-    DateFormatInfo dfi = getDateFormatInfo();
-    dfi.pos.setIndex(0);
-    dfi.pos.setErrorIndex(-1);
-    return dfi.df.parse(dateStr.trim(), dfi.pos);
-  }
-
-
-  @Override
-  public void close() throws IOException {
-    // TODO implement?
-  }
-  
-  @Override
-  public DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException {
-    File f = null;
-    String name = null;
-    synchronized (this) {
-      if (nextFile >= inputFiles.size()) {
-        // exhausted files, start a new round, unless forever set to false.
-        if (!forever) {
-          throw new NoMoreDataException();
-        }
-        nextFile = 0;
-        iteration++;
-      }
-      f = inputFiles.get(nextFile++);
-      name = f.getCanonicalPath() + "_" + iteration;
-    }
-
-    BufferedReader reader = new BufferedReader(new FileReader(f));
-    try {
-      // First line is the date, 3rd is the title, rest is body
-      String dateStr = reader.readLine();
-      reader.readLine();// skip an empty line
-      String title = reader.readLine();
-      reader.readLine();// skip an empty line
-      StringBuilder bodyBuf = new StringBuilder(1024);
-      String line = null;
-      while ((line = reader.readLine()) != null) {
-        bodyBuf.append(line).append(' ');
-      }
-      reader.close();
-      
-      addBytes(f.length());
-      
-      Date date = parseDate(dateStr.trim());
-      
-      docData.clear();
-      docData.setName(name);
-      docData.setBody(bodyBuf.toString());
-      docData.setTitle(title);
-      docData.setDate(date);
-      return docData;
-    } finally {
-      reader.close();
-    }
-  }
-
-  @Override
-  public synchronized void resetInputs() throws IOException {
-    super.resetInputs();
-    nextFile = 0;
-    iteration = 0;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersQueryMaker.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersQueryMaker.java
deleted file mode 100644
index c12ed07..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersQueryMaker.java
+++ /dev/null
@@ -1,120 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.WildcardQuery;
-import org.apache.lucene.search.spans.SpanFirstQuery;
-import org.apache.lucene.search.spans.SpanNearQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.apache.lucene.search.spans.SpanTermQuery;
-import org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask;
-import org.apache.lucene.util.Version;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-
-
-/**
- * A QueryMaker that makes queries devised manually (by Grant Ingersoll) for
- * searching in the Reuters collection.
- */
-public class ReutersQueryMaker extends AbstractQueryMaker implements QueryMaker {
-
-  private static String [] STANDARD_QUERIES = {
-    //Start with some short queries
-    "Salomon", "Comex", "night trading", "Japan Sony",
-    //Try some Phrase Queries
-    "\"Sony Japan\"", "\"food needs\"~3",
-    "\"World Bank\"^2 AND Nigeria", "\"World Bank\" -Nigeria",
-    "\"Ford Credit\"~5",
-    //Try some longer queries
-    "airline Europe Canada destination",
-    "Long term pressure by trade " +
-    "ministers is necessary if the current Uruguay round of talks on " +
-    "the General Agreement on Trade and Tariffs (GATT) is to " +
-    "succeed"
-  };
-  
-  private static Query[] getPrebuiltQueries(String field) {
-    //  be wary of unanalyzed text
-    return new Query[] {
-        new SpanFirstQuery(new SpanTermQuery(new Term(field, "ford")), 5),
-        new SpanNearQuery(new SpanQuery[]{new SpanTermQuery(new Term(field, "night")), new SpanTermQuery(new Term(field, "trading"))}, 4, false),
-        new SpanNearQuery(new SpanQuery[]{new SpanFirstQuery(new SpanTermQuery(new Term(field, "ford")), 10), new SpanTermQuery(new Term(field, "credit"))}, 10, false),
-        new WildcardQuery(new Term(field, "fo*")),
-    };
-  }
-  
-  /**
-   * Parse the strings containing Lucene queries.
-   *
-   * @param qs array of strings containing query expressions
-   * @param a  analyzer to use when parsing queries
-   * @return array of Lucene queries
-   */
-  private static Query[] createQueries(List<Object> qs, Analyzer a) {
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, DocMaker.BODY_FIELD, a);
-    List<Object> queries = new ArrayList<Object>();
-    for (int i = 0; i < qs.size(); i++)  {
-      try {
-        
-        Object query = qs.get(i);
-        Query q = null;
-        if (query instanceof String) {
-          q = qp.parse((String) query);
-          
-        } else if (query instanceof Query) {
-          q = (Query) query;
-          
-        } else {
-          System.err.println("Unsupported Query Type: " + query);
-        }
-        
-        if (q != null) {
-          queries.add(q);
-        }
-        
-      } catch (Exception e)  {
-        e.printStackTrace();
-      }
-    }
-    
-    return queries.toArray(new Query[0]);
-  }
-  
-  @Override
-  protected Query[] prepareQueries() throws Exception {
-    // analyzer (default is standard analyzer)
-    Analyzer anlzr= NewAnalyzerTask.createAnalyzer(config.get("analyzer",
-    "org.apache.lucene.analysis.standard.StandardAnalyzer")); 
-    
-    List<Object> queryList = new ArrayList<Object>(20);
-    queryList.addAll(Arrays.asList(STANDARD_QUERIES));
-    queryList.addAll(Arrays.asList(getPrebuiltQueries(DocMaker.BODY_FIELD)));
-    return createQueries(queryList, anlzr);
-  }
-
-
-  
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleQueryMaker.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleQueryMaker.java
deleted file mode 100644
index c550f33..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleQueryMaker.java
+++ /dev/null
@@ -1,71 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask;
-import org.apache.lucene.util.Version;
-
-import java.util.ArrayList;
-
-/**
- * A QueryMaker that makes queries for a collection created 
- * using {@link org.apache.lucene.benchmark.byTask.feeds.SingleDocSource}.
- */
-public class SimpleQueryMaker extends AbstractQueryMaker implements QueryMaker {
-
-
-  /**
-   * Prepare the queries for this test.
-   * Extending classes can override this method for preparing different queries. 
-   * @return prepared queries.
-   * @throws Exception if cannot prepare the queries.
-   */
-  @Override
-  protected Query[] prepareQueries() throws Exception {
-    // analyzer (default is standard analyzer)
-    Analyzer anlzr= NewAnalyzerTask.createAnalyzer(config.get("analyzer",
-        "org.apache.lucene.analysis.standard.StandardAnalyzer")); 
-    
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, DocMaker.BODY_FIELD,anlzr);
-    ArrayList<Query> qq = new ArrayList<Query>();
-    Query q1 = new TermQuery(new Term(DocMaker.ID_FIELD,"doc2"));
-    qq.add(q1);
-    Query q2 = new TermQuery(new Term(DocMaker.BODY_FIELD,"simple"));
-    qq.add(q2);
-    BooleanQuery bq = new BooleanQuery();
-    bq.add(q1,Occur.MUST);
-    bq.add(q2,Occur.MUST);
-    qq.add(bq);
-    qq.add(qp.parse("synthetic body"));
-    qq.add(qp.parse("\"synthetic body\""));
-    qq.add(qp.parse("synthetic text"));
-    qq.add(qp.parse("\"synthetic text\""));
-    qq.add(qp.parse("\"synthetic text\"~3"));
-    qq.add(qp.parse("zoom*"));
-    qq.add(qp.parse("synth*"));
-    return  qq.toArray(new Query[0]);
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker.java
deleted file mode 100644
index 84930b6..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker.java
+++ /dev/null
@@ -1,83 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.StringTokenizer;
-
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.PhraseQuery;
-import org.apache.lucene.search.Query;
-
-/**
- * Create sloppy phrase queries for performance test, in an index created using simple doc maker.
- */
-public class SimpleSloppyPhraseQueryMaker extends SimpleQueryMaker {
-
-  /* (non-Javadoc)
-   * @see org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker#prepareQueries()
-   */
-  @Override
-  protected Query[] prepareQueries() throws Exception {
-    // extract some 100 words from doc text to an array
-    String words[];
-    ArrayList<String> w = new ArrayList<String>();
-    StringTokenizer st = new StringTokenizer(SingleDocSource.DOC_TEXT);
-    while (st.hasMoreTokens() && w.size()<100) {
-      w.add(st.nextToken());
-    }
-    words = w.toArray(new String[0]);
-
-    // create queries (that would find stuff) with varying slops
-    ArrayList<Query> queries = new ArrayList<Query>(); 
-    for (int slop=0; slop<8; slop++) {
-      for (int qlen=2; qlen<6; qlen++) {
-        for (int wd=0; wd<words.length-qlen-slop; wd++) {
-          // ordered
-          int remainedSlop = slop;
-          PhraseQuery q = new PhraseQuery();
-          q.setSlop(slop);
-          int wind = wd;
-          for (int i=0; i<qlen; i++) {
-            q.add(new Term(DocMaker.BODY_FIELD,words[wind++]));
-            if (remainedSlop>0) {
-              remainedSlop--;
-              wind++;
-            }
-          }
-          queries.add(q);
-          // reversed
-          remainedSlop = slop;
-          q = new PhraseQuery();
-          q.setSlop(slop+2*qlen);
-          wind = wd+qlen+remainedSlop-1;
-          for (int i=0; i<qlen; i++) {
-            q.add(new Term(DocMaker.BODY_FIELD,words[wind--]));
-            if (remainedSlop>0) {
-              remainedSlop--;
-              wind--;
-            }
-          }
-          queries.add(q);
-        }
-      }
-    }
-    return queries.toArray(new Query[0]);
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SingleDocSource.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SingleDocSource.java
deleted file mode 100644
index 547b17f..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SingleDocSource.java
+++ /dev/null
@@ -1,72 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-import java.io.IOException;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Creates the same document each time {@link #getNextDocData(DocData)} is called.
- */
-public class SingleDocSource extends ContentSource {
-  
-  private int docID = 0;
-
-  static final String DOC_TEXT =  
-    "Well, this is just some plain text we use for creating the " +
-    "test documents. It used to be a text from an online collection " +
-    "devoted to first aid, but if there was there an (online) lawyers " +
-    "first aid collection with legal advices, \"it\" might have quite " +
-    "probably advised one not to include \"it\"'s text or the text of " +
-    "any other online collection in one's code, unless one has money " +
-    "that one don't need and one is happy to donate for lawyers " +
-    "charity. Anyhow at some point, rechecking the usage of this text, " +
-    "it became uncertain that this text is free to use, because " +
-    "the web site in the disclaimer of he eBook containing that text " +
-    "was not responding anymore, and at the same time, in projGut, " +
-    "searching for first aid no longer found that eBook as well. " +
-    "So here we are, with a perhaps much less interesting " +
-    "text for the test, but oh much much safer. ";
-  
-  // return a new docid
-  private synchronized int newdocid() throws NoMoreDataException {
-    if (docID > 0 && !forever) {
-      throw new NoMoreDataException();
-    }
-    return docID++;
-  }
-
-  @Override
-  public void close() throws IOException {}
-  
-  @Override
-  public DocData getNextDocData(DocData docData) throws NoMoreDataException {
-    int id = newdocid();
-    addBytes(DOC_TEXT.length());
-    docData.clear();
-    docData.setName("doc" + id);
-    docData.setBody(DOC_TEXT);
-    return docData;
-  }
-
-  @Override
-  public synchronized void resetInputs() throws IOException {
-    super.resetInputs();
-    docID = 0;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SortableSingleDocSource.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SortableSingleDocSource.java
deleted file mode 100644
index a7da954..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SortableSingleDocSource.java
+++ /dev/null
@@ -1,114 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Properties;
-import java.util.Random;
-
-import org.apache.lucene.benchmark.byTask.utils.Config;
-
-/**
- * Adds fields appropriate for sorting: country, random_string and sort_field
- * (int). Supports the following parameters:
- * <ul>
- * <li><b>sort.rng</b> - defines the range for sort-by-int field (default
- * <b>20000</b>).
- * <li><b>rand.seed</b> - defines the seed to initialize Random with (default
- * <b>13</b>).
- * </ul>
- */
-public class SortableSingleDocSource extends SingleDocSource {
-  
-  private static String[] COUNTRIES = new String[] {
-    "European Union", "United States", "Japan", "Germany", "China (PRC)", 
-    "United Kingdom", "France", "Italy", "Spain", "Canada", "Brazil", "Russia",
-    "India", "South Korea", "Australia", "Mexico", "Netherlands", "Turkey", 
-    "Sweden", "Belgium", "Indonesia", "Switzerland", "Poland", "Norway", 
-    "Republic of China", "Saudi Arabia", "Austria", "Greece", "Denmark", "Iran", 
-    "South Africa", "Argentina", "Ireland", "Thailand", "Finland", "Venezuela", 
-    "Portugal", "Hong Kong", "United Arab Emirates", "Malaysia", 
-    "Czech Republic", "Colombia", "Nigeria", "Romania", "Chile", "Israel", 
-    "Singapore", "Philippines", "Pakistan", "Ukraine", "Hungary", "Algeria", 
-    "New Zealand", "Egypt", "Kuwait", "Peru", "Kazakhstan", "Slovakia", 
-    "Morocco", "Bangladesh", "Vietnam", "Qatar", "Angola", "Libya", "Iraq", 
-    "Croatia", "Luxembourg", "Sudan", "Slovenia", "Cuba", "Belarus", "Ecuador", 
-    "Serbia", "Oman", "Bulgaria", "Lithuania", "Syria", "Dominican Republic", 
-    "Tunisia", "Guatemala", "Azerbaijan", "Sri Lanka", "Kenya", "Latvia", 
-    "Turkmenistan", "Costa Rica", "Lebanon", "Uruguay", "Uzbekistan", "Yemen", 
-    "Cyprus", "Estonia", "Trinidad and Tobago", "Cameroon", "El Salvador", 
-    "Iceland", "Panama", "Bahrain", "Ivory Coast", "Ethiopia", "Tanzania", 
-    "Jordan", "Ghana", "Bosnia and Herzegovina", "Macau", "Burma", "Bolivia", 
-    "Brunei", "Botswana", "Honduras", "Gabon", "Uganda", "Jamaica", "Zambia", 
-    "Senegal", "Paraguay", "Albania", "Equatorial Guinea", "Georgia", 
-    "Democratic Republic of the Congo", "Nepal", "Afghanistan", "Cambodia", 
-    "Armenia", "Republic of the Congo", "Mozambique", "Republic of Macedonia", 
-    "Malta", "Namibia", "Madagascar", "Chad", "Burkina Faso", "Mauritius", 
-    "Mali", "The Bahamas", "Papua New Guinea", "Nicaragua", "Haiti", "Benin", 
-    "alestinian flag West Bank and Gaza", "Jersey", "Fiji", "Guinea", "Moldova", 
-    "Niger", "Laos", "Mongolia", "French Polynesia", "Kyrgyzstan", "Barbados", 
-    "Tajikistan", "Malawi", "Liechtenstein", "New Caledonia", "Kosovo", 
-    "Rwanda", "Montenegro", "Swaziland", "Guam", "Mauritania", "Guernsey", 
-    "Isle of Man", "Togo", "Somalia", "Suriname", "Aruba", "North Korea", 
-    "Zimbabwe", "Central African Republic", "Faroe Islands", "Greenland", 
-    "Sierra Leone", "Lesotho", "Cape Verde", "Eritrea", "Bhutan", "Belize", 
-    "Antigua and Barbuda", "Gibraltar", "Maldives", "San Marino", "Guyana", 
-    "Burundi", "Saint Lucia", "Djibouti", "British Virgin Islands", "Liberia", 
-    "Seychelles", "The Gambia", "Northern Mariana Islands", "Grenada", 
-    "Saint Vincent and the Grenadines", "Saint Kitts and Nevis", "East Timor", 
-    "Vanuatu", "Comoros", "Samoa", "Solomon Islands", "Guinea-Bissau", 
-    "American Samoa", "Dominica", "Micronesia", "Tonga", "Cook Islands", 
-    "Palau", "Marshall Islands", "S? Tom? and Pr?cipe", "Anguilla", 
-    "Kiribati", "Tuvalu", "Niue" };
-
-  private int sortRange;
-  private Random r;
-
-  @Override
-  public DocData getNextDocData(DocData docData) throws NoMoreDataException {
-    docData = super.getNextDocData(docData);
-    Properties props = new Properties();
-
-    // random int
-    props.put("sort_field", Integer.toString(r.nextInt(sortRange)));
-
-    // random string
-    int len = nextInt(2, 20);
-    char[] buffer = new char[len];
-    for (int i = 0; i < len; i++) {
-      buffer[i] = (char) r.nextInt(0x80); 
-    }
-    props.put("random_string", new String(buffer));
-
-    // random country
-    props.put("country", COUNTRIES[r.nextInt(COUNTRIES.length)]);
-    docData.setProps(props);
-    return docData;
-  }
-
-  private int nextInt(int start, int end) {
-    return start + r.nextInt(end - start);
-  }
-
-  @Override
-  public void setConfig(Config config) {
-    super.setConfig(config);
-    sortRange = config.get("sort.rng", 20000);
-    r = new Random(config.get("rand.seed", 13));
-  }
-  
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/TrecContentSource.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/TrecContentSource.java
deleted file mode 100644
index 1101e66..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/TrecContentSource.java
+++ /dev/null
@@ -1,349 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.IOException;
-import java.io.InputStreamReader;
-import java.io.Reader;
-import java.text.DateFormat;
-import java.text.ParsePosition;
-import java.text.SimpleDateFormat;
-import java.util.ArrayList;
-import java.util.Date;
-import java.util.Locale;
-import java.util.zip.GZIPInputStream;
-
-import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.benchmark.byTask.utils.StringBuilderReader;
-import org.apache.lucene.util.ThreadInterruptedException;
-
-/**
- * Implements a {@link ContentSource} over the TREC collection.
- * <p>
- * Supports the following configuration parameters (on top of
- * {@link ContentSource}):
- * <ul>
- * <li><b>work.dir</b> - specifies the working directory. Required if "docs.dir"
- * denotes a relative path (<b>default=work</b>).
- * <li><b>docs.dir</b> - specifies the directory where the TREC files reside.
- * Can be set to a relative path if "work.dir" is also specified
- * (<b>default=trec</b>).
- * <li><b>html.parser</b> - specifies the {@link HTMLParser} class to use for
- * parsing the TREC documents content (<b>default=DemoHTMLParser</b>).
- * <li><b>content.source.encoding</b> - if not specified, ISO-8859-1 is used.
- * <li><b>content.source.excludeIteration</b> - if true, do not append iteration number to docname
- * </ul>
- */
-public class TrecContentSource extends ContentSource {
-
-  private static final class DateFormatInfo {
-    DateFormat[] dfs;
-    ParsePosition pos;
-  }
-
-  private static final String DATE = "Date: ";
-  private static final String DOCHDR = "<DOCHDR>";
-  private static final String TERMINATING_DOCHDR = "</DOCHDR>";
-  private static final String DOCNO = "<DOCNO>";
-  private static final String TERMINATING_DOCNO = "</DOCNO>";
-  private static final String DOC = "<DOC>";
-  private static final String TERMINATING_DOC = "</DOC>";
-
-  private static final String NEW_LINE = System.getProperty("line.separator");
-
-  private static final String DATE_FORMATS [] = {
-       "EEE, dd MMM yyyy kk:mm:ss z",	  // Tue, 09 Dec 2003 22:39:08 GMT
-       "EEE MMM dd kk:mm:ss yyyy z",  	// Tue Dec 09 16:45:08 2003 EST
-       "EEE, dd-MMM-':'y kk:mm:ss z", 	// Tue, 09 Dec 2003 22:39:08 GMT
-       "EEE, dd-MMM-yyy kk:mm:ss z", 	  // Tue, 09 Dec 2003 22:39:08 GMT
-       "EEE MMM dd kk:mm:ss yyyy",  	  // Tue Dec 09 16:45:08 2003
-  };
-
-  private ThreadLocal<DateFormatInfo> dateFormats = new ThreadLocal<DateFormatInfo>();
-  private ThreadLocal<StringBuilderReader> trecDocReader = new ThreadLocal<StringBuilderReader>();
-  private ThreadLocal<StringBuilder> trecDocBuffer = new ThreadLocal<StringBuilder>();
-  private File dataDir = null;
-  private ArrayList<File> inputFiles = new ArrayList<File>();
-  private int nextFile = 0;
-  private int rawDocSize;
-
-  // Use to synchronize threads on reading from the TREC documents.
-  private Object lock = new Object();
-
-  // Required for test
-  BufferedReader reader;
-  int iteration = 0;
-  HTMLParser htmlParser;
-  private boolean excludeDocnameIteration;
-  
-  private DateFormatInfo getDateFormatInfo() {
-    DateFormatInfo dfi = dateFormats.get();
-    if (dfi == null) {
-      dfi = new DateFormatInfo();
-      dfi.dfs = new SimpleDateFormat[DATE_FORMATS.length];
-      for (int i = 0; i < dfi.dfs.length; i++) {
-        dfi.dfs[i] = new SimpleDateFormat(DATE_FORMATS[i], Locale.US);
-        dfi.dfs[i].setLenient(true);
-      }
-      dfi.pos = new ParsePosition(0);
-      dateFormats.set(dfi);
-    }
-    return dfi;
-  }
-
-  private StringBuilder getDocBuffer() {
-    StringBuilder sb = trecDocBuffer.get();
-    if (sb == null) {
-      sb = new StringBuilder();
-      trecDocBuffer.set(sb);
-    }
-    return sb;
-  }
-  
-  private Reader getTrecDocReader(StringBuilder docBuffer) {
-    StringBuilderReader r = trecDocReader.get();
-    if (r == null) {
-      r = new StringBuilderReader(docBuffer);
-      trecDocReader.set(r);
-    } else {
-      r.set(docBuffer);
-    }
-    return r;
-  }
-
-  // read until finding a line that starts with the specified prefix, or a terminating tag has been found.
-  private void read(StringBuilder buf, String prefix, boolean collectMatchLine,
-                    boolean collectAll, String terminatingTag)
-      throws IOException, NoMoreDataException {
-    String sep = "";
-    while (true) {
-      String line = reader.readLine();
-
-      if (line == null) {
-        openNextFile();
-        continue;
-      }
-
-      rawDocSize += line.length();
-
-      if (line.startsWith(prefix)) {
-        if (collectMatchLine) {
-          buf.append(sep).append(line);
-          sep = NEW_LINE;
-        }
-        break;
-      }
-
-      if (terminatingTag != null && line.startsWith(terminatingTag)) {
-        // didn't find the prefix that was asked, but the terminating
-        // tag was found. set the length to 0 to signal no match was
-        // found.
-        buf.setLength(0);
-        break;
-      }
-
-      if (collectAll) {
-        buf.append(sep).append(line);
-        sep = NEW_LINE;
-      }
-    }
-  }
-  
-  void openNextFile() throws NoMoreDataException, IOException {
-    close();
-    int retries = 0;
-    while (true) {
-      if (nextFile >= inputFiles.size()) { 
-        // exhausted files, start a new round, unless forever set to false.
-        if (!forever) {
-          throw new NoMoreDataException();
-        }
-        nextFile = 0;
-        iteration++;
-      }
-      File f = inputFiles.get(nextFile++);
-      if (verbose) {
-        System.out.println("opening: " + f + " length: " + f.length());
-      }
-      try {
-        GZIPInputStream zis = new GZIPInputStream(new FileInputStream(f), BUFFER_SIZE);
-        reader = new BufferedReader(new InputStreamReader(zis, encoding), BUFFER_SIZE);
-        return;
-      } catch (Exception e) {
-        retries++;
-        if (retries < 20 && verbose) {
-          System.out.println("Skipping 'bad' file " + f.getAbsolutePath() + "  #retries=" + retries);
-          continue;
-        }
-        throw new NoMoreDataException();
-      }
-    }
-  }
-
-  Date parseDate(String dateStr) {
-    dateStr = dateStr.trim();
-    DateFormatInfo dfi = getDateFormatInfo();
-    for (int i = 0; i < dfi.dfs.length; i++) {
-      DateFormat df = dfi.dfs[i];
-      dfi.pos.setIndex(0);
-      dfi.pos.setErrorIndex(-1);
-      Date d = df.parse(dateStr, dfi.pos);
-      if (d != null) {
-        // Parse succeeded.
-        return d;
-      }
-    }
-    // do not fail test just because a date could not be parsed
-    if (verbose) {
-      System.out.println("failed to parse date (assigning 'now') for: " + dateStr);
-    }
-    return null; 
-  }
-  
-  @Override
-  public void close() throws IOException {
-    if (reader == null) {
-      return;
-    }
-
-    try {
-      reader.close();
-    } catch (IOException e) {
-      if (verbose) {
-        System.out.println("failed to close reader !");
-        e.printStackTrace(System.out);
-      }
-    }
-    reader = null;
-  }
-
-  @Override
-  public DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException {
-    String dateStr = null, name = null;
-    Reader r = null;
-    // protect reading from the TREC files by multiple threads. The rest of the
-    // method, i.e., parsing the content and returning the DocData can run
-    // unprotected.
-    synchronized (lock) {
-      if (reader == null) {
-        openNextFile();
-      }
-
-      StringBuilder docBuf = getDocBuffer();
-      
-      // 1. skip until doc start
-      docBuf.setLength(0);
-      read(docBuf, DOC, false, false, null);
-
-      // 2. name
-      docBuf.setLength(0);
-      read(docBuf, DOCNO, true, false, null);
-      name = docBuf.substring(DOCNO.length(), docBuf.indexOf(TERMINATING_DOCNO,
-          DOCNO.length()));
-      if (!excludeDocnameIteration)
-        name = name + "_" + iteration;
-
-      // 3. skip until doc header
-      docBuf.setLength(0);
-      read(docBuf, DOCHDR, false, false, null);
-
-      boolean findTerminatingDocHdr = false;
-
-      // 4. date - look for the date only until /DOCHDR
-      docBuf.setLength(0);
-      read(docBuf, DATE, true, false, TERMINATING_DOCHDR);
-      if (docBuf.length() != 0) {
-        // Date found.
-        dateStr = docBuf.substring(DATE.length());
-        findTerminatingDocHdr = true;
-      }
-
-      // 5. skip until end of doc header
-      if (findTerminatingDocHdr) {
-        docBuf.setLength(0);
-        read(docBuf, TERMINATING_DOCHDR, false, false, null);
-      }
-
-      // 6. collect until end of doc
-      docBuf.setLength(0);
-      read(docBuf, TERMINATING_DOC, false, true, null);
-      
-      // 7. Set up a Reader over the read content
-      r = getTrecDocReader(docBuf);
-      // Resetting the thread's reader means it will reuse the instance
-      // allocated as well as re-read from docBuf.
-      r.reset();
-      
-      // count char length of parsed html text (larger than the plain doc body text).
-      addBytes(docBuf.length()); 
-    }
-
-    // This code segment relies on HtmlParser being thread safe. When we get 
-    // here, everything else is already private to that thread, so we're safe.
-    Date date = dateStr != null ? parseDate(dateStr) : null;
-    try {
-      docData = htmlParser.parse(docData, name, date, r, null);
-      addDoc();
-    } catch (InterruptedException ie) {
-      throw new ThreadInterruptedException(ie);
-    }
-
-    return docData;
-  }
-
-  @Override
-  public void resetInputs() throws IOException {
-    synchronized (lock) {
-      super.resetInputs();
-      close();
-      nextFile = 0;
-      iteration = 0;
-    }
-  }
-
-  @Override
-  public void setConfig(Config config) {
-    super.setConfig(config);
-    File workDir = new File(config.get("work.dir", "work"));
-    String d = config.get("docs.dir", "trec");
-    dataDir = new File(d);
-    if (!dataDir.isAbsolute()) {
-      dataDir = new File(workDir, d);
-    }
-    collectFiles(dataDir, inputFiles);
-    if (inputFiles.size() == 0) {
-      throw new IllegalArgumentException("No files in dataDir: " + dataDir);
-    }
-    try {
-      String parserClassName = config.get("html.parser",
-          "org.apache.lucene.benchmark.byTask.feeds.DemoHTMLParser");
-      htmlParser = Class.forName(parserClassName).asSubclass(HTMLParser.class).newInstance();
-    } catch (Exception e) {
-      // Should not get here. Throw runtime exception.
-      throw new RuntimeException(e);
-    }
-    if (encoding == null) {
-      encoding = "ISO-8859-1";
-    }
-    excludeDocnameIteration = config.get("content.source.excludeIteration", false);
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/package.html b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/package.html
deleted file mode 100644
index 3feb9e3..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/package.html
+++ /dev/null
@@ -1,23 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<body>
-Sources for benchmark inputs: documents and queries.
-</body>
-
-</html>
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/package.html b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/package.html
deleted file mode 100644
index f5440bd..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/package.html
+++ /dev/null
@@ -1,717 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<HTML>
-<HEAD>
-    <TITLE>Benchmarking Lucene By Tasks</TITLE>
-</HEAD>
-<BODY>
-<DIV>
-Benchmarking Lucene By Tasks.
-<p>
-This package provides "task based" performance benchmarking of Lucene.
-One can use the predefined benchmarks, or create new ones.
-</p>
-<p>
-Contained packages:
-</p>
-
-<table border=1 cellpadding=4>
- <tr>
-   <td><b>Package</b></td>
-   <td><b>Description</b></td>
- </tr>
- <tr>
-   <td><a href="stats/package-summary.html">stats</a></td>
-   <td>Statistics maintained when running benchmark tasks.</td>
- </tr>
- <tr>
-   <td><a href="tasks/package-summary.html">tasks</a></td>
-   <td>Benchmark tasks.</td>
- </tr>
- <tr>
-   <td><a href="feeds/package-summary.html">feeds</a></td>
-   <td>Sources for benchmark inputs: documents and queries.</td>
- </tr>
- <tr>
-   <td><a href="utils/package-summary.html">utils</a></td>
-   <td>Utilities used for the benchmark, and for the reports.</td>
- </tr>
- <tr>
-   <td><a href="programmatic/package-summary.html">programmatic</a></td>
-   <td>Sample performance test written programatically.</td>
- </tr>
-</table>
-
-<h2>Table Of Contents</h2>
-<p>
-    <ol>
-        <li><a href="#concept">Benchmarking By Tasks</a></li>
-        <li><a href="#usage">How to use</a></li>
-        <li><a href="#algorithm">Benchmark "algorithm"</a></li>
-        <li><a href="#tasks">Supported tasks/commands</a></li>
-        <li><a href="#properties">Benchmark properties</a></li>
-        <li><a href="#example">Example input algorithm and the result benchmark
-                    report.</a></li>
-        <li><a href="#recsCounting">Results record counting clarified</a></li>
-    </ol>
-</p>
-<a name="concept"></a>
-<h2>Benchmarking By Tasks</h2>
-<p>
-Benchmark Lucene using task primitives.
-</p>
-
-<p>
-A benchmark is composed of some predefined tasks, allowing for creating an
-index, adding documents,
-optimizing, searching, generating reports, and more. A benchmark run takes an
-"algorithm" file
-that contains a description of the sequence of tasks making up the run, and some
-properties defining a few
-additional characteristics of the benchmark run.
-</p>
-
-<a name="usage"></a>
-<h2>How to use</h2>
-<p>
-Easiest way to run a benchmarks is using the predefined ant task:
-<ul>
- <li>ant run-task
-     <br>- would run the <code>micro-standard.alg</code> "algorithm".
- </li>
- <li>ant run-task -Dtask.alg=conf/compound-penalty.alg
-     <br>- would run the <code>compound-penalty.alg</code> "algorithm".
- </li>
- <li>ant run-task -Dtask.alg=[full-path-to-your-alg-file]
-     <br>- would run <code>your perf test</code> "algorithm".
- </li>
- <li>java org.apache.lucene.benchmark.byTask.programmatic.Sample
-     <br>- would run a performance test programmatically - without using an alg
-     file. This is less readable, and less convinient, but possible.
- </li>
-</ul>
-</p>
-
-<p>
-You may find existing tasks sufficient for defining the benchmark <i>you</i>
-need, otherwise, you can extend the framework to meet your needs, as explained
-herein.
-</p>
-
-<p>
-Each benchmark run has a DocMaker and a QueryMaker. These two should usually
-match, so that "meaningful" queries are used for a certain collection.
-Properties set at the header of the alg file define which "makers" should be
-used. You can also specify your own makers, extending DocMaker and implementing
-QureyMaker.
-	<blockquote>
-		<b>Note:</b> since 2.9, DocMaker is a concrete class which accepts a 
-		ContentSource. In most cases, you can use the DocMaker class to create 
-		Documents, while providing your own ContentSource implementation. For 
-		example, the current Benchmark package includes ContentSource 
-		implementations for TREC, Enwiki and Reuters collections, as well as 
-		others like LineDocSource which reads a 'line' file produced by 
-		WriteLineDocTask.
-	</blockquote>
-</p>
-
-<p>
-Benchmark .alg file contains the benchmark "algorithm". The syntax is described
-below. Within the algorithm, you can specify groups of commands, assign them
-names, specify commands that should be repeated,
-do commands in serial or in parallel,
-and also control the speed of "firing" the commands.
-</p>
-
-<p>
-This allows, for instance, to specify
-that an index should be opened for update,
-documents should be added to it one by one but not faster than 20 docs a minute,
-and, in parallel with this,
-some N queries should be searched against that index,
-again, no more than 2 queries a second.
-You can have the searches all share an index reader,
-or have them each open its own reader and close it afterwords.
-</p>
-
-<p>
-If the commands available for use in the algorithm do not meet your needs,
-you can add commands by adding a new task under
-org.apache.lucene.benchmark.byTask.tasks -
-you should extend the PerfTask abstract class.
-Make sure that your new task class name is suffixed by Task.
-Assume you added the class "WonderfulTask" - doing so also enables the
-command "Wonderful" to be used in the algorithm.
-</p>
-
-<p>
-<u>External classes</u>: It is sometimes useful to invoke the benchmark
-package with your external alg file that configures the use of your own
-doc/query maker and or html parser. You can work this out without
-modifying the benchmark package code, by passing your class path
-with the benchmark.ext.classpath property:
-<ul>
-  <li>ant run-task -Dtask.alg=[full-path-to-your-alg-file]
-      <font color="#FF0000">-Dbenchmark.ext.classpath=/mydir/classes
-      </font> -Dtask.mem=512M</li>
-</ul>
-</p>
-
-<a name="algorithm"></a>
-<h2>Benchmark "algorithm"</h2>
-
-<p>
-The following is an informal description of the supported syntax.
-</p>
-
-<ol>
- <li>
- <b>Measuring</b>: When a command is executed, statistics for the elapsed
- execution time and memory consumption are collected.
- At any time, those statistics can be printed, using one of the
- available ReportTasks.
- </li>
- <li>
- <b>Comments</b> start with '<font color="#FF0066">#</font>'.
- </li>
- <li>
- <b>Serial</b> sequences are enclosed within '<font color="#FF0066">{ }</font>'.
- </li>
- <li>
- <b>Parallel</b> sequences are enclosed within
- '<font color="#FF0066">[ ]</font>'
- </li>
- <li>
- <b>Sequence naming:</b> To name a sequence, put
- '<font color="#FF0066">"name"</font>' just after
- '<font color="#FF0066">{</font>' or '<font color="#FF0066">[</font>'.
- <br>Example - <font color="#FF0066">{ "ManyAdds" AddDoc } : 1000000</font> -
- would
- name the sequence of 1M add docs "ManyAdds", and this name would later appear
- in statistic reports.
- If you don't specify a name for a sequence, it is given one: you can see it as
- the  algorithm is printed just before benchmark execution starts.
- </li>
- <li>
- <b>Repeating</b>:
- To repeat sequence tasks N times, add '<font color="#FF0066">: N</font>' just
- after the
- sequence closing tag - '<font color="#FF0066">}</font>' or
- '<font color="#FF0066">]</font>' or '<font color="#FF0066">></font>'.
- <br>Example -  <font color="#FF0066">[ AddDoc ] : 4</font>  - would do 4 addDoc
- in parallel, spawning 4 threads at once.
- <br>Example -  <font color="#FF0066">[ AddDoc AddDoc ] : 4</font>  - would do
- 8 addDoc in parallel, spawning 8 threads at once.
- <br>Example -  <font color="#FF0066">{ AddDoc } : 30</font> - would do addDoc
- 30 times in a row.
- <br>Example -  <font color="#FF0066">{ AddDoc AddDoc } : 30</font> - would do
- addDoc 60 times in a row.
- <br><b>Exhaustive repeating</b>: use <font color="#FF0066">*</font> instead of
- a number to repeat exhaustively.
- This is sometimes useful, for adding as many files as a doc maker can create,
- without iterating over the same file again, especially when the exact
- number of documents is not known in advance. For insance, TREC files extracted
- from a zip file. Note: when using this, you must also set
- <font color="#FF0066">doc.maker.forever</font> to false.
- <br>Example -  <font color="#FF0066">{ AddDoc } : *</font>  - would add docs
- until the doc maker is "exhausted".
- </li>
- <li>
- <b>Command parameter</b>: a command can optionally take a single parameter.
- If the certain command does not support a parameter, or if the parameter is of
- the wrong type,
- reading the algorithm will fail with an exception and the test would not start.
- Currently the following tasks take optional parameters:
- <ul>
-   <li><b>AddDoc</b> takes a numeric parameter, indicating the required size of
-       added document. Note: if the DocMaker implementation used in the test
-       does not support makeDoc(size), an exception would be thrown and the test
-       would fail.
-   </li>
-   <li><b>DeleteDoc</b> takes numeric parameter, indicating the docid to be
-       deleted. The latter is not very useful for loops, since the docid is
-       fixed, so for deletion in loops it is better to use the
-       <code>doc.delete.step</code> property.
-   </li>
-   <li><b>SetProp</b> takes a <code>name,value<code> mandatory param,
-       ',' used as a separator.
-   </li>
-   <li><b>SearchTravRetTask</b> and <b>SearchTravTask</b> take a numeric
-              parameter, indicating the required traversal size.
-   </li>
-   <li><b>SearchTravRetLoadFieldSelectorTask</b> takes a string
-              parameter: a comma separated list of Fields to load.
-   </li>
-   <li><b>SearchTravRetHighlighterTask</b> takes a string
-              parameter: a comma separated list of parameters to define highlighting.  See that
-     tasks javadocs for more information
-   </li>
- </ul>
- <br>Example - <font color="#FF0066">AddDoc(2000)</font> - would add a document
- of size 2000 (~bytes).
- <br>See conf/task-sample.alg for how this can be used, for instance, to check
- which is faster, adding
- many smaller documents, or few larger documents.
- Next candidates for supporting a parameter may be the Search tasks,
- for controlling the qurey size.
- </li>
- <li>
- <b>Statistic recording elimination</b>: - a sequence can also end with
- '<font color="#FF0066">></font>',
- in which case child tasks would not store their statistics.
- This can be useful to avoid exploding stats data, for adding say 1M docs.
- <br>Example - <font color="#FF0066">{ "ManyAdds" AddDoc > : 1000000</font> -
- would add million docs, measure that total, but not save stats for each addDoc.
- <br>Notice that the granularity of System.currentTimeMillis() (which is used
- here) is system dependant,
- and in some systems an operation that takes 5 ms to complete may show 0 ms
- latency time in performance measurements.
- Therefore it is sometimes more accurate to look at the elapsed time of a larger
- sequence, as demonstrated here.
- </li>
- <li>
- <b>Rate</b>:
- To set a rate (ops/sec or ops/min) for a sequence, add
- '<font color="#FF0066">: N : R</font>' just after sequence closing tag.
- This would specify repetition of N with rate of R operations/sec.
- Use '<font color="#FF0066">R/sec</font>' or
- '<font color="#FF0066">R/min</font>'
- to explicitely specify that the rate is per second or per minute.
- The default is per second,
- <br>Example -  <font color="#FF0066">[ AddDoc ] : 400 : 3</font> - would do 400
- addDoc in parallel, starting up to 3 threads per second.
- <br>Example -  <font color="#FF0066">{ AddDoc } : 100 : 200/min</font> - would
- do 100 addDoc serially,
- waiting before starting next add, if otherwise rate would exceed 200 adds/min.
- </li>
- <li>
- <b>Disable Counting</b>: Each task executed contributes to the records count.
- This count is reflected in reports under recs/s and under recsPerRun.
- Most tasks count 1, some count 0, and some count more.
- (See <a href="#recsCounting">Results record counting clarified</a> for more details.)
- It is possible to disable counting for a task by preceding it with <font color="#FF0066">-</font>.
- <br>Example -  <font color="#FF0066"> -CreateIndex </font> - would count 0 while
- the default behavior for CreateIndex is to count 1.
- </li>
- <li>
- <b>Command names</b>: Each class "AnyNameTask" in the
- package org.apache.lucene.benchmark.byTask.tasks,
- that extends PerfTask, is supported as command "AnyName" that can be
- used in the benchmark "algorithm" description.
- This allows to add new commands by just adding such classes.
- </li>
-</ol>
-
-
-<a name="tasks"></a>
-<h2>Supported tasks/commands</h2>
-
-<p>
-Existing tasks can be divided into a few groups:
-regular index/search work tasks, report tasks, and control tasks.
-</p>
-
-<ol>
-
- <li>
- <b>Report tasks</b>: There are a few Report commands for generating reports.
- Only task runs that were completed are reported.
- (The 'Report tasks' themselves are not measured and not reported.)
- <ul>
-             <li>
-            <font color="#FF0066">RepAll</font> - all (completed) task runs.
-            </li>
-            <li>
-            <font color="#FF0066">RepSumByName</font> - all statistics,
-            aggregated by name. So, if AddDoc was executed 2000 times,
-            only 1 report line would be created for it, aggregating all those
-            2000 statistic records.
-            </li>
-            <li>
-            <font color="#FF0066">RepSelectByPref &nbsp; prefixWord</font> - all
-            records for tasks whose name start with
-            <font color="#FF0066">prefixWord</font>.
-            </li>
-            <li>
-            <font color="#FF0066">RepSumByPref &nbsp; prefixWord</font> - all
-            records for tasks whose name start with
-            <font color="#FF0066">prefixWord</font>,
-            aggregated by their full task name.
-            </li>
-            <li>
-            <font color="#FF0066">RepSumByNameRound</font> - all statistics,
-            aggregated by name and by <font color="#FF0066">Round</font>.
-            So, if AddDoc was executed 2000 times in each of 3
-            <font color="#FF0066">rounds</font>, 3 report lines would be
-            created for it,
-            aggregating all those 2000 statistic records in each round.
-            See more about rounds in the <font color="#FF0066">NewRound</font>
-            command description below.
-            </li>
-            <li>
-            <font color="#FF0066">RepSumByPrefRound &nbsp; prefixWord</font> -
-            similar to <font color="#FF0066">RepSumByNameRound</font>,
-            just that only tasks whose name starts with
-            <font color="#FF0066">prefixWord</font> are included.
-            </li>
- </ul>
- If needed, additional reports can be added by extending the abstract class
- ReportTask, and by
- manipulating the statistics data in Points and TaskStats.
- </li>
-
- <li><b>Control tasks</b>: Few of the tasks control the benchmark algorithm
- all over:
- <ul>
-     <li>
-     <font color="#FF0066">ClearStats</font> - clears the entire statistics.
-     Further reports would only include task runs that would start after this
-     call.
-     </li>
-     <li>
-     <font color="#FF0066">NewRound</font> - virtually start a new round of
-     performance test.
-     Although this command can be placed anywhere, it mostly makes sense at
-     the end of an outermost sequence.
-     <br>This increments a global "round counter". All task runs that
-     would start now would
-     record the new, updated round counter as their round number.
-     This would appear in reports.
-     In particular, see <font color="#FF0066">RepSumByNameRound</font> above.
-     <br>An additional effect of NewRound, is that numeric and boolean
-     properties defined (at the head
-     of the .alg file) as a sequence of values, e.g. <font color="#FF0066">
-     merge.factor=mrg:10:100:10:100</font> would
-     increment (cyclic) to the next value.
-     Note: this would also be reflected in the reports, in this case under a
-     column that would be named "mrg".
-     </li>
-     <li>
-     <font color="#FF0066">ResetInputs</font> - DocMaker and the
-     various QueryMakers
-     would reset their counters to start.
-     The way these Maker interfaces work, each call for makeDocument()
-     or makeQuery() creates the next document or query
-     that it "knows" to create.
-     If that pool is "exhausted", the "maker" start over again.
-     The resetInpus command
-     therefore allows to make the rounds comparable.
-     It is therefore useful to invoke ResetInputs together with NewRound.
-     </li>
-     <li>
-     <font color="#FF0066">ResetSystemErase</font> - reset all index
-     and input data and call gc.
-     Does NOT reset statistics. This contains ResetInputs.
-     All writers/readers are nullified, deleted, closed.
-     Index is erased.
-     Directory is erased.
-     You would have to call CreateIndex once this was called...
-     </li>
-     <li>
-     <font color="#FF0066">ResetSystemSoft</font> -  reset all
-     index and input data and call gc.
-     Does NOT reset statistics. This contains ResetInputs.
-     All writers/readers are nullified, closed.
-     Index is NOT erased.
-     Directory is NOT erased.
-     This is useful for testing performance on an existing index,
-     for instance if the construction of a large index
-     took a very long time and now you would to test
-     its search or update performance.
-     </li>
- </ul>
- </li>
-
- <li>
- Other existing tasks are quite straightforward and would
- just be briefly described here.
- <ul>
-     <li>
-     <font color="#FF0066">CreateIndex</font> and
-     <font color="#FF0066">OpenIndex</font> both leave the
-     index open for later update operations.
-     <font color="#FF0066">CloseIndex</font> would close it.
-     <li>
-     <font color="#FF0066">OpenReader</font>, similarly, would
-     leave an index reader open for later search operations.
-     But this have further semantics.
-     If a Read operation is performed, and an open reader exists,
-     it would be used.
-     Otherwise, the read operation would open its own reader
-     and close it when the read operation is done.
-     This allows testing various scenarios - sharing a reader,
-     searching with "cold" reader, with "warmed" reader, etc.
-     The read operations affected by this are:
-     <font color="#FF0066">Warm</font>,
-     <font color="#FF0066">Search</font>,
-     <font color="#FF0066">SearchTrav</font> (search and traverse),
-     and <font color="#FF0066">SearchTravRet</font> (search
-     and traverse and retrieve).
-     Notice that each of the 3 search task types maintains
-     its own queryMaker instance.
-	 <li>
-	 <font color="#FF0066">CommitIndex</font> and 
-	 <font color="#FF0066">Optimize</font> can be used to commit
-	 changes to the index and/or optimize the index created thus
-	 far.
-	 <li>
-	 <font color="#FF0066">WriteLineDoc</font> prepares a 'line'
-	 file where each line holds a document with <i>title</i>, 
-	 <i>date</i> and <i>body</i> elements, seperated by [TAB].
-	 A line file is useful if one wants to measure pure indexing
-	 performance, without the overhead of parsing the data.<br>
-	 You can use LineDocSource as a ContentSource over a 'line'
-	 file.
-	 <li>
-	 <font color="#FF0066">ConsumeContentSource</font> consumes
-	 a ContentSource. Useful for e.g. testing a ContentSource
-	 performance, without the overhead of preparing a Document
-	 out of it.
- </ul>
- </li>
- </ol>
-
-<a name="properties"></a>
-<h2>Benchmark properties</h2>
-
-<p>
-Properties are read from the header of the .alg file, and
-define several parameters of the performance test.
-As mentioned above for the <font color="#FF0066">NewRound</font> task,
-numeric and boolean properties that are defined as a sequence
-of values, e.g. <font color="#FF0066">merge.factor=mrg:10:100:10:100</font>
-would increment (cyclic) to the next value,
-when NewRound is called, and would also
-appear as a named column in the reports (column
-name would be "mrg" in this example).
-</p>
-
-<p>
-Some of the currently defined properties are:
-</p>
-
-<ol>
-    <li>
-    <font color="#FF0066">analyzer</font> - full
-    class name for the analyzer to use.
-    Same analyzer would be used in the entire test.
-    </li>
-
-    <li>
-    <font color="#FF0066">directory</font> - valid values are
-    This tells which directory to use for the performance test.
-    </li>
-
-    <li>
-    <b>Index work parameters</b>:
-    Multi int/boolean values would be iterated with calls to NewRound.
-    There would be also added as columns in the reports, first string in the
-    sequence is the column name.
-    (Make sure it is no shorter than any value in the sequence).
-    <ul>
-        <li><font color="#FF0066">max.buffered</font>
-        <br>Example: max.buffered=buf:10:10:100:100 -
-        this would define using maxBufferedDocs of 10 in iterations 0 and 1,
-        and 100 in iterations 2 and 3.
-        </li>
-        <li>
-        <font color="#FF0066">merge.factor</font> - which
-        merge factor to use.
-        </li>
-        <li>
-        <font color="#FF0066">compound</font> - whether the index is
-        using the compound format or not. Valid values are "true" and "false".
-        </li>
-    </ul>
-</ol>
-
-<p>
-Here is a list of currently defined properties:
-</p>
-<ol>
-
-  <li><b>Root directory for data and indexes:</b></li>
-    <ul><li>work.dir (default is System property "benchmark.work.dir" or "work".)
-    </li></ul>
-  </li>
-
-  <li><b>Docs and queries creation:</b></li>
-    <ul><li>analyzer
-    </li><li>doc.maker
-    </li><li>doc.maker.forever
-    </li><li>html.parser
-    </li><li>doc.stored
-    </li><li>doc.tokenized
-    </li><li>doc.term.vector
-    </li><li>doc.term.vector.positions
-    </li><li>doc.term.vector.offsets
-    </li><li>doc.store.body.bytes
-    </li><li>docs.dir
-    </li><li>query.maker
-    </li><li>file.query.maker.file
-    </li><li>file.query.maker.default.field
-    </li><li>search.num.hits
-    </li></ul>
-  </li>
-
-  <li><b>Logging</b>:
-    <ul><li>log.step
-	</li><li>log.step.[class name]Task ie log.step.DeleteDoc (e.g. log.step.Wonderful for the WonderfulTask example above).
-    </li><li>log.queries
-    </li><li>task.max.depth.log
-    </li></ul>
-  </li>
-
-  <li><b>Index writing</b>:
-    <ul><li>compound
-    </li><li>merge.factor
-    </li><li>max.buffered
-    </li><li>directory
-    </li><li>ram.flush.mb
-    </li></ul>
-  </li>
-
-  <li><b>Doc deletion</b>:
-    <ul><li>doc.delete.step
-    </li></ul>
-  </li>
-
-</ol>
-
-<p>
-For sample use of these properties see the *.alg files under conf.
-</p>
-
-<a name="example"></a>
-<h2>Example input algorithm and the result benchmark report</h2>
-<p>
-The following example is in conf/sample.alg:
-<pre>
-<font color="#003333"># --------------------------------------------------------
-#
-# Sample: what is the effect of doc size on indexing time?
-#
-# There are two parts in this test:
-# - PopulateShort adds 2N documents of length  L
-# - PopulateLong  adds  N documents of length 2L
-# Which one would be faster?
-# The comparison is done twice.
-#
-# --------------------------------------------------------
-</font>
-<font color="#990066"># -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-merge.factor=mrg:10:20
-max.buffered=buf:100:1000
-compound=true
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-doc.add.log.step=500
-
-docs.dir=reuters-out
-
-doc.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleDocMaker
-
-query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=false
-# -------------------------------------------------------------------------------------</font>
-<font color="#3300FF">{
-
-    { "PopulateShort"
-        CreateIndex
-        { AddDoc(4000) > : 20000
-        Optimize
-        CloseIndex
-    >
-
-    ResetSystemErase
-
-    { "PopulateLong"
-        CreateIndex
-        { AddDoc(8000) > : 10000
-        Optimize
-        CloseIndex
-    >
-
-    ResetSystemErase
-
-    NewRound
-
-} : 2
-
-RepSumByName
-RepSelectByPref Populate
-</font>
-</pre>
-</p>
-
-<p>
-The command line for running this sample:
-<br><code>ant run-task -Dtask.alg=conf/sample.alg</code>
-</p>
-
-<p>
-The output report from running this test contains the following:
-<pre>
-Operation     round mrg  buf   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
-PopulateShort     0  10  100        1        20003        119.6      167.26    12,959,120     14,241,792
-PopulateLong -  - 0  10  100 -  -   1 -  -   10003 -  -  - 74.3 -  - 134.57 -  17,085,208 -   20,635,648
-PopulateShort     1  20 1000        1        20003        143.5      139.39    63,982,040     94,756,864
-PopulateLong -  - 1  20 1000 -  -   1 -  -   10003 -  -  - 77.0 -  - 129.92 -  87,309,608 -  100,831,232
-</pre>
-</p>
-
-<a name="recsCounting"></a>
-<h2>Results record counting clarified</h2>
-<p>
-Two columns in the results table indicate records counts: records-per-run and
-records-per-second. What does it mean?
-</p><p>
-Almost every task gets 1 in this count just for being executed.
-Task sequences aggregate the counts of their child tasks,
-plus their own count of 1.
-So, a task sequence containing 5 other task sequences, each running a single
-other task 10 times, would have a count of 1 + 5 * (1 + 10) = 56.
-</p><p>
-The traverse and retrieve tasks "count" more: a traverse task
-would add 1 for each traversed result (hit), and a retrieve task would
-additionally add 1 for each retrieved doc. So, regular Search would
-count 1, SearchTrav that traverses 10 hits would count 11, and a
-SearchTravRet task that retrieves (and traverses) 10, would count 21.
-</p><p>
-Confusing? this might help: always examine the <code>elapsedSec</code> column,
-and always compare "apples to apples", .i.e. it is interesting to check how the
-<code>rec/s</code> changed for the same task (or sequence) between two
-different runs, but it is not very useful to know how the <code>rec/s</code>
-differs between <code>Search</code> and <code>SearchTrav</code> tasks. For
-the latter, <code>elapsedSec</code> would bring more insight.
-</p>
-
-</DIV>
-<DIV>&nbsp;</DIV>
-</BODY>
-</HTML>
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/programmatic/Sample.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/programmatic/Sample.java
deleted file mode 100644
index 6a1a603..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/programmatic/Sample.java
+++ /dev/null
@@ -1,100 +0,0 @@
-package org.apache.lucene.benchmark.byTask.programmatic;
-
-/**
- * Copyright 2005 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Properties;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.tasks.AddDocTask;
-import org.apache.lucene.benchmark.byTask.tasks.CloseIndexTask;
-import org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask;
-import org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask;
-import org.apache.lucene.benchmark.byTask.tasks.TaskSequence;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-
-/**
- * Sample performance test written programmatically - no algorithm file is needed here.
- */
-public class Sample {
-
-  /**
-   * @param args
-   * @throws Exception 
-   * @throws IOException 
-   */
-  public static void main(String[] args) throws Exception {
-    Properties p = initProps();
-    Config conf = new Config(p);
-    PerfRunData runData = new PerfRunData(conf);
-    
-    // 1. top sequence
-    TaskSequence top = new TaskSequence(runData,null,null,false); // top level, not parallel
-    
-    // 2. task to create the index
-    CreateIndexTask create = new CreateIndexTask(runData);
-    top.addTask(create);
-    
-    // 3. task seq to add 500 docs (order matters - top to bottom - add seq to top, only then add to seq)
-    TaskSequence seq1 = new TaskSequence(runData,"AddDocs",top,false);
-    seq1.setRepetitions(500);
-    seq1.setNoChildReport();
-    top.addTask(seq1);
-
-    // 4. task to add the doc
-    AddDocTask addDoc = new AddDocTask(runData);
-    //addDoc.setParams("1200"); // doc size limit if supported
-    seq1.addTask(addDoc); // order matters 9see comment above)
-
-    // 5. task to close the index
-    CloseIndexTask close = new CloseIndexTask(runData);
-    top.addTask(close);
-
-    // task to report
-    RepSumByNameTask rep = new RepSumByNameTask(runData);
-    top.addTask(rep);
-
-    // print algorithm
-    System.out.println(top.toString());
-    
-    // execute
-    top.doLogic();
-  }
-
-  // Sample programmatic settings. Could also read from file.
-  private static Properties initProps() {
-    Properties p = new Properties();
-    p.setProperty ( "task.max.depth.log"  , "3" );
-    p.setProperty ( "max.buffered"        , "buf:10:10:100:100:10:10:100:100" );
-    p.setProperty ( "doc.maker"           , "org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource" );
-    p.setProperty ( "log.step"            , "2000" );
-    p.setProperty ( "doc.delete.step"     , "8" );
-    p.setProperty ( "analyzer"            , "org.apache.lucene.analysis.standard.StandardAnalyzer" );
-    p.setProperty ( "doc.term.vector"     , "false" );
-    p.setProperty ( "directory"           , "FSDirectory" );
-    p.setProperty ( "query.maker"         , "org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker" );
-    p.setProperty ( "doc.stored"          , "true" );
-    p.setProperty ( "docs.dir"            , "reuters-out" );
-    p.setProperty ( "compound"            , "cmpnd:true:true:true:true:false:false:false:false" );
-    p.setProperty ( "doc.tokenized"       , "true" );
-    p.setProperty ( "merge.factor"        , "mrg:10:100:10:100:10:100:10:100" );
-    return p;
-  }
-  
-  
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/programmatic/package.html b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/programmatic/package.html
deleted file mode 100644
index 7221c42..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/programmatic/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<body>
-Sample performance test written programmatically - no algorithm file is needed here.
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/Points.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/Points.java
deleted file mode 100644
index d2f8b2b..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/Points.java
+++ /dev/null
@@ -1,95 +0,0 @@
-package org.apache.lucene.benchmark.byTask.stats;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.benchmark.byTask.tasks.PerfTask;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-
-
-/**
- * Test run data points collected as the test proceeds.
- */
-public class Points {
-
-  // stat points ordered by their start time. 
-  // for now we collect points as TaskStats objects.
-  // later might optimize to collect only native data.
-  private ArrayList<TaskStats> points = new ArrayList<TaskStats>();
-
-  private int nextTaskRunNum = 0;
-
-  private TaskStats currentStats;
-
-  /**
-   * Create a Points statistics object. 
-   */
-  public Points (Config config) {
-  }
-
-  /**
-   * Return the current task stats.
-   * the actual task stats are returned, so caller should not modify this task stats. 
-   * @return current {@link TaskStats}.
-   */
-  public List<TaskStats> taskStats () {
-    return points;
-  }
-
-  /**
-   * Mark that a task is starting. 
-   * Create a task stats for it and store it as a point.
-   * @param task the starting task.
-   * @return the new task stats created for the starting task.
-   */
-  public synchronized TaskStats markTaskStart (PerfTask task, int round) {
-    TaskStats stats = new TaskStats(task, nextTaskRunNum(), round);
-    this.currentStats = stats;
-    points.add(stats);
-    return stats;
-  }
-
-  public TaskStats getCurrentStats() {
-    return currentStats;
-  }
-  
-  // return next task num
-  private synchronized int nextTaskRunNum() {
-    return nextTaskRunNum++;
-  }
-  
-  /**
-   * mark the end of a task
-   */
-  public synchronized void markTaskEnd (TaskStats stats, int count) {
-    int numParallelTasks = nextTaskRunNum - 1 - stats.getTaskRunNum();
-    // note: if the stats were cleared, might be that this stats object is 
-    // no longer in points, but this is just ok.
-    stats.markEnd(numParallelTasks, count);
-  }
-
-  /**
-   * Clear all data, prepare for more tests.
-   */
-  public void clearData() {
-    points.clear();
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/Report.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/Report.java
deleted file mode 100644
index 1db8f6e..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/Report.java
+++ /dev/null
@@ -1,64 +0,0 @@
-package org.apache.lucene.benchmark.byTask.stats;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Textual report of current statistics.
- */
-public class Report {
-
-  private String text;
-  private int size;
-  private int outOf;
-  private int reported;
-
-  public Report (String text, int size, int reported, int outOf) {
-    this.text = text;
-    this.size = size;
-    this.reported = reported;
-    this.outOf = outOf;
-  }
-
-  /**
-   * Returns total number of stats points when this report was created.
-   */
-  public int getOutOf() {
-    return outOf;
-  }
-
-  /**
-   * Returns number of lines in the report.
-   */
-  public int getSize() {
-    return size;
-  }
-
-  /**
-   * Returns the report text.
-   */
-  public String getText() {
-    return text;
-  }
-
-  /**
-   * Returns number of stats points represented in this report.
-   */
-  public int getReported() {
-    return reported;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/TaskStats.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/TaskStats.java
deleted file mode 100644
index 6924670..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/TaskStats.java
+++ /dev/null
@@ -1,226 +0,0 @@
-package org.apache.lucene.benchmark.byTask.stats;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.tasks.PerfTask;
-
-/**
- * Statistics for a task run. 
- * <br>The same task can run more than once, but, if that task records statistics, 
- * each run would create its own TaskStats.
- */
-public class TaskStats implements Cloneable {
-
-  /** task for which data was collected */
-  private PerfTask task; 
-
-  /** round in which task run started */
-  private int round;
-
-  /** task start time */
-  private long start;
-  
-  /** task elapsed time.  elapsed >= 0 indicates run completion! */
-  private long elapsed = -1;
-  
-  /** max tot mem during task */
-  private long maxTotMem;
-  
-  /** max used mem during task */
-  private long maxUsedMem;
-  
-  /** serial run number of this task run in the perf run */
-  private int taskRunNum;
-  
-  /** number of other tasks that started to run while this task was still running */ 
-  private int numParallelTasks;
-  
-  /** number of work items done by this task.
-   * For indexing that can be number of docs added.
-   * For warming that can be number of scanned items, etc. 
-   * For repeating tasks, this is a sum over repetitions.
-   */
-  private int count;
-
-  /** Number of similar tasks aggregated into this record.   
-   * Used when summing up on few runs/instances of similar tasks.
-   */
-  private int numRuns = 1;
-  
-  /**
-   * Create a run data for a task that is starting now.
-   * To be called from Points.
-   */
-  TaskStats (PerfTask task, int taskRunNum, int round) {
-    this.task = task;
-    this.taskRunNum = taskRunNum;
-    this.round = round;
-    maxTotMem = Runtime.getRuntime().totalMemory();
-    maxUsedMem = maxTotMem - Runtime.getRuntime().freeMemory();
-    start = System.currentTimeMillis();
-  }
-  
-  /**
-   * mark the end of a task
-   */
-  void markEnd (int numParallelTasks, int count) {
-    elapsed = System.currentTimeMillis() - start;
-    long totMem = Runtime.getRuntime().totalMemory();
-    if (totMem > maxTotMem) {
-      maxTotMem = totMem;
-    }
-    long usedMem = totMem - Runtime.getRuntime().freeMemory();
-    if (usedMem > maxUsedMem) {
-      maxUsedMem = usedMem;
-    }
-    this.numParallelTasks = numParallelTasks;
-    this.count = count;
-  }
-  
-  private int[] countsByTime;
-  private long countsByTimeStepMSec;
-
-  public void setCountsByTime(int[] counts, long msecStep) {
-    countsByTime = counts;
-    countsByTimeStepMSec = msecStep;
-  }
-
-  public int[] getCountsByTime() {
-    return countsByTime;
-  }
-
-  public long getCountsByTimeStepMSec() {
-    return countsByTimeStepMSec;
-  }
-
-  /**
-   * @return the taskRunNum.
-   */
-  public int getTaskRunNum() {
-    return taskRunNum;
-  }
-
-  /* (non-Javadoc)
-   * @see java.lang.Object#toString()
-   */
-  @Override
-  public String toString() {
-    StringBuilder res = new StringBuilder(task.getName());
-    res.append(" ");
-    res.append(count);
-    res.append(" ");
-    res.append(elapsed);
-    return res.toString();
-  }
-
-  /**
-   * @return Returns the count.
-   */
-  public int getCount() {
-    return count;
-  }
-
-  /**
-   * @return elapsed time.
-   */
-  public long getElapsed() {
-    return elapsed;
-  }
-
-  /**
-   * @return Returns the maxTotMem.
-   */
-  public long getMaxTotMem() {
-    return maxTotMem;
-  }
-
-  /**
-   * @return Returns the maxUsedMem.
-   */
-  public long getMaxUsedMem() {
-    return maxUsedMem;
-  }
-
-  /**
-   * @return Returns the numParallelTasks.
-   */
-  public int getNumParallelTasks() {
-    return numParallelTasks;
-  }
-
-  /**
-   * @return Returns the task.
-   */
-  public PerfTask getTask() {
-    return task;
-  }
-
-  /**
-   * @return Returns the numRuns.
-   */
-  public int getNumRuns() {
-    return numRuns;
-  }
-
-  /**
-   * Add data from another stat, for aggregation
-   * @param stat2 the added stat data.
-   */
-  public void add(TaskStats stat2) {
-    numRuns += stat2.getNumRuns();
-    elapsed += stat2.getElapsed();
-    maxTotMem += stat2.getMaxTotMem();
-    maxUsedMem += stat2.getMaxUsedMem();
-    count += stat2.getCount();
-    if (round != stat2.round) {
-      round = -1; // no meaning if aggregating tasks of different round. 
-    }
-
-    if (countsByTime != null && stat2.countsByTime != null) {
-      if (countsByTimeStepMSec != stat2.countsByTimeStepMSec) {
-        throw new IllegalStateException("different by-time msec step");
-      }
-      if (countsByTime.length != stat2.countsByTime.length) {
-        throw new IllegalStateException("different by-time msec count");
-      }
-      for(int i=0;i<stat2.countsByTime.length;i++) {
-        countsByTime[i] += stat2.countsByTime[i];
-      }
-    }
-  }
-
-  /* (non-Javadoc)
-   * @see java.lang.Object#clone()
-   */
-  @Override
-  public Object clone() throws CloneNotSupportedException {
-    TaskStats c = (TaskStats) super.clone();
-    if (c.countsByTime != null) {
-      c.countsByTime = c.countsByTime.clone();
-    }
-    return c;
-  }
-
-  /**
-   * @return the round number.
-   */
-  public int getRound() {
-    return round;
-  }
-  
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/package.html b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/package.html
deleted file mode 100644
index fb44623..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<body>
-  Statistics maintained when running benchmark tasks.
-</body>
-</html>
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddDocTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddDocTask.java
deleted file mode 100644
index 672d736..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddDocTask.java
+++ /dev/null
@@ -1,86 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
-import org.apache.lucene.document.Document;
-
-/**
- * Add a document, optionally with of a certain size.
- * <br>Other side effects: none.
- * <br>Takes optional param: document size. 
- */
-public class AddDocTask extends PerfTask {
-
-  public AddDocTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  private int docSize = 0;
-  
-  // volatile data passed between setup(), doLogic(), tearDown().
-  private Document doc = null;
-  
-  @Override
-  public void setup() throws Exception {
-    super.setup();
-    DocMaker docMaker = getRunData().getDocMaker();
-    if (docSize > 0) {
-      doc = docMaker.makeDocument(docSize);
-    } else {
-      doc = docMaker.makeDocument();
-    }
-  }
-
-  @Override
-  public void tearDown() throws Exception {
-    doc = null;
-    super.tearDown();
-  }
-
-  @Override
-  protected String getLogMessage(int recsCount) {
-    return "added " + recsCount + " docs";
-  }
-  
-  @Override
-  public int doLogic() throws Exception {
-    getRunData().getIndexWriter().addDocument(doc);
-    return 1;
-  }
-
-  /**
-   * Set the params (docSize only)
-   * @param params docSize, or 0 for no limit.
-   */
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    docSize = (int) Float.parseFloat(params); 
-  }
-
-  /* (non-Javadoc)
-   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#supportsParams()
-   */
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-  
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/BenchmarkHighlighter.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/BenchmarkHighlighter.java
deleted file mode 100644
index c20720b..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/BenchmarkHighlighter.java
+++ /dev/null
@@ -1,27 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.IndexReader;
-
-public abstract class BenchmarkHighlighter {
-  public abstract int doHighlight( IndexReader reader, int doc, String field,
-      Document document, Analyzer analyzer, String text ) throws Exception ;
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ClearStatsTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ClearStatsTask.java
deleted file mode 100644
index d1172d8..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ClearStatsTask.java
+++ /dev/null
@@ -1,46 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Clear statistics data.
- * <br>Other side effects: None.
- */
-public class ClearStatsTask extends PerfTask {
-
-  public ClearStatsTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public int doLogic() throws Exception {
-    getRunData().getPoints().clearData();
-    return 0;
-  }
-
-  /* (non-Javadoc)
-   * @see PerfTask#shouldNotRecordStats()
-   */
-  @Override
-  protected boolean shouldNotRecordStats() {
-    return true;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CloseIndexTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CloseIndexTask.java
deleted file mode 100644
index 992a3e59..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CloseIndexTask.java
+++ /dev/null
@@ -1,65 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.PrintStream;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.index.IndexWriter;
-
-/**
- * Close index writer.
- * <br>Other side effects: index writer object in perfRunData is nullified.
- * <br>Takes optional param "doWait": if false, then close(false) is called.
- */
-public class CloseIndexTask extends PerfTask {
-
-  public CloseIndexTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  boolean doWait = true;
-
-  @Override
-  public int doLogic() throws IOException {
-    IndexWriter iw = getRunData().getIndexWriter();
-    if (iw != null) {
-      // If infoStream was set to output to a file, close it.
-      PrintStream infoStream = iw.getInfoStream();
-      if (infoStream != null && infoStream != System.out
-          && infoStream != System.err) {
-        infoStream.close();
-      }
-      iw.close(doWait);
-      getRunData().setIndexWriter(null);
-    }
-    return 1;
-  }
-
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    doWait = Boolean.valueOf(params).booleanValue();
-  }
-
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CloseReaderTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CloseReaderTask.java
deleted file mode 100644
index 85c4d65..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CloseReaderTask.java
+++ /dev/null
@@ -1,47 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.index.IndexReader;
-
-/**
- * Close index reader.
- * <br>Other side effects: index reader in perfRunData is nullified.
- * <br>This would cause read related tasks to reopen their own reader. 
- */
-public class CloseReaderTask extends PerfTask {
-
-  public CloseReaderTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public int doLogic() throws IOException {
-    IndexReader reader = getRunData().getIndexReader();
-    getRunData().setIndexReader(null);
-    if (reader.getRefCount() != 1) {
-      System.out.println("WARNING: CloseReader: reference count is currently " + reader.getRefCount());
-    }
-    reader.decRef();
-    return 1;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CommitIndexTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CommitIndexTask.java
deleted file mode 100644
index 2b026d8..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CommitIndexTask.java
+++ /dev/null
@@ -1,66 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexReader;
-
-/**
- * Commits the IndexWriter.
- *
- */
-public class CommitIndexTask extends PerfTask {
-  Map<String,String> commitUserData;
-
-  public CommitIndexTask(PerfRunData runData) {
-    super(runData);
-  }
-  
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-  
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    commitUserData = new HashMap<String,String>();
-    commitUserData.put(OpenReaderTask.USER_DATA, params);
-  }
-  
-  @Override
-  public int doLogic() throws Exception {
-    IndexWriter iw = getRunData().getIndexWriter();
-    if (iw != null) {
-      iw.commit(commitUserData);
-    } else {
-      IndexReader r = getRunData().getIndexReader();
-      if (r != null) {
-        r.commit(commitUserData);
-        r.decRef();
-      } else {
-        throw new IllegalStateException("neither IndexWriter nor IndexReader is currently open");
-      }
-    }
-    
-    return 1;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ConsumeContentSourceTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ConsumeContentSourceTask.java
deleted file mode 100644
index 5dbed92..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ConsumeContentSourceTask.java
+++ /dev/null
@@ -1,70 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.feeds.ContentSource;
-import org.apache.lucene.benchmark.byTask.feeds.DocData;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-
-/**
- * Consumes a {@link org.apache.lucene.benchmark.byTask.feeds.ContentSource}.
- * Supports the following parameters:
- * <ul>
- * <li>content.source - the content source to use. (mandatory)
- * </ul>
- */
-public class ConsumeContentSourceTask extends PerfTask {
-
-  private ContentSource source;
-  private DocData dd = new DocData();
-  
-  public ConsumeContentSourceTask(PerfRunData runData) {
-    super(runData);
-    Config config = runData.getConfig();
-    String sourceClass = config.get("content.source", null);
-    if (sourceClass == null) {
-      throw new IllegalArgumentException("content.source must be defined");
-    }
-    try {
-      source = Class.forName(sourceClass).asSubclass(ContentSource.class).newInstance();
-      source.setConfig(config);
-      source.resetInputs();
-    } catch (Exception e) {
-      throw new RuntimeException(e);
-    }
-  }
-
-  @Override
-  protected String getLogMessage(int recsCount) {
-    return "read " + recsCount + " documents from the content source";
-  }
-  
-  @Override
-  public void close() throws Exception {
-    source.close();
-    super.close();
-  }
-
-  @Override
-  public int doLogic() throws Exception {
-    dd = source.getNextDocData(dd);
-    return 1;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java
deleted file mode 100644
index a347c9c..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java
+++ /dev/null
@@ -1,185 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexCommit;
-import org.apache.lucene.index.IndexDeletionPolicy;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.LogMergePolicy;
-import org.apache.lucene.index.MergeScheduler;
-import org.apache.lucene.index.ConcurrentMergeScheduler;
-import org.apache.lucene.index.MergePolicy;
-import org.apache.lucene.index.NoDeletionPolicy;
-import org.apache.lucene.index.NoMergePolicy;
-import org.apache.lucene.index.NoMergeScheduler;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.codecs.CodecProvider;
-import org.apache.lucene.store.LockObtainFailedException;
-import org.apache.lucene.util.Version;
-
-import java.io.BufferedOutputStream;
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.io.PrintStream;
-
-/**
- * Create an index. <br>
- * Other side effects: index writer object in perfRunData is set. <br>
- * Relevant properties: <code>merge.factor (default 10),
- * max.buffered (default no flush), max.field.length (default
- * 10,000 tokens), max.field.length, compound (default true), ram.flush.mb [default 0],
- * merge.policy (default org.apache.lucene.index.LogByteSizeMergePolicy),
- * merge.scheduler (default
- * org.apache.lucene.index.ConcurrentMergeScheduler),
- * concurrent.merge.scheduler.max.thread.count and
- * concurrent.merge.scheduler.max.merge.count (defaults per
- * ConcurrentMergeScheduler), default.codec </code>.
- * <p>
- * This task also supports a "writer.info.stream" property with the following
- * values:
- * <ul>
- * <li>SystemOut - sets {@link IndexWriter#setInfoStream(java.io.PrintStream)}
- * to {@link System#out}.
- * <li>SystemErr - sets {@link IndexWriter#setInfoStream(java.io.PrintStream)}
- * to {@link System#err}.
- * <li>&lt;file_name&gt; - attempts to create a file given that name and sets
- * {@link IndexWriter#setInfoStream(java.io.PrintStream)} to that file. If this
- * denotes an invalid file name, or some error occurs, an exception will be
- * thrown.
- * </ul>
- */
-public class CreateIndexTask extends PerfTask {
-
-  public CreateIndexTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  
-  
-  public static IndexDeletionPolicy getIndexDeletionPolicy(Config config) {
-    String deletionPolicyName = config.get("deletion.policy", "org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy");
-    if (deletionPolicyName.equals(NoDeletionPolicy.class.getName())) {
-      return NoDeletionPolicy.INSTANCE;
-    } else {
-      try {
-        return Class.forName(deletionPolicyName).asSubclass(IndexDeletionPolicy.class).newInstance();
-      } catch (Exception e) {
-        throw new RuntimeException("unable to instantiate class '" + deletionPolicyName + "' as IndexDeletionPolicy", e);
-      }
-    }
-  }
-  
-  @Override
-  public int doLogic() throws IOException {
-    PerfRunData runData = getRunData();
-    Config config = runData.getConfig();
-    runData.setIndexWriter(configureWriter(config, runData, OpenMode.CREATE, null));
-    return 1;
-  }
-  
-  public static IndexWriterConfig createWriterConfig(Config config, PerfRunData runData, OpenMode mode, IndexCommit commit) {
-    Version version = Version.valueOf(config.get("writer.version", Version.LUCENE_40.toString()));
-    IndexWriterConfig iwConf = new IndexWriterConfig(version, runData.getAnalyzer());
-    iwConf.setOpenMode(mode);
-    IndexDeletionPolicy indexDeletionPolicy = getIndexDeletionPolicy(config);
-    iwConf.setIndexDeletionPolicy(indexDeletionPolicy);
-    if(commit != null)
-      iwConf.setIndexCommit(commit);
-    
-
-    final String mergeScheduler = config.get("merge.scheduler",
-                                             "org.apache.lucene.index.ConcurrentMergeScheduler");
-    if (mergeScheduler.equals(NoMergeScheduler.class.getName())) {
-      iwConf.setMergeScheduler(NoMergeScheduler.INSTANCE);
-    } else {
-      try {
-        iwConf.setMergeScheduler(Class.forName(mergeScheduler).asSubclass(MergeScheduler.class).newInstance());
-      } catch (Exception e) {
-        throw new RuntimeException("unable to instantiate class '" + mergeScheduler + "' as merge scheduler", e);
-      }
-      
-      if (mergeScheduler.equals("org.apache.lucene.index.ConcurrentMergeScheduler")) {
-        ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) iwConf.getMergeScheduler();
-        int v = config.get("concurrent.merge.scheduler.max.thread.count", -1);
-        if (v != -1) {
-          cms.setMaxThreadCount(v);
-        }
-        v = config.get("concurrent.merge.scheduler.max.merge.count", -1);
-        if (v != -1) {
-          cms.setMaxMergeCount(v);
-        }
-      }
-    }
-
-    final String defaultCodec = config.get("default.codec", null);
-    if (defaultCodec != null) {
-      CodecProvider.getDefault().setDefaultFieldCodec(defaultCodec);
-    }
-
-    final String mergePolicy = config.get("merge.policy",
-                                          "org.apache.lucene.index.LogByteSizeMergePolicy");
-    boolean isCompound = config.get("compound", true);
-    if (mergePolicy.equals(NoMergePolicy.class.getName())) {
-      iwConf.setMergePolicy(isCompound ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES);
-    } else {
-      try {
-        iwConf.setMergePolicy(Class.forName(mergePolicy).asSubclass(MergePolicy.class).newInstance());
-      } catch (Exception e) {
-        throw new RuntimeException("unable to instantiate class '" + mergePolicy + "' as merge policy", e);
-      }
-      if(iwConf.getMergePolicy() instanceof LogMergePolicy) {
-        LogMergePolicy logMergePolicy = (LogMergePolicy) iwConf.getMergePolicy();
-        logMergePolicy.setUseCompoundFile(isCompound);
-        logMergePolicy.setMergeFactor(config.get("merge.factor",OpenIndexTask.DEFAULT_MERGE_PFACTOR));
-      }
-    }
-    iwConf.setMaxFieldLength(config.get("max.field.length",OpenIndexTask.DEFAULT_MAX_FIELD_LENGTH));
-    final double ramBuffer = config.get("ram.flush.mb",OpenIndexTask.DEFAULT_RAM_FLUSH_MB);
-    final int maxBuffered = config.get("max.buffered",OpenIndexTask.DEFAULT_MAX_BUFFERED);
-    if (maxBuffered == IndexWriterConfig.DISABLE_AUTO_FLUSH) {
-      iwConf.setRAMBufferSizeMB(ramBuffer);
-      iwConf.setMaxBufferedDocs(maxBuffered);
-    } else {
-      iwConf.setMaxBufferedDocs(maxBuffered);
-      iwConf.setRAMBufferSizeMB(ramBuffer);
-    }
-    
-    return iwConf;
-  }
-  
-  public static IndexWriter configureWriter(Config config, PerfRunData runData, OpenMode mode, IndexCommit commit) throws CorruptIndexException, LockObtainFailedException, IOException {
-    IndexWriter writer = new IndexWriter(runData.getDirectory(), createWriterConfig(config, runData, mode, commit));
-    String infoStreamVal = config.get("writer.info.stream", null);
-    if (infoStreamVal != null) {
-      if (infoStreamVal.equals("SystemOut")) {
-        writer.setInfoStream(System.out);
-      } else if (infoStreamVal.equals("SystemErr")) {
-        writer.setInfoStream(System.err);
-      } else {
-        File f = new File(infoStreamVal).getAbsoluteFile();
-        writer.setInfoStream(new PrintStream(new BufferedOutputStream(new FileOutputStream(f))));
-      }
-    }
-    return writer;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/DeleteByPercentTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/DeleteByPercentTask.java
deleted file mode 100644
index 46b603f..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/DeleteByPercentTask.java
+++ /dev/null
@@ -1,95 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Random;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.util.Bits;
-
-/**
- * Deletes a percentage of documents from an index randomly
- * over the number of documents.  The parameter, X, is in
- * percent.  EG 50 means 1/2 of all documents will be
- * deleted.
- *
- * <p><b>NOTE</b>: the param is an absolute percentage of
- * maxDoc().  This means if you delete 50%, and then delete
- * 50% again, the 2nd delete will do nothing.
- *
- * <p> Parameters:
- * <ul>
- * <li> delete.percent.rand.seed - defines the seed to
- * initialize Random (default 1717)
- * </ul>
- */
-public class DeleteByPercentTask extends PerfTask {
-  double percent;
-  int numDeleted = 0;
-  final Random random;
-
-  public DeleteByPercentTask(PerfRunData runData) {
-    super(runData);
-    random = new Random(runData.getConfig().get("delete.percent.rand.seed", 1717));
-  }
-  
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    percent = Double.parseDouble(params)/100;
-  }
-
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-
-  @Override
-  public int doLogic() throws Exception {
-    IndexReader r = getRunData().getIndexReader();
-    int maxDoc = r.maxDoc();
-    int numDeleted = 0;
-    // percent is an absolute target:
-    int numToDelete = ((int) (maxDoc * percent)) - r.numDeletedDocs();
-    if (numToDelete < 0) {
-      r.undeleteAll();
-      numToDelete = (int) (maxDoc * percent);
-    }
-    while (numDeleted < numToDelete) {
-      double delRate = ((double) (numToDelete-numDeleted))/r.numDocs();
-      Bits delDocs = MultiFields.getDeletedDocs(r);
-      int doc = 0;
-      while (doc < maxDoc && numDeleted < numToDelete) {
-        if ((delDocs == null || !delDocs.get(doc)) && random.nextDouble() <= delRate) {
-          r.deleteDocument(doc);
-          numDeleted++;
-          if (delDocs == null) {
-            delDocs = MultiFields.getDeletedDocs(r);
-            assert delDocs != null;
-          }
-        }
-        doc++;
-      }
-    }
-    System.out.println("--> processed (delete) " + numDeleted + " docs");
-    r.decRef();
-    return numDeleted;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/DeleteDocTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/DeleteDocTask.java
deleted file mode 100644
index 52bf501..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/DeleteDocTask.java
+++ /dev/null
@@ -1,90 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.index.IndexReader;
-
-/**
- * Delete a document by docid. If no docid param is supplied, deletes doc with
- * <code>id = last-deleted-doc + doc.delete.step</code>.
- */
-public class DeleteDocTask extends PerfTask {
-
-  /**
-   * Gap between ids of deleted docs, applies when no docid param is provided.
-   */
-  public static final int DEFAULT_DOC_DELETE_STEP = 8;
-  
-  public DeleteDocTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  private int deleteStep = -1;
-  private static int lastDeleted = -1;
-
-  private int docid = -1;
-  private boolean byStep = true;
-  
-  @Override
-  public int doLogic() throws Exception {
-    IndexReader r = getRunData().getIndexReader();
-    r.deleteDocument(docid);
-    lastDeleted = docid;
-    r.decRef();
-    return 1; // one work item done here
-  }
-
-  /* (non-Javadoc)
-   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#setup()
-   */
-  @Override
-  public void setup() throws Exception {
-    super.setup();
-    if (deleteStep<0) {
-      deleteStep = getRunData().getConfig().get("doc.delete.step",DEFAULT_DOC_DELETE_STEP);
-    }
-    // set the docid to be deleted
-    docid = (byStep ? lastDeleted + deleteStep : docid);
-  }
-
-  @Override
-  protected String getLogMessage(int recsCount) {
-    return "deleted " + recsCount + " docs, last deleted: " + lastDeleted;
-  }
-  
-  /**
-   * Set the params (docid only)
-   * @param params docid to delete, or -1 for deleting by delete gap settings.
-   */
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    docid = (int) Float.parseFloat(params);
-    byStep = (docid < 0);
-  }
-  
-  /* (non-Javadoc)
-   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#supportsParams()
-   */
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/FlushReaderTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/FlushReaderTask.java
deleted file mode 100644
index 1a9d8ec..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/FlushReaderTask.java
+++ /dev/null
@@ -1,58 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.index.IndexReader;
-
-public class FlushReaderTask extends PerfTask {
-  String userData = null;
-  
-  public FlushReaderTask(PerfRunData runData) {
-    super(runData);
-  }
-  
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-  
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    userData = params;
-  }
-  
-  @Override
-  public int doLogic() throws IOException {
-    IndexReader reader = getRunData().getIndexReader();
-    if (userData != null) {
-      Map<String,String> map = new HashMap<String,String>();
-      map.put(OpenReaderTask.USER_DATA, userData);
-      reader.flush(map);
-    } else {
-      reader.flush();
-    }
-    reader.decRef();
-    return 1;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NearRealtimeReaderTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NearRealtimeReaderTask.java
deleted file mode 100644
index 398c72f..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NearRealtimeReaderTask.java
+++ /dev/null
@@ -1,118 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.util.ArrayUtil;
-
-/**
- * Spawns a BG thread that periodically (defaults to 3.0
- * seconds, but accepts param in seconds) wakes up and asks
- * IndexWriter for a near real-time reader.  Then runs a
- * single query (body: 1) sorted by docdate, and prints
- * time to reopen and time to run the search.
- *
- * @lucene.experimental It's also not generally usable, eg
- * you cannot change which query is executed.
- */
-public class NearRealtimeReaderTask extends PerfTask {
-
-  long pauseMSec = 3000L;
-
-  int reopenCount;
-  int[] reopenTimes = new int[1];
-
-  public NearRealtimeReaderTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public int doLogic() throws Exception {
-
-    final PerfRunData runData = getRunData();
-
-    // Get initial reader
-    IndexWriter w = runData.getIndexWriter();
-    if (w == null) {
-      throw new RuntimeException("please open the writer before invoking NearRealtimeReader");
-    }
-
-    if (runData.getIndexReader() != null) {
-      throw new RuntimeException("please close the existing reader before invoking NearRealtimeReader");
-    }
-    
-    long t = System.currentTimeMillis();
-    IndexReader r = IndexReader.open(w);
-    runData.setIndexReader(r);
-    // Transfer our reference to runData
-    r.decRef();
-
-    // TODO: gather basic metrics for reporting -- eg mean,
-    // stddev, min/max reopen latencies
-
-    // Parent sequence sets stopNow
-    reopenCount = 0;
-    while(!stopNow) {
-      long waitForMsec = (pauseMSec - (System.currentTimeMillis() - t));
-      if (waitForMsec > 0) {
-        Thread.sleep(waitForMsec);
-        //System.out.println("NRT wait: " + waitForMsec + " msec");
-      }
-
-      t = System.currentTimeMillis();
-      final IndexReader newReader = r.reopen();
-      if (r != newReader) {
-        final int delay = (int) (System.currentTimeMillis()-t);
-        if (reopenTimes.length == reopenCount) {
-          reopenTimes = ArrayUtil.grow(reopenTimes, 1+reopenCount);
-        }
-        reopenTimes[reopenCount++] = delay;
-        // TODO: somehow we need to enable warming, here
-        runData.setIndexReader(newReader);
-        // Transfer our reference to runData
-        newReader.decRef();
-        r = newReader;
-      }
-    }
-    stopNow = false;
-
-    return reopenCount;
-  }
-
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    pauseMSec = (long) (1000.0*Float.parseFloat(params));
-  }
-
-  @Override
-  public void close() {
-    System.out.println("NRT reopen times:");
-    for(int i=0;i<reopenCount;i++) {
-      System.out.print(" " + reopenTimes[i]);
-    }
-    System.out.println();
-  }
-
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewAnalyzerTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewAnalyzerTask.java
deleted file mode 100644
index 6d30114..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewAnalyzerTask.java
+++ /dev/null
@@ -1,101 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-/**
- * Copyright 2005 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.util.Version;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.StringTokenizer;
-import java.lang.reflect.Constructor;
-
-/**
- * Create a new {@link org.apache.lucene.analysis.Analyzer} and set it it in the getRunData() for use by all future tasks.
- *
- */
-public class NewAnalyzerTask extends PerfTask {
-  private List<String> analyzerClassNames;
-  private int current;
-
-  public NewAnalyzerTask(PerfRunData runData) {
-    super(runData);
-    analyzerClassNames = new ArrayList<String>();
-  }
-  
-  public static final Analyzer createAnalyzer(String className) throws Exception{
-    final Class<? extends Analyzer> clazz = Class.forName(className).asSubclass(Analyzer.class);
-    try {
-      // first try to use a ctor with version parameter (needed for many new Analyzers that have no default one anymore
-      Constructor<? extends Analyzer> cnstr = clazz.getConstructor(Version.class);
-      return cnstr.newInstance(Version.LUCENE_CURRENT);
-    } catch (NoSuchMethodException nsme) {
-      // otherwise use default ctor
-      return clazz.newInstance();
-    }
-  }
-
-  @Override
-  public int doLogic() throws IOException {
-    String className = null;
-    try {
-      if (current >= analyzerClassNames.size())
-      {
-        current = 0;
-      }
-      className = analyzerClassNames.get(current++);
-      if (className == null || className.equals(""))
-      {
-        className = "org.apache.lucene.analysis.standard.StandardAnalyzer"; 
-      }
-      if (className.indexOf(".") == -1  || className.startsWith("standard."))//there is no package name, assume o.a.l.analysis
-      {
-        className = "org.apache.lucene.analysis." + className;
-      }
-      getRunData().setAnalyzer(createAnalyzer(className));
-      System.out.println("Changed Analyzer to: " + className);
-    } catch (Exception e) {
-      throw new RuntimeException("Error creating Analyzer: " + className, e);
-    }
-    return 1;
-  }
-
-  /**
-   * Set the params (analyzerClassName only),  Comma-separate list of Analyzer class names.  If the Analyzer lives in
-   * org.apache.lucene.analysis, the name can be shortened by dropping the o.a.l.a part of the Fully Qualified Class Name.
-   * <p/>
-   * Example Declaration: {"NewAnalyzer" NewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer) >
-   * @param params analyzerClassName, or empty for the StandardAnalyzer
-   */
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    for (StringTokenizer tokenizer = new StringTokenizer(params, ","); tokenizer.hasMoreTokens();) {
-      String s = tokenizer.nextToken();
-      analyzerClassNames.add(s.trim());
-    }
-  }
-
-  /* (non-Javadoc)
-   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#supportsParams()
-   */
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewCollationAnalyzerTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewCollationAnalyzerTask.java
deleted file mode 100644
index 2dd29ec..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewCollationAnalyzerTask.java
+++ /dev/null
@@ -1,117 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.lang.reflect.Constructor;
-import java.lang.reflect.Method;
-import java.util.Locale;
-import java.util.StringTokenizer;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-
-/**
- * Task to support benchmarking collation.
- * <p>
- * <ul>
- *  <li> <code>NewCollationAnalyzer</code> with the default jdk impl
- *  <li> <code>NewCollationAnalyzer(impl:icu)</code> specify an impl (jdk,icu)
- * </ul>
- * </p>
- */
-public class NewCollationAnalyzerTask extends PerfTask {
-  public enum Implementation { 
-    JDK("org.apache.lucene.collation.CollationKeyAnalyzer", 
-        "java.text.Collator"),
-    ICU("org.apache.lucene.collation.ICUCollationKeyAnalyzer", 
-        "com.ibm.icu.text.Collator");
-    
-    String className;
-    String collatorClassName;
-    
-    Implementation(String className, String collatorClassName) {
-      this.className = className;
-      this.collatorClassName = collatorClassName;
-    }
-  }
-  
-  private Implementation impl = Implementation.JDK;
-
-  public NewCollationAnalyzerTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  static Analyzer createAnalyzer(Locale locale, Implementation impl)
-      throws Exception {
-    final Class<?> collatorClazz = Class.forName(impl.collatorClassName);
-    Method collatorMethod = collatorClazz.getMethod("getInstance",
-        new Class[] {Locale.class});
-    Object collator = collatorMethod.invoke(null, locale);
-    
-    final Class<? extends Analyzer> clazz = Class.forName(impl.className)
-        .asSubclass(Analyzer.class);
-    Constructor<? extends Analyzer> ctor = clazz.getConstructor(collatorClazz);
-    return ctor.newInstance(collator);
-  }
-  
-  @Override
-  public int doLogic() throws Exception {
-    try {
-      Locale locale = getRunData().getLocale();
-      if (locale == null) throw new RuntimeException(
-          "Locale must be set with the NewLocale task!");
-      Analyzer analyzer = createAnalyzer(locale, impl);
-      getRunData().setAnalyzer(analyzer);
-      System.out.println("Changed Analyzer to: "
-          + analyzer.getClass().getName() + "(" + locale + ")");
-    } catch (Exception e) {
-      throw new RuntimeException("Error creating Analyzer: impl=" + impl, e);
-    }
-    return 1;
-  }
-  
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    
-    StringTokenizer st = new StringTokenizer(params, ",");
-    while (st.hasMoreTokens()) {
-      String param = st.nextToken();
-      StringTokenizer expr = new StringTokenizer(param, ":");
-      String key = expr.nextToken();
-      String value = expr.nextToken();
-      // for now we only support the "impl" parameter.
-      // TODO: add strength, decomposition, etc
-      if (key.equals("impl")) {
-        if (value.equalsIgnoreCase("icu"))
-          impl = Implementation.ICU;
-        else if (value.equalsIgnoreCase("jdk"))
-          impl = Implementation.JDK;
-        else
-          throw new RuntimeException("Unknown parameter " + param);
-      } else {
-        throw new RuntimeException("Unknown parameter " + param);
-      }
-    }
-  }
-
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewLocaleTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewLocaleTask.java
deleted file mode 100644
index 196af26..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewLocaleTask.java
+++ /dev/null
@@ -1,89 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Locale;
-import java.util.StringTokenizer;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-
-/**
- * Set a {@link java.util.Locale} for use in benchmarking.
- * <p>
- * Locales can be specified in the following ways:
- * <ul>
- *  <li><code>de</code>: Language "de"
- *  <li><code>en,US</code>: Language "en", country "US"
- *  <li><code>no,NO,NY</code>: Language "no", country "NO", variant "NY" 
- *  <li><code>ROOT</code>: The root (language-agnostic) Locale
- *  <li>&lt;empty string&gt;: Erase the Locale (null)
- * </ul>
- * </p>
- */
-public class NewLocaleTask extends PerfTask {
-  private String language;
-  private String country;
-  private String variant;
-  
-  /**
-   * Create a new {@link java.util.Locale} and set it it in the getRunData() for
-   * use by all future tasks.
-   */
-  public NewLocaleTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  static Locale createLocale(String language, String country, String variant) {
-    if (language == null || language.length() == 0) 
-      return null;
-    
-    String lang = language;
-    if (lang.equalsIgnoreCase("ROOT"))
-      lang = ""; // empty language is the root locale in the JDK
-      
-    return new Locale(lang, country, variant);
-  }
-  
-  @Override
-  public int doLogic() throws Exception {
-    Locale locale = createLocale(language, country, variant);
-    getRunData().setLocale(locale);
-    System.out.println("Changed Locale to: " + 
-        (locale == null ? "null" : 
-        (locale.getDisplayName().length() == 0) ? "root locale" : locale));
-    return 1;
-  }
-  
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    language = country = variant = "";
-    StringTokenizer st = new StringTokenizer(params, ",");
-    if (st.hasMoreTokens())
-      language = st.nextToken();
-    if (st.hasMoreTokens())
-      country = st.nextToken();
-    if (st.hasMoreTokens())
-      variant = st.nextToken();
-  }
-
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewRoundTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewRoundTask.java
deleted file mode 100644
index ec16979..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewRoundTask.java
+++ /dev/null
@@ -1,46 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/**
- * Increment the counter for properties maintained by Round Number.
- * <br>Other side effects: if there are props by round number, log value change.
- */
-public class NewRoundTask extends PerfTask {
-
-  public NewRoundTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public int doLogic() throws Exception {
-    getRunData().getConfig().newRound();
-    return 0;
-  }
-
-  /* (non-Javadoc)
-   * @see PerfTask#shouldNotRecordStats()
-   */
-  @Override
-  protected boolean shouldNotRecordStats() {
-    return true;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewShingleAnalyzerTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewShingleAnalyzerTask.java
deleted file mode 100644
index 27b805c..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewShingleAnalyzerTask.java
+++ /dev/null
@@ -1,113 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.lang.reflect.Constructor;
-import java.util.StringTokenizer;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.shingle.ShingleAnalyzerWrapper;
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.util.Version;
-
-/**
- * Task to support benchmarking ShingleFilter / ShingleAnalyzerWrapper
- * <p>
- * <ul>
- *  <li> <code>NewShingleAnalyzer</code> (constructs with all defaults)
- *  <li> <code>NewShingleAnalyzer(analyzer:o.a.l.analysis.StandardAnalyzer,maxShingleSize:2,outputUnigrams:true)</code>
- * </ul>
- * </p>
- */
-public class NewShingleAnalyzerTask extends PerfTask {
-
-  private String analyzerClassName = "standard.StandardAnalyzer";
-  private int maxShingleSize = 2;
-  private boolean outputUnigrams = true;
-  
-  public NewShingleAnalyzerTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  private void setAnalyzer() throws Exception {
-    Class<? extends Analyzer> clazz = null;
-    Analyzer wrappedAnalyzer;
-    try {
-      if (analyzerClassName == null || analyzerClassName.equals("")) {
-        analyzerClassName 
-          = "org.apache.lucene.analysis.standard.StandardAnalyzer"; 
-      }
-      if (analyzerClassName.indexOf(".") == -1 
-          || analyzerClassName.startsWith("standard.")) {
-        //there is no package name, assume o.a.l.analysis
-        analyzerClassName = "org.apache.lucene.analysis." + analyzerClassName;
-      }
-      clazz = Class.forName(analyzerClassName).asSubclass(Analyzer.class);
-      // first try to use a ctor with version parameter (needed for many new 
-      // Analyzers that have no default one anymore)
-      Constructor<? extends Analyzer> ctor = clazz.getConstructor(Version.class);
-      wrappedAnalyzer = ctor.newInstance(Version.LUCENE_CURRENT);
-    } catch (NoSuchMethodException e) {
-      // otherwise use default ctor
-      wrappedAnalyzer = clazz.newInstance();
-    }
-    ShingleAnalyzerWrapper analyzer 
-      = new ShingleAnalyzerWrapper(wrappedAnalyzer, maxShingleSize);
-    analyzer.setOutputUnigrams(outputUnigrams);
-    getRunData().setAnalyzer(analyzer);
-  }
-  
-  @Override
-  public int doLogic() throws Exception {
-    try {
-      setAnalyzer();
-      System.out.println
-        ("Changed Analyzer to: ShingleAnalyzerWrapper, wrapping ShingleFilter over" 
-         + analyzerClassName);
-    } catch (Exception e) {
-      throw new RuntimeException("Error creating Analyzer", e);
-    }
-    return 1;
-  }
-  
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    StringTokenizer st = new StringTokenizer(params, ",");
-    while (st.hasMoreTokens()) {
-      String param = st.nextToken();
-      StringTokenizer expr = new StringTokenizer(param, ":");
-      String key = expr.nextToken();
-      String value = expr.nextToken();
-      if (key.equalsIgnoreCase("analyzer")) {
-        analyzerClassName = value;
-      } else if (key.equalsIgnoreCase("outputUnigrams")) {
-        outputUnigrams = Boolean.parseBoolean(value);
-      } else if (key.equalsIgnoreCase("maxShingleSize")) {
-        maxShingleSize = (int)Double.parseDouble(value);
-      } else {
-        throw new RuntimeException("Unknown parameter " + param);
-      }
-    }
-  }
-
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenIndexTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenIndexTask.java
deleted file mode 100644
index fe61e44..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenIndexTask.java
+++ /dev/null
@@ -1,82 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexCommit;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.LogMergePolicy;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import java.io.IOException;
-
-
-/**
- * Open an index writer.
- * <br>Other side effects: index writer object in perfRunData is set.
- * <br>Relevant properties: <code>merge.factor, max.buffered,
- * max.field.length, ram.flush.mb [default 0]</code>.
- *
- * <p> Accepts a param specifying the commit point as
- * previously saved with CommitIndexTask.  If you specify
- * this, it rolls the index back to that commit on opening
- * the IndexWriter.
- */
-public class OpenIndexTask extends PerfTask {
-
-  public static final int DEFAULT_MAX_BUFFERED = IndexWriterConfig.DEFAULT_MAX_BUFFERED_DOCS;
-  public static final int DEFAULT_MAX_FIELD_LENGTH = IndexWriterConfig.UNLIMITED_FIELD_LENGTH;
-  public static final int DEFAULT_MERGE_PFACTOR = LogMergePolicy.DEFAULT_MERGE_FACTOR;
-  public static final double DEFAULT_RAM_FLUSH_MB = (int) IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB;
-  private String commitUserData;
-
-  public OpenIndexTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public int doLogic() throws IOException {
-    PerfRunData runData = getRunData();
-    Config config = runData.getConfig();
-    final IndexCommit ic;
-    if (commitUserData != null) {
-      ic = OpenReaderTask.findIndexCommit(runData.getDirectory(), commitUserData);
-    } else {
-      ic = null;
-    }
-    
-    final IndexWriter writer = CreateIndexTask.configureWriter(config, runData, OpenMode.APPEND, ic);
-    runData.setIndexWriter(writer);
-    return 1;
-  }
-
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    if (params != null) {
-      // specifies which commit point to open
-      commitUserData = params;
-    }
-  }
-
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenReaderTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenReaderTask.java
deleted file mode 100644
index 8bc1c94..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenReaderTask.java
+++ /dev/null
@@ -1,105 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Map;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.index.IndexCommit;
-import org.apache.lucene.index.IndexDeletionPolicy;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.store.Directory;
-
-/**
- * Open an index reader.
- * <br>Other side effects: index reader object in perfRunData is set.
- * <br> Optional params readOnly,commitUserData eg. OpenReader(false,commit1)
- */
-public class OpenReaderTask extends PerfTask {
-  public static final String USER_DATA = "userData";
-  private boolean readOnly = true;
-  private String commitUserData = null;
-
-  public OpenReaderTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public int doLogic() throws IOException {
-    Directory dir = getRunData().getDirectory();
-    Config config = getRunData().getConfig();
-    IndexReader r = null;
-    final IndexDeletionPolicy deletionPolicy;
-    if (readOnly) {
-      deletionPolicy = null;
-    } else {
-      deletionPolicy = CreateIndexTask.getIndexDeletionPolicy(config);
-    }
-    if (commitUserData != null) {
-      r = IndexReader.open(OpenReaderTask.findIndexCommit(dir, commitUserData),
-                           deletionPolicy,
-                           readOnly); 
-    } else {
-      r = IndexReader.open(dir,
-                           deletionPolicy,
-                           readOnly); 
-    }
-    getRunData().setIndexReader(r);
-    // We transfer reference to the run data
-    r.decRef();
-    return 1;
-  }
- 
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    if (params != null) {
-      String[] split = params.split(",");
-      if (split.length > 0) {
-        readOnly = Boolean.valueOf(split[0]).booleanValue();
-      }
-      if (split.length > 1) {
-        commitUserData = split[1];
-      }
-    }
-  }
-
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-
-  public static IndexCommit findIndexCommit(Directory dir, String userData) throws IOException {
-    Collection<IndexCommit> commits = IndexReader.listCommits(dir);
-    for (final IndexCommit ic : commits) {
-      Map<String,String> map = ic.getUserData();
-      String ud = null;
-      if (map != null) {
-        ud = map.get(USER_DATA);
-      }
-      if (ud != null && ud.equals(userData)) {
-        return ic;
-      }
-    }
-
-    throw new IOException("index does not contain commit with userData: " + userData);
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OptimizeTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OptimizeTask.java
deleted file mode 100644
index 19947b6..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OptimizeTask.java
+++ /dev/null
@@ -1,53 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.index.IndexWriter;
-
-/**
- * Optimize the index.
- * <br>Other side effects: none.
- */
-public class OptimizeTask extends PerfTask {
-
-  public OptimizeTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  int maxNumSegments = 1;
-
-  @Override
-  public int doLogic() throws Exception {
-    IndexWriter iw = getRunData().getIndexWriter();
-    iw.optimize(maxNumSegments);
-    //System.out.println("optimize called");
-    return 1;
-  }
-
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    maxNumSegments = Double.valueOf(params).intValue();
-  }
-
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PerfTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PerfTask.java
deleted file mode 100644
index 7ac051b..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PerfTask.java
+++ /dev/null
@@ -1,320 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.text.NumberFormat;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.stats.Points;
-import org.apache.lucene.benchmark.byTask.stats.TaskStats;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-
-/**
- * An abstract task to be tested for performance. <br>
- * Every performance task extends this class, and provides its own
- * {@link #doLogic()} method, which performs the actual task. <br>
- * Tasks performing some work that should be measured for the task, can override
- * {@link #setup()} and/or {@link #tearDown()} and place that work there. <br>
- * Relevant properties: <code>task.max.depth.log</code>.<br>
- * Also supports the following logging attributes:
- * <ul>
- * <li>log.step - specifies how often to log messages about the current running
- * task. Default is 1000 {@link #doLogic()} invocations. Set to -1 to disable
- * logging.
- * <li>log.step.[class Task Name] - specifies the same as 'log.step', only for a
- * particular task name. For example, log.step.AddDoc will be applied only for
- * {@link AddDocTask}, but not for {@link DeleteDocTask}. It's a way to control
- * per task logging settings. If you want to omit logging for any other task,
- * include log.step=-1. The syntax is "log.step." together with the Task's
- * 'short' name (i.e., without the 'Task' part).
- * </ul>
- */
-public abstract class PerfTask implements Cloneable {
-
-  static final int DEFAULT_LOG_STEP = 1000;
-  
-  private PerfRunData runData;
-  
-  // propeties that all tasks have
-  private String name;
-  private int depth = 0;
-  protected int logStep;
-  private int logStepCount = 0;
-  private int maxDepthLogStart = 0;
-  private boolean disableCounting = false;
-  protected String params = null;
-
-  private boolean runInBackground;
-  private int deltaPri;
-
-  protected static final String NEW_LINE = System.getProperty("line.separator");
-
-  /** Should not be used externally */
-  private PerfTask() {
-    name = getClass().getSimpleName();
-    if (name.endsWith("Task")) {
-      name = name.substring(0, name.length() - 4);
-    }
-  }
-
-  public void setRunInBackground(int deltaPri) {
-    runInBackground = true;
-    this.deltaPri = deltaPri;
-  }
-
-  public boolean getRunInBackground() {
-    return runInBackground;
-  }
-
-  public int getBackgroundDeltaPriority() {
-    return deltaPri;
-  }
-
-  protected volatile boolean stopNow;
-
-  public void stopNow() {
-    stopNow = true;
-  }
-
-  public PerfTask(PerfRunData runData) {
-    this();
-    this.runData = runData;
-    Config config = runData.getConfig();
-    this.maxDepthLogStart = config.get("task.max.depth.log",0);
-
-    String logStepAtt = "log.step";
-    String taskLogStepAtt = "log.step." + name;
-    if (config.get(taskLogStepAtt, null) != null) {
-      logStepAtt = taskLogStepAtt;
-    }
-
-    // It's important to read this from Config, to support vals-by-round.
-    logStep = config.get(logStepAtt, DEFAULT_LOG_STEP);
-    // To avoid the check 'if (logStep > 0)' in tearDown(). This effectively
-    // turns logging off.
-    if (logStep <= 0) {
-      logStep = Integer.MAX_VALUE;
-    }
-  }
-  
-  @Override
-  protected Object clone() throws CloneNotSupportedException {
-    // tasks having non primitive data structures should override this.
-    // otherwise parallel running of a task sequence might not run correctly. 
-    return super.clone();
-  }
-
-  public void close() throws Exception {
-  }
-
-  /**
-   * Run the task, record statistics.
-   * @return number of work items done by this task.
-   */
-  public final int runAndMaybeStats(boolean reportStats) throws Exception {
-    if (!reportStats || shouldNotRecordStats()) {
-      setup();
-      int count = doLogic();
-      count = disableCounting ? 0 : count;
-      tearDown();
-      return count;
-    }
-    if (reportStats && depth <= maxDepthLogStart && !shouldNeverLogAtStart()) {
-      System.out.println("------------> starting task: " + getName());
-    }
-    setup();
-    Points pnts = runData.getPoints();
-    TaskStats ts = pnts.markTaskStart(this, runData.getConfig().getRoundNumber());
-    int count = doLogic();
-    count = disableCounting ? 0 : count;
-    pnts.markTaskEnd(ts, count);
-    tearDown();
-    return count;
-  }
-
-  /**
-   * Perform the task once (ignoring repetitions specification)
-   * Return number of work items done by this task.
-   * For indexing that can be number of docs added.
-   * For warming that can be number of scanned items, etc.
-   * @return number of work items done by this task.
-   */
-  public abstract int doLogic() throws Exception;
-  
-  /**
-   * @return Returns the name.
-   */
-  public String getName() {
-    if (params==null) {
-      return name;
-    } 
-    return new StringBuilder(name).append('(').append(params).append(')').toString();
-  }
-
-  /**
-   * @param name The name to set.
-   */
-  protected void setName(String name) {
-    this.name = name;
-  }
-
-  /**
-   * @return Returns the run data.
-   */
-  public PerfRunData getRunData() {
-    return runData;
-  }
-
-  /**
-   * @return Returns the depth.
-   */
-  public int getDepth() {
-    return depth;
-  }
-
-  /**
-   * @param depth The depth to set.
-   */
-  public void setDepth(int depth) {
-    this.depth = depth;
-  }
-  
-  // compute a blank string padding for printing this task indented by its depth  
-  String getPadding () {
-    char c[] = new char[4*getDepth()];
-    for (int i = 0; i < c.length; i++) c[i] = ' ';
-    return new String(c);
-  }
-  
-  /* (non-Javadoc)
-   * @see java.lang.Object#toString()
-   */
-  @Override
-  public String toString() {
-    String padd = getPadding();
-    StringBuilder sb = new StringBuilder(padd);
-    if (disableCounting) {
-      sb.append('-');
-    }
-    sb.append(getName());
-    if (getRunInBackground()) {
-      sb.append(" &");
-      int x = getBackgroundDeltaPriority();
-      if (x != 0) {
-        sb.append(x);
-      }
-    }
-    return sb.toString();
-  }
-
-  /**
-   * @return Returns the maxDepthLogStart.
-   */
-  int getMaxDepthLogStart() {
-    return maxDepthLogStart;
-  }
-
-  protected String getLogMessage(int recsCount) {
-    return "processed " + recsCount + " records";
-  }
-  
-  /**
-   * Tasks that should never log at start can override this.  
-   * @return true if this task should never log when it start.
-   */
-  protected boolean shouldNeverLogAtStart () {
-    return false;
-  }
-  
-  /**
-   * Tasks that should not record statistics can override this.  
-   * @return true if this task should never record its statistics.
-   */
-  protected boolean shouldNotRecordStats () {
-    return false;
-  }
-
-  /**
-   * Task setup work that should not be measured for that specific task.
-   * By default it does nothing, but tasks can implement this, moving work from 
-   * doLogic() to this method. Only the work done in doLogicis measured for this task.
-   * Notice that higher level (sequence) tasks containing this task would then 
-   * measure larger time than the sum of their contained tasks.
-   * @throws Exception 
-   */
-  public void setup () throws Exception {
-  }
-  
-  /**
-   * Task tearDown work that should not be measured for that specific task.
-   * By default it does nothing, but tasks can implement this, moving work from 
-   * doLogic() to this method. Only the work done in doLogicis measured for this task.
-   * Notice that higher level (sequence) tasks containing this task would then 
-   * measure larger time than the sum of their contained tasks.
-   */
-  public void tearDown() throws Exception {
-    if (++logStepCount % logStep == 0) {
-      double time = (System.currentTimeMillis() - runData.getStartTimeMillis()) / 1000.0;
-      NumberFormat nf = NumberFormat.getInstance();
-      nf.setMaximumFractionDigits(2);
-      System.out.println(nf.format(time) + " sec --> "
-          + Thread.currentThread().getName() + " " + getLogMessage(logStepCount));
-    }
-  }
-
-  /**
-   * Sub classes that supports parameters must override this method to return true.
-   * @return true iff this task supports command line params.
-   */
-  public boolean supportsParams () {
-    return false;
-  }
-  
-  /**
-   * Set the params of this task.
-   * @exception UnsupportedOperationException for tasks supporting command line parameters.
-   */
-  public void setParams(String params) {
-    if (!supportsParams()) {
-      throw new UnsupportedOperationException(getName()+" does not support command line parameters.");
-    }
-    this.params = params;
-  }
-  
-  /**
-   * @return Returns the Params.
-   */
-  public String getParams() {
-    return params;
-  }
-
-  /**
-   * Return true if counting is disabled for this task.
-   */
-  public boolean isDisableCounting() {
-    return disableCounting;
-  }
-
-  /**
-   * See {@link #isDisableCounting()}
-   */
-  public void setDisableCounting(boolean disableCounting) {
-    this.disableCounting = disableCounting;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PrintReaderTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PrintReaderTask.java
deleted file mode 100644
index f8d9f96..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PrintReaderTask.java
+++ /dev/null
@@ -1,56 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.store.Directory;
-
-public class PrintReaderTask extends PerfTask {
-  private String userData = null;
-  
-  public PrintReaderTask(PerfRunData runData) {
-    super(runData);
-  }
-  
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    userData = params;
-  }
-  
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-  
-  @Override
-  public int doLogic() throws Exception {
-    Directory dir = getRunData().getDirectory();
-    IndexReader r = null;
-    if (userData == null) 
-      r = IndexReader.open(dir, true);
-    else
-      r = IndexReader.open(OpenReaderTask.findIndexCommit(dir, userData),
-                           null,
-                           true);
-    System.out.println("--> numDocs:"+r.numDocs()+" dels:"+r.numDeletedDocs());
-    r.close();
-    return 1;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java
deleted file mode 100644
index b684814..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java
+++ /dev/null
@@ -1,307 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.HashSet;
-
-import java.util.List;
-import java.util.Set;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.feeds.QueryMaker;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Fieldable;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.TopFieldCollector;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.TopScoreDocCollector;
-import org.apache.lucene.search.Weight;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.Bits;
-
-
-/**
- * Read index (abstract) task.
- * Sub classes implement withSearch(), withWarm(), withTraverse() and withRetrieve()
- * methods to configure the actual action.
- * <p/>
- * <p>Note: All ReadTasks reuse the reader if it is already open.
- * Otherwise a reader is opened at start and closed at the end.
- * <p>
- * The <code>search.num.hits</code> config parameter sets
- * the top number of hits to collect during searching.  If
- * <code>print.hits.field</code> is set, then each hit is
- * printed along with the value of that field.</p>
- *
- * <p>Other side effects: none.
- */
-public abstract class ReadTask extends PerfTask {
-
-  private final QueryMaker queryMaker;
-
-  public ReadTask(PerfRunData runData) {
-    super(runData);
-    if (withSearch()) {
-      queryMaker = getQueryMaker();
-    } else {
-      queryMaker = null;
-    }
-  }
-  @Override
-  public int doLogic() throws Exception {
-    int res = 0;
-
-    // open reader or use existing one
-    IndexSearcher searcher = getRunData().getIndexSearcher();
-
-    IndexReader reader;
-
-    final boolean closeSearcher;
-    if (searcher == null) {
-      // open our own reader
-      Directory dir = getRunData().getDirectory();
-      reader = IndexReader.open(dir, true);
-      searcher = new IndexSearcher(reader);
-      closeSearcher = true;
-    } else {
-      // use existing one; this passes +1 ref to us
-      reader = searcher.getIndexReader();
-      closeSearcher = false;
-    }
-
-    // optionally warm and add num docs traversed to count
-    if (withWarm()) {
-      Document doc = null;
-      Bits delDocs = reader.getDeletedDocs();
-      for (int m = 0; m < reader.maxDoc(); m++) {
-        if (!delDocs.get(m)) {
-          doc = reader.document(m);
-          res += (doc == null ? 0 : 1);
-        }
-      }
-    }
-
-    if (withSearch()) {
-      res++;
-      Query q = queryMaker.makeQuery();
-      Sort sort = getSort();
-      TopDocs hits = null;
-      final int numHits = numHits();
-      if (numHits > 0) {
-        if (withCollector() == false) {
-          if (sort != null) {
-            Weight w = q.weight(searcher);
-            TopFieldCollector collector = TopFieldCollector.create(sort, numHits,
-                                                                   true, withScore(),
-                                                                   withMaxScore(),
-                                                                   !w.scoresDocsOutOfOrder());
-            searcher.search(w, null, collector);
-            hits = collector.topDocs();
-          } else {
-            hits = searcher.search(q, numHits);
-          }
-        } else {
-          Collector collector = createCollector();
-          searcher.search(q, null, collector);
-          //hits = collector.topDocs();
-        }
-
-        final String printHitsField = getRunData().getConfig().get("print.hits.field", null);
-        if (hits != null && printHitsField != null && printHitsField.length() > 0) {
-          if (q instanceof MultiTermQuery) {
-            System.out.println("MultiTermQuery term count = " + ((MultiTermQuery) q).getTotalNumberOfTerms());
-          }
-          System.out.println("totalHits = " + hits.totalHits);
-          System.out.println("maxDoc()  = " + reader.maxDoc());
-          System.out.println("numDocs() = " + reader.numDocs());
-          for(int i=0;i<hits.scoreDocs.length;i++) {
-            final int docID = hits.scoreDocs[i].doc;
-            final Document doc = reader.document(docID);
-            System.out.println("  " + i + ": doc=" + docID + " score=" + hits.scoreDocs[i].score + " " + printHitsField + " =" + doc.get(printHitsField));
-          }
-        }
-
-        if (withTraverse()) {
-          final ScoreDoc[] scoreDocs = hits.scoreDocs;
-          int traversalSize = Math.min(scoreDocs.length, traversalSize());
-
-          if (traversalSize > 0) {
-            boolean retrieve = withRetrieve();
-            int numHighlight = Math.min(numToHighlight(), scoreDocs.length);
-            Analyzer analyzer = getRunData().getAnalyzer();
-            BenchmarkHighlighter highlighter = null;
-            if (numHighlight > 0) {
-              highlighter = getBenchmarkHighlighter(q);
-            }
-            for (int m = 0; m < traversalSize; m++) {
-              int id = scoreDocs[m].doc;
-              res++;
-              if (retrieve) {
-                Document document = retrieveDoc(reader, id);
-                res += document != null ? 1 : 0;
-                if (numHighlight > 0 && m < numHighlight) {
-                  Collection<String> fieldsToHighlight = getFieldsToHighlight(document);
-                  for (final String field : fieldsToHighlight) {
-                    String text = document.get(field);
-                    res += highlighter.doHighlight(reader, id, field, document, analyzer, text);
-                  }
-                }
-              }
-            }
-          }
-        }
-      }
-    }
-
-    if (closeSearcher) {
-      searcher.close();
-      reader.close();
-    } else {
-      // Release our +1 ref from above
-      reader.decRef();
-    }
-    return res;
-  }
-
-  protected Collector createCollector() throws Exception {
-    return TopScoreDocCollector.create(numHits(), true);
-  }
-
-
-  protected Document retrieveDoc(IndexReader ir, int id) throws IOException {
-    return ir.document(id);
-  }
-
-  /**
-   * Return query maker used for this task.
-   */
-  public abstract QueryMaker getQueryMaker();
-
-  /**
-   * Return true if search should be performed.
-   */
-  public abstract boolean withSearch();
-
-  public boolean withCollector(){
-    return false;
-  }
-  
-
-  /**
-   * Return true if warming should be performed.
-   */
-  public abstract boolean withWarm();
-
-  /**
-   * Return true if, with search, results should be traversed.
-   */
-  public abstract boolean withTraverse();
-
-  /** Whether scores should be computed (only useful with
-   *  field sort) */
-  public boolean withScore() {
-    return true;
-  }
-
-  /** Whether maxScores should be computed (only useful with
-   *  field sort) */
-  public boolean withMaxScore() {
-    return true;
-  }
-
-  /**
-   * Specify the number of hits to traverse.  Tasks should override this if they want to restrict the number
-   * of hits that are traversed when {@link #withTraverse()} is true. Must be greater than 0.
-   * <p/>
-   * Read task calculates the traversal as: Math.min(hits.length(), traversalSize())
-   *
-   * @return Integer.MAX_VALUE
-   */
-  public int traversalSize() {
-    return Integer.MAX_VALUE;
-  }
-
-  static final int DEFAULT_SEARCH_NUM_HITS = 10;
-  private int numHits;
-
-  @Override
-  public void setup() throws Exception {
-    super.setup();
-    numHits = getRunData().getConfig().get("search.num.hits", DEFAULT_SEARCH_NUM_HITS);
-  }
-
-  /**
-   * Specify the number of hits to retrieve.  Tasks should override this if they want to restrict the number
-   * of hits that are collected during searching. Must be greater than 0.
-   *
-   * @return 10 by default, or search.num.hits config if set.
-   */
-  public int numHits() {
-    return numHits;
-  }
-
-  /**
-   * Return true if, with search & results traversing, docs should be retrieved.
-   */
-  public abstract boolean withRetrieve();
-
-  /**
-   * Set to the number of documents to highlight.
-   *
-   * @return The number of the results to highlight.  O means no docs will be highlighted.
-   */
-  public int numToHighlight() {
-    return 0;
-  }
-
-  /**
-   * Return an appropriate highlighter to be used with
-   * highlighting tasks
-   */
-  protected BenchmarkHighlighter getBenchmarkHighlighter(Query q){
-    return null;
-  }
-  
-  protected Sort getSort() {
-    return null;
-  }
-
-  /**
-   * Define the fields to highlight.  Base implementation returns all fields
-   * @param document The Document
-   * @return A Collection of Field names (Strings)
-   */
-  protected Collection<String> getFieldsToHighlight(Document document) {
-    List<Fieldable> fieldables = document.getFields();
-    Set<String> result = new HashSet<String>(fieldables.size());
-    for (final Fieldable fieldable : fieldables) {
-      result.add(fieldable.name());
-    }
-    return result;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask.java
deleted file mode 100644
index fa0ae99..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask.java
+++ /dev/null
@@ -1,146 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-import java.util.List;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Fieldable;
-import org.apache.lucene.document.NumericField;
-
-/**
- * Simple task to test performance of tokenizers.  It just
- * creates a token stream for each field of the document and
- * read all tokens out of that stream.
- */
-public class ReadTokensTask extends PerfTask {
-
-  public ReadTokensTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  private int totalTokenCount = 0;
-  
-  // volatile data passed between setup(), doLogic(), tearDown().
-  private Document doc = null;
-  
-  @Override
-  public void setup() throws Exception {
-    super.setup();
-    DocMaker docMaker = getRunData().getDocMaker();
-    doc = docMaker.makeDocument();
-  }
-
-  @Override
-  protected String getLogMessage(int recsCount) {
-    return "read " + recsCount + " docs; " + totalTokenCount + " tokens";
-  }
-  
-  @Override
-  public void tearDown() throws Exception {
-    doc = null;
-    super.tearDown();
-  }
-
-  @Override
-  public int doLogic() throws Exception {
-    List<Fieldable> fields = doc.getFields();
-    Analyzer analyzer = getRunData().getAnalyzer();
-    int tokenCount = 0;
-    for(final Fieldable field : fields) {
-      if (!field.isTokenized() || field instanceof NumericField) continue;
-      
-      final TokenStream stream;
-      final TokenStream streamValue = field.tokenStreamValue();
-
-      if (streamValue != null) 
-        stream = streamValue;
-      else {
-        // the field does not have a TokenStream,
-        // so we have to obtain one from the analyzer
-        final Reader reader;			  // find or make Reader
-        final Reader readerValue = field.readerValue();
-
-        if (readerValue != null)
-          reader = readerValue;
-        else {
-          String stringValue = field.stringValue();
-          if (stringValue == null)
-            throw new IllegalArgumentException("field must have either TokenStream, String or Reader value");
-          stringReader.init(stringValue);
-          reader = stringReader;
-        }
-        
-        // Tokenize field
-        stream = analyzer.reusableTokenStream(field.name(), reader);
-      }
-
-      // reset the TokenStream to the first token
-      stream.reset();
-
-      while(stream.incrementToken())
-        tokenCount++;
-    }
-    totalTokenCount += tokenCount;
-    return tokenCount;
-  }
-
-  /* Simple StringReader that can be reset to a new string;
-   * we use this when tokenizing the string value from a
-   * Field. */
-  ReusableStringReader stringReader = new ReusableStringReader();
-
-  private final static class ReusableStringReader extends Reader {
-    int upto;
-    int left;
-    String s;
-    void init(String s) {
-      this.s = s;
-      left = s.length();
-      this.upto = 0;
-    }
-    @Override
-    public int read(char[] c) {
-      return read(c, 0, c.length);
-    }
-    @Override
-    public int read(char[] c, int off, int len) {
-      if (left > len) {
-        s.getChars(upto, upto+len, c, off);
-        upto += len;
-        left -= len;
-        return len;
-      } else if (0 == left) {
-        return -1;
-      } else {
-        s.getChars(upto, upto+left, c, off);
-        int r = left;
-        left = 0;
-        upto = s.length();
-        return r;
-      }
-    }
-    @Override
-    public void close() {}
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReopenReaderTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReopenReaderTask.java
deleted file mode 100644
index 10198c5..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReopenReaderTask.java
+++ /dev/null
@@ -1,45 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import java.io.IOException;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.index.IndexReader;
-
-/**
-* Reopens IndexReader and closes old IndexReader.
-*
-*/
-public class ReopenReaderTask extends PerfTask {
-  public ReopenReaderTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public int doLogic() throws IOException {
-    IndexReader r = getRunData().getIndexReader();
-    IndexReader nr = r.reopen();
-    if (nr != r) {
-      getRunData().setIndexReader(nr);
-      nr.decRef();
-    }
-    r.decRef();
-    return 1;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepAllTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepAllTask.java
deleted file mode 100644
index 89ae30d..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepAllTask.java
+++ /dev/null
@@ -1,76 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.List;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.stats.Report;
-import org.apache.lucene.benchmark.byTask.stats.TaskStats;
-
-/**
- * Report all statistics with no aggregations.
- * <br>Other side effects: None.
- */
-public class RepAllTask extends ReportTask {
-
-  public RepAllTask(PerfRunData runData) {
-    super(runData);
-   }
-
-  @Override
-  public int doLogic() throws Exception {
-    Report rp = reportAll(getRunData().getPoints().taskStats());
-    
-    System.out.println();
-    System.out.println("------------> Report All ("+rp.getSize()+" out of "+rp.getOutOf()+")");
-    System.out.println(rp.getText());
-    System.out.println();
-    return 0;
-  }
-  
-  /**
-   * Report detailed statistics as a string
-   * @return the report
-   */
-  protected Report reportAll(List<TaskStats> taskStats) {
-    String longestOp = longestOp(taskStats);
-    boolean first = true;
-    StringBuilder sb = new StringBuilder();
-    sb.append(tableTitle(longestOp));
-    sb.append(newline);
-    int reported = 0;
-    for (final TaskStats stat : taskStats) {
-      if (stat.getElapsed()>=0) { // consider only tasks that ended
-        if (!first) {
-          sb.append(newline);
-        }
-        first = false;
-        String line = taskReportLine(longestOp, stat);
-        reported++;
-        if (taskStats.size()>2 && reported%2==0) {
-          line = line.replaceAll("   "," - ");
-        }
-        sb.append(line);
-      }
-    }
-    String reptxt = (reported==0 ? "No Matching Entries Were Found!" : sb.toString());
-    return new Report(reptxt,reported,reported,taskStats.size());
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSelectByPrefTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSelectByPrefTask.java
deleted file mode 100644
index 30c1b60..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSelectByPrefTask.java
+++ /dev/null
@@ -1,74 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.List;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.stats.Report;
-import org.apache.lucene.benchmark.byTask.stats.TaskStats;
-
-/**
- * Report by-name-prefix statistics with no aggregations.
- * <br>Other side effects: None.
- */
-public class RepSelectByPrefTask extends RepSumByPrefTask {
-
-  public RepSelectByPrefTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public int doLogic() throws Exception {
-    Report rp = reportSelectByPrefix(getRunData().getPoints().taskStats());
-    
-    System.out.println();
-    System.out.println("------------> Report Select By Prefix ("+prefix+") ("+
-        rp.getSize()+" about "+rp.getReported()+" out of "+rp.getOutOf()+")");
-    System.out.println(rp.getText());
-    System.out.println();
-
-    return 0;
-  }
-  
-  protected Report reportSelectByPrefix(List<TaskStats> taskStats) {
-    String longestOp = longestOp(taskStats);
-    boolean first = true;
-    StringBuilder sb = new StringBuilder();
-    sb.append(tableTitle(longestOp));
-    sb.append(newline);
-    int reported = 0;
-    for (final TaskStats stat : taskStats) {
-      if (stat.getElapsed()>=0 && stat.getTask().getName().startsWith(prefix)) { // only ended tasks with proper name
-        reported++;
-        if (!first) {
-          sb.append(newline);
-        }
-        first = false;
-        String line = taskReportLine(longestOp,stat);
-        if (taskStats.size()>2 && reported%2==0) {
-          line = line.replaceAll("   "," - ");
-        }
-        sb.append(line);
-      }
-    }
-    String reptxt = (reported==0 ? "No Matching Entries Were Found!" : sb.toString());
-    return new Report(reptxt,reported,reported, taskStats.size());
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameRoundTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameRoundTask.java
deleted file mode 100644
index 2fd31f3..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameRoundTask.java
+++ /dev/null
@@ -1,80 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.LinkedHashMap;
-import java.util.List;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.stats.Report;
-import org.apache.lucene.benchmark.byTask.stats.TaskStats;
-
-/**
- * Report all statistics grouped/aggregated by name and round.
- * <br>Other side effects: None.
- */
-public class RepSumByNameRoundTask extends ReportTask {
-
-  public RepSumByNameRoundTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public int doLogic() throws Exception {
-    Report rp = reportSumByNameRound(getRunData().getPoints().taskStats());
-
-    System.out.println();
-    System.out.println("------------> Report Sum By (any) Name and Round ("+
-        rp.getSize()+" about "+rp.getReported()+" out of "+rp.getOutOf()+")");
-    System.out.println(rp.getText());
-    System.out.println();
-    
-    return 0;
-  }
-
-  /**
-   * Report statistics as a string, aggregate for tasks named the same, and from the same round.
-   * @return the report
-   */
-  protected Report reportSumByNameRound(List<TaskStats> taskStats) {
-    // aggregate by task name and round
-    LinkedHashMap<String,TaskStats> p2 = new LinkedHashMap<String,TaskStats>();
-    int reported = 0;
-    for (final TaskStats stat1 : taskStats) {
-      if (stat1.getElapsed()>=0) { // consider only tasks that ended
-        reported++;
-        String name = stat1.getTask().getName();
-        String rname = stat1.getRound()+"."+name; // group by round
-        TaskStats stat2 = p2.get(rname);
-        if (stat2 == null) {
-          try {
-            stat2 = (TaskStats) stat1.clone();
-          } catch (CloneNotSupportedException e) {
-            throw new RuntimeException(e);
-          }
-          p2.put(rname,stat2);
-        } else {
-          stat2.add(stat1);
-        }
-      }
-    }
-    // now generate report from secondary list p2    
-    return genPartialReport(reported, p2, taskStats.size());
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameTask.java
deleted file mode 100644
index a55d26d..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameTask.java
+++ /dev/null
@@ -1,80 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.LinkedHashMap;
-import java.util.List;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.stats.Report;
-import org.apache.lucene.benchmark.byTask.stats.TaskStats;
-
-/**
- * Report all statistics aggregated by name.
- * <br>Other side effects: None.
- */
-public class RepSumByNameTask extends ReportTask {
-
-  public RepSumByNameTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public int doLogic() throws Exception {
-    Report rp = reportSumByName(getRunData().getPoints().taskStats());
-
-    System.out.println();
-    System.out.println("------------> Report Sum By (any) Name ("+
-        rp.getSize()+" about "+rp.getReported()+" out of "+rp.getOutOf()+")");
-    System.out.println(rp.getText());
-    System.out.println();
-
-    return 0;
-  }
-
-  /**
-   * Report statistics as a string, aggregate for tasks named the same.
-   * @return the report
-   */
-  protected Report reportSumByName(List<TaskStats> taskStats) {
-    // aggregate by task name
-    int reported = 0;
-    LinkedHashMap<String,TaskStats> p2 = new LinkedHashMap<String,TaskStats>();
-    for (final TaskStats stat1: taskStats) {
-      if (stat1.getElapsed()>=0) { // consider only tasks that ended
-        reported++;
-        String name = stat1.getTask().getName();
-        TaskStats stat2 = p2.get(name);
-        if (stat2 == null) {
-          try {
-            stat2 = (TaskStats) stat1.clone();
-          } catch (CloneNotSupportedException e) {
-            throw new RuntimeException(e);
-          }
-          p2.put(name,stat2);
-        } else {
-          stat2.add(stat1);
-        }
-      }
-    }
-    // now generate report from secondary list p2    
-    return genPartialReport(reported, p2, taskStats.size());
-  }
-
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefRoundTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefRoundTask.java
deleted file mode 100644
index 070927b..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefRoundTask.java
+++ /dev/null
@@ -1,77 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.LinkedHashMap;
-import java.util.List;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.stats.Report;
-import org.apache.lucene.benchmark.byTask.stats.TaskStats;
-
-/**
- * Report all prefix matching statistics grouped/aggregated by name and round.
- * <br>Other side effects: None.
- */
-public class RepSumByPrefRoundTask extends RepSumByPrefTask {
-
-  public RepSumByPrefRoundTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public int doLogic() throws Exception {
-    Report rp = reportSumByPrefixRound(getRunData().getPoints().taskStats());
-    
-    System.out.println();
-    System.out.println("------------> Report sum by Prefix ("+prefix+") and Round ("+
-        rp.getSize()+" about "+rp.getReported()+" out of "+rp.getOutOf()+")");
-    System.out.println(rp.getText());
-    System.out.println();
-
-    return 0;
-  }
-
-  protected Report reportSumByPrefixRound(List<TaskStats> taskStats) {
-    // aggregate by task name and by round
-    int reported = 0;
-    LinkedHashMap<String,TaskStats> p2 = new LinkedHashMap<String,TaskStats>();
-    for (final TaskStats stat1 : taskStats) {
-      if (stat1.getElapsed()>=0 && stat1.getTask().getName().startsWith(prefix)) { // only ended tasks with proper name
-        reported++;
-        String name = stat1.getTask().getName();
-        String rname = stat1.getRound()+"."+name; // group by round
-        TaskStats stat2 = p2.get(rname);
-        if (stat2 == null) {
-          try {
-            stat2 = (TaskStats) stat1.clone();
-          } catch (CloneNotSupportedException e) {
-            throw new RuntimeException(e);
-          }
-          p2.put(rname,stat2);
-        } else {
-          stat2.add(stat1);
-        }
-      }
-    }
-    // now generate report from secondary list p2    
-    return genPartialReport(reported, p2, taskStats.size());
-  }
-
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefTask.java
deleted file mode 100644
index 610f282..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefTask.java
+++ /dev/null
@@ -1,90 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.stats.Report;
-import org.apache.lucene.benchmark.byTask.stats.TaskStats;
-
-import java.util.LinkedHashMap;
-import java.util.List;
-
-/**
- * Report by-name-prefix statistics aggregated by name.
- * <br>Other side effects: None.
- */
-public class RepSumByPrefTask extends ReportTask {
-
-  public RepSumByPrefTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  protected String prefix;
-
-  @Override
-  public int doLogic() throws Exception {
-    Report rp = reportSumByPrefix(getRunData().getPoints().taskStats());
-    
-    System.out.println();
-    System.out.println("------------> Report Sum By Prefix ("+prefix+") ("+
-        rp.getSize()+" about "+rp.getReported()+" out of "+rp.getOutOf()+")");
-    System.out.println(rp.getText());
-    System.out.println();
-
-    return 0;
-  }
-
-  protected Report reportSumByPrefix (List<TaskStats> taskStats) {
-    // aggregate by task name
-    int reported = 0;
-    LinkedHashMap<String,TaskStats> p2 = new LinkedHashMap<String,TaskStats>();
-    for (final TaskStats stat1 : taskStats) {
-      if (stat1.getElapsed()>=0 && stat1.getTask().getName().startsWith(prefix)) { // only ended tasks with proper name
-        reported++;
-        String name = stat1.getTask().getName();
-        TaskStats stat2 = p2.get(name);
-        if (stat2 == null) {
-          try {
-            stat2 = (TaskStats) stat1.clone();
-          } catch (CloneNotSupportedException e) {
-            throw new RuntimeException(e);
-          }
-          p2.put(name,stat2);
-        } else {
-          stat2.add(stat1);
-        }
-      }
-    }
-    // now generate report from secondary list p2    
-    return genPartialReport(reported, p2, taskStats.size());
-  }
-  
-
-  public void setPrefix(String prefix) {
-    this.prefix = prefix;
-  }
-
-  /* (non-Javadoc)
-   * @see PerfTask#toString()
-   */
-  @Override
-  public String toString() {
-    return super.toString()+" "+prefix;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReportTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReportTask.java
deleted file mode 100644
index ed990a4..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReportTask.java
+++ /dev/null
@@ -1,176 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-import java.util.LinkedHashMap;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.stats.Report;
-import org.apache.lucene.benchmark.byTask.stats.TaskStats;
-import org.apache.lucene.benchmark.byTask.utils.Format;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Report (abstract) task - all report tasks extend this task.
- */
-public abstract class ReportTask extends PerfTask {
-
-  public ReportTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  /* (non-Javadoc)
-   * @see PerfTask#shouldNeverLogAtStart()
-   */
-  @Override
-  protected boolean shouldNeverLogAtStart() {
-    return true;
-  }
-
-  /* (non-Javadoc)
-   * @see PerfTask#shouldNotRecordStats()
-   */
-  @Override
-  protected boolean shouldNotRecordStats() {
-    return true;
-  }
-
-  /*
-   * From here start the code used to generate the reports. 
-   * Subclasses would use this part to generate reports.
-   */
-  
-  protected static final String newline = System.getProperty("line.separator");
-  
-  /**
-   * Get a textual summary of the benchmark results, average from all test runs.
-   */
-  protected static final String OP =          "Operation  ";
-  protected static final String ROUND =       " round";
-  protected static final String RUNCNT =      "   runCnt";
-  protected static final String RECCNT =      "   recsPerRun";
-  protected static final String RECSEC =      "        rec/s";
-  protected static final String ELAPSED =     "  elapsedSec";
-  protected static final String USEDMEM =     "    avgUsedMem";
-  protected static final String TOTMEM =      "    avgTotalMem";
-  protected static final String COLS[] = {
-      RUNCNT,
-      RECCNT,
-      RECSEC,
-      ELAPSED,
-      USEDMEM,
-      TOTMEM
-  };
-
-  /**
-   * Compute a title line for a report table
-   * @param longestOp size of longest op name in the table
-   * @return the table title line.
-   */
-  protected String tableTitle (String longestOp) {
-    StringBuilder sb = new StringBuilder();
-    sb.append(Format.format(OP,longestOp));
-    sb.append(ROUND);
-    sb.append(getRunData().getConfig().getColsNamesForValsByRound());
-    for (int i = 0; i < COLS.length; i++) {
-      sb.append(COLS[i]);
-    }
-    return sb.toString(); 
-  }
-  
-  /**
-   * find the longest op name out of completed tasks.  
-   * @param taskStats completed tasks to be considered.
-   * @return the longest op name out of completed tasks.
-   */
-  protected String longestOp(Iterable<TaskStats> taskStats) {
-    String longest = OP;
-    for (final TaskStats stat : taskStats) {
-      if (stat.getElapsed()>=0) { // consider only tasks that ended
-        String name = stat.getTask().getName();
-        if (name.length() > longest.length()) {
-          longest = name;
-        }
-      }
-    }
-    return longest;
-  }
-  
-  /**
-   * Compute a report line for the given task stat.
-   * @param longestOp size of longest op name in the table.
-   * @param stat task stat to be printed.
-   * @return the report line.
-   */
-  protected String taskReportLine(String longestOp, TaskStats stat) {
-    PerfTask task = stat.getTask();
-    StringBuilder sb = new StringBuilder();
-    sb.append(Format.format(task.getName(), longestOp));
-    String round = (stat.getRound()>=0 ? ""+stat.getRound() : "-");
-    sb.append(Format.formatPaddLeft(round, ROUND));
-    sb.append(getRunData().getConfig().getColsValuesForValsByRound(stat.getRound()));
-    sb.append(Format.format(stat.getNumRuns(), RUNCNT)); 
-    sb.append(Format.format(stat.getCount() / stat.getNumRuns(), RECCNT));
-    long elapsed = (stat.getElapsed()>0 ? stat.getElapsed() : 1); // assume at least 1ms
-    sb.append(Format.format(2, (float) (stat.getCount() * 1000.0 / elapsed), RECSEC));
-    sb.append(Format.format(2, (float) stat.getElapsed() / 1000, ELAPSED));
-    sb.append(Format.format(0, (float) stat.getMaxUsedMem() / stat.getNumRuns(), USEDMEM)); 
-    sb.append(Format.format(0, (float) stat.getMaxTotMem() / stat.getNumRuns(), TOTMEM));
-    return sb.toString();
-  }
-
-  protected Report genPartialReport(int reported, LinkedHashMap<String,TaskStats> partOfTasks, int totalSize) {
-    String longetOp = longestOp(partOfTasks.values());
-    boolean first = true;
-    StringBuilder sb = new StringBuilder();
-    sb.append(tableTitle(longetOp));
-    sb.append(newline);
-    int lineNum = 0;
-    for (final TaskStats stat : partOfTasks.values()) {
-      if (!first) {
-        sb.append(newline);
-      }
-      first = false;
-      String line = taskReportLine(longetOp,stat);
-      lineNum++;
-      if (partOfTasks.size()>2 && lineNum%2==0) {
-        line = line.replaceAll("   "," - ");
-      }
-      sb.append(line);
-      int[] byTime = stat.getCountsByTime();
-      if (byTime != null) {
-        sb.append(newline);
-        int end = -1;
-        for(int i=byTime.length-1;i>=0;i--) {
-          if (byTime[i] != 0) {
-            end = i;
-            break;
-          }
-        }
-        if (end != -1) {
-          sb.append("  by time:");
-          for(int i=0;i<end;i++) {
-            sb.append(' ').append(byTime[i]);
-          }
-        }
-      }
-    }
-    
-    String reptxt = (reported==0 ? "No Matching Entries Were Found!" : sb.toString());
-    return new Report(reptxt,partOfTasks.size(),reported,totalSize);
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetInputsTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetInputsTask.java
deleted file mode 100644
index 57f7b69..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetInputsTask.java
+++ /dev/null
@@ -1,50 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-
-/**
- * Reset inputs so that the test run would behave, input wise, 
- * as if it just started. This affects e.g. the generation of docs and queries.
- */
-public class ResetInputsTask extends PerfTask {
-
-  public ResetInputsTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public int doLogic() throws Exception {
-    getRunData().resetInputs();
-    return 0;
-  }
-  
-  /*
-   * (non-Javadoc)
-   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#shouldNotRecordStats()
-   */
-  @Override
-  protected boolean shouldNotRecordStats() {
-    return true;
-  }
-
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetSystemEraseTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetSystemEraseTask.java
deleted file mode 100644
index ba6dc66..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetSystemEraseTask.java
+++ /dev/null
@@ -1,43 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-
-/**
- * Reset all index and input data and call gc, erase index and dir, does NOT clear statistics.
- * <br>This contains ResetInputs.
- * <br>Other side effects: writers/readers nullified, deleted, closed.
- * Index is erased.
- * Directory is erased.
- */
-public class ResetSystemEraseTask extends ResetSystemSoftTask {
-
-  public ResetSystemEraseTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public int doLogic() throws Exception {
-    getRunData().reinit(true);
-    return 0;
-  }
-  
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetSystemSoftTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetSystemSoftTask.java
deleted file mode 100644
index 80087ed..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetSystemSoftTask.java
+++ /dev/null
@@ -1,43 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-
-/**
- * Reset all index and input data and call gc, does NOT erase index/dir, does NOT clear statistics.
- * This contains ResetInputs.
- * <br>Other side effects: writers/readers nullified, closed.
- * Index is NOT erased.
- * Directory is NOT erased.
- */
-public class ResetSystemSoftTask extends ResetInputsTask {
-
-  public ResetSystemSoftTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public int doLogic() throws Exception {
-    getRunData().reinit(false);
-    return 0;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RollbackIndexTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RollbackIndexTask.java
deleted file mode 100644
index 64ee89d..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RollbackIndexTask.java
+++ /dev/null
@@ -1,52 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.PrintStream;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.index.IndexWriter;
-
-/**
- * Rollback the index writer.
- */
-public class RollbackIndexTask extends PerfTask {
-
-  public RollbackIndexTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  boolean doWait = true;
-
-  @Override
-  public int doLogic() throws IOException {
-    IndexWriter iw = getRunData().getIndexWriter();
-    if (iw != null) {
-      // If infoStream was set to output to a file, close it.
-      PrintStream infoStream = iw.getInfoStream();
-      if (infoStream != null && infoStream != System.out
-          && infoStream != System.err) {
-        infoStream.close();
-      }
-      iw.rollback();
-      getRunData().setIndexWriter(null);
-    }
-    return 1;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTask.java
deleted file mode 100644
index da5aa92..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTask.java
+++ /dev/null
@@ -1,61 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.feeds.QueryMaker;
-
-/**
- * Search task.
- * 
- * <p>Note: This task reuses the reader if it is already open. 
- * Otherwise a reader is opened at start and closed at the end.
- */
-public class SearchTask extends ReadTask {
-
-  public SearchTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public boolean withRetrieve() {
-    return false;
-  }
-
-  @Override
-  public boolean withSearch() {
-    return true;
-  }
-
-  @Override
-  public boolean withTraverse() {
-    return false;
-  }
-
-  @Override
-  public boolean withWarm() {
-    return false;
-  }
-
-  @Override
-  public QueryMaker getQueryMaker() {
-    return getRunData().getQueryMaker(this);
-  }
-
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetHighlightTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetHighlightTask.java
deleted file mode 100644
index 9ca1813..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetHighlightTask.java
+++ /dev/null
@@ -1,152 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.highlight.Highlighter;
-import org.apache.lucene.search.highlight.QueryScorer;
-import org.apache.lucene.search.highlight.SimpleHTMLFormatter;
-import org.apache.lucene.search.highlight.TextFragment;
-import org.apache.lucene.search.highlight.TokenSources;
-
-import java.util.Set;
-import java.util.Collection;
-import java.util.HashSet;
-import java.util.Collections;
-
-/**
- * Search and Traverse and Retrieve docs task.  Highlight the fields in the retrieved documents.
- *
- * Uses the {@link org.apache.lucene.search.highlight.SimpleHTMLFormatter} for formatting.
- *
- * <p>Note: This task reuses the reader if it is already open.
- * Otherwise a reader is opened at start and closed at the end.
- * </p>
- *
- * <p>Takes optional multivalued, comma separated param string as: size[&lt;traversal size&gt;],highlight[&lt;int&gt;],maxFrags[&lt;int&gt;],mergeContiguous[&lt;boolean&gt;],fields[name1;name2;...]</p>
- * <ul>
- * <li>traversal size - The number of hits to traverse, otherwise all will be traversed</li>
- * <li>highlight - The number of the hits to highlight.  Will always be less than or equal to traversal size.  Default is Integer.MAX_VALUE (i.e. hits.length())</li>
- * <li>maxFrags - The maximum number of fragments to score by the highlighter</li>
- * <li>mergeContiguous - true if contiguous fragments should be merged.</li>
- * <li>fields - The fields to highlight.  If not specified all fields will be highlighted (or at least attempted)</li>
- * </ul>
- * Example:
- * <pre>"SearchHlgtSameRdr" SearchTravRetHighlight(size[10],highlight[10],mergeContiguous[true],maxFrags[3],fields[body]) > : 1000
- * </pre>
- *
- * Documents must be stored in order for this task to work.  Additionally, term vector positions can be used as well.
- *
- * <p>Other side effects: counts additional 1 (record) for each traversed hit,
- * and 1 more for each retrieved (non null) document and 1 for each fragment returned.</p>
- */
-public class SearchTravRetHighlightTask extends SearchTravTask {
-
-  protected int numToHighlight = Integer.MAX_VALUE;
-  protected boolean mergeContiguous;
-  protected int maxFrags = 2;
-  protected Set<String> paramFields = Collections.emptySet();
-  protected Highlighter highlighter;
-  protected int maxDocCharsToAnalyze;
-
-  public SearchTravRetHighlightTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public void setup() throws Exception {
-    super.setup();
-    //check to make sure either the doc is being stored
-    PerfRunData data = getRunData();
-    if (data.getConfig().get("doc.stored", false) == false){
-      throw new Exception("doc.stored must be set to true");
-    }
-    maxDocCharsToAnalyze = data.getConfig().get("highlighter.maxDocCharsToAnalyze", Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);
-  }
-
-  @Override
-  public boolean withRetrieve() {
-    return true;
-  }
-
-  @Override
-  public int numToHighlight() {
-    return numToHighlight;
-  }
-  
-  @Override
-  protected BenchmarkHighlighter getBenchmarkHighlighter(Query q){
-    highlighter = new Highlighter(new SimpleHTMLFormatter(), new QueryScorer(q));
-    highlighter.setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);
-    return new BenchmarkHighlighter(){
-      @Override
-      public int doHighlight(IndexReader reader, int doc, String field,
-          Document document, Analyzer analyzer, String text) throws Exception {
-        TokenStream ts = TokenSources.getAnyTokenStream(reader, doc, field, document, analyzer);
-        TextFragment[] frag = highlighter.getBestTextFragments(ts, text, mergeContiguous, maxFrags);
-        return frag != null ? frag.length : 0;
-      }
-    };
-  }
-
-  @Override
-  protected Collection<String> getFieldsToHighlight(Document document) {
-    Collection<String> result = super.getFieldsToHighlight(document);
-    //if stored is false, then result will be empty, in which case just get all the param fields
-    if (paramFields.isEmpty() == false && result.isEmpty() == false) {
-      result.retainAll(paramFields);
-    } else {
-      result = paramFields;
-    }
-    return result;
-  }
-
-  @Override
-  public void setParams(String params) {
-    // can't call super because super doesn't understand our
-    // params syntax
-    this.params = params;
-    String [] splits = params.split(",");
-    for (int i = 0; i < splits.length; i++) {
-      if (splits[i].startsWith("size[") == true){
-        traversalSize = (int)Float.parseFloat(splits[i].substring("size[".length(),splits[i].length() - 1));
-      } else if (splits[i].startsWith("highlight[") == true){
-        numToHighlight = (int)Float.parseFloat(splits[i].substring("highlight[".length(),splits[i].length() - 1));
-      } else if (splits[i].startsWith("maxFrags[") == true){
-        maxFrags = (int)Float.parseFloat(splits[i].substring("maxFrags[".length(),splits[i].length() - 1));
-      } else if (splits[i].startsWith("mergeContiguous[") == true){
-        mergeContiguous = Boolean.valueOf(splits[i].substring("mergeContiguous[".length(),splits[i].length() - 1)).booleanValue();
-      } else if (splits[i].startsWith("fields[") == true){
-        paramFields = new HashSet<String>();
-        String fieldNames = splits[i].substring("fields[".length(), splits[i].length() - 1);
-        String [] fieldSplits = fieldNames.split(";");
-        for (int j = 0; j < fieldSplits.length; j++) {
-          paramFields.add(fieldSplits[j]);          
-        }
-
-      }
-    }
-  }
-
-
-}
\ No newline at end of file
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetLoadFieldSelectorTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetLoadFieldSelectorTask.java
deleted file mode 100644
index 26050b4..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetLoadFieldSelectorTask.java
+++ /dev/null
@@ -1,80 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-/**
- * Copyright 2005 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.document.FieldSelector;
-import org.apache.lucene.document.SetBasedFieldSelector;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.IndexReader;
-
-import java.util.StringTokenizer;
-import java.util.Set;
-import java.util.HashSet;
-import java.util.Collections;
-import java.io.IOException;
-
-/**
- * Search and Traverse and Retrieve docs task using a SetBasedFieldSelector.
- *
- * <p>Note: This task reuses the reader if it is already open.
- * Otherwise a reader is opened at start and closed at the end.
- *
- * <p>Takes optional param: comma separated list of Fields to load.</p>
- * 
- * <p>Other side effects: counts additional 1 (record) for each traversed hit, 
- * and 1 more for each retrieved (non null) document.</p>
- */
-public class SearchTravRetLoadFieldSelectorTask extends SearchTravTask {
-
-  protected FieldSelector fieldSelector;
-  public SearchTravRetLoadFieldSelectorTask(PerfRunData runData) {
-    super(runData);
-    
-  }
-
-  @Override
-  public boolean withRetrieve() {
-    return true;
-  }
-
-
-  @Override
-  protected Document retrieveDoc(IndexReader ir, int id) throws IOException {
-    return ir.document(id, fieldSelector);
-  }
-
-  @Override
-  public void setParams(String params) {
-    this.params = params; // cannot just call super.setParams(), b/c it's params differ.
-    Set<String> fieldsToLoad = new HashSet<String>();
-    for (StringTokenizer tokenizer = new StringTokenizer(params, ","); tokenizer.hasMoreTokens();) {
-      String s = tokenizer.nextToken();
-      fieldsToLoad.add(s);
-    }
-    fieldSelector = new SetBasedFieldSelector(fieldsToLoad, Collections.<String> emptySet());
-  }
-
-
-  /* (non-Javadoc)
-  * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#supportsParams()
-  */
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetTask.java
deleted file mode 100644
index fc450a3..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetTask.java
+++ /dev/null
@@ -1,45 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-
-/**
- * Search and Traverse and Retrieve docs task.
- * 
- * <p>Note: This task reuses the reader if it is already open. 
- * Otherwise a reader is opened at start and closed at the end.
- * </p>
- * 
- * <p>Takes optional param: traversal size (otherwise all results are traversed).</p>
- * 
- * <p>Other side effects: counts additional 1 (record) for each traversed hit, 
- * and 1 more for each retrieved (non null) document.</p>
- */
-public class SearchTravRetTask extends SearchTravTask {
-
-  public SearchTravRetTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public boolean withRetrieve() {
-    return true;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetVectorHighlightTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetVectorHighlightTask.java
deleted file mode 100644
index 19570af..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetVectorHighlightTask.java
+++ /dev/null
@@ -1,146 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.vectorhighlight.FastVectorHighlighter;
-import org.apache.lucene.search.vectorhighlight.FieldQuery;
-
-import java.util.Set;
-import java.util.Collection;
-import java.util.HashSet;
-import java.util.Collections;
-
-/**
- * Search and Traverse and Retrieve docs task.  Highlight the fields in the retrieved documents by using FastVectorHighlighter.
- *
- * <p>Note: This task reuses the reader if it is already open.
- * Otherwise a reader is opened at start and closed at the end.
- * </p>
- *
- * <p>Takes optional multivalued, comma separated param string as: size[&lt;traversal size&gt;],highlight[&lt;int&gt;],maxFrags[&lt;int&gt;],mergeContiguous[&lt;boolean&gt;],fields[name1;name2;...]</p>
- * <ul>
- * <li>traversal size - The number of hits to traverse, otherwise all will be traversed</li>
- * <li>highlight - The number of the hits to highlight.  Will always be less than or equal to traversal size.  Default is Integer.MAX_VALUE (i.e. hits.length())</li>
- * <li>maxFrags - The maximum number of fragments to score by the highlighter</li>
- * <li>fragSize - The length of fragments</li>
- * <li>fields - The fields to highlight.  If not specified all fields will be highlighted (or at least attempted)</li>
- * </ul>
- * Example:
- * <pre>"SearchVecHlgtSameRdr" SearchTravRetVectorHighlight(size[10],highlight[10],maxFrags[3],fields[body]) > : 1000
- * </pre>
- *
- * Fields must be stored and term vector offsets and positions in order must be true for this task to work.
- *
- * <p>Other side effects: counts additional 1 (record) for each traversed hit,
- * and 1 more for each retrieved (non null) document and 1 for each fragment returned.</p>
- */
-public class SearchTravRetVectorHighlightTask extends SearchTravTask {
-
-  protected int numToHighlight = Integer.MAX_VALUE;
-  protected int maxFrags = 2;
-  protected int fragSize = 100;
-  protected Set<String> paramFields = Collections.emptySet();
-  protected FastVectorHighlighter highlighter;
-
-  public SearchTravRetVectorHighlightTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public void setup() throws Exception {
-    super.setup();
-    //check to make sure either the doc is being stored
-    PerfRunData data = getRunData();
-    if (data.getConfig().get("doc.stored", false) == false){
-      throw new Exception("doc.stored must be set to true");
-    }
-    if (data.getConfig().get("doc.term.vector.offsets", false) == false){
-      throw new Exception("doc.term.vector.offsets must be set to true");
-    }
-    if (data.getConfig().get("doc.term.vector.positions", false) == false){
-      throw new Exception("doc.term.vector.positions must be set to true");
-    }
-  }
-
-  @Override
-  public boolean withRetrieve() {
-    return true;
-  }
-
-  @Override
-  public int numToHighlight() {
-    return numToHighlight;
-  }
-  
-  @Override
-  protected BenchmarkHighlighter getBenchmarkHighlighter(Query q){
-    highlighter = new FastVectorHighlighter( false, false );
-    final FieldQuery fq = highlighter.getFieldQuery( q );
-    return new BenchmarkHighlighter(){
-      @Override
-      public int doHighlight(IndexReader reader, int doc, String field,
-          Document document, Analyzer analyzer, String text) throws Exception {
-        String[] fragments = highlighter.getBestFragments(fq, reader, doc, field, fragSize, maxFrags);
-        return fragments != null ? fragments.length : 0;
-      }
-    };
-  }
-
-  @Override
-  protected Collection<String> getFieldsToHighlight(Document document) {
-    Collection<String> result = super.getFieldsToHighlight(document);
-    //if stored is false, then result will be empty, in which case just get all the param fields
-    if (paramFields.isEmpty() == false && result.isEmpty() == false) {
-      result.retainAll(paramFields);
-    } else {
-      result = paramFields;
-    }
-    return result;
-  }
-
-  @Override
-  public void setParams(String params) {
-    // can't call super because super doesn't understand our
-    // params syntax
-    final String [] splits = params.split(",");
-    for (int i = 0; i < splits.length; i++) {
-      if (splits[i].startsWith("size[") == true){
-        traversalSize = (int)Float.parseFloat(splits[i].substring("size[".length(),splits[i].length() - 1));
-      } else if (splits[i].startsWith("highlight[") == true){
-        numToHighlight = (int)Float.parseFloat(splits[i].substring("highlight[".length(),splits[i].length() - 1));
-      } else if (splits[i].startsWith("maxFrags[") == true){
-        maxFrags = (int)Float.parseFloat(splits[i].substring("maxFrags[".length(),splits[i].length() - 1));
-      } else if (splits[i].startsWith("fragSize[") == true){
-        fragSize = (int)Float.parseFloat(splits[i].substring("fragSize[".length(),splits[i].length() - 1));
-      } else if (splits[i].startsWith("fields[") == true){
-        paramFields = new HashSet<String>();
-        String fieldNames = splits[i].substring("fields[".length(), splits[i].length() - 1);
-        String [] fieldSplits = fieldNames.split(";");
-        for (int j = 0; j < fieldSplits.length; j++) {
-          paramFields.add(fieldSplits[j]);          
-        }
-
-      }
-    }
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravTask.java
deleted file mode 100644
index 56893fd..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravTask.java
+++ /dev/null
@@ -1,86 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.feeds.QueryMaker;
-
-/**
- * Search and Traverse task.
- * 
- * <p>Note: This task reuses the reader if it is already open. 
- * Otherwise a reader is opened at start and closed at the end.
- * <p/>
- * 
- * <p>Takes optional param: traversal size (otherwise all results are traversed).</p>
- * 
- * <p>Other side effects: counts additional 1 (record) for each traversed hit.</p>
- */
-public class SearchTravTask extends ReadTask {
-  protected int traversalSize = Integer.MAX_VALUE;
-
-  public SearchTravTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public boolean withRetrieve() {
-    return false;
-  }
-
-  @Override
-  public boolean withSearch() {
-    return true;
-  }
-
-  @Override
-  public boolean withTraverse() {
-    return true;
-  }
-
-  @Override
-  public boolean withWarm() {
-    return false;
-  }
-
-  
-
-  @Override
-  public QueryMaker getQueryMaker() {
-    return getRunData().getQueryMaker(this);
-  }
-
-  @Override
-  public int traversalSize() {
-    return traversalSize;
-  }
-
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    traversalSize = (int)Float.parseFloat(params);
-  }
-
-  /* (non-Javadoc)
-   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#supportsParams()
-   */
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchWithCollectorTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchWithCollectorTask.java
deleted file mode 100644
index b654540..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchWithCollectorTask.java
+++ /dev/null
@@ -1,93 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.feeds.QueryMaker;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.TopScoreDocCollector;
-
-/**
- * Does search w/ a custom collector
- */
-public class SearchWithCollectorTask extends SearchTask {
-
-  protected String clnName;
-
-  public SearchWithCollectorTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public void setup() throws Exception {
-    super.setup();
-    //check to make sure either the doc is being stored
-    PerfRunData runData = getRunData();
-    Config config = runData.getConfig();
-    clnName = config.get("collector.class", "");
-  }
-
-  
-
-  @Override
-  public boolean withCollector() {
-    return true;
-  }
-
-  @Override
-  protected Collector createCollector() throws Exception {
-    Collector collector = null;
-    if (clnName.equalsIgnoreCase("topScoreDocOrdered") == true) {
-      collector = TopScoreDocCollector.create(numHits(), true);
-    } else if (clnName.equalsIgnoreCase("topScoreDocUnOrdered") == true) {
-      collector = TopScoreDocCollector.create(numHits(), false);
-    } else if (clnName.length() > 0){
-      collector = Class.forName(clnName).asSubclass(Collector.class).newInstance();
-
-    } else {
-      collector = super.createCollector();
-    }
-    return collector;
-  }
-
-  @Override
-  public QueryMaker getQueryMaker() {
-    return getRunData().getQueryMaker(this);
-  }
-
-  @Override
-  public boolean withRetrieve() {
-    return false;
-  }
-
-  @Override
-  public boolean withSearch() {
-    return true;
-  }
-
-  @Override
-  public boolean withTraverse() {
-    return false;
-  }
-
-  @Override
-  public boolean withWarm() {
-    return false;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchWithSortTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchWithSortTask.java
deleted file mode 100644
index e00583f..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchWithSortTask.java
+++ /dev/null
@@ -1,164 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.feeds.QueryMaker;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.SortField;
-
-/**
- * Does sort search on specified field.
- * 
- */
-public class SearchWithSortTask extends ReadTask {
-
-  private boolean doScore = true;
-  private boolean doMaxScore = true;
-  private Sort sort;
-
-  public SearchWithSortTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  /**
-   * SortFields: field:type,field:type[,noscore][,nomaxscore]
-   *
-   * If noscore is present, then we turn off score tracking
-   * in {@link org.apache.lucene.search.TopFieldCollector}.
-   * If nomaxscore is present, then we turn off maxScore tracking
-   * in {@link org.apache.lucene.search.TopFieldCollector}.
-   * 
-   * name:string,page:int,subject:string
-   * 
-   */
-  @Override
-  public void setParams(String sortField) {
-    super.setParams(sortField);
-    String[] fields = sortField.split(",");
-    SortField[] sortFields = new SortField[fields.length];
-    int upto = 0;
-    for (int i = 0; i < fields.length; i++) {
-      String field = fields[i];
-      SortField sortField0;
-      if (field.equals("doc")) {
-        sortField0 = SortField.FIELD_DOC;
-      } if (field.equals("score")) {
-        sortField0 = SortField.FIELD_SCORE;
-      } else if (field.equals("noscore")) {
-        doScore = false;
-        continue;
-      } else if (field.equals("nomaxscore")) {
-        doMaxScore = false;
-        continue;
-      } else {
-        int index = field.lastIndexOf(":");
-        String fieldName;
-        String typeString;
-        if (index != -1) {
-          fieldName = field.substring(0, index);
-          typeString = field.substring(1+index, field.length());
-        } else {
-          throw new RuntimeException("You must specify the sort type ie page:int,subject:string");
-        }
-        int type = getType(typeString);
-        sortField0 = new SortField(fieldName, type);
-      }
-      sortFields[upto++] = sortField0;
-    }
-
-    if (upto < sortFields.length) {
-      SortField[] newSortFields = new SortField[upto];
-      System.arraycopy(sortFields, 0, newSortFields, 0, upto);
-      sortFields = newSortFields;
-    }
-    this.sort = new Sort(sortFields);
-  }
-
-  private int getType(String typeString) {
-    int type;
-    if (typeString.equals("float")) {
-      type = SortField.FLOAT;
-    } else if (typeString.equals("double")) {
-      type = SortField.DOUBLE;
-    } else if (typeString.equals("byte")) {
-      type = SortField.BYTE;
-    } else if (typeString.equals("short")) {
-      type = SortField.SHORT;
-    } else if (typeString.equals("int")) {
-      type = SortField.INT;
-    } else if (typeString.equals("long")) {
-      type = SortField.LONG;
-    } else if (typeString.equals("string")) {
-      type = SortField.STRING;
-    } else if (typeString.equals("string_val")) {
-      type = SortField.STRING_VAL;
-    } else {
-      throw new RuntimeException("Unrecognized sort field type " + typeString);
-    }
-    return type;
-  }
-
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-
-  @Override
-  public QueryMaker getQueryMaker() {
-    return getRunData().getQueryMaker(this);
-  }
-
-  @Override
-  public boolean withRetrieve() {
-    return false;
-  }
-
-  @Override
-  public boolean withSearch() {
-    return true;
-  }
-
-  @Override
-  public boolean withTraverse() {
-    return false;
-  }
-
-  @Override
-  public boolean withWarm() {
-    return false;
-  }
-
-  @Override
-  public boolean withScore() {
-    return doScore;
-  }
-
-  @Override
-  public boolean withMaxScore() {
-    return doMaxScore;
-  }
-  
-  @Override
-  public Sort getSort() {
-    if (sort == null) {
-      throw new IllegalStateException("No sort field was set");
-    }
-    return sort;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SetPropTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SetPropTask.java
deleted file mode 100644
index 23c2799..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SetPropTask.java
+++ /dev/null
@@ -1,69 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Set a performance test configuration property.
- * A property may have a single value, or a sequence of values, separated by ":". 
- * If a sequence of values is specified, each time a new round starts, 
- * the next (cyclic) value is taken.  
- * <br>Other side effects: none.
- * <br>Takes mandatory param: "name,value" pair. 
- * @see org.apache.lucene.benchmark.byTask.tasks.NewRoundTask
- */
-public class SetPropTask extends PerfTask {
-
-  public SetPropTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  private String name;
-  private String value;
-  
-  @Override
-  public int doLogic() throws Exception {
-    if (name==null || value==null) {
-      throw new Exception(getName()+" - undefined name or value: name="+name+" value="+value);
-    }
-    getRunData().getConfig().set(name,value);
-    return 0;
-  }
-
-  /**
-   * Set the params (property name and value).
-   * @param params property name and value separated by ','.
-   */
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    int k = params.indexOf(",");
-    name = params.substring(0,k).trim();
-    value = params.substring(k+1).trim();
-  }
-
-  /* (non-Javadoc)
-   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#supportsParams()
-   */
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/TaskSequence.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/TaskSequence.java
deleted file mode 100644
index 6e3b687..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/TaskSequence.java
+++ /dev/null
@@ -1,527 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.List;
-import java.text.NumberFormat;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.feeds.NoMoreDataException;
-import org.apache.lucene.benchmark.byTask.stats.TaskStats;
-import org.apache.lucene.util.ArrayUtil;
-
-/**
- * Sequence of parallel or sequential tasks.
- */
-public class TaskSequence extends PerfTask {
-  public static int REPEAT_EXHAUST = -2; 
-  private ArrayList<PerfTask> tasks;
-  private int repetitions = 1;
-  private boolean parallel;
-  private TaskSequence parent;
-  private boolean letChildReport = true;
-  private int rate = 0;
-  private boolean perMin = false; // rate, if set, is, by default, be sec.
-  private String seqName;
-  private boolean exhausted = false;
-  private boolean resetExhausted = false;
-  private PerfTask[] tasksArray;
-  private boolean anyExhaustibleTasks;
-  private boolean collapsable = false; // to not collapse external sequence named in alg.  
-  
-  private boolean fixedTime;                      // true if we run for fixed time
-  private double runTimeSec;                      // how long to run for
-  private final long logByTimeMsec;
-
-  public TaskSequence (PerfRunData runData, String name, TaskSequence parent, boolean parallel) {
-    super(runData);
-    collapsable = (name == null);
-    name = (name!=null ? name : (parallel ? "Par" : "Seq"));
-    setName(name);
-    setSequenceName();
-    this.parent = parent;
-    this.parallel = parallel;
-    tasks = new ArrayList<PerfTask>();
-    logByTimeMsec = runData.getConfig().get("report.time.step.msec", 0);
-  }
-
-  @Override
-  public void close() throws Exception {
-    initTasksArray();
-    for(int i=0;i<tasksArray.length;i++) {
-      tasksArray[i].close();
-    }
-    getRunData().getDocMaker().close();
-  }
-
-  private void initTasksArray() {
-    if (tasksArray == null) {
-      final int numTasks = tasks.size();
-      tasksArray = new PerfTask[numTasks];
-      for(int k=0;k<numTasks;k++) {
-        tasksArray[k] = tasks.get(k);
-        anyExhaustibleTasks |= tasksArray[k] instanceof ResetInputsTask;
-        anyExhaustibleTasks |= tasksArray[k] instanceof TaskSequence;
-      }
-    }
-    if (!parallel && logByTimeMsec != 0 && !letChildReport) {
-      countsByTime = new int[1];
-    }
-  }
-
-  /**
-   * @return Returns the parallel.
-   */
-  public boolean isParallel() {
-    return parallel;
-  }
-
-  /**
-   * @return Returns the repetitions.
-   */
-  public int getRepetitions() {
-    return repetitions;
-  }
-
-  private int[] countsByTime;
-
-  public void setRunTime(double sec) throws Exception {
-    runTimeSec = sec;
-    fixedTime = true;
-  }
-
-  /**
-   * @param repetitions The repetitions to set.
-   * @throws Exception 
-   */
-  public void setRepetitions(int repetitions) throws Exception {
-    fixedTime = false;
-    this.repetitions = repetitions;
-    if (repetitions==REPEAT_EXHAUST) {
-      if (isParallel()) {
-        throw new Exception("REPEAT_EXHAUST is not allowed for parallel tasks");
-      }
-    }
-    setSequenceName();
-  }
-
-  /**
-   * @return Returns the parent.
-   */
-  public TaskSequence getParent() {
-    return parent;
-  }
-
-  /*
-   * (non-Javadoc)
-   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#doLogic()
-   */
-  @Override
-  public int doLogic() throws Exception {
-    exhausted = resetExhausted = false;
-    return ( parallel ? doParallelTasks() : doSerialTasks());
-  }
-
-  private static class RunBackgroundTask extends Thread {
-    private final PerfTask task;
-    private final boolean letChildReport;
-    private volatile int count;
-
-    public RunBackgroundTask(PerfTask task, boolean letChildReport) {
-      this.task = task;
-      this.letChildReport = letChildReport;
-    }
-
-    public void stopNow() throws InterruptedException {
-      task.stopNow();
-    }
-
-    public int getCount() {
-      return count;
-    }
-
-    @Override
-    public void run() {
-      try {
-        count = task.runAndMaybeStats(letChildReport);
-      } catch (Exception e) {
-        throw new RuntimeException(e);
-      }
-    }
-  }
-
-  private int doSerialTasks() throws Exception {
-    if (rate > 0) {
-      return doSerialTasksWithRate();
-    }
-    
-    initTasksArray();
-    int count = 0;
-
-    final long runTime = (long) (runTimeSec*1000);
-    List<RunBackgroundTask> bgTasks = null;
-
-    final long t0 = System.currentTimeMillis();
-    for (int k=0; fixedTime || (repetitions==REPEAT_EXHAUST && !exhausted) || k<repetitions; k++) {
-      if (stopNow) {
-        break;
-      }
-      for(int l=0;l<tasksArray.length;l++) {
-        final PerfTask task = tasksArray[l];
-        if (task.getRunInBackground()) {
-          if (bgTasks == null) {
-            bgTasks = new ArrayList<RunBackgroundTask>();
-          }
-          RunBackgroundTask bgTask = new RunBackgroundTask(task, letChildReport);
-          bgTask.setPriority(task.getBackgroundDeltaPriority() + Thread.currentThread().getPriority());
-          bgTask.start();
-          bgTasks.add(bgTask);
-        } else {
-          try {
-            final int inc = task.runAndMaybeStats(letChildReport);
-            count += inc;
-            if (countsByTime != null) {
-              final int slot = (int) ((System.currentTimeMillis()-t0)/logByTimeMsec);
-              if (slot >= countsByTime.length) {
-                countsByTime = ArrayUtil.grow(countsByTime, 1+slot);
-              }
-              countsByTime[slot] += inc;
-            }
-            if (anyExhaustibleTasks)
-              updateExhausted(task);
-          } catch (NoMoreDataException e) {
-            exhausted = true;
-          }
-        }
-      }
-      if (fixedTime && System.currentTimeMillis()-t0 > runTime) {
-        repetitions = k+1;
-        break;
-      }
-    }
-
-    if (bgTasks != null) {
-      for(RunBackgroundTask bgTask : bgTasks) {
-        bgTask.stopNow();
-      }
-      for(RunBackgroundTask bgTask : bgTasks) {
-        bgTask.join();
-        count += bgTask.getCount();
-      }
-    }
-
-    if (countsByTime != null) {
-      getRunData().getPoints().getCurrentStats().setCountsByTime(countsByTime, logByTimeMsec);
-    }
-
-    stopNow = false;
-
-    return count;
-  }
-
-  private int doSerialTasksWithRate() throws Exception {
-    initTasksArray();
-    long delayStep = (perMin ? 60000 : 1000) /rate;
-    long nextStartTime = System.currentTimeMillis();
-    int count = 0;
-    final long t0 = System.currentTimeMillis();
-    for (int k=0; (repetitions==REPEAT_EXHAUST && !exhausted) || k<repetitions; k++) {
-      if (stopNow) {
-        break;
-      }
-      for (int l=0;l<tasksArray.length;l++) {
-        final PerfTask task = tasksArray[l];
-        while(!stopNow) {
-          long waitMore = nextStartTime - System.currentTimeMillis();
-          if (waitMore > 0) {
-            // TODO: better to use condition to notify
-            Thread.sleep(1);
-          } else {
-            break;
-          }
-        }
-        if (stopNow) {
-          break;
-        }
-        nextStartTime += delayStep; // this aims at avarage rate. 
-        try {
-          final int inc = task.runAndMaybeStats(letChildReport);
-          count += inc;
-          if (countsByTime != null) {
-            final int slot = (int) ((System.currentTimeMillis()-t0)/logByTimeMsec);
-            if (slot >= countsByTime.length) {
-              countsByTime = ArrayUtil.grow(countsByTime, 1+slot);
-            }
-            countsByTime[slot] += inc;
-          }
-
-          if (anyExhaustibleTasks)
-            updateExhausted(task);
-        } catch (NoMoreDataException e) {
-          exhausted = true;
-        }
-      }
-    }
-    stopNow = false;
-    return count;
-  }
-
-  // update state regarding exhaustion.
-  private void updateExhausted(PerfTask task) {
-    if (task instanceof ResetInputsTask) {
-      exhausted = false;
-      resetExhausted = true;
-    } else if (task instanceof TaskSequence) {
-      TaskSequence t = (TaskSequence) task;
-      if (t.resetExhausted) {
-        exhausted = false;
-        resetExhausted = true;
-        t.resetExhausted = false;
-      } else {
-        exhausted |= t.exhausted;
-      }
-    }
-  }
-
-  private class ParallelTask extends Thread {
-
-    public int count;
-    public final PerfTask task;
-
-    public ParallelTask(PerfTask task) {
-      this.task = task;
-    }
-
-    @Override
-    public void run() {
-      try {
-        int n = task.runAndMaybeStats(letChildReport);
-        if (anyExhaustibleTasks) {
-          updateExhausted(task);
-        }
-        count += n;
-      } catch (NoMoreDataException e) {
-        exhausted = true;
-      } catch (Exception e) {
-        throw new RuntimeException(e);
-      }
-    }
-  }
-
-  @Override
-  public void stopNow() {
-    super.stopNow();
-    // Forwards top request to children
-    if (runningParallelTasks != null) {
-      for(ParallelTask t : runningParallelTasks) {
-        t.task.stopNow();
-      }
-    }
-  }
-
-  ParallelTask[] runningParallelTasks;
-
-  private int doParallelTasks() throws Exception {
-
-    final TaskStats stats = getRunData().getPoints().getCurrentStats();
-
-    initTasksArray();
-    ParallelTask t[] = runningParallelTasks = new ParallelTask[repetitions * tasks.size()];
-    // prepare threads
-    int index = 0;
-    for (int k=0; k<repetitions; k++) {
-      for (int i = 0; i < tasksArray.length; i++) {
-        final PerfTask task = (PerfTask) tasksArray[i].clone();
-        t[index++] = new ParallelTask(task);
-      }
-    }
-    // run threads
-    startThreads(t);
-
-    // wait for all threads to complete
-    int count = 0;
-    for (int i = 0; i < t.length; i++) {
-      t[i].join();
-      count += t[i].count;
-      if (t[i].task instanceof TaskSequence) {
-        TaskSequence sub = (TaskSequence) t[i].task;
-        if (sub.countsByTime != null) {
-          if (countsByTime == null) {
-            countsByTime = new int[sub.countsByTime.length];
-          } else if (countsByTime.length < sub.countsByTime.length) {
-            countsByTime = ArrayUtil.grow(countsByTime, sub.countsByTime.length);
-          }
-          for(int j=0;j<sub.countsByTime.length;j++) {
-            countsByTime[j] += sub.countsByTime[j];
-          }
-        }
-      }
-    }
-
-    if (countsByTime != null) {
-      stats.setCountsByTime(countsByTime, logByTimeMsec);
-    }
-
-    // return total count
-    return count;
-  }
-
-  // run threads
-  private void startThreads(ParallelTask[] t) throws InterruptedException {
-    if (rate > 0) {
-      startlThreadsWithRate(t);
-      return;
-    }
-    for (int i = 0; i < t.length; i++) {
-      t[i].start();
-    }
-  }
-
-  // run threads with rate
-  private void startlThreadsWithRate(ParallelTask[] t) throws InterruptedException {
-    long delayStep = (perMin ? 60000 : 1000) /rate;
-    long nextStartTime = System.currentTimeMillis();
-    for (int i = 0; i < t.length; i++) {
-      long waitMore = nextStartTime - System.currentTimeMillis();
-      if (waitMore > 0) {
-        Thread.sleep(waitMore);
-      }
-      nextStartTime += delayStep; // this aims at average rate of starting threads. 
-      t[i].start();
-    }
-  }
-
-  public void addTask(PerfTask task) {
-    tasks.add(task);
-    task.setDepth(getDepth()+1);
-  }
-  
-  /* (non-Javadoc)
-   * @see java.lang.Object#toString()
-   */
-  @Override
-  public String toString() {
-    String padd = getPadding();
-    StringBuilder sb = new StringBuilder(super.toString());
-    sb.append(parallel ? " [" : " {");
-    sb.append(NEW_LINE);
-    for (final PerfTask task : tasks) {
-      sb.append(task.toString());
-      sb.append(NEW_LINE);
-    }
-    sb.append(padd);
-    sb.append(!letChildReport ? ">" : (parallel ? "]" : "}"));
-    if (fixedTime) {
-      sb.append(" " + NumberFormat.getNumberInstance().format(runTimeSec) + "s");
-    } else if (repetitions>1) {
-      sb.append(" * " + repetitions);
-    } else if (repetitions==REPEAT_EXHAUST) {
-      sb.append(" * EXHAUST");
-    }
-    if (rate>0) {
-      sb.append(",  rate: " + rate+"/"+(perMin?"min":"sec"));
-    }
-    if (getRunInBackground()) {
-      sb.append(" &");
-      int x = getBackgroundDeltaPriority();
-      if (x != 0) {
-        sb.append(x);
-      }
-    }
-    return sb.toString();
-  }
-
-  /**
-   * Execute child tasks in a way that they do not report their time separately.
-   */
-  public void setNoChildReport() {
-    letChildReport  = false;
-    for (final PerfTask task : tasks) {
-      if (task instanceof TaskSequence) {
-        ((TaskSequence)task).setNoChildReport();
-  }
-    }
-  }
-
-  /**
-   * Returns the rate per minute: how many operations should be performed in a minute.
-   * If 0 this has no effect.
-   * @return the rate per min: how many operations should be performed in a minute.
-   */
-  public int getRate() {
-    return (perMin ? rate : 60*rate);
-  }
-
-  /**
-   * @param rate The rate to set.
-   */
-  public void setRate(int rate, boolean perMin) {
-    this.rate = rate;
-    this.perMin = perMin;
-    setSequenceName();
-  }
-
-  private void setSequenceName() {
-    seqName = super.getName();
-    if (repetitions==REPEAT_EXHAUST) {
-      seqName += "_Exhaust";
-    } else if (repetitions>1) {
-      seqName += "_"+repetitions;
-    }
-    if (rate>0) {
-      seqName += "_" + rate + (perMin?"/min":"/sec"); 
-    }
-    if (parallel && seqName.toLowerCase().indexOf("par")<0) {
-      seqName += "_Par";
-    }
-  }
-
-  @Override
-  public String getName() {
-    return seqName; // override to include more info 
-  }
-
-  /**
-   * @return Returns the tasks.
-   */
-  public ArrayList<PerfTask> getTasks() {
-    return tasks;
-  }
-
-  /* (non-Javadoc)
-   * @see java.lang.Object#clone()
-   */
-  @Override
-  protected Object clone() throws CloneNotSupportedException {
-    TaskSequence res = (TaskSequence) super.clone();
-    res.tasks = new ArrayList<PerfTask>();
-    for (int i = 0; i < tasks.size(); i++) {
-      res.tasks.add((PerfTask)tasks.get(i).clone());
-    }
-    return res;
-  }
-
-  /**
-   * Return true if can be collapsed in case it is outermost sequence
-   */
-  public boolean isCollapsable() {
-    return collapsable;
-  }
-  
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/UpdateDocTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/UpdateDocTask.java
deleted file mode 100644
index 759029b..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/UpdateDocTask.java
+++ /dev/null
@@ -1,94 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexWriter;
-
-/**
- * Update a document, using IndexWriter.updateDocument,
- * optionally with of a certain size.
- * <br>Other side effects: none.
- * <br>Takes optional param: document size. 
- */
-public class UpdateDocTask extends PerfTask {
-
-  public UpdateDocTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  private int docSize = 0;
-  
-  // volatile data passed between setup(), doLogic(), tearDown().
-  private Document doc = null;
-  
-  @Override
-  public void setup() throws Exception {
-    super.setup();
-    DocMaker docMaker = getRunData().getDocMaker();
-    if (docSize > 0) {
-      doc = docMaker.makeDocument(docSize);
-    } else {
-      doc = docMaker.makeDocument();
-    }
-  }
-
-  @Override
-  public void tearDown() throws Exception {
-    doc = null;
-    super.tearDown();
-  }
-
-  @Override
-  public int doLogic() throws Exception {
-    final String docID = doc.get(DocMaker.ID_FIELD);
-    if (docID == null) {
-      throw new IllegalStateException("document must define the docid field");
-    }
-    final IndexWriter iw = getRunData().getIndexWriter();
-    iw.updateDocument(new Term(DocMaker.ID_FIELD, docID), doc);
-    return 1;
-  }
-
-  @Override
-  protected String getLogMessage(int recsCount) {
-    return "updated " + recsCount + " docs";
-  }
-  
-  /**
-   * Set the params (docSize only)
-   * @param params docSize, or 0 for no limit.
-   */
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    docSize = (int) Float.parseFloat(params); 
-  }
-
-  /* (non-Javadoc)
-   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#supportsParams()
-   */
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-  
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WaitTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WaitTask.java
deleted file mode 100644
index 39d526c..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WaitTask.java
+++ /dev/null
@@ -1,75 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-
-/**
- * Simply waits for the specified (via the parameter) amount
- * of time.  For example Wait(30s) waits for 30 seconds.
- * This is useful with background tasks to control how long
- * the tasks run.
- *
- *<p>You can specify h, m, or s (hours, minutes, seconds) as
- *the trailing time unit.  No unit is interpreted as
- *seconds.</p>
- */
-public class WaitTask extends PerfTask {
-
-  private double waitTimeSec;
-
-  public WaitTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    if (params != null) {
-      int multiplier;
-      if (params.endsWith("s")) {
-        multiplier = 1;
-        params = params.substring(0, params.length()-1);
-      } else if (params.endsWith("m")) {
-        multiplier = 60;
-        params = params.substring(0, params.length()-1);
-      } else if (params.endsWith("h")) {
-        multiplier = 3600;
-        params = params.substring(0, params.length()-1);
-      } else {
-        // Assume seconds
-        multiplier = 1;
-      }
-
-      waitTimeSec = Double.parseDouble(params) * multiplier;
-    } else {
-      throw new IllegalArgumentException("you must specify the wait time, eg: 10.0s, 4.5m, 2h");
-    }
-  }
-
-  @Override
-  public int doLogic() throws Exception {
-    Thread.sleep((long) (1000*waitTimeSec));
-    return 0;
-  }
-
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WarmTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WarmTask.java
deleted file mode 100644
index ba1a666..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WarmTask.java
+++ /dev/null
@@ -1,65 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.feeds.QueryMaker;
-
-/**
- * Warm reader task: retrieve all reader documents.
- * 
- * <p>Note: This task reuses the reader if it is already open. 
- * Otherwise a reader is opened at start and closed at the end.
- * </p>
- * 
- * <p>Other side effects: counts additional 1 (record) for each 
- * retrieved (non null) document.</p>
- */
-public class WarmTask extends ReadTask {
-
-  public WarmTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public boolean withRetrieve() {
-    return false;
-  }
-
-  @Override
-  public boolean withSearch() {
-    return false;
-  }
-
-  @Override
-  public boolean withTraverse() {
-    return false;
-  }
-
-  @Override
-  public boolean withWarm() {
-    return true;
-  }
-
-  @Override
-  public QueryMaker getQueryMaker() {
-    return null; // not required for this task.
-  }
-
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTask.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTask.java
deleted file mode 100644
index c5a3adb..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTask.java
+++ /dev/null
@@ -1,154 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.BufferedOutputStream;
-import java.io.BufferedWriter;
-import java.io.FileOutputStream;
-import java.io.OutputStream;
-import java.io.OutputStreamWriter;
-import java.io.PrintWriter;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-import org.apache.commons.compress.compressors.CompressorStreamFactory;
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-
-/**
- * A task which writes documents, one line per document. Each line is in the
- * following format: title &lt;TAB&gt; date &lt;TAB&gt; body. The output of this
- * task can be consumed by
- * {@link org.apache.lucene.benchmark.byTask.feeds.LineDocSource} and is intended
- * to save the IO overhead of opening a file per document to be indexed.<br>
- * Supports the following parameters:
- * <ul>
- * <li>line.file.out - the name of the file to write the output to. That
- * parameter is mandatory. <b>NOTE:</b> the file is re-created.
- * <li>bzip.compression - whether the output should be bzip-compressed. This is
- * recommended when the output file is expected to be large. (optional, default:
- * false).
- * </ul>
- * <b>NOTE:</b> this class is not thread-safe and if used by multiple threads the
- * output is unspecified (as all will write to the same output file in a
- * non-synchronized way).
- */
-public class WriteLineDocTask extends PerfTask {
-
-  public final static char SEP = '\t';
-  
-  private int docSize = 0;
-  private PrintWriter lineFileOut = null;
-  private DocMaker docMaker;
-  private ThreadLocal<StringBuilder> threadBuffer = new ThreadLocal<StringBuilder>();
-  private ThreadLocal<Matcher> threadNormalizer = new ThreadLocal<Matcher>();
-  
-  public WriteLineDocTask(PerfRunData runData) throws Exception {
-    super(runData);
-    Config config = runData.getConfig();
-    String fileName = config.get("line.file.out", null);
-    if (fileName == null) {
-      throw new IllegalArgumentException("line.file.out must be set");
-    }
-
-    OutputStream out = new FileOutputStream(fileName);
-    boolean doBzipCompression = false;
-    String doBZCompress = config.get("bzip.compression", null);
-    if (doBZCompress != null) {
-      // Property was set, use the value.
-      doBzipCompression = Boolean.valueOf(doBZCompress).booleanValue();
-    } else {
-      // Property was not set, attempt to detect based on file's extension
-      doBzipCompression = fileName.endsWith("bz2");
-    }
-
-    if (doBzipCompression) {
-      // Wrap with BOS since BZip2CompressorOutputStream calls out.write(int) 
-      // and does not use the write(byte[]) version. This proved to speed the 
-      // compression process by 70% !
-      out = new BufferedOutputStream(out, 1 << 16);
-      out = new CompressorStreamFactory().createCompressorOutputStream("bzip2", out);
-    }
-    lineFileOut = new PrintWriter(new BufferedWriter(new OutputStreamWriter(out, "UTF-8"), 1 << 16));
-    docMaker = runData.getDocMaker();
-  }
-
-  @Override
-  protected String getLogMessage(int recsCount) {
-    return "Wrote " + recsCount + " line docs";
-  }
-  
-  @Override
-  public int doLogic() throws Exception {
-    Document doc = docSize > 0 ? docMaker.makeDocument(docSize) : docMaker.makeDocument();
-
-    Matcher matcher = threadNormalizer.get();
-    if (matcher == null) {
-      matcher = Pattern.compile("[\t\r\n]+").matcher("");
-      threadNormalizer.set(matcher);
-    }
-    
-    Field f = doc.getField(DocMaker.BODY_FIELD);
-    String body = f != null ? matcher.reset(f.stringValue()).replaceAll(" ") : "";
-    
-    f = doc.getField(DocMaker.TITLE_FIELD);
-    String title = f != null ? matcher.reset(f.stringValue()).replaceAll(" ") : "";
-    
-    if (body.length() > 0 || title.length() > 0) {
-      
-      f = doc.getField(DocMaker.DATE_FIELD);
-      String date = f != null ? matcher.reset(f.stringValue()).replaceAll(" ") : "";
-      
-      StringBuilder sb = threadBuffer.get();
-      if (sb == null) {
-        sb = new StringBuilder();
-        threadBuffer.set(sb);
-      }
-      sb.setLength(0);
-      sb.append(title).append(SEP).append(date).append(SEP).append(body);
-      // lineFileOut is a PrintWriter, which synchronizes internally in println.
-      lineFileOut.println(sb.toString());
-    }
-    return 1;
-  }
-
-  @Override
-  public void close() throws Exception {
-    lineFileOut.close();
-    super.close();
-  }
-  
-  /**
-   * Set the params (docSize only)
-   * @param params docSize, or 0 for no limit.
-   */
-  @Override
-  public void setParams(String params) {
-    super.setParams(params);
-    docSize = (int) Float.parseFloat(params); 
-  }
-
-  @Override
-  public boolean supportsParams() {
-    return true;
-  }
-  
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/package.html b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/package.html
deleted file mode 100644
index 9c17edc..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/package.html
+++ /dev/null
@@ -1,26 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-   <meta name="Author" content="Doron Cohen">
-</head>
-<body>
-Extendable benchmark tasks.
-</body>
-</html>
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Algorithm.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Algorithm.java
deleted file mode 100644
index b4d6198..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Algorithm.java
+++ /dev/null
@@ -1,301 +0,0 @@
-package org.apache.lucene.benchmark.byTask.utils;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.StreamTokenizer;
-import java.io.StringReader;
-import java.lang.reflect.Constructor;
-import java.util.ArrayList;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.tasks.PerfTask;
-import org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask;
-import org.apache.lucene.benchmark.byTask.tasks.TaskSequence;
-
-/**
- * Test algorithm, as read from file
- */
-public class Algorithm {
-  
-  private TaskSequence sequence;
-  
-  /**
-   * Read algorithm from file
-   * @param runData perf-run-data used at running the tasks.
-   * @throws Exception if errors while parsing the algorithm 
-   */
-  @SuppressWarnings("fallthrough")
-  public Algorithm (PerfRunData runData) throws Exception {
-    String algTxt = runData.getConfig().getAlgorithmText();
-    sequence = new TaskSequence(runData,null,null,false);
-    TaskSequence currSequence = sequence;
-    PerfTask prevTask = null;
-    StreamTokenizer stok = new StreamTokenizer(new StringReader(algTxt));
-    stok.commentChar('#');
-    stok.eolIsSignificant(false);
-    stok.ordinaryChar('"');
-    stok.ordinaryChar('/');
-    stok.ordinaryChar('(');
-    stok.ordinaryChar(')');
-    boolean colonOk = false; 
-    boolean isDisableCountNextTask = false; // only for primitive tasks
-    currSequence.setDepth(0);
-    String taskPackage = PerfTask.class.getPackage().getName() + ".";
-    
-    while (stok.nextToken() != StreamTokenizer.TT_EOF) { 
-      switch(stok.ttype) {
-  
-        case StreamTokenizer.TT_WORD:
-          String s = stok.sval;
-          Constructor<? extends PerfTask> cnstr = Class.forName(taskPackage+s+"Task")
-            .asSubclass(PerfTask.class).getConstructor(PerfRunData.class);
-          PerfTask task = cnstr.newInstance(runData);
-          task.setDisableCounting(isDisableCountNextTask);
-          isDisableCountNextTask = false;
-          currSequence.addTask(task);
-          if (task instanceof RepSumByPrefTask) {
-            stok.nextToken();
-            String prefix = stok.sval;
-            if (prefix==null || prefix.length()==0) { 
-              throw new Exception("named report prefix problem - "+stok.toString()); 
-            }
-            ((RepSumByPrefTask) task).setPrefix(prefix);
-          }
-          // check for task param: '(' someParam ')'
-          stok.nextToken();
-          if (stok.ttype!='(') {
-            stok.pushBack();
-          } else {
-            // get params, for tasks that supports them, - anything until next ')'
-            StringBuilder params = new StringBuilder();
-            stok.nextToken();
-            while (stok.ttype!=')') { 
-              switch (stok.ttype) {
-                case StreamTokenizer.TT_NUMBER:  
-                  params.append(stok.nval);
-                  break;
-                case StreamTokenizer.TT_WORD:    
-                  params.append(stok.sval);             
-                  break;
-                case StreamTokenizer.TT_EOF:     
-                  throw new Exception("unexpexted EOF: - "+stok.toString());
-                default:
-                  params.append((char)stok.ttype);
-              }
-              stok.nextToken();
-            }
-            String prm = params.toString().trim();
-            if (prm.length()>0) {
-              task.setParams(prm);
-            }
-          }
-
-          // ---------------------------------------
-          colonOk = false; prevTask = task;
-          break;
-  
-        default:
-          char c = (char)stok.ttype;
-          
-          switch(c) {
-          
-            case ':' :
-              if (!colonOk) throw new Exception("colon unexpexted: - "+stok.toString());
-              colonOk = false;
-              // get repetitions number
-              stok.nextToken();
-              if ((char)stok.ttype == '*') {
-                ((TaskSequence)prevTask).setRepetitions(TaskSequence.REPEAT_EXHAUST);
-              } else {
-                if (stok.ttype!=StreamTokenizer.TT_NUMBER)  {
-                  throw new Exception("expected repetitions number or XXXs: - "+stok.toString());
-                } else {
-                  double num = stok.nval;
-                  stok.nextToken();
-                  if (stok.ttype == StreamTokenizer.TT_WORD && stok.sval.equals("s")) {
-                    ((TaskSequence) prevTask).setRunTime(num);
-                  } else {
-                    stok.pushBack();
-                    ((TaskSequence) prevTask).setRepetitions((int) num);
-                  }
-                }
-              }
-              // check for rate specification (ops/min)
-              stok.nextToken();
-              if (stok.ttype!=':') {
-                stok.pushBack();
-              } else {
-                // get rate number
-                stok.nextToken();
-                if (stok.ttype!=StreamTokenizer.TT_NUMBER) throw new Exception("expected rate number: - "+stok.toString());
-                // check for unit - min or sec, sec is default
-                stok.nextToken();
-                if (stok.ttype!='/') {
-                  stok.pushBack();
-                  ((TaskSequence)prevTask).setRate((int)stok.nval,false); // set rate per sec
-                } else {
-                  stok.nextToken();
-                  if (stok.ttype!=StreamTokenizer.TT_WORD) throw new Exception("expected rate unit: 'min' or 'sec' - "+stok.toString());
-                  String unit = stok.sval.toLowerCase();
-                  if ("min".equals(unit)) {
-                    ((TaskSequence)prevTask).setRate((int)stok.nval,true); // set rate per min
-                  } else if ("sec".equals(unit)) {
-                    ((TaskSequence)prevTask).setRate((int)stok.nval,false); // set rate per sec
-                  } else {
-                    throw new Exception("expected rate unit: 'min' or 'sec' - "+stok.toString());
-                  }
-                }
-              }
-              colonOk = false;
-              break;
-    
-            case '{' : 
-            case '[' :  
-              // a sequence
-              // check for sequence name
-              String name = null;
-              stok.nextToken();
-              if (stok.ttype!='"') {
-                stok.pushBack();
-              } else {
-                stok.nextToken();
-                name = stok.sval;
-                stok.nextToken();
-                if (stok.ttype!='"' || name==null || name.length()==0) { 
-                  throw new Exception("sequence name problem - "+stok.toString()); 
-                }
-              }
-              // start the sequence
-              TaskSequence seq2 = new TaskSequence(runData, name, currSequence, c=='[');
-              currSequence.addTask(seq2);
-              currSequence = seq2;
-              colonOk = false;
-              break;
-
-            case '&' :
-              if (currSequence.isParallel()) {
-                throw new Exception("Can only create background tasks within a serial task");
-              }
-              stok.nextToken();
-              final int deltaPri;
-              if (stok.ttype != StreamTokenizer.TT_NUMBER) {
-                stok.pushBack();
-                deltaPri = 0;
-              } else {
-                // priority
-                deltaPri = (int) stok.nval;
-              }
-
-              if (prevTask == null) {
-                throw new Exception("& was unexpected");
-              } else if (prevTask.getRunInBackground()) {
-                throw new Exception("double & was unexpected");
-              } else {
-                prevTask.setRunInBackground(deltaPri);
-              }
-              break;
-    
-            case '>' :
-              currSequence.setNoChildReport(); /* intentional fallthrough */
-            case '}' : 
-            case ']' : 
-              // end sequence
-              colonOk = true; prevTask = currSequence;
-              currSequence = currSequence.getParent();
-              break;
-          
-            case '-' :
-              isDisableCountNextTask = true;
-              break;
-              
-          } //switch(c)
-          break;
-          
-      } //switch(stok.ttype)
-      
-    }
-    
-    if (sequence != currSequence) {
-      throw new Exception("Unmatched sequences");
-    }
-    
-    // remove redundant top level enclosing sequences
-    while (sequence.isCollapsable() && sequence.getRepetitions()==1 && sequence.getRate()==0) {
-      ArrayList<PerfTask> t = sequence.getTasks();
-      if (t!=null && t.size()==1) {
-        PerfTask p = t.get(0);
-        if (p instanceof TaskSequence) {
-          sequence = (TaskSequence) p;
-          continue;
-        }
-      }
-      break;
-    }
-  }
-
-  /* (non-Javadoc)
-   * @see java.lang.Object#toString()
-   */
-  @Override
-  public String toString() {
-    String newline = System.getProperty("line.separator");
-    StringBuilder sb = new StringBuilder();
-    sb.append(sequence.toString());
-    sb.append(newline);
-    return sb.toString();
-  }
-
-  /**
-   * Execute this algorithm
-   * @throws Exception 
-   */
-  public void execute() throws Exception {
-    try {
-      sequence.runAndMaybeStats(true);
-    } finally {
-      sequence.close();
-    }
-  }
-
-  /**
-   * Expert: for test purposes, return all tasks participating in this algorithm.
-   * @return all tasks participating in this algorithm.
-   */
-  public ArrayList<PerfTask> extractTasks() {
-    ArrayList<PerfTask> res = new ArrayList<PerfTask>();
-    extractTasks(res, sequence);
-    return res;
-  }
-  private void extractTasks (ArrayList<PerfTask> extrct, TaskSequence seq) {
-    if (seq==null) 
-      return;
-    extrct.add(seq);
-    ArrayList<PerfTask> t = sequence.getTasks();
-    if (t==null) 
-      return;
-    for (final PerfTask p : t) {
-      if (p instanceof TaskSequence) {
-        extractTasks(extrct, (TaskSequence)p);
-      } else {
-        extrct.add(p);
-      }
-    }
-  }
-  
-}
-
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java
deleted file mode 100644
index bb166bf..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java
+++ /dev/null
@@ -1,452 +0,0 @@
-package org.apache.lucene.benchmark.byTask.utils;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.BufferedReader;
-import java.io.ByteArrayInputStream;
-import java.io.IOException;
-import java.io.Reader;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Properties;
-import java.util.StringTokenizer;
-
-/**
- * Perf run configuration properties.
- * <p/>
- * Numeric property containing ":", e.g. "10:100:5" is interpreted
- * as array of numeric values. It is extracted once, on first use, and
- * maintain a round number to return the appropriate value.
- * <p/>
- * The config property "work.dir" tells where is the root of
- * docs data dirs and indexes dirs. It is set to either of: <ul>
- * <li>value supplied for it in the alg file;</li>
- * <li>otherwise, value of System property "benchmark.work.dir";</li>
- * <li>otherwise, "work".</li>
- * </ul>
- */
-public class Config {
-
-  // For tests, if verbose is not turned on, don't print the props.
-  private static final String DEFAULT_PRINT_PROPS = System.getProperty("tests.verbose", "true");
-  private static final String NEW_LINE = System.getProperty("line.separator");
-
-  private int roundNumber = 0;
-  private Properties props;
-  private HashMap<String, Object> valByRound = new HashMap<String, Object>();
-  private HashMap<String, String> colForValByRound = new HashMap<String, String>();
-  private String algorithmText;
-
-  /**
-   * Read both algorithm and config properties.
-   *
-   * @param algReader from where to read algorithm and config properties.
-   * @throws IOException
-   */
-  public Config(Reader algReader) throws IOException {
-    // read alg file to array of lines
-    ArrayList<String> lines = new ArrayList<String>();
-    BufferedReader r = new BufferedReader(algReader);
-    int lastConfigLine = 0;
-    for (String line = r.readLine(); line != null; line = r.readLine()) {
-      lines.add(line);
-      if (line.indexOf('=') > 0) {
-        lastConfigLine = lines.size();
-      }
-    }
-    r.close();
-    // copy props lines to string
-    StringBuilder sb = new StringBuilder();
-    for (int i = 0; i < lastConfigLine; i++) {
-      sb.append(lines.get(i));
-      sb.append(NEW_LINE);
-    }
-    // read props from string
-    this.props = new Properties();
-    props.load(new ByteArrayInputStream(sb.toString().getBytes()));
-
-    // make sure work dir is set properly 
-    if (props.get("work.dir") == null) {
-      props.setProperty("work.dir", System.getProperty("benchmark.work.dir", "work"));
-    }
-
-    if (Boolean.valueOf(props.getProperty("print.props", DEFAULT_PRINT_PROPS)).booleanValue()) {
-      printProps();
-    }
-
-    // copy algorithm lines
-    sb = new StringBuilder();
-    for (int i = lastConfigLine; i < lines.size(); i++) {
-      sb.append(lines.get(i));
-      sb.append(NEW_LINE);
-    }
-    algorithmText = sb.toString();
-  }
-
-  /**
-   * Create config without algorithm - useful for a programmatic perf test.
-   * @param props - configuration properties.
-   */
-  public Config (Properties props) {
-    this.props = props;
-    if (Boolean.valueOf(props.getProperty("print.props",DEFAULT_PRINT_PROPS)).booleanValue()) {
-      printProps();
-    }
-  }
-
-  @SuppressWarnings({"unchecked", "rawtypes"})
-  private void printProps() {
-    System.out.println("------------> config properties:");
-    List<String> propKeys = new ArrayList(props.keySet());
-    Collections.sort(propKeys);
-    for (final String propName : propKeys) {
-      System.out.println(propName + " = " + props.getProperty(propName));
-    }
-    System.out.println("-------------------------------");
-  }
-
-  /**
-   * Return a string property.
-   *
-   * @param name name of property.
-   * @param dflt default value.
-   * @return a string property.
-   */
-  public String get(String name, String dflt) {
-    String vals[] = (String[]) valByRound.get(name);
-    if (vals != null) {
-      return vals[roundNumber % vals.length];
-    }
-    // done if not by round
-    String sval = props.getProperty(name, dflt);
-    if (sval == null) {
-      return null;
-    }
-    if (sval.indexOf(":") < 0) {
-      return sval;
-    } else if (sval.indexOf(":\\") >= 0 || sval.indexOf(":/") >= 0) {
-      // this previously messed up absolute path names on Windows. Assuming
-      // there is no real value that starts with \ or /
-      return sval;
-    }
-    // first time this prop is extracted by round
-    int k = sval.indexOf(":");
-    String colName = sval.substring(0, k);
-    sval = sval.substring(k + 1);
-    colForValByRound.put(name, colName);
-    vals = propToStringArray(sval);
-    valByRound.put(name, vals);
-    return vals[roundNumber % vals.length];
-  }
-
-  /**
-   * Set a property.
-   * Note: once a multiple values property is set, it can no longer be modified.
-   *
-   * @param name  name of property.
-   * @param value either single or multiple property value (multiple values are separated by ":")
-   * @throws Exception
-   */
-  public void set(String name, String value) throws Exception {
-    if (valByRound.get(name) != null) {
-      throw new Exception("Cannot modify a multi value property!");
-    }
-    props.setProperty(name, value);
-  }
-
-  /**
-   * Return an int property.
-   * If the property contain ":", e.g. "10:100:5", it is interpreted
-   * as array of ints. It is extracted once, on first call
-   * to get() it, and a by-round-value is returned.
-   *
-   * @param name name of property
-   * @param dflt default value
-   * @return a int property.
-   */
-  public int get(String name, int dflt) {
-    // use value by round if already parsed
-    int vals[] = (int[]) valByRound.get(name);
-    if (vals != null) {
-      return vals[roundNumber % vals.length];
-    }
-    // done if not by round 
-    String sval = props.getProperty(name, "" + dflt);
-    if (sval.indexOf(":") < 0) {
-      return Integer.parseInt(sval);
-    }
-    // first time this prop is extracted by round
-    int k = sval.indexOf(":");
-    String colName = sval.substring(0, k);
-    sval = sval.substring(k + 1);
-    colForValByRound.put(name, colName);
-    vals = propToIntArray(sval);
-    valByRound.put(name, vals);
-    return vals[roundNumber % vals.length];
-  }
-
-  /**
-   * Return a double property.
-   * If the property contain ":", e.g. "10:100:5", it is interpreted
-   * as array of doubles. It is extracted once, on first call
-   * to get() it, and a by-round-value is returned.
-   *
-   * @param name name of property
-   * @param dflt default value
-   * @return a double property.
-   */
-  public double get(String name, double dflt) {
-    // use value by round if already parsed
-    double vals[] = (double[]) valByRound.get(name);
-    if (vals != null) {
-      return vals[roundNumber % vals.length];
-    }
-    // done if not by round 
-    String sval = props.getProperty(name, "" + dflt);
-    if (sval.indexOf(":") < 0) {
-      return Double.parseDouble(sval);
-    }
-    // first time this prop is extracted by round
-    int k = sval.indexOf(":");
-    String colName = sval.substring(0, k);
-    sval = sval.substring(k + 1);
-    colForValByRound.put(name, colName);
-    vals = propToDoubleArray(sval);
-    valByRound.put(name, vals);
-    return vals[roundNumber % vals.length];
-  }
-
-  /**
-   * Return a boolean property.
-   * If the property contain ":", e.g. "true.true.false", it is interpreted
-   * as array of booleans. It is extracted once, on first call
-   * to get() it, and a by-round-value is returned.
-   *
-   * @param name name of property
-   * @param dflt default value
-   * @return a int property.
-   */
-  public boolean get(String name, boolean dflt) {
-    // use value by round if already parsed
-    boolean vals[] = (boolean[]) valByRound.get(name);
-    if (vals != null) {
-      return vals[roundNumber % vals.length];
-    }
-    // done if not by round 
-    String sval = props.getProperty(name, "" + dflt);
-    if (sval.indexOf(":") < 0) {
-      return Boolean.valueOf(sval).booleanValue();
-    }
-    // first time this prop is extracted by round 
-    int k = sval.indexOf(":");
-    String colName = sval.substring(0, k);
-    sval = sval.substring(k + 1);
-    colForValByRound.put(name, colName);
-    vals = propToBooleanArray(sval);
-    valByRound.put(name, vals);
-    return vals[roundNumber % vals.length];
-  }
-
-  /**
-   * Increment the round number, for config values that are extracted by round number.
-   *
-   * @return the new round number.
-   */
-  public int newRound() {
-    roundNumber++;
-
-    StringBuilder sb = new StringBuilder("--> Round ").append(roundNumber - 1).append("-->").append(roundNumber);
-
-    // log changes in values
-    if (valByRound.size() > 0) {
-      sb.append(": ");
-      for (final String name : valByRound.keySet()) {
-        Object a = valByRound.get(name);
-        if (a instanceof int[]) {
-          int ai[] = (int[]) a;
-          int n1 = (roundNumber - 1) % ai.length;
-          int n2 = roundNumber % ai.length;
-          sb.append("  ").append(name).append(":").append(ai[n1]).append("-->").append(ai[n2]);
-        } else if (a instanceof double[]) {
-          double ad[] = (double[]) a;
-          int n1 = (roundNumber - 1) % ad.length;
-          int n2 = roundNumber % ad.length;
-          sb.append("  ").append(name).append(":").append(ad[n1]).append("-->").append(ad[n2]);
-        } else if (a instanceof String[]) {
-          String ad[] = (String[]) a;
-          int n1 = (roundNumber - 1) % ad.length;
-          int n2 = roundNumber % ad.length;
-          sb.append("  ").append(name).append(":").append(ad[n1]).append("-->").append(ad[n2]);
-        } else {
-          boolean ab[] = (boolean[]) a;
-          int n1 = (roundNumber - 1) % ab.length;
-          int n2 = roundNumber % ab.length;
-          sb.append("  ").append(name).append(":").append(ab[n1]).append("-->").append(ab[n2]);
-        }
-      }
-    }
-
-    System.out.println();
-    System.out.println(sb.toString());
-    System.out.println();
-
-    return roundNumber;
-  }
-
-  private String[] propToStringArray(String s) {
-    if (s.indexOf(":") < 0) {
-      return new String[]{s};
-    }
-
-    ArrayList<String> a = new ArrayList<String>();
-    StringTokenizer st = new StringTokenizer(s, ":");
-    while (st.hasMoreTokens()) {
-      String t = st.nextToken();
-      a.add(t);
-    }
-    return a.toArray(new String[a.size()]);
-  }
-
-  // extract properties to array, e.g. for "10:100:5" return int[]{10,100,5}. 
-  private int[] propToIntArray(String s) {
-    if (s.indexOf(":") < 0) {
-      return new int[]{Integer.parseInt(s)};
-    }
-
-    ArrayList<Integer> a = new ArrayList<Integer>();
-    StringTokenizer st = new StringTokenizer(s, ":");
-    while (st.hasMoreTokens()) {
-      String t = st.nextToken();
-      a.add(Integer.valueOf(t));
-    }
-    int res[] = new int[a.size()];
-    for (int i = 0; i < a.size(); i++) {
-      res[i] = a.get(i).intValue();
-    }
-    return res;
-  }
-
-  // extract properties to array, e.g. for "10.7:100.4:-2.3" return int[]{10.7,100.4,-2.3}. 
-  private double[] propToDoubleArray(String s) {
-    if (s.indexOf(":") < 0) {
-      return new double[]{Double.parseDouble(s)};
-    }
-
-    ArrayList<Double> a = new ArrayList<Double>();
-    StringTokenizer st = new StringTokenizer(s, ":");
-    while (st.hasMoreTokens()) {
-      String t = st.nextToken();
-      a.add(Double.valueOf(t));
-    }
-    double res[] = new double[a.size()];
-    for (int i = 0; i < a.size(); i++) {
-      res[i] = a.get(i).doubleValue();
-    }
-    return res;
-  }
-
-  // extract properties to array, e.g. for "true:true:false" return boolean[]{true,false,false}. 
-  private boolean[] propToBooleanArray(String s) {
-    if (s.indexOf(":") < 0) {
-      return new boolean[]{Boolean.valueOf(s).booleanValue()};
-    }
-
-    ArrayList<Boolean> a = new ArrayList<Boolean>();
-    StringTokenizer st = new StringTokenizer(s, ":");
-    while (st.hasMoreTokens()) {
-      String t = st.nextToken();
-      a.add(new Boolean(t));
-    }
-    boolean res[] = new boolean[a.size()];
-    for (int i = 0; i < a.size(); i++) {
-      res[i] = a.get(i).booleanValue();
-    }
-    return res;
-  }
-
-  /**
-   * @return names of params set by round, for reports title
-   */
-  public String getColsNamesForValsByRound() {
-    if (colForValByRound.size() == 0) {
-      return "";
-    }
-    StringBuilder sb = new StringBuilder();
-    for (final String name : colForValByRound.keySet()) {
-      String colName = colForValByRound.get(name);
-      sb.append(" ").append(colName);
-    }
-    return sb.toString();
-  }
-
-  /**
-   * @return values of params set by round, for reports lines.
-   */
-  public String getColsValuesForValsByRound(int roundNum) {
-    if (colForValByRound.size() == 0) {
-      return "";
-    }
-    StringBuilder sb = new StringBuilder();
-    for (final String name : colForValByRound.keySet()) {
-      String colName = colForValByRound.get(name);
-      String template = " " + colName;
-      if (roundNum < 0) {
-        // just append blanks
-        sb.append(Format.formatPaddLeft("-", template));
-      } else {
-        // append actual values, for that round
-        Object a = valByRound.get(name);
-        if (a instanceof int[]) {
-          int ai[] = (int[]) a;
-          int n = roundNum % ai.length;
-          sb.append(Format.format(ai[n], template));
-        } else if (a instanceof double[]) {
-          double ad[] = (double[]) a;
-          int n = roundNum % ad.length;
-          sb.append(Format.format(2, ad[n], template));
-        } else if (a instanceof String[]) {
-          String ad[] = (String[]) a;
-          int n = roundNum % ad.length;
-          sb.append(ad[n]);
-        } else {
-          boolean ab[] = (boolean[]) a;
-          int n = roundNum % ab.length;
-          sb.append(Format.formatPaddLeft("" + ab[n], template));
-        }
-      }
-    }
-    return sb.toString();
-  }
-
-  /**
-   * @return the round number.
-   */
-  public int getRoundNumber() {
-    return roundNumber;
-  }
-
-  /**
-   * @return Returns the algorithmText.
-   */
-  public String getAlgorithmText() {
-    return algorithmText;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/FileUtils.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/FileUtils.java
deleted file mode 100644
index 1712038..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/FileUtils.java
+++ /dev/null
@@ -1,54 +0,0 @@
-package org.apache.lucene.benchmark.byTask.utils;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.io.IOException;
-
-/**
- * File utilities.
- */
-public class FileUtils {
-
-  /**
-   * Delete files and directories, even if non-empty.
-   *
-   * @param dir file or directory
-   * @return true on success, false if no or part of files have been deleted
-   * @throws java.io.IOException
-   */
-  public static boolean fullyDelete(File dir) throws IOException {
-    if (dir == null || !dir.exists()) return false;
-    File contents[] = dir.listFiles();
-    if (contents != null) {
-      for (int i = 0; i < contents.length; i++) {
-        if (contents[i].isFile()) {
-          if (!contents[i].delete()) {
-            return false;
-          }
-        } else {
-          if (!fullyDelete(contents[i])) {
-            return false;
-          }
-        }
-      }
-    }
-    return dir.delete();
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Format.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Format.java
deleted file mode 100644
index a571fb5..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Format.java
+++ /dev/null
@@ -1,110 +0,0 @@
-package org.apache.lucene.benchmark.byTask.utils;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.text.NumberFormat;
-
-/**
- * Formatting utilities (for reports).
- */
-public class Format {
-
-  private static NumberFormat numFormat [] = { 
-    NumberFormat.getInstance(), 
-    NumberFormat.getInstance(),
-    NumberFormat.getInstance(),
-  };
-  private static final String padd = "                                                 ";
-  
-  static {
-    numFormat[0].setMaximumFractionDigits(0);
-    numFormat[0].setMinimumFractionDigits(0);
-    numFormat[1].setMaximumFractionDigits(1);
-    numFormat[1].setMinimumFractionDigits(1);
-    numFormat[2].setMaximumFractionDigits(2);
-    numFormat[2].setMinimumFractionDigits(2);
-  }
-
-  /**
-   * Padd a number from left.
-   * @param numFracDigits number of digits in fraction part - must be 0 or 1 or 2.
-   * @param f number to be formatted.
-   * @param col column name (used for deciding on length).
-   * @return formatted string.
-   */
-  public static String format(int numFracDigits, float f, String col) {
-    String res = padd + numFormat[numFracDigits].format(f);
-    return res.substring(res.length() - col.length());
-  }
-
-  public static String format(int numFracDigits, double f, String col) {
-    String res = padd + numFormat[numFracDigits].format(f);
-    return res.substring(res.length() - col.length());
-  }
-
-  /**
-   * Pad a number from right.
-   * @param numFracDigits number of digits in fraction part - must be 0 or 1 or 2.
-   * @param f number to be formatted.
-   * @param col column name (used for deciding on length).
-   * @return formatted string.
-   */
-  public static String formatPaddRight(int numFracDigits, float f, String col) {
-    String res = numFormat[numFracDigits].format(f) + padd;
-    return res.substring(0, col.length());
-  }
-
-  public static String formatPaddRight(int numFracDigits, double f, String col) {
-    String res = numFormat[numFracDigits].format(f) + padd;
-    return res.substring(0, col.length());
-  }
-
-  /**
-   * Pad a number from left.
-   * @param n number to be formatted.
-   * @param col column name (used for deciding on length).
-   * @return formatted string.
-   */
-  public static String format(int n, String col) {
-    String res = padd + n;
-    return res.substring(res.length() - col.length());
-  }
-
-  /**
-   * Pad a string from right.
-   * @param s string to be formatted.
-   * @param col column name (used for deciding on length).
-   * @return formatted string.
-   */
-  public static String format(String s, String col) {
-    String s1 = (s + padd);
-    return s1.substring(0, Math.min(col.length(), s1.length()));
-  }
-
-  /**
-   * Pad a string from left.
-   * @param s string to be formatted.
-   * @param col column name (used for deciding on length).
-   * @return formatted string.
-   */
-  public static String formatPaddLeft(String s, String col) {
-    String res = padd + s;
-    return res.substring(res.length() - col.length());
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/StringBuilderReader.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/StringBuilderReader.java
deleted file mode 100644
index c6e9510..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/StringBuilderReader.java
+++ /dev/null
@@ -1,179 +0,0 @@
-package org.apache.lucene.benchmark.byTask.utils;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.Reader;
-
-/**
- * Implements a {@link Reader} over a {@link StringBuilder} instance. Although
- * one can use {@link java.io.StringReader} by passing it
- * {@link StringBuilder#toString()}, it is better to use this class, as it
- * doesn't mark the passed-in {@link StringBuilder} as shared (which will cause
- * inner char[] allocations at the next append() attempt).<br>
- * Notes:
- * <ul>
- * <li>This implementation assumes the underlying {@link StringBuilder} is not
- * changed during the use of this {@link Reader} implementation.
- * <li>This implementation is thread-safe.
- * <li>The implementation looks very much like {@link java.io.StringReader} (for
- * the right reasons).
- * <li>If one wants to reuse that instance, then the following needs to be done:
- * <pre>
- * StringBuilder sb = new StringBuilder("some text");
- * Reader reader = new StringBuilderReader(sb);
- * ... read from reader - don't close it ! ...
- * sb.setLength(0);
- * sb.append("some new text");
- * reader.reset();
- * ... read the new string from the reader ...
- * </pre>
- * </ul>
- */
-public class StringBuilderReader extends Reader {
-  
-  // The StringBuilder to read from.
-  private StringBuilder sb;
-
-  // The length of 'sb'.
-  private int length;
-
-  // The next position to read from the StringBuilder.
-  private int next = 0;
-
-  // The mark position. The default value 0 means the start of the text.
-  private int mark = 0;
-
-  public StringBuilderReader(StringBuilder sb) {
-    set(sb);
-  }
-
-  /** Check to make sure that the stream has not been closed. */
-  private void ensureOpen() throws IOException {
-    if (sb == null) {
-      throw new IOException("Stream has already been closed");
-    }
-  }
-
-  @Override
-  public void close() {
-    synchronized (lock) {
-      sb = null;
-    }
-  }
-
-  /**
-   * Mark the present position in the stream. Subsequent calls to reset() will
-   * reposition the stream to this point.
-   * 
-   * @param readAheadLimit Limit on the number of characters that may be read
-   *        while still preserving the mark. Because the stream's input comes
-   *        from a StringBuilder, there is no actual limit, so this argument 
-   *        must not be negative, but is otherwise ignored.
-   * @exception IllegalArgumentException If readAheadLimit is < 0
-   * @exception IOException If an I/O error occurs
-   */
-  @Override
-  public void mark(int readAheadLimit) throws IOException {
-    if (readAheadLimit < 0){
-      throw new IllegalArgumentException("Read-ahead limit cannpt be negative: " + readAheadLimit);
-    }
-    synchronized (lock) {
-      ensureOpen();
-      mark = next;
-    }
-  }
-
-  @Override
-  public boolean markSupported() {
-    return true;
-  }
-
-  @Override
-  public int read() throws IOException {
-    synchronized (lock) {
-      ensureOpen();
-      return next >= length ? -1 : sb.charAt(next++);
-    }
-  }
-
-  @Override
-  public int read(char cbuf[], int off, int len) throws IOException {
-    synchronized (lock) {
-      ensureOpen();
-
-      // Validate parameters
-      if (off < 0 || off > cbuf.length || len < 0 || off + len > cbuf.length) {
-        throw new IndexOutOfBoundsException("off=" + off + " len=" + len + " cbuf.length=" + cbuf.length);
-      }
-
-      if (len == 0) {
-        return 0;
-      }
-
-      if (next >= length) {
-        return -1;
-      }
-
-      int n = Math.min(length - next, len);
-      sb.getChars(next, next + n, cbuf, off);
-      next += n;
-      return n;
-    }
-  }
-
-  @Override
-  public boolean ready() throws IOException {
-    synchronized (lock) {
-      ensureOpen();
-      return true;
-    }
-  }
-
-  @Override
-  public void reset() throws IOException {
-    synchronized (lock) {
-      ensureOpen();
-      next = mark;
-      length = sb.length();
-    }
-  }
-
-  public void set(StringBuilder sb) {
-    synchronized (lock) {
-      this.sb = sb;
-      length = sb.length();
-    }
-  }
-  @Override
-  public long skip(long ns) throws IOException {
-    synchronized (lock) {
-      ensureOpen();
-      if (next >= length) {
-        return 0;
-      }
-
-      // Bound skip by beginning and end of the source
-      long n = Math.min(length - next, ns);
-      n = Math.max(-next, n);
-      next += n;
-      return n;
-    }
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/package.html b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/package.html
deleted file mode 100644
index 6a71c2f..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<body>
-Utilities used for the benchmark, and for the reports.
-</body>
-</html>
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/package.html b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/package.html
deleted file mode 100644
index dc28bc8..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/package.html
+++ /dev/null
@@ -1,45 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<HTML>
-<HEAD>
-    <TITLE>Lucene Benchmarking Package</TITLE>
-</HEAD>
-<BODY>
-<DIV>
-    <p/>
-    The benchmark contribution contains tools for benchmarking Lucene using standard, freely available corpora. ANT will
-    download the corpus automatically, place it in a temp directory and then unpack it to the working.dir directory specified in the build.
-    The temp directory
-    and working directory can be safely removed after a run. However, the next time the task is run, it will need to download the files again.
-<p/>
-    Classes implementing the Benchmarker interface should have a no-argument constructor if they are to be used with the Driver class. The Driver
-    class is provided for convenience only. Feel free to implement your own main class for your benchmarker.
-<p/>
-    The StandardBenchmarker is meant to be just that, a standard that runs out of the box with no configuration or changes needed.
-    Other benchmarking classes may derive from it to provide alternate views or to take in command line options. When reporting benchmarking runs
-    you should state any alterations you have made.
-    <p/>
-    To run the short version of the StandardBenchmarker, call "ant run-micro-standard". This should take a minute or so to complete and give you a preliminary idea of how your change affects the code
-    <p/>
-    To run the long version of the StandardBenchmarker, call "ant run-standard". This takes considerably longer.
-    <p/>
-    The original code for these classes was donated by Andrzej Bialecki at http://issues.apache.org/jira/browse/LUCENE-675 and has been updated by Grant Ingersoll to make some parts of the code reusable in other benchmarkers
-</DIV>
-<DIV>&nbsp;</DIV>
-</BODY>
-</HTML>
\ No newline at end of file
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/Judge.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/Judge.java
deleted file mode 100755
index 78e5718..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/Judge.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.benchmark.quality;
-
-import java.io.PrintWriter;
-
-/**
- * Judge if a document is relevant for a quality query.
- */
-public interface Judge {
-
-  /**
-   * Judge if document <code>docName</code> is relevant for the given quality query.
-   * @param docName name of doc tested for relevancy.
-   * @param query tested quality query. 
-   * @return true if relevant, false if not.
-   */
-  public boolean isRelevant(String docName, QualityQuery query);
-
-  /**
-   * Validate that queries and this Judge match each other.
-   * To be perfectly valid, this Judge must have some data for each and every 
-   * input quality query, and must not have any data on any other quality query.  
-   * <b>Note</b>: the quality benchmark run would not fail in case of imperfect
-   * validity, just a warning message would be logged.  
-   * @param qq quality queries to be validated.
-   * @param logger if not null, validation issues are logged.
-   * @return true if perfectly valid, false if not.
-   */
-  public boolean validateData (QualityQuery qq[], PrintWriter logger);
-  
-  /**
-   * Return the maximal recall for the input quality query. 
-   * It is the number of relevant docs this Judge "knows" for the query. 
-   * @param query the query whose maximal recall is needed.
-   */
-  public int maxRecall (QualityQuery query);
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityBenchmark.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityBenchmark.java
deleted file mode 100644
index 73a76b5..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityBenchmark.java
+++ /dev/null
@@ -1,161 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.benchmark.quality;
-
-import java.io.IOException;
-import java.io.PrintWriter;
-
-import org.apache.lucene.benchmark.quality.utils.DocNameExtractor;
-import org.apache.lucene.benchmark.quality.utils.SubmissionReport;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.Searcher;
-import org.apache.lucene.search.TopDocs;
-
-/**
- * Main entry point for running a quality benchmark.
- * <p>
- * There are two main configurations for running a quality benchmark: <ul>
- * <li>Against existing judgements.</li>
- * <li>For submission (e.g. for a contest).</li>
- * </ul>
- * The first configuration requires a non null
- * {@link org.apache.lucene.benchmark.quality.Judge Judge}. 
- * The second configuration requires a non null 
- * {@link org.apache.lucene.benchmark.quality.utils.SubmissionReport SubmissionLogger}.
- */
-public class QualityBenchmark {
-
-  /** Quality Queries that this quality benchmark would execute. */
-  protected QualityQuery qualityQueries[];
-  
-  /** Parser for turning QualityQueries into Lucene Queries. */
-  protected QualityQueryParser qqParser;
-  
-  /** Index to be searched. */
-  protected Searcher searcher;
-
-  /** index field to extract doc name for each search result; used for judging the results. */  
-  protected String docNameField;
-  
-  /** maximal number of queries that this quality benchmark runs. Default: maxint. Useful for debugging. */
-  private int maxQueries = Integer.MAX_VALUE;
-  
-  /** maximal number of results to collect for each query. Default: 1000. */
-  private int maxResults = 1000;
-
-  /**
-   * Create a QualityBenchmark.
-   * @param qqs quality queries to run.
-   * @param qqParser parser for turning QualityQueries into Lucene Queries. 
-   * @param searcher index to be searched.
-   * @param docNameField name of field containing the document name.
-   *        This allows to extract the doc name for search results,
-   *        and is important for judging the results.  
-   */
-  public QualityBenchmark(QualityQuery qqs[], QualityQueryParser qqParser, 
-      Searcher searcher, String docNameField) {
-    this.qualityQueries = qqs;
-    this.qqParser = qqParser;
-    this.searcher = searcher;
-    this.docNameField = docNameField;
-  }
-
-  /**
-   * Run the quality benchmark.
-   * @param judge the judge that can tell if a certain result doc is relevant for a certain quality query. 
-   *        If null, no judgements would be made. Usually null for a submission run. 
-   * @param submitRep submission report is created if non null.
-   * @param qualityLog If not null, quality run data would be printed for each query.
-   * @return QualityStats of each quality query that was executed.
-   * @throws Exception if quality benchmark failed to run.
-   */
-  public  QualityStats [] execute(Judge judge, SubmissionReport submitRep, 
-                                  PrintWriter qualityLog) throws Exception {
-    int nQueries = Math.min(maxQueries, qualityQueries.length);
-    QualityStats stats[] = new QualityStats[nQueries]; 
-    for (int i=0; i<nQueries; i++) {
-      QualityQuery qq = qualityQueries[i];
-      // generate query
-      Query q = qqParser.parse(qq);
-      // search with this query 
-      long t1 = System.currentTimeMillis();
-      TopDocs td = searcher.search(q,null,maxResults);
-      long searchTime = System.currentTimeMillis()-t1;
-      //most likely we either submit or judge, but check both 
-      if (judge!=null) {
-        stats[i] = analyzeQueryResults(qq, q, td, judge, qualityLog, searchTime);
-      }
-      if (submitRep!=null) {
-        submitRep.report(qq,td,docNameField,searcher);
-      }
-    } 
-    if (submitRep!=null) {
-      submitRep.flush();
-    }
-    return stats;
-  }
-  
-  /* Analyze/judge results for a single quality query; optionally log them. */  
-  private QualityStats analyzeQueryResults(QualityQuery qq, Query q, TopDocs td, Judge judge, PrintWriter logger, long searchTime) throws IOException {
-    QualityStats stts = new QualityStats(judge.maxRecall(qq),searchTime);
-    ScoreDoc sd[] = td.scoreDocs;
-    long t1 = System.currentTimeMillis(); // extraction of first doc name we measure also construction of doc name extractor, just in case.
-    DocNameExtractor xt = new DocNameExtractor(docNameField);
-    for (int i=0; i<sd.length; i++) {
-      String docName = xt.docName(searcher,sd[i].doc);
-      long docNameExtractTime = System.currentTimeMillis() - t1;
-      t1 = System.currentTimeMillis();
-      boolean isRelevant = judge.isRelevant(docName,qq);
-      stts.addResult(i+1,isRelevant, docNameExtractTime);
-    }
-    if (logger!=null) {
-      logger.println(qq.getQueryID()+"  -  "+q);
-      stts.log(qq.getQueryID()+" Stats:",1,logger,"  ");
-    }
-    return stts;
-  }
-
-  /**
-   * @return the maximum number of quality queries to run. Useful at debugging.
-   */
-  public int getMaxQueries() {
-    return maxQueries;
-  }
-
-  /**
-   * Set the maximum number of quality queries to run. Useful at debugging.
-   */
-  public void setMaxQueries(int maxQueries) {
-    this.maxQueries = maxQueries;
-  }
-
-  /**
-   * @return the maximum number of results to collect for each quality query.
-   */
-  public int getMaxResults() {
-    return maxResults;
-  }
-
-  /**
-   * set the maximum number of results to collect for each quality query.
-   */
-  public void setMaxResults(int maxResults) {
-    this.maxResults = maxResults;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQuery.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQuery.java
deleted file mode 100755
index deb5005..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQuery.java
+++ /dev/null
@@ -1,86 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.benchmark.quality;
-
-import java.util.Map;
-
-/**
- * A QualityQuery has an ID and some name-value pairs.
- * <p> 
- * The ID allows to map the quality query with its judgements.
- * <p>
- * The name-value pairs are used by a 
- * {@link org.apache.lucene.benchmark.quality.QualityQueryParser}
- * to create a Lucene {@link org.apache.lucene.search.Query}.
- * <p>
- * It is very likely that name-value-pairs would be mapped into fields in a Lucene query,
- * but it is up to the QualityQueryParser how to map - e.g. all values in a single field, 
- * or each pair as its own field, etc., - and this of course must match the way the 
- * searched index was constructed.
- */
-public class QualityQuery implements Comparable<QualityQuery> {
-  private String queryID;
-  private Map<String,String> nameValPairs;
-
-  /**
-   * Create a QualityQuery with given ID and name-value pairs.
-   * @param queryID ID of this quality query.
-   * @param nameValPairs the contents of this quality query.
-   */
-  public QualityQuery(String queryID, Map<String,String> nameValPairs) {
-    this.queryID = queryID;
-    this.nameValPairs = nameValPairs;
-  }
-  
-  /**
-   * Return all the names of name-value-pairs in this QualityQuery.
-   */
-  public String[] getNames() {
-    return nameValPairs.keySet().toArray(new String[0]);
-  }
-
-  /**
-   * Return the value of a certain name-value pair.
-   * @param name the name whose value should be returned. 
-   */
-  public String getValue(String name) {
-    return nameValPairs.get(name);
-  }
-
-  /**
-   * Return the ID of this query.
-   * The ID allows to map the quality query with its judgements.
-   */
-  public String getQueryID() {
-    return queryID;
-  }
-
-  /* for a nicer sort of input queries before running them.
-   * Try first as ints, fall back to string if not int. */ 
-  public int compareTo(QualityQuery other) {
-    try {
-      // compare as ints when ids ints
-      int n = Integer.parseInt(queryID);
-      int nOther = Integer.parseInt(other.queryID);
-      return n - nOther;
-    } catch (NumberFormatException e) {
-      // fall back to string comparison
-      return queryID.compareTo(other.queryID);
-    }
-  }
-  
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQueryParser.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQueryParser.java
deleted file mode 100755
index 66bd275..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQueryParser.java
+++ /dev/null
@@ -1,34 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.benchmark.quality;
-
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.search.Query;
-
-/**
- * Parse a QualityQuery into a Lucene query.
- */
-public interface QualityQueryParser {
-
-  /**
-   * Parse a given QualityQuery into a Lucene query.
-   * @param qq the quality query to be parsed.
-   * @throws ParseException if parsing failed.
-   */
-  public Query parse(QualityQuery qq) throws ParseException;
-  
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityStats.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityStats.java
deleted file mode 100644
index ccf606f..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityStats.java
+++ /dev/null
@@ -1,294 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.benchmark.quality;
-
-import java.io.PrintWriter;
-import java.text.NumberFormat;
-import java.util.ArrayList;
-
-/**
- * Results of quality benchmark run for a single query or for a set of queries.
- */
-public class QualityStats {
-
-  /** Number of points for which precision is computed. */
-  public static final int MAX_POINTS = 20;
-  
-  private double maxGoodPoints;
-  private double recall;
-  private double pAt[];
-  private double pReleventSum = 0;
-  private double numPoints = 0;
-  private double numGoodPoints = 0;
-  private double mrr = 0;
-  private long searchTime;
-  private long docNamesExtractTime;
-
-  /**
-   * A certain rank in which a relevant doc was found.
-   */
-  public static class RecallPoint {
-    private int rank;
-    private double recall;
-    private RecallPoint(int rank, double recall) {
-      this.rank = rank;
-      this.recall = recall;
-    }
-    /** Returns the rank: where on the list of returned docs this relevant doc appeared. */
-    public int getRank() {
-      return rank;
-    }
-    /** Returns the recall: how many relevant docs were returned up to this point, inclusive. */
-    public double getRecall() {
-      return recall;
-    }
-  }
-  
-  private ArrayList<RecallPoint> recallPoints;
-  
-  /**
-   * Construct a QualityStats object with anticipated maximal number of relevant hits. 
-   * @param maxGoodPoints maximal possible relevant hits.
-   */
-  public QualityStats(double maxGoodPoints, long searchTime) {
-    this.maxGoodPoints = maxGoodPoints;
-    this.searchTime = searchTime;
-    this.recallPoints = new ArrayList<RecallPoint>();
-    pAt = new double[MAX_POINTS+1]; // pAt[0] unused. 
-  }
-
-  /**
-   * Add a (possibly relevant) doc.
-   * @param n rank of the added doc (its ordinal position within the query results).
-   * @param isRelevant true if the added doc is relevant, false otherwise.
-   */
-  public void addResult(int n, boolean isRelevant, long docNameExtractTime) {
-    if (Math.abs(numPoints+1 - n) > 1E-6) {
-      throw new IllegalArgumentException("point "+n+" illegal after "+numPoints+" points!");
-    }
-    if (isRelevant) {
-      numGoodPoints+=1;
-      recallPoints.add(new RecallPoint(n,numGoodPoints));
-      if (recallPoints.size()==1 && n<=5) { // first point, but only within 5 top scores. 
-        mrr =  1.0 / n;
-      }
-    }
-    numPoints = n;
-    double p = numGoodPoints / numPoints;
-    if (isRelevant) {
-      pReleventSum += p;
-    }
-    if (n<pAt.length) {
-      pAt[n] = p;
-    }
-    recall = maxGoodPoints<=0 ? p : numGoodPoints/maxGoodPoints;
-    docNamesExtractTime += docNameExtractTime;
-  }
-
-  /**
-   * Return the precision at rank n:
-   * |{relevant hits within first <code>n</code> hits}| / <code>n</code>.
-   * @param n requested precision point, must be at least 1 and at most {@link #MAX_POINTS}. 
-   */
-  public double getPrecisionAt(int n) {
-    if (n<1 || n>MAX_POINTS) {
-      throw new IllegalArgumentException("n="+n+" - but it must be in [1,"+MAX_POINTS+"] range!"); 
-    }
-    if (n>numPoints) {
-      return (numPoints * pAt[(int)numPoints])/n;
-    }
-    return pAt[n];
-  }
-
-  /**
-   * Return the average precision at recall points.
-   */
-  public double getAvp() {
-    return maxGoodPoints==0 ? 0 : pReleventSum/maxGoodPoints;
-  }
-  
-  /**
-   * Return the recall: |{relevant hits found}| / |{relevant hits existing}|.
-   */
-  public double getRecall() {
-    return recall;
-  }
-
-  /**
-   * Log information on this QualityStats object.
-   * @param logger Logger.
-   * @param prefix prefix before each log line.
-   */
-  public void log(String title, int paddLines, PrintWriter logger, String prefix) {
-    for (int i=0; i<paddLines; i++) {  
-      logger.println();
-    }
-    if (title!=null && title.trim().length()>0) {
-      logger.println(title);
-    }
-    prefix = prefix==null ? "" : prefix;
-    NumberFormat nf = NumberFormat.getInstance();
-    nf.setMaximumFractionDigits(3);
-    nf.setMinimumFractionDigits(3);
-    nf.setGroupingUsed(true);
-    int M = 19;
-    logger.println(prefix+format("Search Seconds: ",M)+
-        fracFormat(nf.format((double)searchTime/1000)));
-    logger.println(prefix+format("DocName Seconds: ",M)+
-        fracFormat(nf.format((double)docNamesExtractTime/1000)));
-    logger.println(prefix+format("Num Points: ",M)+
-        fracFormat(nf.format(numPoints)));
-    logger.println(prefix+format("Num Good Points: ",M)+
-        fracFormat(nf.format(numGoodPoints)));
-    logger.println(prefix+format("Max Good Points: ",M)+
-        fracFormat(nf.format(maxGoodPoints)));
-    logger.println(prefix+format("Average Precision: ",M)+
-        fracFormat(nf.format(getAvp())));
-    logger.println(prefix+format("MRR: ",M)+
-        fracFormat(nf.format(getMRR())));
-    logger.println(prefix+format("Recall: ",M)+
-        fracFormat(nf.format(getRecall())));
-    for (int i=1; i<(int)numPoints && i<pAt.length; i++) {
-      logger.println(prefix+format("Precision At "+i+": ",M)+
-          fracFormat(nf.format(getPrecisionAt(i))));
-    }
-    for (int i=0; i<paddLines; i++) {  
-      logger.println();
-    }
-  }
-
-  private static String padd = "                                    ";
-  private String format(String s, int minLen) {
-    s = (s==null ? "" : s);
-    int n = Math.max(minLen,s.length());
-    return (s+padd).substring(0,n);
-  }
-  private String fracFormat(String frac) {
-    int k = frac.indexOf('.');
-    String s1 = padd+frac.substring(0,k);
-    int n = Math.max(k,6);
-    s1 = s1.substring(s1.length()-n);
-    return s1 + frac.substring(k);
-  }
-  
-  /**
-   * Create a QualityStats object that is the average of the input QualityStats objects. 
-   * @param stats array of input stats to be averaged.
-   * @return an average over the input stats.
-   */
-  public static QualityStats average(QualityStats[] stats) {
-    QualityStats avg = new QualityStats(0,0);
-    if (stats.length==0) {
-      // weired, no stats to average!
-      return avg;
-    }
-    int m = 0; // queries with positive judgements
-    // aggregate
-    for (int i=0; i<stats.length; i++) {
-      avg.searchTime += stats[i].searchTime;
-      avg.docNamesExtractTime += stats[i].docNamesExtractTime;
-      if (stats[i].maxGoodPoints>0) {
-        m++;
-        avg.numGoodPoints += stats[i].numGoodPoints;
-        avg.numPoints += stats[i].numPoints;
-        avg.pReleventSum += stats[i].getAvp();
-        avg.recall += stats[i].recall;
-        avg.mrr += stats[i].getMRR();
-        avg.maxGoodPoints += stats[i].maxGoodPoints;
-        for (int j=1; j<avg.pAt.length; j++) {
-          avg.pAt[j] += stats[i].getPrecisionAt(j);
-        }
-      }
-    }
-    assert m>0 : "Fishy: no \"good\" queries!";
-    // take average: times go by all queries, other measures go by "good" queries only.
-    avg.searchTime /= stats.length;
-    avg.docNamesExtractTime /= stats.length;
-    avg.numGoodPoints /= m;
-    avg.numPoints /= m;
-    avg.recall /= m;
-    avg.mrr /= m;
-    avg.maxGoodPoints /= m;
-    for (int j=1; j<avg.pAt.length; j++) {
-      avg.pAt[j] /= m;
-    }
-    avg.pReleventSum /= m;                 // this is actually avgp now 
-    avg.pReleventSum *= avg.maxGoodPoints; // so that getAvgP() would be correct
-    
-    return avg;
-  }
-
-  /**
-   * Returns the time it took to extract doc names for judging the measured query, in milliseconds.
-   */
-  public long getDocNamesExtractTime() {
-    return docNamesExtractTime;
-  }
-
-  /**
-   * Returns the maximal number of good points.
-   * This is the number of relevant docs known by the judge for the measured query.
-   */
-  public double getMaxGoodPoints() {
-    return maxGoodPoints;
-  }
-
-  /**
-   * Returns the number of good points (only relevant points).
-   */
-  public double getNumGoodPoints() {
-    return numGoodPoints;
-  }
-
-  /**
-   * Returns the number of points (both relevant and irrelevant points).
-   */
-  public double getNumPoints() {
-    return numPoints;
-  }
-
-  /**
-   * Returns the recallPoints.
-   */
-  public RecallPoint [] getRecallPoints() {
-    return recallPoints.toArray(new RecallPoint[0]);
-  }
-
-  /**
-   * Returns the Mean reciprocal rank over the queries or RR for a single query.
-   * <p>
-   * Reciprocal rank is defined as <code>1/r</code> where <code>r</code> is the 
-   * rank of the first correct result, or <code>0</code> if there are no correct 
-   * results within the top 5 results. 
-   * <p>
-   * This follows the definition in 
-   * <a href="http://www.cnlp.org/publications/02cnlptrec10.pdf"> 
-   * Question Answering - CNLP at the TREC-10 Question Answering Track</a>.
-   */
-  public double getMRR() {
-    return mrr;
-  }
-
-  
-  /**
-   * Returns the search time in milliseconds for the measured query.
-   */
-  public long getSearchTime() {
-    return searchTime;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/package.html b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/package.html
deleted file mode 100755
index be2622c..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/package.html
+++ /dev/null
@@ -1,82 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<body>
-<h2>Search Quality Benchmarking.</h2>
-<p>
-This package allows to benchmark search quality of a Lucene application.
-<p>
-In order to use this package you should provide:
-<ul>
-  <li>A <a href="../../search/Searcher.html">searcher</a>.</li>
-  <li><a href="QualityQuery.html">Quality queries</a>.</li>
-  <li><a href="Judge.html">Judging object</a>.</li>
-  <li><a href="utils/SubmissionReport.html">Reporting object</a>.</li>
-</ul>
-<p>
-For benchmarking TREC collections with TREC QRels, take a look at the 
-<a href="trec/package-summary.html">trec package</a>.
-<p>
-Here is a sample code used to run the TREC 2006 queries 701-850 on the .Gov2 collection:
-
-<pre>
-    File topicsFile = new File("topics-701-850.txt");
-    File qrelsFile = new File("qrels-701-850.txt");
-    Searcher searcher = new IndexSearcher("index");
-
-    int maxResults = 1000;
-    String docNameField = "docname"; 
-    
-    PrintWriter logger = new PrintWriter(System.out,true); 
-
-    // use trec utilities to read trec topics into quality queries
-    TrecTopicsReader qReader = new TrecTopicsReader();
-    QualityQuery qqs[] = qReader.readQueries(new BufferedReader(new FileReader(topicsFile)));
-    
-    // prepare judge, with trec utilities that read from a QRels file
-    Judge judge = new TrecJudge(new BufferedReader(new FileReader(qrelsFile)));
-    
-    // validate topics & judgments match each other
-    judge.validateData(qqs, logger);
-    
-    // set the parsing of quality queries into Lucene queries.
-    QualityQueryParser qqParser = new SimpleQQParser("title", "body");
-    
-    // run the benchmark
-    QualityBenchmark qrun = new QualityBenchmark(qqs, qqParser, searcher, docNameField);
-    SubmissionReport submitLog = null;
-    QualityStats stats[] = qrun.execute(maxResults, judge, submitLog, logger);
-    
-    // print an avarage sum of the results
-    QualityStats avg = QualityStats.average(stats);
-    avg.log("SUMMARY",2,logger, "  ");
-</pre>
-
-<p>
-Some immediate ways to modify this program to your needs are:
-<ul>
-  <li>To run on different formats of queries and judgements provide your own 
-      <a href="Judge.html">Judge</a> and 
-      <a href="QualityQuery.html">Quality queries</a>.</li>
-  <li>Create sophisticated Lucene queries by supplying a different 
-  <a href="QualityQueryParser.html">Quality query parser</a>.</li>
-</ul>
-
-</body>
-
-</html>
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/QueryDriver.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/QueryDriver.java
deleted file mode 100644
index 5f92f08..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/QueryDriver.java
+++ /dev/null
@@ -1,93 +0,0 @@
-package org.apache.lucene.benchmark.quality.trec;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.quality.trec.TrecJudge;
-import org.apache.lucene.benchmark.quality.trec.TrecTopicsReader;
-import org.apache.lucene.benchmark.quality.utils.SimpleQQParser;
-import org.apache.lucene.benchmark.quality.utils.SubmissionReport;
-import org.apache.lucene.benchmark.quality.*;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Searcher;
-import org.apache.lucene.store.FSDirectory;
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileReader;
-import java.io.PrintWriter;
-import java.util.HashSet;
-import java.util.Set;
-
-
-/**
- *
- *
- **/
-public class QueryDriver {
-  public static void main(String[] args) throws Exception {
-    if (args.length < 4 || args.length > 5) {
-      System.err.println("Usage: QueryDriver <topicsFile> <qrelsFile> <submissionFile> <indexDir> [querySpec]");
-      System.err.println("topicsFile: input file containing queries");
-      System.err.println("qrelsFile: input file containing relevance judgements");
-      System.err.println("submissionFile: output submission file for trec_eval");
-      System.err.println("indexDir: index directory");
-      System.err.println("querySpec: string composed of fields to use in query consisting of T=title,D=description,N=narrative:");
-      System.err.println("\texample: TD (query on Title + Description). The default is T (title only)");
-      System.exit(1);
-    }
-    
-    File topicsFile = new File(args[0]);
-    File qrelsFile = new File(args[1]);
-    SubmissionReport submitLog = new SubmissionReport(new PrintWriter(args[2]), "lucene");
-    FSDirectory dir = FSDirectory.open(new File(args[3]));
-    String fieldSpec = args.length == 5 ? args[4] : "T"; // default to Title-only if not specified.
-    Searcher searcher = new IndexSearcher(dir, true);
-
-    int maxResults = 1000;
-    String docNameField = "docname";
-
-    PrintWriter logger = new PrintWriter(System.out, true);
-
-    // use trec utilities to read trec topics into quality queries
-    TrecTopicsReader qReader = new TrecTopicsReader();
-    QualityQuery qqs[] = qReader.readQueries(new BufferedReader(new FileReader(topicsFile)));
-
-    // prepare judge, with trec utilities that read from a QRels file
-    Judge judge = new TrecJudge(new BufferedReader(new FileReader(qrelsFile)));
-
-    // validate topics & judgments match each other
-    judge.validateData(qqs, logger);
-
-    Set<String> fieldSet = new HashSet<String>();
-    if (fieldSpec.indexOf('T') >= 0) fieldSet.add("title");
-    if (fieldSpec.indexOf('D') >= 0) fieldSet.add("description");
-    if (fieldSpec.indexOf('N') >= 0) fieldSet.add("narrative");
-    
-    // set the parsing of quality queries into Lucene queries.
-    QualityQueryParser qqParser = new SimpleQQParser(fieldSet.toArray(new String[0]), "body");
-
-    // run the benchmark
-    QualityBenchmark qrun = new QualityBenchmark(qqs, qqParser, searcher, docNameField);
-    qrun.setMaxResults(maxResults);
-    QualityStats stats[] = qrun.execute(judge, submitLog, logger);
-
-    // print an avarage sum of the results
-    QualityStats avg = QualityStats.average(stats);
-    avg.log("SUMMARY", 2, logger, "  ");
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/Trec1MQReader.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/Trec1MQReader.java
deleted file mode 100755
index 02dddd3..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/Trec1MQReader.java
+++ /dev/null
@@ -1,87 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.benchmark.quality.trec;
-
-import java.io.BufferedReader;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-
-import org.apache.lucene.benchmark.quality.QualityQuery;
-
-/**
- * Read topics of TREC 1MQ track.
- * <p>
- * Expects this topic format -
- * <pre>
- *   qnum:qtext
- * </pre>
- * Comment lines starting with '#' are ignored.
- * <p>
- * All topics will have a single name value pair.
- */
-public class Trec1MQReader {
-
-  private String name;
-  
-  /**
-   *  Constructor for Trec's 1MQ TopicsReader
-   *  @param name name of name-value pair to set for all queries.
-   */
-  public Trec1MQReader(String name) {
-    super();
-    this.name = name;
-  }
-
-  /**
-   * Read quality queries from trec 1MQ format topics file.
-   * @param reader where queries are read from.
-   * @return the result quality queries.
-   * @throws IOException if cannot read the queries.
-   */
-  public QualityQuery[] readQueries(BufferedReader reader) throws IOException {
-    ArrayList<QualityQuery> res = new ArrayList<QualityQuery>();
-    String line;
-    try {
-      while (null!=(line=reader.readLine())) {
-        line = line.trim();
-        if (line.startsWith("#")) {
-          continue;
-        }
-        // id
-        int k = line.indexOf(":");
-        String id = line.substring(0,k).trim();
-        // qtext
-        String qtext = line.substring(k+1).trim();
-        // we got a topic!
-        HashMap<String,String> fields = new HashMap<String,String>();
-        fields.put(name,qtext);
-        //System.out.println("id: "+id+" qtext: "+qtext+"  line: "+line);
-        QualityQuery topic = new QualityQuery(id,fields);
-        res.add(topic);
-      }
-    } finally {
-      reader.close();
-    }
-    // sort result array (by ID) 
-    QualityQuery qq[] = res.toArray(new QualityQuery[0]);
-    Arrays.sort(qq);
-    return qq;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecJudge.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecJudge.java
deleted file mode 100644
index 3ca3877..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecJudge.java
+++ /dev/null
@@ -1,156 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.benchmark.quality.trec;
-
-import java.io.BufferedReader;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.StringTokenizer;
-
-import org.apache.lucene.benchmark.quality.Judge;
-import org.apache.lucene.benchmark.quality.QualityQuery;
-
-/**
- * Judge if given document is relevant to given quality query, based on Trec format for judgements.
- */
-public class TrecJudge implements Judge {
-
-  HashMap<String,QRelJudgement> judgements;
-  
-  /**
-   * Constructor from a reader.
-   * <p>
-   * Expected input format:
-   * <pre>
-   *     qnum  0   doc-name     is-relevant
-   * </pre> 
-   * Two sample lines:
-   * <pre> 
-   *     19    0   doc303       1
-   *     19    0   doc7295      0
-   * </pre> 
-   * @param reader where judgments are read from.
-   * @throws IOException 
-   */
-  public TrecJudge (BufferedReader reader) throws IOException {
-    judgements = new HashMap<String,QRelJudgement>();
-    QRelJudgement curr = null;
-    String zero = "0";
-    String line;
-    
-    try {
-      while (null!=(line=reader.readLine())) {
-        line = line.trim();
-        if (line.length()==0 || '#'==line.charAt(0)) {
-          continue;
-        }
-        StringTokenizer st = new StringTokenizer(line);
-        String queryID = st.nextToken();
-        st.nextToken();
-        String docName = st.nextToken();
-        boolean relevant = !zero.equals(st.nextToken());
-        assert !st.hasMoreTokens() : "wrong format: "+line+"  next: "+st.nextToken();
-        if (relevant) { // only keep relevant docs
-          if (curr==null || !curr.queryID.equals(queryID)) {
-            curr = judgements.get(queryID);
-            if (curr==null) {
-              curr = new QRelJudgement(queryID);
-              judgements.put(queryID,curr);
-            }
-          }
-          curr.addRelevandDoc(docName);
-        }
-      }
-    } finally {
-      reader.close();
-    }
-  }
-  
-  // inherit javadocs
-  public boolean isRelevant(String docName, QualityQuery query) {
-    QRelJudgement qrj = judgements.get(query.getQueryID());
-    return qrj!=null && qrj.isRelevant(docName);
-  }
-
-  /** single Judgement of a trec quality query */
-  private static class QRelJudgement {
-    private String queryID;
-    private HashMap<String,String> relevantDocs;
-    
-    QRelJudgement(String queryID) {
-      this.queryID = queryID;
-      relevantDocs = new HashMap<String,String>();
-    }
-    
-    public void addRelevandDoc(String docName) {
-      relevantDocs.put(docName,docName);
-    }
-
-    boolean isRelevant(String docName) {
-      return relevantDocs.containsKey(docName);
-    }
-
-    public int maxRecall() {
-      return relevantDocs.size();
-    }
-  }
-
-  // inherit javadocs
-  public boolean validateData(QualityQuery[] qq, PrintWriter logger) {
-    HashMap<String,QRelJudgement> missingQueries = new HashMap<String, QRelJudgement>(judgements);
-    ArrayList<String> missingJudgements = new ArrayList<String>();
-    for (int i=0; i<qq.length; i++) {
-      String id = qq[i].getQueryID();
-      if (missingQueries.containsKey(id)) {
-        missingQueries.remove(id);
-      } else {
-        missingJudgements.add(id);
-      }
-    }
-    boolean isValid = true;
-    if (missingJudgements.size()>0) {
-      isValid = false;
-      if (logger!=null) {
-        logger.println("WARNING: "+missingJudgements.size()+" queries have no judgments! - ");
-        for (int i=0; i<missingJudgements.size(); i++) {
-          logger.println("   "+ missingJudgements.get(i));
-        }
-      }
-    }
-    if (missingQueries.size()>0) {
-      isValid = false;
-      if (logger!=null) {
-        logger.println("WARNING: "+missingQueries.size()+" judgments match no query! - ");
-        for (final String id : missingQueries.keySet()) {
-          logger.println("   "+id);
-        }
-      }
-    }
-    return isValid;
-  }
-
-  // inherit javadocs
-  public int maxRecall(QualityQuery query) {
-    QRelJudgement qrj = judgements.get(query.getQueryID());
-    if (qrj!=null) {
-      return qrj.maxRecall();
-    }
-    return 0;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecTopicsReader.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecTopicsReader.java
deleted file mode 100644
index e84aa51..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecTopicsReader.java
+++ /dev/null
@@ -1,138 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.benchmark.quality.trec;
-
-import java.io.BufferedReader;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-
-import org.apache.lucene.benchmark.quality.QualityQuery;
-
-/**
- * Read TREC topics.
- * <p>
- * Expects this topic format -
- * <pre>
- *   &lt;top&gt;
- *   &lt;num&gt; Number: nnn
- *     
- *   &lt;title&gt; title of the topic
- *     
- *   &lt;desc&gt; Description:
- *   description of the topic
- *     
- *   &lt;narr&gt; Narrative:
- *   "story" composed by assessors.
- *    
- *   &lt;/top&gt;
- * </pre>
- * Comment lines starting with '#' are ignored.
- */
-public class TrecTopicsReader {
-
-  private static final String newline = System.getProperty("line.separator");
-  
-  /**
-   *  Constructor for Trec's TopicsReader
-   */
-  public TrecTopicsReader() {
-    super();
-  }
-
-  /**
-   * Read quality queries from trec format topics file.
-   * @param reader where queries are read from.
-   * @return the result quality queries.
-   * @throws IOException if cannot read the queries.
-   */
-  public QualityQuery[] readQueries(BufferedReader reader) throws IOException {
-    ArrayList<QualityQuery> res = new ArrayList<QualityQuery>();
-    StringBuilder sb;
-    try {
-      while (null!=(sb=read(reader,"<top>",null,false,false))) {
-        HashMap<String,String> fields = new HashMap<String,String>();
-        // id
-        sb = read(reader,"<num>",null,true,false);
-        int k = sb.indexOf(":");
-        String id = sb.substring(k+1).trim();
-        // title
-        sb = read(reader,"<title>",null,true,false);
-        k = sb.indexOf(">");
-        String title = sb.substring(k+1).trim();
-        // description
-        read(reader,"<desc>",null,false,false);
-        sb.setLength(0);
-        String line = null;
-        while ((line = reader.readLine()) != null) {
-          if (line.startsWith("<narr>"))
-            break;
-          if (sb.length() > 0) sb.append(' ');
-          sb.append(line);
-        }
-        String description = sb.toString().trim();
-        // narrative
-        sb.setLength(0);
-        while ((line = reader.readLine()) != null) {
-          if (line.startsWith("</top>"))
-            break;
-          if (sb.length() > 0) sb.append(' ');
-          sb.append(line);
-        }
-        String narrative = sb.toString().trim();
-        // we got a topic!
-        fields.put("title",title);
-        fields.put("description",description);
-        fields.put("narrative", narrative);
-        QualityQuery topic = new QualityQuery(id,fields);
-        res.add(topic);
-      }
-    } finally {
-      reader.close();
-    }
-    // sort result array (by ID) 
-    QualityQuery qq[] = res.toArray(new QualityQuery[0]);
-    Arrays.sort(qq);
-    return qq;
-  }
-
-  // read until finding a line that starts with the specified prefix
-  private StringBuilder read (BufferedReader reader, String prefix, StringBuilder sb, boolean collectMatchLine, boolean collectAll) throws IOException {
-    sb = (sb==null ? new StringBuilder() : sb);
-    String sep = "";
-    while (true) {
-      String line = reader.readLine();
-      if (line==null) {
-        return null;
-      }
-      if (line.startsWith(prefix)) {
-        if (collectMatchLine) {
-          sb.append(sep+line);
-          sep = newline;
-        }
-        break;
-      }
-      if (collectAll) {
-        sb.append(sep+line);
-        sep = newline;
-      }
-    }
-    //System.out.println("read: "+sb);
-    return sb;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/package.html b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/package.html
deleted file mode 100755
index dafccb7..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/package.html
+++ /dev/null
@@ -1,23 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<body>
-Utilities for Trec related quality benchmarking, feeding from Trec Topics and QRels inputs.
-</body>
-
-</html>
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/DocNameExtractor.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/DocNameExtractor.java
deleted file mode 100755
index 3639956..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/DocNameExtractor.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.benchmark.quality.utils;
-
-import java.io.IOException;
-
-import org.apache.lucene.document.FieldSelector;
-import org.apache.lucene.document.FieldSelectorResult;
-import org.apache.lucene.search.Searcher;
-
-/**
- * Utility: extract doc names from an index
- */
-public class DocNameExtractor {
-
-  private FieldSelector fldSel;
-  private String docNameField;
-  
-  /**
-   * Constructor for DocNameExtractor.
-   * @param docNameField name of the stored field containing the doc name. 
-   */
-  public DocNameExtractor (final String docNameField) {
-    this.docNameField = docNameField;
-    fldSel = new FieldSelector() {
-      public FieldSelectorResult accept(String fieldName) {
-        return fieldName.equals(docNameField) ? 
-            FieldSelectorResult.LOAD_AND_BREAK :
-              FieldSelectorResult.NO_LOAD;
-      }
-    };
-  }
-  
-  /**
-   * Extract the name of the input doc from the index.
-   * @param searcher access to the index.
-   * @param docid ID of doc whose name is needed.
-   * @return the name of the input doc as extracted from the index.
-   * @throws IOException if cannot extract the doc name from the index.
-   */
-  public String docName(Searcher searcher, int docid) throws IOException {
-    return searcher.doc(docid,fldSel).get(docNameField);
-  }
-  
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/QualityQueriesFinder.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/QualityQueriesFinder.java
deleted file mode 100755
index 228b7fb..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/QualityQueriesFinder.java
+++ /dev/null
@@ -1,135 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.benchmark.quality.utils;
-
-import java.io.File;
-import java.io.IOException;
-
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.util.PriorityQueue;
-
-/**
- * Suggest Quality queries based on an index contents.
- * Utility class, used for making quality test benchmarks.
- */
-public class QualityQueriesFinder {
-
-  private static final String newline = System.getProperty("line.separator");
-  private Directory dir;
-  
-  /**
-   * Constructor over a directory containing the index.
-   * @param dir directory containing the index we search for the quality test. 
-   */
-  private QualityQueriesFinder(Directory dir) {
-    this.dir = dir;
-  }
-
-  /**
-   * @param args {index-dir}
-   * @throws IOException  if cannot access the index.
-   */
-  public static void main(String[] args) throws IOException {
-    if (args.length<1) {
-      System.err.println("Usage: java QualityQueriesFinder <index-dir>");
-      System.exit(1);
-    }
-    QualityQueriesFinder qqf = new QualityQueriesFinder(FSDirectory.open(new File(args[0])));
-    String q[] = qqf.bestQueries("body",20);
-    for (int i=0; i<q.length; i++) {
-      System.out.println(newline+formatQueryAsTrecTopic(i,q[i],null,null));
-    }
-  }
-
-  private String [] bestQueries(String field,int numQueries) throws IOException {
-    String words[] = bestTerms("body",4*numQueries);
-    int n = words.length;
-    int m = n/4;
-    String res[] = new String[m];
-    for (int i=0; i<res.length; i++) {
-      res[i] = words[i] + " " + words[m+i]+ "  " + words[n-1-m-i]  + " " + words[n-1-i];
-      //System.out.println("query["+i+"]:  "+res[i]);
-    }
-    return res;
-  }
-  
-  private static String formatQueryAsTrecTopic (int qnum, String title, String description, String narrative) {
-    return 
-      "<top>" + newline +
-      "<num> Number: " + qnum             + newline + newline + 
-      "<title> " + (title==null?"":title) + newline + newline + 
-      "<desc> Description:"               + newline +
-      (description==null?"":description)  + newline + newline +
-      "<narr> Narrative:"                 + newline +
-      (narrative==null?"":narrative)      + newline + newline +
-      "</top>";
-  }
-  
-  private String [] bestTerms(String field,int numTerms) throws IOException {
-    PriorityQueue<TermDf> pq = new TermsDfQueue(numTerms);
-    IndexReader ir = IndexReader.open(dir, true);
-    try {
-      int threshold = ir.maxDoc() / 10; // ignore words too common.
-      Terms terms = MultiFields.getTerms(ir, field);
-      if (terms != null) {
-        TermsEnum termsEnum = terms.iterator();
-        while (termsEnum.next() != null) {
-          int df = termsEnum.docFreq();
-          if (df<threshold) {
-            String ttxt = termsEnum.term().utf8ToString();
-            pq.insertWithOverflow(new TermDf(ttxt,df));
-          }
-        }
-      }
-    } finally {
-      ir.close();
-    }
-    String res[] = new String[pq.size()];
-    int i = 0;
-    while (pq.size()>0) {
-      TermDf tdf = pq.pop(); 
-      res[i++] = tdf.word;
-      System.out.println(i+".   word:  "+tdf.df+"   "+tdf.word);
-    }
-    return res;
-  }
-
-  private static class TermDf {
-    String word;
-    int df;
-    TermDf (String word, int freq) {
-      this.word = word;
-      this.df = freq;
-    }
-  }
-  
-  private static class TermsDfQueue extends PriorityQueue<TermDf> {
-    TermsDfQueue (int maxSize) {
-      initialize(maxSize);
-    }
-    @Override
-    protected boolean lessThan(TermDf tf1, TermDf tf2) {
-      return tf1.df < tf2.df;
-    }
-  }
-  
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java
deleted file mode 100755
index 765bdc6..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java
+++ /dev/null
@@ -1,74 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.benchmark.quality.utils;
-
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.benchmark.quality.QualityQuery;
-import org.apache.lucene.benchmark.quality.QualityQueryParser;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.util.Version;
-
-/**
- * Simplistic quality query parser. A Lucene query is created by passing 
- * the value of the specified QualityQuery name-value pair(s) into 
- * a Lucene's QueryParser using StandardAnalyzer. */
-public class SimpleQQParser implements QualityQueryParser {
-
-  private String qqNames[];
-  private String indexField;
-  ThreadLocal<QueryParser> queryParser = new ThreadLocal<QueryParser>();
-
-  /**
-   * Constructor of a simple qq parser.
-   * @param qqNames name-value pairs of quality query to use for creating the query
-   * @param indexField corresponding index field  
-   */
-  public SimpleQQParser(String qqNames[], String indexField) {
-    this.qqNames = qqNames;
-    this.indexField = indexField;
-  }
-
-  /**
-   * Constructor of a simple qq parser.
-   * @param qqName name-value pair of quality query to use for creating the query
-   * @param indexField corresponding index field  
-   */
-  public SimpleQQParser(String qqName, String indexField) {
-    this(new String[] { qqName }, indexField);
-  }
-
-  /* (non-Javadoc)
-   * @see org.apache.lucene.benchmark.quality.QualityQueryParser#parse(org.apache.lucene.benchmark.quality.QualityQuery)
-   */
-  public Query parse(QualityQuery qq) throws ParseException {
-    QueryParser qp = queryParser.get();
-    if (qp==null) {
-      qp = new QueryParser(Version.LUCENE_CURRENT, indexField, new StandardAnalyzer(Version.LUCENE_CURRENT));
-      queryParser.set(qp);
-    }
-    BooleanQuery bq = new BooleanQuery();
-    for (int i = 0; i < qqNames.length; i++)
-      bq.add(qp.parse(QueryParser.escape(qq.getValue(qqNames[i]))), BooleanClause.Occur.SHOULD);
-    
-    return bq;
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SubmissionReport.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SubmissionReport.java
deleted file mode 100644
index f9ea2d0..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SubmissionReport.java
+++ /dev/null
@@ -1,93 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.benchmark.quality.utils;
-
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.text.NumberFormat;
-
-import org.apache.lucene.benchmark.quality.QualityQuery;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.Searcher;
-import org.apache.lucene.search.TopDocs;
-
-/**
- * Create a log ready for submission.
- * Extend this class and override
- * {@link #report(QualityQuery, TopDocs, String, Searcher)}
- * to create different reports. 
- */
-public class SubmissionReport {
-
-  private NumberFormat nf;
-  private PrintWriter logger;
-  private String name;
-  
-  /**
-   * Constructor for SubmissionReport.
-   * @param logger if null, no submission data is created. 
-   * @param name name of this run.
-   */
-  public SubmissionReport (PrintWriter logger, String name) {
-    this.logger = logger;
-    this.name = name;
-    nf = NumberFormat.getInstance();
-    nf.setMaximumFractionDigits(4);
-    nf.setMinimumFractionDigits(4);
-  }
-  
-  /**
-   * Report a search result for a certain quality query.
-   * @param qq quality query for which the results are reported.
-   * @param td search results for the query.
-   * @param docNameField stored field used for fetching the result doc name.  
-   * @param searcher index access for fetching doc name.
-   * @throws IOException in case of a problem.
-   */
-  public void report(QualityQuery qq, TopDocs td, String docNameField, Searcher searcher) throws IOException {
-    if (logger==null) {
-      return;
-    }
-    ScoreDoc sd[] = td.scoreDocs;
-    String sep = " \t ";
-    DocNameExtractor xt = new DocNameExtractor(docNameField);
-    for (int i=0; i<sd.length; i++) {
-      String docName = xt.docName(searcher,sd[i].doc);
-      logger.println(
-          qq.getQueryID()       + sep +
-          "Q0"                   + sep +
-          format(docName,20)    + sep +
-          format(""+i,7)        + sep +
-          nf.format(sd[i].score) + sep +
-          name
-          );
-    }
-  }
-
-  public void flush() {
-    if (logger!=null) {
-      logger.flush();
-    }
-  }
-  
-  private static String padd = "                                    ";
-  private String format(String s, int minLen) {
-    s = (s==null ? "" : s);
-    int n = Math.max(minLen,s.length());
-    return (s+padd).substring(0,n);
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/package.html b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/package.html
deleted file mode 100755
index 7fde1a8..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/package.html
+++ /dev/null
@@ -1,23 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<body>
-Miscellaneous utilities for search quality benchmarking: query parsing, submission reports.
-</body>
-
-</html>
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/stats/MemUsage.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/stats/MemUsage.java
deleted file mode 100644
index 2ebb015..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/stats/MemUsage.java
+++ /dev/null
@@ -1,43 +0,0 @@
-package org.apache.lucene.benchmark.stats;
-/**
- * Copyright 2005 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * This class holds a set of memory usage values.
- *
- */
-public class MemUsage {
-  public long maxFree, minFree, avgFree;
-
-  public long maxTotal, minTotal, avgTotal;
-
-  @Override
-  public String toString() {
-    return toScaledString(1, "B");
-  }
-
-  /** Scale down the values by divisor, append the unit string. */
-  public String toScaledString(int div, String unit) {
-    StringBuilder sb = new StringBuilder();
-      sb.append("free=").append(minFree / div);
-      sb.append("/").append(avgFree / div);
-      sb.append("/").append(maxFree / div).append(" ").append(unit);
-      sb.append(", total=").append(minTotal / div);
-      sb.append("/").append(avgTotal / div);
-      sb.append("/").append(maxTotal / div).append(" ").append(unit);
-    return sb.toString();
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/stats/QueryData.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/stats/QueryData.java
deleted file mode 100644
index cdf7f52..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/stats/QueryData.java
+++ /dev/null
@@ -1,79 +0,0 @@
-package org.apache.lucene.benchmark.stats;
-/**
- * Copyright 2005 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Vector;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.benchmark.Constants;
-
-/**
- * This class holds parameters for a query benchmark.
- *
- */
-public class QueryData {
-  /** Benchmark id */
-  public String id;
-  /** Lucene query */
-  public Query q;
-  /** If true, re-open index reader before benchmark. */
-  public boolean reopen;
-  /** If true, warm-up the index reader before searching by sequentially
-   * retrieving all documents from index.
-   */
-  public boolean warmup;
-  /**
-   * If true, actually retrieve documents returned in Hits.
-   */
-  public boolean retrieve;
-
-  /**
-   * Prepare a list of benchmark data, using all possible combinations of
-   * benchmark parameters.
-   * @param queries source Lucene queries
-   * @return The QueryData
-   */
-  public static QueryData[] getAll(Query[] queries) {
-    Vector<QueryData> vqd = new Vector<QueryData>();
-    for (int i = 0; i < queries.length; i++) {
-      for (int r = 1; r >= 0; r--) {
-        for (int w = 1; w >= 0; w--) {
-          for (int t = 0; t < 2; t++) {
-            QueryData qd = new QueryData();
-            qd.id="qd-" + i + r + w + t;
-            qd.reopen = Constants.BOOLEANS[r].booleanValue();
-            qd.warmup = Constants.BOOLEANS[w].booleanValue();
-            qd.retrieve = Constants.BOOLEANS[t].booleanValue();
-            qd.q = queries[i];
-            vqd.add(qd);
-          }
-        }
-      }
-    }
-    return vqd.toArray(new QueryData[0]);
-  }
-
-  /** Short legend for interpreting toString() output. */
-  public static String getLabels() {
-    return "# Query data: R-reopen, W-warmup, T-retrieve, N-no";
-  }
-
-  @Override
-  public String toString() {
-    return id + " " + (reopen ? "R" : "NR") + " " + (warmup ? "W" : "NW") +
-      " " + (retrieve ? "T" : "NT") + " [" + q.toString() + "]";
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/stats/TestData.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/stats/TestData.java
deleted file mode 100644
index 58b840d..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/stats/TestData.java
+++ /dev/null
@@ -1,574 +0,0 @@
-package org.apache.lucene.benchmark.stats;
-/**
- * Copyright 2005 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import java.io.File;
-import java.text.NumberFormat;
-import java.util.ArrayList;
-
-import java.util.Date;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Vector;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.benchmark.Constants;
-import org.apache.lucene.store.Directory;
-
-
-/**
- * This class holds together all parameters related to a test. Single test is
- * performed several times, and all results are averaged.
- *
- */
-public class TestData
-{
-    public static int[] MAX_BUFFERED_DOCS_COUNTS = new int[]{10, 20, 50, 100, 200, 500};
-    public static int[] MERGEFACTOR_COUNTS = new int[]{10, 20, 50, 100, 200, 500};
-
-    /**
-     * ID of this test data.
-     */
-    private String id;
-    /**
-     * Heap size.
-     */
-    private long heap;
-    /**
-     * List of results for each test run with these parameters.
-     */
-    private Vector<TestRunData> runData = new Vector<TestRunData>();
-    private int maxBufferedDocs, mergeFactor;
-    /**
-     * Directory containing source files.
-     */
-    private File source;
-    /**
-     * Lucene Directory implementation for creating an index.
-     */
-    private Directory directory;
-    /**
-     * Analyzer to use when adding documents.
-     */
-    private Analyzer analyzer;
-    /**
-     * If true, use compound file format.
-     */
-    private boolean compound;
-    /**
-     * If true, optimize index when finished adding documents.
-     */
-    private boolean optimize;
-    /**
-     * Data for search benchmarks.
-     */
-    private QueryData[] queries;
-
-    public TestData()
-    {
-        heap = Runtime.getRuntime().maxMemory();
-    }
-
-    private static class DCounter
-    {
-        double total;
-        int count, recordCount;
-    }
-
-    private static class LCounter
-    {
-        long total;
-        int count;
-    }
-
-    private static class LDCounter
-    {
-      double Dtotal;
-      int Dcount, DrecordCount;
-      long Ltotal0;
-      int Lcount0;
-      long Ltotal1;
-      int Lcount1;
-    }
-
-    /**
-     * Get a textual summary of the benchmark results, average from all test runs.
-     */
-    static final String ID =      "# testData id     ";
-    static final String OP =      "operation      ";
-    static final String RUNCNT =  "     runCnt";
-    static final String RECCNT =  "     recCnt";
-    static final String RECSEC =  "          rec/s";
-    static final String FREEMEM = "       avgFreeMem";
-    static final String TOTMEM =  "      avgTotalMem";
-    static final String COLS[] = {
-        ID,
-        OP,
-        RUNCNT,
-        RECCNT,
-        RECSEC,
-        FREEMEM,
-        TOTMEM
-    };
-    public String showRunData(String prefix)
-    {
-        if (runData.size() == 0)
-        {
-            return "# [NO RUN DATA]";
-        }
-        HashMap<String,LDCounter> resByTask = new HashMap<String,LDCounter>(); 
-        StringBuilder sb = new StringBuilder();
-        String lineSep = System.getProperty("line.separator");
-        sb.append("warm = Warm Index Reader").append(lineSep).append("srch = Search Index").append(lineSep).append("trav = Traverse Hits list, optionally retrieving document").append(lineSep).append(lineSep);
-        for (int i = 0; i < COLS.length; i++) {
-          sb.append(COLS[i]);
-        }
-        sb.append("\n");
-        LinkedHashMap<String,TestData.LCounter[]> mapMem = new LinkedHashMap<String,TestData.LCounter[]>();
-        LinkedHashMap<String,DCounter> mapSpeed = new LinkedHashMap<String,DCounter>();
-        for (int i = 0; i < runData.size(); i++)
-        {
-            TestRunData trd = runData.get(i);
-            for (final String label : trd.getLabels()) 
-            {
-                MemUsage mem = trd.getMemUsage(label);
-                if (mem != null)
-                {
-                    TestData.LCounter[] tm = mapMem.get(label);
-                    if (tm == null)
-                    {
-                        tm = new TestData.LCounter[2];
-                        tm[0] = new TestData.LCounter();
-                        tm[1] = new TestData.LCounter();
-                        mapMem.put(label, tm);
-                    }
-                    tm[0].total += mem.avgFree;
-                    tm[0].count++;
-                    tm[1].total += mem.avgTotal;
-                    tm[1].count++;
-                }
-                TimeData td = trd.getTotals(label);
-                if (td != null)
-                {
-                    TestData.DCounter dc = mapSpeed.get(label);
-                    if (dc == null)
-                    {
-                        dc = new TestData.DCounter();
-                        mapSpeed.put(label, dc);
-                    }
-                    dc.count++;
-                    //dc.total += td.getRate();
-                    dc.total += (td.count>0 && td.elapsed<=0 ? 1 : td.elapsed); // assume at least 1ms for any countable op
-                    dc.recordCount += td.count;
-                }
-            }
-        }
-        LinkedHashMap<String,String> res = new LinkedHashMap<String,String>();
-        Iterator<String> it = mapSpeed.keySet().iterator();
-        while (it.hasNext())
-        {
-            String label = it.next();
-            TestData.DCounter dc = mapSpeed.get(label);
-            res.put(label, 
-                format(dc.count, RUNCNT) + 
-                format(dc.recordCount / dc.count, RECCNT) +
-                format(1,(float) (dc.recordCount * 1000.0 / (dc.total>0 ? dc.total : 1.0)), RECSEC)
-                //format((float) (dc.total / (double) dc.count), RECSEC)
-                );
-            
-            // also sum by task
-            String task = label.substring(label.lastIndexOf("-")+1);
-            LDCounter ldc = resByTask.get(task);
-            if (ldc==null) {
-              ldc = new LDCounter();
-              resByTask.put(task,ldc);
-            }
-            ldc.Dcount += dc.count;
-            ldc.DrecordCount += dc.recordCount;
-            ldc.Dtotal += (dc.count>0 && dc.total<=0 ? 1 : dc.total); // assume at least 1ms for any countable op 
-        }
-        it = mapMem.keySet().iterator();
-        while (it.hasNext())
-        {
-            String label = it.next();
-            TestData.LCounter[] lc =  mapMem.get(label);
-            String speed = res.get(label);
-            boolean makeSpeed = false;
-            if (speed == null)
-            {
-                makeSpeed = true;
-                speed =  
-                  format(lc[0].count, RUNCNT) + 
-                  format(0, RECCNT) + 
-                  format(0,(float)0.0, RECSEC);
-            }
-            res.put(label, speed + 
-                format(0, lc[0].total / lc[0].count, FREEMEM) + 
-                format(0, lc[1].total / lc[1].count, TOTMEM));
-            
-            // also sum by task
-            String task = label.substring(label.lastIndexOf("-")+1);
-            LDCounter ldc = resByTask.get(task);
-            if (ldc==null) {
-              ldc = new LDCounter();
-              resByTask.put(task,ldc);
-              makeSpeed = true;
-            }
-            if (makeSpeed) {
-              ldc.Dcount += lc[0].count;
-            }
-            ldc.Lcount0 += lc[0].count;
-            ldc.Lcount1 += lc[1].count;
-            ldc.Ltotal0 += lc[0].total;
-            ldc.Ltotal1 += lc[1].total;
-        }
-        it = res.keySet().iterator();
-        while (it.hasNext())
-        {
-            String label = it.next();
-            sb.append(format(prefix, ID));
-            sb.append(format(label, OP));
-            sb.append(res.get(label)).append("\n");
-        }
-        // show results by task (srch, optimize, etc.) 
-        sb.append("\n");
-        for (int i = 0; i < COLS.length; i++) {
-          sb.append(COLS[i]);
-        }
-        sb.append("\n");
-        it = resByTask.keySet().iterator();
-        while (it.hasNext())
-        {
-            String task = it.next();
-            LDCounter ldc = resByTask.get(task);
-            sb.append(format("    ", ID));
-            sb.append(format(task, OP));
-            sb.append(format(ldc.Dcount, RUNCNT)); 
-            sb.append(format(ldc.DrecordCount / ldc.Dcount, RECCNT));
-            sb.append(format(1,(float) (ldc.DrecordCount * 1000.0 / (ldc.Dtotal>0 ? ldc.Dtotal : 1.0)), RECSEC));
-            sb.append(format(0, ldc.Ltotal0 / ldc.Lcount0, FREEMEM)); 
-            sb.append(format(0, ldc.Ltotal1 / ldc.Lcount1, TOTMEM));
-            sb.append("\n");
-        }
-        return sb.toString();
-    }
-
-    private static NumberFormat numFormat [] = { NumberFormat.getInstance(), NumberFormat.getInstance()};
-    private static final String padd = "                                  ";
-    static {
-      numFormat[0].setMaximumFractionDigits(0);
-      numFormat[0].setMinimumFractionDigits(0);
-      numFormat[1].setMaximumFractionDigits(1);
-      numFormat[1].setMinimumFractionDigits(1);
-    }
-
-    // pad number from left
-    // numFracDigits must be 0 or 1.
-    static String format(int numFracDigits, float f, String col) {
-      String res = padd + numFormat[numFracDigits].format(f);
-      return res.substring(res.length() - col.length());
-    }
-
-    // pad number from left
-    static String format(int n, String col) {
-      String res = padd + n;
-      return res.substring(res.length() - col.length());
-    }
-
-    // pad string from right
-    static String format(String s, String col) {
-      return (s + padd).substring(0,col.length());
-    }
-
-    /**
-     * Prepare a list of benchmark data, using all possible combinations of
-     * benchmark parameters.
-     *
-     * @param sources   list of directories containing different source document
-     *                  collections
-     * @param analyzers of analyzers to use.
-     */
-    public static TestData[] getAll(File[] sources, Analyzer[] analyzers)
-    {
-        List<TestData> res = new ArrayList<TestData>(50);
-        TestData ref = new TestData();
-        for (int q = 0; q < analyzers.length; q++)
-        {
-            for (int m = 0; m < sources.length; m++)
-            {
-                for (int i = 0; i < MAX_BUFFERED_DOCS_COUNTS.length; i++)
-                {
-                    for (int k = 0; k < MERGEFACTOR_COUNTS.length; k++)
-                    {
-                        for (int n = 0; n < Constants.BOOLEANS.length; n++)
-                        {
-                            for (int p = 0; p < Constants.BOOLEANS.length; p++)
-                            {
-                                ref.id = "td-" + q + m + i + k + n + p;
-                                ref.source = sources[m];
-                                ref.analyzer = analyzers[q];
-                                ref.maxBufferedDocs = MAX_BUFFERED_DOCS_COUNTS[i];
-                                ref.mergeFactor = MERGEFACTOR_COUNTS[k];
-                                ref.compound = Constants.BOOLEANS[n].booleanValue();
-                                ref.optimize = Constants.BOOLEANS[p].booleanValue();
-                                try
-                                {
-                                    res.add((TestData)ref.clone());
-                                }
-                                catch (Exception e)
-                                {
-                                    e.printStackTrace();
-                                }
-                            }
-                        }
-                    }
-                }
-            }
-        }
-        return res.toArray(new TestData[0]);
-    }
-
-    /**
-     * Similar to {@link #getAll(java.io.File[], org.apache.lucene.analysis.Analyzer[])} but only uses
-     * maxBufferedDocs of 10 and 100 and same for mergeFactor, thus reducing the number of permutations significantly.
-     * It also only uses compound file and optimize is always true.
-     *
-     * @param sources
-     * @param analyzers
-     * @return An Array of {@link TestData}
-     */
-    public static TestData[] getTestDataMinMaxMergeAndMaxBuffered(File[] sources, Analyzer[] analyzers)
-    {
-        List<TestData> res = new ArrayList<TestData>(50);
-        TestData ref = new TestData();
-        for (int q = 0; q < analyzers.length; q++)
-        {
-            for (int m = 0; m < sources.length; m++)
-            {
-                ref.id = "td-" + q + m + "_" + 10 + "_" + 10;
-                ref.source = sources[m];
-                ref.analyzer = analyzers[q];
-                ref.maxBufferedDocs = 10;
-                ref.mergeFactor = 10;//MERGEFACTOR_COUNTS[k];
-                ref.compound = true;
-                ref.optimize = true;
-                try
-                {
-                    res.add((TestData)ref.clone());
-                }
-                catch (Exception e)
-                {
-                    e.printStackTrace();
-                }
-                ref.id = "td-" + q + m  + "_" + 10 + "_" + 100;
-                ref.source = sources[m];
-                ref.analyzer = analyzers[q];
-                ref.maxBufferedDocs = 10;
-                ref.mergeFactor = 100;//MERGEFACTOR_COUNTS[k];
-                ref.compound = true;
-                ref.optimize = true;
-                try
-                {
-                    res.add((TestData)ref.clone());
-                }
-                catch (Exception e)
-                {
-                    e.printStackTrace();
-                }
-                ref.id = "td-" + q + m + "_" + 100 + "_" + 10;
-                ref.source = sources[m];
-                ref.analyzer = analyzers[q];
-                ref.maxBufferedDocs = 100;
-                ref.mergeFactor = 10;//MERGEFACTOR_COUNTS[k];
-                ref.compound = true;
-                ref.optimize = true;
-                try
-                {
-                    res.add((TestData)ref.clone());
-                }
-                catch (Exception e)
-                {
-                    e.printStackTrace();
-                }
-                ref.id = "td-" + q + m + "_" + 100 + "_" + 100;
-                ref.source = sources[m];
-                ref.analyzer = analyzers[q];
-                ref.maxBufferedDocs = 100;
-                ref.mergeFactor = 100;//MERGEFACTOR_COUNTS[k];
-                ref.compound = true;
-                ref.optimize = true;
-                try
-                {
-                    res.add((TestData)ref.clone());
-                }
-                catch (Exception e)
-                {
-                    e.printStackTrace();
-                }
-            }
-        }
-        return res.toArray(new TestData[0]);
-    }
-
-    @Override
-    protected Object clone()
-    {
-        TestData cl = new TestData();
-        cl.id = id;
-        cl.compound = compound;
-        cl.heap = heap;
-        cl.mergeFactor = mergeFactor;
-        cl.maxBufferedDocs = maxBufferedDocs;
-        cl.optimize = optimize;
-        cl.source = source;
-        cl.directory = directory;
-        cl.analyzer = analyzer;
-        // don't clone runData
-        return cl;
-    }
-
-    @Override
-    public String toString()
-    {
-        StringBuilder res = new StringBuilder();
-        res.append("#-- ID: ").append(id).append(", ").append(new Date()).append(", heap=").append(heap).append(" --\n");
-        res.append("# source=").append(source).append(", directory=").append(directory).append("\n");
-        res.append("# maxBufferedDocs=").append(maxBufferedDocs).append(", mergeFactor=").append(mergeFactor);
-        res.append(", compound=").append(compound).append(", optimize=").append(optimize).append("\n");
-        if (queries != null)
-        {
-            res.append(QueryData.getLabels()).append("\n");
-            for (int i = 0; i < queries.length; i++)
-            {
-                res.append("# ").append(queries[i].toString()).append("\n");
-            }
-        }
-        return res.toString();
-    }
-
-    public Analyzer getAnalyzer()
-    {
-        return analyzer;
-    }
-
-    public void setAnalyzer(Analyzer analyzer)
-    {
-        this.analyzer = analyzer;
-    }
-
-    public boolean isCompound()
-    {
-        return compound;
-    }
-
-    public void setCompound(boolean compound)
-    {
-        this.compound = compound;
-    }
-
-    public Directory getDirectory()
-    {
-        return directory;
-    }
-
-    public void setDirectory(Directory directory)
-    {
-        this.directory = directory;
-    }
-
-    public long getHeap()
-    {
-        return heap;
-    }
-
-    public void setHeap(long heap)
-    {
-        this.heap = heap;
-    }
-
-    public String getId()
-    {
-        return id;
-    }
-
-    public void setId(String id)
-    {
-        this.id = id;
-    }
-
-    public int getMaxBufferedDocs()
-    {
-        return maxBufferedDocs;
-    }
-
-    public void setMaxBufferedDocs(int maxBufferedDocs)
-    {
-        this.maxBufferedDocs = maxBufferedDocs;
-    }
-
-    public int getMergeFactor()
-    {
-        return mergeFactor;
-    }
-
-    public void setMergeFactor(int mergeFactor)
-    {
-        this.mergeFactor = mergeFactor;
-    }
-
-    public boolean isOptimize()
-    {
-        return optimize;
-    }
-
-    public void setOptimize(boolean optimize)
-    {
-        this.optimize = optimize;
-    }
-
-    public QueryData[] getQueries()
-    {
-        return queries;
-    }
-
-    public void setQueries(QueryData[] queries)
-    {
-        this.queries = queries;
-    }
-
-    public Vector<TestRunData> getRunData()
-    {
-        return runData;
-    }
-
-    public void setRunData(Vector<TestRunData> runData)
-    {
-        this.runData = runData;
-    }
-
-    public File getSource()
-    {
-        return source;
-    }
-
-    public void setSource(File source)
-    {
-        this.source = source;
-    }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/stats/TestRunData.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/stats/TestRunData.java
deleted file mode 100644
index d047155..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/stats/TestRunData.java
+++ /dev/null
@@ -1,172 +0,0 @@
-package org.apache.lucene.benchmark.stats;
-/**
- * Copyright 2005 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import java.util.LinkedHashMap;
-import java.util.Vector;
-import java.util.Collection;
-import java.util.Iterator;
-
-/**
- * This class holds series of TimeData related to a single test run. TimeData
- * values may contribute to different measurements, so this class provides also
- * some useful methods to separate them.
- *
- */
-public class TestRunData {
-  private String id;
-
-  /** Start and end time of this test run. */
-  private long start = 0L, end = 0L;
-
-  private LinkedHashMap<String,Vector<TimeData>> data = new LinkedHashMap<String,Vector<TimeData>>();
-
-  public TestRunData() {}
-
-  public TestRunData(String id) {
-    this.id = id;
-  }
-
-    public LinkedHashMap<String,Vector<TimeData>> getData()
-    {
-        return data;
-    }
-
-    public String getId()
-    {
-        return id;
-    }
-
-    public void setId(String id)
-    {
-        this.id = id;
-    }
-
-    public long getEnd()
-    {
-        return end;
-    }
-
-    public long getStart()
-    {
-        return start;
-    }
-
-    /** Mark the starting time of this test run. */
-  public void startRun() {
-    start = System.currentTimeMillis();
-  }
-
-  /** Mark the ending time of this test run. */
-  public void endRun() {
-    end = System.currentTimeMillis();
-  }
-
-  /** Add a data point. */
-  public void addData(TimeData td) {
-    td.recordMemUsage();
-    Vector<TimeData> v = data.get(td.name);
-    if (v == null) {
-      v = new Vector<TimeData>();
-      data.put(td.name, v);
-    }
-    v.add((TimeData)td.clone());
-  }
-
-  /** Get a list of all available types of data points. */
-  public Collection<String> getLabels() {
-    return data.keySet();
-  }
-
-  /** Get total values from all data points of a given type. */
-  public TimeData getTotals(String label) {
-    Vector<TimeData> v = data.get(label);
-      if (v == null)
-      {
-          return null;
-      }
-    TimeData res = new TimeData("TOTAL " + label);
-    for (int i = 0; i < v.size(); i++) {
-      TimeData td = v.get(i);
-      res.count += td.count;
-      res.elapsed += td.elapsed;
-    }
-    return res;
-  }
-
-  /** Get total values from all data points of all types.
-   * @return a list of TimeData values for all types.
-   */
-  public Vector<TimeData> getTotals() {
-    Collection<String> labels = getLabels();
-    Vector<TimeData> v = new Vector<TimeData>();
-    Iterator<String> it = labels.iterator();
-    while (it.hasNext()) {
-      TimeData td = getTotals(it.next());
-      v.add(td);
-    }
-    return v;
-  }
-
-  /** Get memory usage stats for a given data type. */
-  public MemUsage getMemUsage(String label) {
-    Vector<TimeData> v = data.get(label);
-      if (v == null)
-      {
-          return null;
-      }
-    MemUsage res = new MemUsage();
-    res.minFree = Long.MAX_VALUE;
-    res.minTotal = Long.MAX_VALUE;
-    long avgFree = 0L, avgTotal = 0L;
-    for (int i = 0; i < v.size(); i++) {
-      TimeData td = v.get(i);
-        if (res.maxFree < td.freeMem)
-        {
-            res.maxFree = td.freeMem;
-        }
-        if (res.maxTotal < td.totalMem)
-        {
-            res.maxTotal = td.totalMem;
-        }
-        if (res.minFree > td.freeMem)
-        {
-            res.minFree = td.freeMem;
-        }
-        if (res.minTotal > td.totalMem)
-        {
-            res.minTotal = td.totalMem;
-        }
-      avgFree += td.freeMem;
-      avgTotal += td.totalMem;
-    }
-    res.avgFree = avgFree / v.size();
-    res.avgTotal = avgTotal / v.size();
-    return res;
-  }
-
-  /** Return a string representation. */
-  @Override
-  public String toString() {
-    StringBuilder sb = new StringBuilder();
-    for (final String label : getLabels()) {
-        sb.append(id).append("-").append(label).append(" ").append(getTotals(label).toString(false)).append(" ");
-        sb.append(getMemUsage(label).toScaledString(1024 * 1024, "MB")).append("\n");
-    }
-    return sb.toString();
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/stats/TimeData.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/stats/TimeData.java
deleted file mode 100644
index c210939..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/stats/TimeData.java
+++ /dev/null
@@ -1,103 +0,0 @@
-package org.apache.lucene.benchmark.stats;
-/**
- * Copyright 2005 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/**
- * This class holds a data point measuring speed of processing.
- *
- */
-public class TimeData {
-  /** Name of the data point - usually one of a data series with the same name */
-  public String name;
-  /** Number of records processed. */
-  public long count = 0;
-  /** Elapsed time in milliseconds. */
-  public long elapsed = 0L;
-
-  private long delta = 0L;
-  /** Free memory at the end of measurement interval. */
-  public long freeMem = 0L;
-  /** Total memory at the end of measurement interval. */
-  public long totalMem = 0L;
-
-  public TimeData() {}
-
-  public TimeData(String name) {
-    this.name = name;
-  }
-
-  /** Start counting elapsed time. */
-  public void start() {
-    delta = System.currentTimeMillis();
-  }
-
-  /** Stop counting elapsed time. */
-  public void stop() {
-    count++;
-    elapsed += (System.currentTimeMillis() - delta);
-  }
-
-  /** Record memory usage. */
-  public void recordMemUsage() {
-    freeMem = Runtime.getRuntime().freeMemory();
-    totalMem = Runtime.getRuntime().totalMemory();
-  }
-
-  /** Reset counters. */
-  public void reset() {
-    count = 0;
-    elapsed = 0L;
-    delta = elapsed;
-  }
-
-  @Override
-  protected Object clone() {
-    TimeData td = new TimeData(name);
-    td.name = name;
-    td.elapsed = elapsed;
-    td.count = count;
-    td.delta = delta;
-    td.freeMem = freeMem;
-    td.totalMem = totalMem;
-    return td;
-  }
-
-  /** Get rate of processing, defined as number of processed records per second. */
-  public double getRate() {
-    double rps = count * 1000.0 / (elapsed > 0 ? elapsed : 1); // assume at least 1ms for any countable op
-    return rps;
-  }
-
-  /** Get a short legend for toString() output. */
-  public static String getLabels() {
-    return "# count\telapsed\trec/s\tfreeMem\ttotalMem";
-  }
-
-  @Override
-  public String toString() { return toString(true); }
-  /**
-   * Return a tab-separated string containing this data.
-   * @param withMem if true, append also memory information
-   * @return The String
-   */
-  public String toString(boolean withMem) {
-    StringBuilder sb = new StringBuilder();
-    sb.append(count + "\t" + elapsed + "\t" + getRate());
-    if (withMem) sb.append("\t" + freeMem + "\t" + totalMem);
-    return sb.toString();
-  }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/utils/ExtractReuters.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/utils/ExtractReuters.java
deleted file mode 100644
index 3e4104b..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/utils/ExtractReuters.java
+++ /dev/null
@@ -1,174 +0,0 @@
-package org.apache.lucene.benchmark.utils;
-/**
- * Copyright 2005 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileFilter;
-import java.io.FileReader;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-
-/**
- * Split the Reuters SGML documents into Simple Text files containing: Title, Date, Dateline, Body
- */
-public class ExtractReuters
-{
-    private File reutersDir;
-    private File outputDir;
-    private static final String LINE_SEPARATOR = System.getProperty("line.separator");
-
-    public ExtractReuters(File reutersDir, File outputDir)
-    {
-        this.reutersDir = reutersDir;
-        this.outputDir = outputDir;
-        System.out.println("Deleting all files in " + outputDir);
-        File [] files = outputDir.listFiles();
-        for (int i = 0; i < files.length; i++)
-        {
-            files[i].delete();
-        }
-
-    }
-
-    public void extract()
-    {
-        File [] sgmFiles = reutersDir.listFiles(new FileFilter()
-        {
-            public boolean accept(File file)
-            {
-                return file.getName().endsWith(".sgm");
-            }
-        });
-        if (sgmFiles != null && sgmFiles.length > 0)
-        {
-            for (int i = 0; i < sgmFiles.length; i++)
-            {
-                File sgmFile = sgmFiles[i];
-                extractFile(sgmFile);
-            }
-        }
-        else
-        {
-            System.err.println("No .sgm files in " + reutersDir);
-        }
-    }
-
-    Pattern EXTRACTION_PATTERN = Pattern.compile("<TITLE>(.*?)</TITLE>|<DATE>(.*?)</DATE>|<BODY>(.*?)</BODY>");
-
-    private static String[] META_CHARS
-            = {"&", "<", ">", "\"", "'"};
-
-    private static String[] META_CHARS_SERIALIZATIONS
-            = {"&amp;", "&lt;", "&gt;", "&quot;", "&apos;"};
-
-    /**
-     * Override if you wish to change what is extracted
-     *
-     * @param sgmFile
-     */
-    protected void extractFile(File sgmFile)
-    {
-        try
-        {
-            BufferedReader reader = new BufferedReader(new FileReader(sgmFile));
-
-            StringBuilder buffer = new StringBuilder(1024);
-            StringBuilder outBuffer = new StringBuilder(1024);
-
-            String line = null;
-            int docNumber = 0;
-            while ((line = reader.readLine()) != null)
-            {
-                //when we see a closing reuters tag, flush the file
-
-                if (line.indexOf("</REUTERS") == -1) {
-                    //Replace the SGM escape sequences
-
-                    buffer.append(line).append(' ');//accumulate the strings for now, then apply regular expression to get the pieces,
-                }
-                else
-                {
-                    //Extract the relevant pieces and write to a file in the output dir
-                    Matcher matcher = EXTRACTION_PATTERN.matcher(buffer);
-                    while (matcher.find())
-                    {
-                        for (int i = 1; i <= matcher.groupCount(); i++)
-                        {
-                            if (matcher.group(i) != null)
-                            {
-                                outBuffer.append(matcher.group(i));
-                            }
-                        }
-                        outBuffer.append(LINE_SEPARATOR).append(LINE_SEPARATOR);
-                    }
-                    String out = outBuffer.toString();
-                    for (int i = 0; i < META_CHARS_SERIALIZATIONS.length; i++)
-                    {
-                        out = out.replaceAll(META_CHARS_SERIALIZATIONS[i], META_CHARS[i]);
-                    }
-                    File outFile = new File(outputDir, sgmFile.getName() + "-" + (docNumber++) + ".txt");
-                    //System.out.println("Writing " + outFile);
-                    FileWriter writer = new FileWriter(outFile);
-                    writer.write(out);
-                    writer.close();
-                    outBuffer.setLength(0);
-                    buffer.setLength(0);
-                }
-            }
-            reader.close();
-        }
-
-        catch (
-                IOException e
-                )
-
-        {
-            throw new RuntimeException(e);
-        }
-    }
-
-
-    public static void main(String[] args)
-    {
-        if (args.length != 2)
-        {
-            printUsage();
-        }
-        File reutersDir = new File(args[0]);
-
-        if (reutersDir.exists())
-        {
-            File outputDir = new File(args[1]);
-            outputDir.mkdirs();
-            ExtractReuters extractor = new ExtractReuters(reutersDir, outputDir);
-            extractor.extract();
-        }
-        else
-        {
-            printUsage();
-        }
-    }
-
-    private static void printUsage()
-    {
-        System.err.println("Usage: java -cp <...> org.apache.lucene.benchmark.utils.ExtractReuters <Path to Reuters SGM files> <Output Path>");
-    }
-}
diff --git a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/utils/ExtractWikipedia.java b/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/utils/ExtractWikipedia.java
deleted file mode 100644
index feeb6da..0000000
--- a/lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/utils/ExtractWikipedia.java
+++ /dev/null
@@ -1,151 +0,0 @@
-package org.apache.lucene.benchmark.utils;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.util.Properties;
-
-import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
-import org.apache.lucene.benchmark.byTask.feeds.NoMoreDataException;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.document.Document;
-
-/**
- * Extract the downloaded Wikipedia dump into separate files for indexing.
- */
-public class ExtractWikipedia {
-
-  private File outputDir;
-
-  static public int count = 0;
-
-  static final int BASE = 10;
-  protected DocMaker docMaker;
-
-  public ExtractWikipedia(DocMaker docMaker, File outputDir) {
-    this.outputDir = outputDir;
-    this.docMaker = docMaker;
-    System.out.println("Deleting all files in " + outputDir);
-    File[] files = outputDir.listFiles();
-    for (int i = 0; i < files.length; i++) {
-      files[i].delete();
-    }
-  }
-
-  public File directory(int count, File directory) {
-    if (directory == null) {
-      directory = outputDir;
-    }
-    int base = BASE;
-    while (base <= count) {
-      base *= BASE;
-    }
-    if (count < BASE) {
-      return directory;
-    }
-    directory = new File(directory, (Integer.toString(base / BASE)));
-    directory = new File(directory, (Integer.toString(count / (base / BASE))));
-    return directory(count % (base / BASE), directory);
-  }
-
-  public void create(String id, String title, String time, String body) {
-
-    File d = directory(count++, null);
-    d.mkdirs();
-    File f = new File(d, id + ".txt");
-
-    StringBuilder contents = new StringBuilder();
-
-    contents.append(time);
-    contents.append("\n\n");
-    contents.append(title);
-    contents.append("\n\n");
-    contents.append(body);
-    contents.append("\n");
-
-    try {
-      FileWriter writer = new FileWriter(f);
-      writer.write(contents.toString());
-      writer.close();
-    } catch (IOException ioe) {
-      throw new RuntimeException(ioe);
-    }
-
-  }
-
-  public void extract() throws Exception {
-    Document doc = null;
-    System.out.println("Starting Extraction");
-    long start = System.currentTimeMillis();
-    try {
-      while ((doc = docMaker.makeDocument()) != null) {
-        create(doc.get(DocMaker.ID_FIELD), doc.get(DocMaker.TITLE_FIELD), doc
-            .get(DocMaker.DATE_FIELD), doc.get(DocMaker.BODY_FIELD));
-      }
-    } catch (NoMoreDataException e) {
-      //continue
-    }
-    long finish = System.currentTimeMillis();
-    System.out.println("Extraction took " + (finish - start) + " ms");
-  }
-
-  public static void main(String[] args) throws Exception {
-
-    File wikipedia = null;
-    File outputDir = new File("./enwiki");
-    boolean keepImageOnlyDocs = true;
-    for (int i = 0; i < args.length; i++) {
-      String arg = args[i];
-      if (arg.equals("--input") || arg.equals("-i")) {
-        wikipedia = new File(args[i + 1]);
-        i++;
-      } else if (arg.equals("--output") || arg.equals("-o")) {
-        outputDir = new File(args[i + 1]);
-        i++;
-      } else if (arg.equals("--discardImageOnlyDocs") || arg.equals("-d")) {
-        keepImageOnlyDocs = false;
-      }
-
-    }
-    DocMaker docMaker = new DocMaker();
-    Properties properties = new Properties();
-    properties.setProperty("content.source", "org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource");
-    properties.setProperty("docs.file", wikipedia.getAbsolutePath());
-    properties.setProperty("content.source.forever", "false");
-    properties.setProperty("keep.image.only.docs", String.valueOf(keepImageOnlyDocs));
-    docMaker.setConfig(new Config(properties));
-    docMaker.resetInputs();
-    if (wikipedia.exists()) {
-      System.out.println("Extracting Wikipedia to: " + outputDir + " using EnwikiContentSource");
-      outputDir.mkdirs();
-      ExtractWikipedia extractor = new ExtractWikipedia(docMaker, outputDir);
-      extractor.extract();
-    } else {
-      printUsage();
-    }
-  }
-
-  private static void printUsage() {
-    System.err.println("Usage: java -cp <...> org.apache.lucene.benchmark.utils.ExtractWikipedia --input|-i <Path to Wikipedia XML file> " +
-            "[--output|-o <Output Path>] [--discardImageOnlyDocs|-d]");
-    System.err.println("--discardImageOnlyDocs tells the extractor to skip Wiki docs that contain only images");
-  }
-
-}
\ No newline at end of file
diff --git a/lucene/contrib/benchmark/src/java/overview.html b/lucene/contrib/benchmark/src/java/overview.html
deleted file mode 100644
index c0f44cd..0000000
--- a/lucene/contrib/benchmark/src/java/overview.html
+++ /dev/null
@@ -1,26 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-  <head>
-    <title>
-      benchmark
-    </title>
-  </head>
-  <body>
-  benchmark
-  </body>
-</html>
\ No newline at end of file
diff --git a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/BenchmarkTestCase.java b/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/BenchmarkTestCase.java
deleted file mode 100644
index 4fbac12..0000000
--- a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/BenchmarkTestCase.java
+++ /dev/null
@@ -1,100 +0,0 @@
-package org.apache.lucene.benchmark;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.io.StringReader;
-
-import org.apache.lucene.benchmark.byTask.Benchmark;
-import org.apache.lucene.util.LuceneTestCase;
-
-/** Base class for all Benchmark unit tests. */
-public abstract class BenchmarkTestCase extends LuceneTestCase {
-  
-  public File getWorkDir() {
-    return TEMP_DIR;
-  }
-  
-  /** Copy a resource into the workdir */
-  public void copyToWorkDir(String resourceName) throws IOException {
-    InputStream resource = getClass().getResourceAsStream(resourceName);
-    OutputStream dest = new FileOutputStream(new File(getWorkDir(), resourceName));
-    byte[] buffer = new byte[8192];
-    int len;
-    
-    while ((len = resource.read(buffer)) > 0) {
-        dest.write(buffer, 0, len);
-    }
-
-    resource.close();
-    dest.close();
-  }
-  
-  /** Return a path, suitable for a .alg config file, for a resource in the workdir */
-  public String getWorkDirResourcePath(String resourceName) {
-    return new File(getWorkDir(), resourceName).getAbsolutePath().replace("\\", "/");
-  }
-  
-  /** Return a path, suitable for a .alg config file, for the workdir */
-  public String getWorkDirPath() {
-    return getWorkDir().getAbsolutePath().replace("\\", "/");
-  }
-  
-  // create the benchmark and execute it. 
-  public Benchmark execBenchmark(String[] algLines) throws Exception {
-    String algText = algLinesToText(algLines);
-    logTstLogic(algText);
-    Benchmark benchmark = new Benchmark(new StringReader(algText));
-    benchmark.execute();
-    return benchmark;
-  }
-  
-  // properties in effect in all tests here
-  final String propLines [] = {
-    "work.dir=" + getWorkDirPath(),
-    "directory=RAMDirectory",
-    "print.props=false",
-  };
-  
-  static final String NEW_LINE = System.getProperty("line.separator");
-  
-  // catenate alg lines to make the alg text
-  private String algLinesToText(String[] algLines) {
-    String indent = "  ";
-    StringBuilder sb = new StringBuilder();
-    for (int i = 0; i < propLines.length; i++) {
-      sb.append(indent).append(propLines[i]).append(NEW_LINE);
-    }
-    for (int i = 0; i < algLines.length; i++) {
-      sb.append(indent).append(algLines[i]).append(NEW_LINE);
-    }
-    return sb.toString();
-  }
-
-  private static void logTstLogic (String txt) {
-    if (!VERBOSE) 
-      return;
-    System.out.println("Test logic of:");
-    System.out.println(txt);
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java b/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
deleted file mode 100755
index 90c970f..0000000
--- a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
+++ /dev/null
@@ -1,1052 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.benchmark.byTask;
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileReader;
-import java.io.StringReader;
-import java.text.Collator;
-import java.util.List;
-import java.util.Locale;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.benchmark.BenchmarkTestCase;
-import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
-import org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker;
-import org.apache.lucene.benchmark.byTask.stats.TaskStats;
-import org.apache.lucene.benchmark.byTask.tasks.CountingHighlighterTestTask;
-import org.apache.lucene.benchmark.byTask.tasks.CountingSearchTestTask;
-import org.apache.lucene.collation.CollationKeyAnalyzer;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.LogDocMergePolicy;
-import org.apache.lucene.index.LogMergePolicy;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.index.SerialMergeScheduler;
-import org.apache.lucene.index.TermFreqVector;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.search.FieldCache.DocTermsIndex;
-import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Test very simply that perf tasks - simple algorithms - are doing what they should.
- */
-public class TestPerfTasksLogic extends BenchmarkTestCase {
-
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    copyToWorkDir("reuters.first20.lines.txt");
-  }
-
-  /**
-   * Test index creation logic
-   */
-  public void testIndexAndSearchTasks() throws Exception {
-    // 1. alg definition (required in every "logic" test)
-    String algLines[] = {
-        "ResetSystemErase",
-        "CreateIndex",
-        "{ AddDoc } : 1000",
-        "Optimize",
-        "CloseIndex",
-        "OpenReader",
-        "{ CountingSearchTest } : 200",
-        "CloseReader",
-        "[ CountingSearchTest > : 70",
-        "[ CountingSearchTest > : 9",
-    };
-    
-    // 2. we test this value later
-    CountingSearchTestTask.numSearches = 0;
-    
-    // 3. execute the algorithm  (required in every "logic" test)
-    Benchmark benchmark = execBenchmark(algLines);
-
-    // 4. test specific checks after the benchmark run completed.
-    assertEquals("TestSearchTask was supposed to be called!",279,CountingSearchTestTask.numSearches);
-    assertTrue("Index does not exist?...!", IndexReader.indexExists(benchmark.getRunData().getDirectory()));
-    // now we should be able to open the index for write. 
-    IndexWriter iw = new IndexWriter(benchmark.getRunData().getDirectory(),
-        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())
-            .setOpenMode(OpenMode.APPEND));
-    iw.close();
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
-    assertEquals("1000 docs were added to the index, this is what we expect to find!",1000,ir.numDocs());
-    ir.close();
-  }
-
-  /**
-   * Test timed sequence task.
-   */
-  public void testTimedSearchTask() throws Exception {
-    String algLines[] = {
-        "log.step=100000",
-        "ResetSystemErase",
-        "CreateIndex",
-        "{ AddDoc } : 100",
-        "Optimize",
-        "CloseIndex",
-        "OpenReader",
-        "{ CountingSearchTest } : .5s",
-        "CloseReader",
-    };
-
-    CountingSearchTestTask.numSearches = 0;
-    execBenchmark(algLines);
-    assertTrue(CountingSearchTestTask.numSearches > 0);
-    long elapsed = CountingSearchTestTask.prevLastMillis - CountingSearchTestTask.startMillis;
-    assertTrue("elapsed time was " + elapsed + " msec", elapsed <= 1500);
-  }
-
-  // disabled until we fix BG thread prio -- this test
-  // causes build to hang
-  public void testBGSearchTaskThreads() throws Exception {
-    String algLines[] = {
-        "log.time.step.msec = 100",
-        "log.step=100000",
-        "ResetSystemErase",
-        "CreateIndex",
-        "{ AddDoc } : 1000",
-        "Optimize",
-        "CloseIndex",
-        "OpenReader",
-        "{",
-        "  [ \"XSearch\" { CountingSearchTest > : * ] : 2 &-1",
-        "  Wait(0.5)",
-        "}",
-        "CloseReader",
-        "RepSumByPref X"
-    };
-
-    CountingSearchTestTask.numSearches = 0;
-    execBenchmark(algLines);
-    assertTrue(CountingSearchTestTask.numSearches > 0);
-  }
-
-  public void testHighlighting() throws Exception {
-    // 1. alg definition (required in every "logic" test)
-    String algLines[] = {
-        "doc.stored=true",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-        "docs.file=" + getReuters20LinesFile(),
-        "query.maker=" + ReutersQueryMaker.class.getName(),
-        "ResetSystemErase",
-        "CreateIndex",
-        "{ AddDoc } : 100",
-        "Optimize",
-        "CloseIndex",
-        "OpenReader(true)",
-        "{ CountingHighlighterTest(size[1],highlight[1],mergeContiguous[true],maxFrags[1],fields[body]) } : 200",
-        "CloseReader",
-    };
-
-    // 2. we test this value later
-    CountingHighlighterTestTask.numHighlightedResults = 0;
-    CountingHighlighterTestTask.numDocsRetrieved = 0;
-    // 3. execute the algorithm  (required in every "logic" test)
-    Benchmark benchmark = execBenchmark(algLines);
-
-    // 4. test specific checks after the benchmark run completed.
-    assertEquals("TestSearchTask was supposed to be called!",92,CountingHighlighterTestTask.numDocsRetrieved);
-    //pretty hard to figure out a priori how many docs are going to have highlighted fragments returned, but we can never have more than the number of docs
-    //we probably should use a different doc/query maker, but...
-    assertTrue("TestSearchTask was supposed to be called!", CountingHighlighterTestTask.numDocsRetrieved >= CountingHighlighterTestTask.numHighlightedResults && CountingHighlighterTestTask.numHighlightedResults > 0);
-
-    assertTrue("Index does not exist?...!", IndexReader.indexExists(benchmark.getRunData().getDirectory()));
-    // now we should be able to open the index for write.
-    IndexWriter iw = new IndexWriter(benchmark.getRunData().getDirectory(), new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));
-    iw.close();
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
-    assertEquals("100 docs were added to the index, this is what we expect to find!",100,ir.numDocs());
-    ir.close();
-  }
-
-  public void testHighlightingTV() throws Exception {
-    // 1. alg definition (required in every "logic" test)
-    String algLines[] = {
-        "doc.stored=true",//doc storage is required in order to have text to highlight
-        "doc.term.vector.offsets=true",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-        "docs.file=" + getReuters20LinesFile(),
-        "query.maker=" + ReutersQueryMaker.class.getName(),
-        "ResetSystemErase",
-        "CreateIndex",
-        "{ AddDoc } : 1000",
-        "Optimize",
-        "CloseIndex",
-        "OpenReader(false)",
-        "{ CountingHighlighterTest(size[1],highlight[1],mergeContiguous[true],maxFrags[1],fields[body]) } : 200",
-        "CloseReader",
-    };
-
-    // 2. we test this value later
-    CountingHighlighterTestTask.numHighlightedResults = 0;
-    CountingHighlighterTestTask.numDocsRetrieved = 0;
-    // 3. execute the algorithm  (required in every "logic" test)
-    Benchmark benchmark = execBenchmark(algLines);
-
-    // 4. test specific checks after the benchmark run completed.
-    assertEquals("TestSearchTask was supposed to be called!",92,CountingHighlighterTestTask.numDocsRetrieved);
-    //pretty hard to figure out a priori how many docs are going to have highlighted fragments returned, but we can never have more than the number of docs
-    //we probably should use a different doc/query maker, but...
-    assertTrue("TestSearchTask was supposed to be called!", CountingHighlighterTestTask.numDocsRetrieved >= CountingHighlighterTestTask.numHighlightedResults && CountingHighlighterTestTask.numHighlightedResults > 0);
-
-    assertTrue("Index does not exist?...!", IndexReader.indexExists(benchmark.getRunData().getDirectory()));
-    // now we should be able to open the index for write.
-    IndexWriter iw = new IndexWriter(benchmark.getRunData().getDirectory(), new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));
-    iw.close();
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
-    assertEquals("1000 docs were added to the index, this is what we expect to find!",1000,ir.numDocs());
-    ir.close();
-  }
-
-  public void testHighlightingNoTvNoStore() throws Exception {
-    // 1. alg definition (required in every "logic" test)
-    String algLines[] = {
-        "doc.stored=false",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-        "docs.file=" + getReuters20LinesFile(),
-        "query.maker=" + ReutersQueryMaker.class.getName(),
-        "ResetSystemErase",
-        "CreateIndex",
-        "{ AddDoc } : 1000",
-        "Optimize",
-        "CloseIndex",
-        "OpenReader",
-        "{ CountingHighlighterTest(size[1],highlight[1],mergeContiguous[true],maxFrags[1],fields[body]) } : 200",
-        "CloseReader",
-    };
-
-    // 2. we test this value later
-    CountingHighlighterTestTask.numHighlightedResults = 0;
-    CountingHighlighterTestTask.numDocsRetrieved = 0;
-    // 3. execute the algorithm  (required in every "logic" test)
-    try {
-      Benchmark benchmark = execBenchmark(algLines);
-      assertTrue("CountingHighlighterTest should have thrown an exception", false);
-      assertNotNull(benchmark); // (avoid compile warning on unused variable)
-    } catch (Exception e) {
-      assertTrue(true);
-    }
-  }
-
-  /**
-   * Test Exhasting Doc Maker logic
-   */
-  public void testExhaustContentSource() throws Exception {
-    // 1. alg definition (required in every "logic" test)
-    String algLines[] = {
-        "# ----- properties ",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource",
-        "content.source.log.step=1",
-        "doc.term.vector=false",
-        "content.source.forever=false",
-        "directory=RAMDirectory",
-        "doc.stored=false",
-        "doc.tokenized=false",
-        "# ----- alg ",
-        "CreateIndex",
-        "{ AddDoc } : * ",
-        "Optimize",
-        "CloseIndex",
-        "OpenReader",
-        "{ CountingSearchTest } : 100",
-        "CloseReader",
-        "[ CountingSearchTest > : 30",
-        "[ CountingSearchTest > : 9",
-    };
-    
-    // 2. we test this value later
-    CountingSearchTestTask.numSearches = 0;
-    
-    // 3. execute the algorithm  (required in every "logic" test)
-    Benchmark benchmark = execBenchmark(algLines);
-
-    // 4. test specific checks after the benchmark run completed.
-    assertEquals("TestSearchTask was supposed to be called!",139,CountingSearchTestTask.numSearches);
-    assertTrue("Index does not exist?...!", IndexReader.indexExists(benchmark.getRunData().getDirectory()));
-    // now we should be able to open the index for write. 
-    IndexWriter iw = new IndexWriter(benchmark.getRunData().getDirectory(), new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));
-    iw.close();
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
-    assertEquals("1 docs were added to the index, this is what we expect to find!",1,ir.numDocs());
-    ir.close();
-  }
-
-  // LUCENE-1994: test thread safety of SortableSingleDocMaker
-  public void testDocMakerThreadSafety() throws Exception {
-    // 1. alg definition (required in every "logic" test)
-    String algLines[] = {
-        "# ----- properties ",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.SortableSingleDocSource",
-        "doc.term.vector=false",
-        "log.step.AddDoc=10000",
-        "content.source.forever=true",
-        "directory=RAMDirectory",
-        "doc.reuse.fields=false",
-        "doc.stored=false",
-        "doc.tokenized=false",
-        "doc.index.props=true",
-        "# ----- alg ",
-        "CreateIndex",
-        "[ { AddDoc > : 250 ] : 4",
-        "CloseIndex",
-    };
-    
-    // 2. we test this value later
-    CountingSearchTestTask.numSearches = 0;
-    
-    // 3. execute the algorithm  (required in every "logic" test)
-    Benchmark benchmark = execBenchmark(algLines);
-
-    IndexReader r = IndexReader.open(benchmark.getRunData().getDirectory(), true);
-    DocTermsIndex idx = FieldCache.DEFAULT.getTermsIndex(r, "country");
-    final int maxDoc = r.maxDoc();
-    assertEquals(1000, maxDoc);
-    BytesRef br = new BytesRef();
-    for(int i=0;i<1000;i++) {
-      assertNotNull("doc " + i + " has null country", idx.getTerm(i, br));
-    }
-    r.close();
-  }
-
-  /**
-   * Test Parallel Doc Maker logic (for LUCENE-940)
-   */
-  public void testParallelDocMaker() throws Exception {
-    // 1. alg definition (required in every "logic" test)
-    String algLines[] = {
-        "# ----- properties ",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-        "docs.file=" + getReuters20LinesFile(),
-        "content.source.log.step=3",
-        "doc.term.vector=false",
-        "content.source.forever=false",
-        "directory=FSDirectory",
-        "doc.stored=false",
-        "doc.tokenized=false",
-        "# ----- alg ",
-        "CreateIndex",
-        "[ { AddDoc } : * ] : 4 ",
-        "CloseIndex",
-    };
-    
-    // 2. execute the algorithm  (required in every "logic" test)
-    Benchmark benchmark = execBenchmark(algLines);
-
-    // 3. test number of docs in the index
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
-    int ndocsExpected = 20; // first 20 reuters docs.
-    assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
-    ir.close();
-  }
-
-  /**
-   * Test WriteLineDoc and LineDocSource.
-   */
-  public void testLineDocFile() throws Exception {
-    File lineFile = new File(TEMP_DIR, "test.reuters.lines.txt");
-
-    // We will call WriteLineDocs this many times
-    final int NUM_TRY_DOCS = 50;
-
-    // Creates a line file with first 50 docs from SingleDocSource
-    String algLines1[] = {
-      "# ----- properties ",
-      "content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource",
-      "content.source.forever=true",
-      "line.file.out=" + lineFile.getAbsolutePath().replace('\\', '/'),
-      "# ----- alg ",
-      "{WriteLineDoc()}:" + NUM_TRY_DOCS,
-    };
-
-    // Run algo
-    Benchmark benchmark = execBenchmark(algLines1);
-
-    BufferedReader r = new BufferedReader(new FileReader(lineFile));
-    int numLines = 0;
-    while(r.readLine() != null)
-      numLines++;
-    r.close();
-    assertEquals("did not see the right number of docs; should be " + NUM_TRY_DOCS + " but was " + numLines, NUM_TRY_DOCS, numLines);
-    
-    // Index the line docs
-    String algLines2[] = {
-      "# ----- properties ",
-      "analyzer=org.apache.lucene.analysis.MockAnalyzer",
-      "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-      "docs.file=" + lineFile.getAbsolutePath().replace('\\', '/'),
-      "content.source.forever=false",
-      "doc.reuse.fields=false",
-      "ram.flush.mb=4",
-      "# ----- alg ",
-      "ResetSystemErase",
-      "CreateIndex",
-      "{AddDoc}: *",
-      "CloseIndex",
-    };
-    
-    // Run algo
-    benchmark = execBenchmark(algLines2);
-
-    // now we should be able to open the index for write. 
-    IndexWriter iw = new IndexWriter(benchmark.getRunData().getDirectory(),
-        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())
-            .setOpenMode(OpenMode.APPEND));
-    iw.close();
-
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
-    assertEquals(numLines + " lines were created but " + ir.numDocs() + " docs are in the index", numLines, ir.numDocs());
-    ir.close();
-
-    lineFile.delete();
-  }
-  
-  /**
-   * Test ReadTokensTask
-   */
-  public void testReadTokens() throws Exception {
-
-    // We will call ReadTokens on this many docs
-    final int NUM_DOCS = 20;
-
-    // Read tokens from first NUM_DOCS docs from Reuters and
-    // then build index from the same docs
-    String algLines1[] = {
-      "# ----- properties ",
-      "analyzer=org.apache.lucene.analysis.MockAnalyzer",
-      "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-      "docs.file=" + getReuters20LinesFile(),
-      "# ----- alg ",
-      "{ReadTokens}: " + NUM_DOCS,
-      "ResetSystemErase",
-      "CreateIndex",
-      "{AddDoc}: " + NUM_DOCS,
-      "CloseIndex",
-    };
-
-    // Run algo
-    Benchmark benchmark = execBenchmark(algLines1);
-
-    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();
-
-    // Count how many tokens all ReadTokens saw
-    int totalTokenCount1 = 0;
-    for (final TaskStats stat : stats) {
-      if (stat.getTask().getName().equals("ReadTokens")) {
-        totalTokenCount1 += stat.getCount();
-      }
-    }
-
-    // Separately count how many tokens are actually in the index:
-    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);
-    assertEquals(NUM_DOCS, reader.numDocs());
-
-    int totalTokenCount2 = 0;
-
-    FieldsEnum fields = MultiFields.getFields(reader).iterator();
-    String fieldName = null;
-    while((fieldName = fields.next()) != null) {
-      if (fieldName == DocMaker.ID_FIELD || fieldName == DocMaker.DATE_MSEC_FIELD || fieldName == DocMaker.TIME_SEC_FIELD) {
-        continue;
-      }
-      TermsEnum terms = fields.terms();
-      DocsEnum docs = null;
-      while(terms.next() != null) {
-        docs = terms.docs(MultiFields.getDeletedDocs(reader), docs);
-        while(docs.nextDoc() != docs.NO_MORE_DOCS) {
-          totalTokenCount2 += docs.freq();
-        }
-      }
-    }
-    reader.close();
-
-    // Make sure they are the same
-    assertEquals(totalTokenCount1, totalTokenCount2);
-  }
-  
-  /**
-   * Test that " {[AddDoc(4000)]: 4} : * " works corrcetly (for LUCENE-941)
-   */
-  public void testParallelExhausted() throws Exception {
-    // 1. alg definition (required in every "logic" test)
-    String algLines[] = {
-        "# ----- properties ",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-        "docs.file=" + getReuters20LinesFile(),
-        "content.source.log.step=3",
-        "doc.term.vector=false",
-        "content.source.forever=false",
-        "directory=RAMDirectory",
-        "doc.stored=false",
-        "doc.tokenized=false",
-        "task.max.depth.log=1",
-        "# ----- alg ",
-        "CreateIndex",
-        "{ [ AddDoc]: 4} : * ",
-        "ResetInputs ",
-        "{ [ AddDoc]: 4} : * ",
-        "CloseIndex",
-    };
-    
-    // 2. execute the algorithm  (required in every "logic" test)
-    Benchmark benchmark = execBenchmark(algLines);
-
-    // 3. test number of docs in the index
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
-    int ndocsExpected = 2 * 20; // first 20 reuters docs.
-    assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
-    ir.close();
-  }
-
-
-  /**
-   * Test that exhaust in loop works as expected (LUCENE-1115).
-   */
-  public void testExhaustedLooped() throws Exception {
-    // 1. alg definition (required in every "logic" test)
-    String algLines[] = {
-        "# ----- properties ",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-        "docs.file=" + getReuters20LinesFile(),
-        "content.source.log.step=3",
-        "doc.term.vector=false",
-        "content.source.forever=false",
-        "directory=RAMDirectory",
-        "doc.stored=false",
-        "doc.tokenized=false",
-        "task.max.depth.log=1",
-        "# ----- alg ",
-        "{ \"Rounds\"",
-        "  ResetSystemErase",
-        "  CreateIndex",
-        "  { \"AddDocs\"  AddDoc > : * ",
-        "  CloseIndex",
-        "} : 2",
-    };
-    
-    // 2. execute the algorithm  (required in every "logic" test)
-    Benchmark benchmark = execBenchmark(algLines);
-
-    // 3. test number of docs in the index
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
-    int ndocsExpected = 20;  // first 20 reuters docs.
-    assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
-    ir.close();
-  }
-  
-  /**
-   * Test that we can close IndexWriter with argument "false".
-   */
-  public void testCloseIndexFalse() throws Exception {
-    // 1. alg definition (required in every "logic" test)
-    String algLines[] = {
-        "# ----- properties ",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-        "docs.file=" + getReuters20LinesFile(),
-        "ram.flush.mb=-1",
-        "max.buffered=2",
-        "content.source.log.step=3",
-        "doc.term.vector=false",
-        "content.source.forever=false",
-        "directory=RAMDirectory",
-        "doc.stored=false",
-        "doc.tokenized=false",
-        "debug.level=1",
-        "# ----- alg ",
-        "{ \"Rounds\"",
-        "  ResetSystemErase",
-        "  CreateIndex",
-        "  { \"AddDocs\"  AddDoc > : * ",
-        "  CloseIndex(false)",
-        "} : 2",
-    };
-    
-    // 2. execute the algorithm  (required in every "logic" test)
-    Benchmark benchmark = execBenchmark(algLines);
-
-    // 3. test number of docs in the index
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
-    int ndocsExpected = 20; // first 20 reuters docs.
-    assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
-    ir.close();
-  }
-
-  public static class MyMergeScheduler extends SerialMergeScheduler {
-    boolean called;
-    public MyMergeScheduler() {
-      super();
-      called = true;
-    }
-  }
-
-  public void testDeleteByPercent() throws Exception {
-    // 1. alg definition (required in every "logic" test)
-    String algLines[] = {
-        "# ----- properties ",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-        "docs.file=" + getReuters20LinesFile(),
-        "ram.flush.mb=-1",
-        "max.buffered=2",
-        "content.source.log.step=3",
-        "doc.term.vector=false",
-        "content.source.forever=false",
-        "directory=RAMDirectory",
-        "doc.stored=false",
-        "doc.tokenized=false",
-        "debug.level=1",
-        "# ----- alg ",
-        "CreateIndex",
-        "{ \"AddDocs\"  AddDoc > : * ",
-        "CloseIndex()",
-        "OpenReader(false)",
-        "DeleteByPercent(20)",
-        "CloseReader"
-    };
-    
-    // 2. execute the algorithm  (required in every "logic" test)
-    Benchmark benchmark = execBenchmark(algLines);
-
-    // 3. test number of docs in the index
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
-    int ndocsExpected = 16; // first 20 reuters docs, minus 20%
-    assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
-    ir.close();
-  }
-
-  /**
-   * Test that we can set merge scheduler".
-   */
-  public void testMergeScheduler() throws Exception {
-    // 1. alg definition (required in every "logic" test)
-    String algLines[] = {
-        "# ----- properties ",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-        "docs.file=" + getReuters20LinesFile(),
-        "content.source.log.step=3",
-        "doc.term.vector=false",
-        "content.source.forever=false",
-        "directory=RAMDirectory",
-        "merge.scheduler=" + MyMergeScheduler.class.getName(),
-        "doc.stored=false",
-        "doc.tokenized=false",
-        "debug.level=1",
-        "# ----- alg ",
-        "{ \"Rounds\"",
-        "  ResetSystemErase",
-        "  CreateIndex",
-        "  { \"AddDocs\"  AddDoc > : * ",
-        "} : 2",
-    };
-    // 2. execute the algorithm  (required in every "logic" test)
-    Benchmark benchmark = execBenchmark(algLines);
-
-    assertTrue("did not use the specified MergeScheduler",
-        ((MyMergeScheduler) benchmark.getRunData().getIndexWriter().getConfig()
-            .getMergeScheduler()).called);
-    benchmark.getRunData().getIndexWriter().close();
-
-    // 3. test number of docs in the index
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
-    int ndocsExpected = 20; // first 20 reuters docs.
-    assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
-    ir.close();
-  }
-
-  public static class MyMergePolicy extends LogDocMergePolicy {
-    boolean called;
-    public MyMergePolicy() {
-      called = true;
-    }
-  }
-  
-  /**
-   * Test that we can set merge policy".
-   */
-  public void testMergePolicy() throws Exception {
-    // 1. alg definition (required in every "logic" test)
-    String algLines[] = {
-        "# ----- properties ",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-        "docs.file=" + getReuters20LinesFile(),
-        "content.source.log.step=3",
-        "ram.flush.mb=-1",
-        "max.buffered=2",
-        "doc.term.vector=false",
-        "content.source.forever=false",
-        "directory=RAMDirectory",
-        "merge.policy=" + MyMergePolicy.class.getName(),
-        "doc.stored=false",
-        "doc.tokenized=false",
-        "debug.level=1",
-        "# ----- alg ",
-        "{ \"Rounds\"",
-        "  ResetSystemErase",
-        "  CreateIndex",
-        "  { \"AddDocs\"  AddDoc > : * ",
-        "} : 2",
-    };
-
-    // 2. execute the algorithm  (required in every "logic" test)
-    Benchmark benchmark = execBenchmark(algLines);
-    assertTrue("did not use the specified MergePolicy", ((MyMergePolicy) benchmark.getRunData().getIndexWriter().getConfig().getMergePolicy()).called);
-    benchmark.getRunData().getIndexWriter().close();
-    
-    // 3. test number of docs in the index
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
-    int ndocsExpected = 20; // first 20 reuters docs.
-    assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
-    ir.close();
-  }
-
-  /**
-   * Test that IndexWriter settings stick.
-   */
-  public void testIndexWriterSettings() throws Exception {
-    // 1. alg definition (required in every "logic" test)
-    String algLines[] = {
-        "# ----- properties ",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-        "docs.file=" + getReuters20LinesFile(),
-        "content.source.log.step=3",
-        "ram.flush.mb=-1",
-        "max.buffered=2",
-        "compound=cmpnd:true:false",
-        "doc.term.vector=vector:false:true",
-        "content.source.forever=false",
-        "directory=RAMDirectory",
-        "doc.stored=false",
-        "merge.factor=3",
-        "doc.tokenized=false",
-        "debug.level=1",
-        "# ----- alg ",
-        "{ \"Rounds\"",
-        "  ResetSystemErase",
-        "  CreateIndex",
-        "  { \"AddDocs\"  AddDoc > : * ",
-        "  NewRound",
-        "} : 2",
-    };
-
-    // 2. execute the algorithm  (required in every "logic" test)
-    Benchmark benchmark = execBenchmark(algLines);
-    final IndexWriter writer = benchmark.getRunData().getIndexWriter();
-    assertEquals(2, writer.getConfig().getMaxBufferedDocs());
-    assertEquals(IndexWriterConfig.DISABLE_AUTO_FLUSH, (int) writer.getConfig().getRAMBufferSizeMB());
-    assertEquals(3, ((LogMergePolicy) writer.getConfig().getMergePolicy()).getMergeFactor());
-    assertFalse(((LogMergePolicy) writer.getConfig().getMergePolicy()).getUseCompoundFile());
-    writer.close();
-    Directory dir = benchmark.getRunData().getDirectory();
-    IndexReader reader = IndexReader.open(dir, true);
-    TermFreqVector [] tfv = reader.getTermFreqVectors(0);
-    assertNotNull(tfv);
-    assertTrue(tfv.length > 0);
-    reader.close();
-  }
-
-  /**
-   * Test that we can call optimize(maxNumSegments).
-   */
-  public void testOptimizeMaxNumSegments() throws Exception {
-    // 1. alg definition (required in every "logic" test)
-    String algLines[] = {
-        "# ----- properties ",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-        "docs.file=" + getReuters20LinesFile(),
-        "content.source.log.step=3",
-        "ram.flush.mb=-1",
-        "max.buffered=3",
-        "doc.term.vector=false",
-        "content.source.forever=false",
-        "directory=RAMDirectory",
-        "merge.policy=org.apache.lucene.index.LogDocMergePolicy",
-        "doc.stored=false",
-        "doc.tokenized=false",
-        "debug.level=1",
-        "# ----- alg ",
-        "{ \"Rounds\"",
-        "  ResetSystemErase",
-        "  CreateIndex",
-        "  { \"AddDocs\"  AddDoc > : * ",
-        "  Optimize(3)",
-        "  CloseIndex()",
-        "} : 2",
-    };
-    
-    // 2. execute the algorithm  (required in every "logic" test)
-    Benchmark benchmark = execBenchmark(algLines);
-
-    // 3. test number of docs in the index
-    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
-    int ndocsExpected = 20; // first 20 reuters docs.
-    assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
-    ir.close();
-
-    // Make sure we have 3 segments:
-    SegmentInfos infos = new SegmentInfos();
-    infos.read(benchmark.getRunData().getDirectory());
-    assertEquals(3, infos.size());
-  }
-  
-  /**
-   * Test disabling task count (LUCENE-1136).
-   */
-  public void testDisableCounting() throws Exception {
-    doTestDisableCounting(true);
-    doTestDisableCounting(false);
-  }
-
-  private void doTestDisableCounting(boolean disable) throws Exception {
-    // 1. alg definition (required in every "logic" test)
-    String algLines[] = disableCountingLines(disable);
-    
-    // 2. execute the algorithm  (required in every "logic" test)
-    Benchmark benchmark = execBenchmark(algLines);
-
-    // 3. test counters
-    int n = disable ? 0 : 1;
-    int nChecked = 0;
-    for (final TaskStats stats : benchmark.getRunData().getPoints().taskStats()) {
-      String taskName = stats.getTask().getName();
-      if (taskName.equals("Rounds")) {
-        assertEquals("Wrong total count!",20+2*n,stats.getCount());
-        nChecked++;
-      } else if (taskName.equals("CreateIndex")) {
-        assertEquals("Wrong count for CreateIndex!",n,stats.getCount());
-        nChecked++;
-      } else if (taskName.equals("CloseIndex")) {
-        assertEquals("Wrong count for CloseIndex!",n,stats.getCount());
-        nChecked++;
-      }
-    }
-    assertEquals("Missing some tasks to check!",3,nChecked);
-  }
-
-  private String[] disableCountingLines (boolean disable) {
-    String dis = disable ? "-" : "";
-    return new String[] {
-        "# ----- properties ",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-        "docs.file=" + getReuters20LinesFile(),
-        "content.source.log.step=30",
-        "doc.term.vector=false",
-        "content.source.forever=false",
-        "directory=RAMDirectory",
-        "doc.stored=false",
-        "doc.tokenized=false",
-        "task.max.depth.log=1",
-        "# ----- alg ",
-        "{ \"Rounds\"",
-        "  ResetSystemErase",
-        "  "+dis+"CreateIndex",            // optionally disable counting here
-        "  { \"AddDocs\"  AddDoc > : * ",
-        "  "+dis+"  CloseIndex",             // optionally disable counting here (with extra blanks)
-        "}",
-        "RepSumByName",
-    };
-  }
-
-  /**
-   * Test that we can change the Locale in the runData,
-   * that it is parsed as we expect.
-   */
-  public void testLocale() throws Exception {
-    // empty Locale: clear it (null)
-    Benchmark benchmark = execBenchmark(getLocaleConfig(""));
-    assertNull(benchmark.getRunData().getLocale());
-
-    // ROOT locale
-    benchmark = execBenchmark(getLocaleConfig("ROOT"));
-    assertEquals(new Locale(""), benchmark.getRunData().getLocale());
-    
-    // specify just a language 
-    benchmark = execBenchmark(getLocaleConfig("de"));
-    assertEquals(new Locale("de"), benchmark.getRunData().getLocale());
-    
-    // specify language + country
-    benchmark = execBenchmark(getLocaleConfig("en,US"));
-    assertEquals(new Locale("en", "US"), benchmark.getRunData().getLocale());
-    
-    // specify language + country + variant
-    benchmark = execBenchmark(getLocaleConfig("no,NO,NY"));
-    assertEquals(new Locale("no", "NO", "NY"), benchmark.getRunData().getLocale());
-  }
-   
-  private String[] getLocaleConfig(String localeParam) {
-    String algLines[] = {
-        "# ----- properties ",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-        "docs.file=" + getReuters20LinesFile(),
-        "content.source.log.step=3",
-        "content.source.forever=false",
-        "directory=RAMDirectory",
-        "# ----- alg ",
-        "{ \"Rounds\"",
-        "  ResetSystemErase",
-        "  NewLocale(" + localeParam + ")",
-        "  CreateIndex",
-        "  { \"AddDocs\"  AddDoc > : * ",
-        "  NewRound",
-        "} : 1",
-    };
-    return algLines;
-  }
-  
-  /**
-   * Test that we can create CollationAnalyzers.
-   */
-  public void testCollator() throws Exception {
-    // ROOT locale
-    Benchmark benchmark = execBenchmark(getCollatorConfig("ROOT", "impl:jdk"));
-    CollationKeyAnalyzer expected = new CollationKeyAnalyzer(Collator
-        .getInstance(new Locale("")));
-    assertEqualCollation(expected, benchmark.getRunData().getAnalyzer(), "foobar");
-    
-    // specify just a language
-    benchmark = execBenchmark(getCollatorConfig("de", "impl:jdk"));
-    expected = new CollationKeyAnalyzer(Collator.getInstance(new Locale("de")));
-    assertEqualCollation(expected, benchmark.getRunData().getAnalyzer(), "foobar");
-    
-    // specify language + country
-    benchmark = execBenchmark(getCollatorConfig("en,US", "impl:jdk"));
-    expected = new CollationKeyAnalyzer(Collator.getInstance(new Locale("en",
-        "US")));
-    assertEqualCollation(expected, benchmark.getRunData().getAnalyzer(), "foobar");
-    
-    // specify language + country + variant
-    benchmark = execBenchmark(getCollatorConfig("no,NO,NY", "impl:jdk"));
-    expected = new CollationKeyAnalyzer(Collator.getInstance(new Locale("no",
-        "NO", "NY")));
-    assertEqualCollation(expected, benchmark.getRunData().getAnalyzer(), "foobar");
-  }
-  
-  private void assertEqualCollation(Analyzer a1, Analyzer a2, String text)
-      throws Exception {
-    TokenStream ts1 = a1.tokenStream("bogus", new StringReader(text));
-    TokenStream ts2 = a2.tokenStream("bogus", new StringReader(text));
-    ts1.reset();
-    ts2.reset();
-    CharTermAttribute termAtt1 = ts1.addAttribute(CharTermAttribute.class);
-    CharTermAttribute termAtt2 = ts2.addAttribute(CharTermAttribute.class);
-    assertTrue(ts1.incrementToken());
-    assertTrue(ts2.incrementToken());
-    assertEquals(termAtt1.toString(), termAtt2.toString());
-    assertFalse(ts1.incrementToken());
-    assertFalse(ts2.incrementToken());
-    ts1.close();
-    ts2.close();
-  }
-  
-  private String[] getCollatorConfig(String localeParam, 
-      String collationParam) {
-    String algLines[] = {
-        "# ----- properties ",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-        "docs.file=" + getReuters20LinesFile(),
-        "content.source.log.step=3",
-        "content.source.forever=false",
-        "directory=RAMDirectory",
-        "# ----- alg ",
-        "{ \"Rounds\"",
-        "  ResetSystemErase",
-        "  NewLocale(" + localeParam + ")",
-        "  NewCollationAnalyzer(" + collationParam + ")",
-        "  CreateIndex",
-        "  { \"AddDocs\"  AddDoc > : * ",
-        "  NewRound",
-        "} : 1",
-    };
-    return algLines;
-  }
-  
-  /**
-   * Test that we can create ShingleAnalyzerWrappers.
-   */
-  public void testShingleAnalyzer() throws Exception {
-    String text = "one,two,three, four five six";
-    
-    // Default analyzer, maxShingleSize, and outputUnigrams
-    Benchmark benchmark = execBenchmark(getShingleConfig(""));
-    benchmark.getRunData().getAnalyzer().tokenStream
-      ("bogus", new StringReader(text)).close();
-    assertEqualShingle(benchmark.getRunData().getAnalyzer(), text,
-                       new String[] {"one", "one two", "two", "two three",
-                                     "three", "three four", "four", "four five",
-                                     "five", "five six", "six"});
-    // Default analyzer, maxShingleSize = 3, and outputUnigrams = false
-    benchmark = execBenchmark
-      (getShingleConfig("maxShingleSize:3,outputUnigrams:false"));
-    assertEqualShingle(benchmark.getRunData().getAnalyzer(), text,
-                       new String[] { "one two", "one two three", "two three",
-                                      "two three four", "three four", 
-                                      "three four five", "four five",
-                                      "four five six", "five six" });
-    // MockAnalyzer, default maxShingleSize and outputUnigrams
-    benchmark = execBenchmark
-      (getShingleConfig("analyzer:MockAnalyzer"));
-    assertEqualShingle(benchmark.getRunData().getAnalyzer(), text,
-                       new String[] { "one,two,three,", "one,two,three, four",
-                                      "four", "four five", "five", "five six", 
-                                      "six" });
-    
-    // MockAnalyzer, maxShingleSize=3 and outputUnigrams=false
-    benchmark = execBenchmark
-      (getShingleConfig
-        ("outputUnigrams:false,maxShingleSize:3,analyzer:MockAnalyzer"));
-    assertEqualShingle(benchmark.getRunData().getAnalyzer(), text,
-                       new String[] { "one,two,three, four", 
-                                      "one,two,three, four five",
-                                      "four five", "four five six",
-                                      "five six" });
-  }
-  
-  private void assertEqualShingle
-    (Analyzer analyzer, String text, String[] expected) throws Exception {
-    BaseTokenStreamTestCase.assertAnalyzesTo(analyzer, text, expected);
-  }
-  
-  private String[] getShingleConfig(String params) { 
-    String algLines[] = {
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-        "docs.file=" + getReuters20LinesFile(),
-        "content.source.forever=false",
-        "directory=RAMDirectory",
-        "NewShingleAnalyzer(" + params + ")",
-        "CreateIndex",
-        "{ \"AddDocs\"  AddDoc > : * "
-    };
-    return algLines;
-  }
-  
-  private String getReuters20LinesFile() {
-    return getWorkDirResourcePath("reuters.first20.lines.txt");
-  }  
-}
diff --git a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksParse.java b/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksParse.java
deleted file mode 100755
index bc4bc90..0000000
--- a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksParse.java
+++ /dev/null
@@ -1,80 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.benchmark.byTask;
-
-import java.io.StringReader;
-import java.util.ArrayList;
-
-import org.apache.lucene.benchmark.byTask.tasks.PerfTask;
-import org.apache.lucene.benchmark.byTask.tasks.TaskSequence;
-import org.apache.lucene.benchmark.byTask.utils.Algorithm;
-import org.apache.lucene.util.LuceneTestCase;
-
-/** Test very simply that perf tasks are parses as expected. */
-public class TestPerfTasksParse extends LuceneTestCase {
-
-  static final String NEW_LINE = System.getProperty("line.separator");
-  static final String INDENT = "  ";
-
-  // properties in effect in all tests here
-  static final String propPart = 
-    INDENT + "directory=RAMDirectory" + NEW_LINE +
-    INDENT + "print.props=false" + NEW_LINE
-  ;
-
-  /** Test the repetiotion parsing for parallel tasks */
-  public void testParseParallelTaskSequenceRepetition() throws Exception {
-    String taskStr = "AddDoc";
-    String parsedTasks = "[ "+taskStr+" ] : 1000";
-    Benchmark benchmark = new Benchmark(new StringReader(propPart+parsedTasks));
-    Algorithm alg = benchmark.getAlgorithm();
-    ArrayList<PerfTask> algTasks = alg.extractTasks();
-    boolean foundAdd = false;
-    for (final PerfTask task : algTasks) {
-       if (task.toString().indexOf(taskStr)>=0) {
-          foundAdd = true;
-       }
-       if (task instanceof TaskSequence) {
-         assertEquals("repetions should be 1000 for "+parsedTasks, 1000, ((TaskSequence) task).getRepetitions());
-         assertTrue("sequence for "+parsedTasks+" should be parallel!", ((TaskSequence) task).isParallel());
-       }
-       assertTrue("Task "+taskStr+" was not found in "+alg.toString(),foundAdd);
-    }
-  }
-
-  /** Test the repetiotion parsing for sequential  tasks */
-  public void testParseTaskSequenceRepetition() throws Exception {
-    String taskStr = "AddDoc";
-    String parsedTasks = "{ "+taskStr+" } : 1000";
-    Benchmark benchmark = new Benchmark(new StringReader(propPart+parsedTasks));
-    Algorithm alg = benchmark.getAlgorithm();
-    ArrayList<PerfTask> algTasks = alg.extractTasks();
-    boolean foundAdd = false;
-    for (final PerfTask task : algTasks) {
-       if (task.toString().indexOf(taskStr)>=0) {
-          foundAdd = true;
-       }
-       if (task instanceof TaskSequence) {
-         assertEquals("repetions should be 1000 for "+parsedTasks, 1000, ((TaskSequence) task).getRepetitions());
-         assertFalse("sequence for "+parsedTasks+" should be sequential!", ((TaskSequence) task).isParallel());
-       }
-       assertTrue("Task "+taskStr+" was not found in "+alg.toString(),foundAdd);
-    }
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/DocMakerTest.java b/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/DocMakerTest.java
deleted file mode 100644
index 5ee7b13..0000000
--- a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/DocMakerTest.java
+++ /dev/null
@@ -1,163 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Properties;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.benchmark.BenchmarkTestCase;
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.tasks.AddDocTask;
-import org.apache.lucene.benchmark.byTask.tasks.CloseIndexTask;
-import org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask;
-import org.apache.lucene.benchmark.byTask.tasks.TaskSequence;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-
-/** Tests the functionality of {@link DocMaker}. */
-public class DocMakerTest extends BenchmarkTestCase {
-
-  static final class OneDocSource extends ContentSource {
-
-    private boolean finish = false;
-    
-    @Override
-    public void close() throws IOException {
-    }
-
-    @Override
-    public DocData getNextDocData(DocData docData) throws NoMoreDataException,
-        IOException {
-      if (finish) {
-        throw new NoMoreDataException();
-      }
-      
-      docData.setBody("body");
-      docData.setDate("date");
-      docData.setTitle("title");
-      Properties props = new Properties();
-      props.setProperty("key", "value");
-      docData.setProps(props);
-      finish = true;
-      
-      return docData;
-    }
-    
-  }
-
-  private void doTestIndexProperties(boolean setIndexProps,
-      boolean indexPropsVal, int numExpectedResults) throws Exception {
-    Properties props = new Properties();
-    
-    // Indexing configuration.
-    props.setProperty("analyzer", MockAnalyzer.class.getName());
-    props.setProperty("content.source", OneDocSource.class.getName());
-    props.setProperty("directory", "RAMDirectory");
-    if (setIndexProps) {
-      props.setProperty("doc.index.props", Boolean.toString(indexPropsVal));
-    }
-    
-    // Create PerfRunData
-    Config config = new Config(props);
-    PerfRunData runData = new PerfRunData(config);
-
-    TaskSequence tasks = new TaskSequence(runData, getName(), null, false);
-    tasks.addTask(new CreateIndexTask(runData));
-    tasks.addTask(new AddDocTask(runData));
-    tasks.addTask(new CloseIndexTask(runData));
-    tasks.doLogic();
-    
-    IndexSearcher searcher = new IndexSearcher(runData.getDirectory(), true);
-    TopDocs td = searcher.search(new TermQuery(new Term("key", "value")), 10);
-    assertEquals(numExpectedResults, td.totalHits);
-    searcher.close();
-  }
-  
-  private Document createTestNormsDocument(boolean setNormsProp,
-      boolean normsPropVal, boolean setBodyNormsProp, boolean bodyNormsVal)
-      throws Exception {
-    Properties props = new Properties();
-    
-    // Indexing configuration.
-    props.setProperty("analyzer", MockAnalyzer.class.getName());
-    props.setProperty("content.source", OneDocSource.class.getName());
-    props.setProperty("directory", "RAMDirectory");
-    if (setNormsProp) {
-      props.setProperty("doc.tokenized.norms", Boolean.toString(normsPropVal));
-    }
-    if (setBodyNormsProp) {
-      props.setProperty("doc.body.tokenized.norms", Boolean.toString(bodyNormsVal));
-    }
-    
-    // Create PerfRunData
-    Config config = new Config(props);
-    
-    DocMaker dm = new DocMaker();
-    dm.setConfig(config);
-    return dm.makeDocument();
-  }
-  
-  /* Tests doc.index.props property. */
-  public void testIndexProperties() throws Exception {
-    // default is to not index properties.
-    doTestIndexProperties(false, false, 0);
-    
-    // set doc.index.props to false.
-    doTestIndexProperties(true, false, 0);
-    
-    // set doc.index.props to true.
-    doTestIndexProperties(true, true, 1);
-  }
-  
-  /* Tests doc.tokenized.norms and doc.body.tokenized.norms properties. */
-  public void testNorms() throws Exception {
-    
-    Document doc;
-    
-    // Don't set anything, use the defaults
-    doc = createTestNormsDocument(false, false, false, false);
-    assertTrue(doc.getField(DocMaker.TITLE_FIELD).getOmitNorms());
-    assertFalse(doc.getField(DocMaker.BODY_FIELD).getOmitNorms());
-    
-    // Set norms to false
-    doc = createTestNormsDocument(true, false, false, false);
-    assertTrue(doc.getField(DocMaker.TITLE_FIELD).getOmitNorms());
-    assertFalse(doc.getField(DocMaker.BODY_FIELD).getOmitNorms());
-    
-    // Set norms to true
-    doc = createTestNormsDocument(true, true, false, false);
-    assertFalse(doc.getField(DocMaker.TITLE_FIELD).getOmitNorms());
-    assertFalse(doc.getField(DocMaker.BODY_FIELD).getOmitNorms());
-    
-    // Set body norms to false
-    doc = createTestNormsDocument(false, false, true, false);
-    assertTrue(doc.getField(DocMaker.TITLE_FIELD).getOmitNorms());
-    assertTrue(doc.getField(DocMaker.BODY_FIELD).getOmitNorms());
-    
-    // Set body norms to true
-    doc = createTestNormsDocument(false, false, true, true);
-    assertTrue(doc.getField(DocMaker.TITLE_FIELD).getOmitNorms());
-    assertFalse(doc.getField(DocMaker.BODY_FIELD).getOmitNorms());
-  }
-  
-}
diff --git a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/LineDocSourceTest.java b/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/LineDocSourceTest.java
deleted file mode 100644
index 8629dd9..0000000
--- a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/LineDocSourceTest.java
+++ /dev/null
@@ -1,145 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.BufferedWriter;
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.OutputStream;
-import java.io.OutputStreamWriter;
-import java.util.Properties;
-
-import org.apache.commons.compress.compressors.CompressorStreamFactory;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.benchmark.BenchmarkTestCase;
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.tasks.AddDocTask;
-import org.apache.lucene.benchmark.byTask.tasks.CloseIndexTask;
-import org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask;
-import org.apache.lucene.benchmark.byTask.tasks.TaskSequence;
-import org.apache.lucene.benchmark.byTask.tasks.WriteLineDocTask;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-
-/** Tests the functionality of {@link LineDocSource}. */
-public class LineDocSourceTest extends BenchmarkTestCase {
-
-  private static final CompressorStreamFactory csFactory = new CompressorStreamFactory();
-
-  private void createBZ2LineFile(File file) throws Exception {
-    OutputStream out = new FileOutputStream(file);
-    out = csFactory.createCompressorOutputStream("bzip2", out);
-    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(out, "utf-8"));
-    StringBuilder doc = new StringBuilder();
-    doc.append("title").append(WriteLineDocTask.SEP).append("date").append(WriteLineDocTask.SEP).append("body");
-    writer.write(doc.toString());
-    writer.newLine();
-    writer.close();
-  }
-
-  private void createRegularLineFile(File file) throws Exception {
-    OutputStream out = new FileOutputStream(file);
-    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(out, "utf-8"));
-    StringBuilder doc = new StringBuilder();
-    doc.append("title").append(WriteLineDocTask.SEP).append("date").append(WriteLineDocTask.SEP).append("body");
-    writer.write(doc.toString());
-    writer.newLine();
-    writer.close();
-  }
-  
-  private void doIndexAndSearchTest(File file, boolean setBZCompress,
-      String bz2CompressVal) throws Exception {
-
-    Properties props = new Properties();
-    
-    // LineDocSource specific settings.
-    props.setProperty("docs.file", file.getAbsolutePath());
-    if (setBZCompress) {
-      props.setProperty("bzip.compression", bz2CompressVal);
-    }
-    
-    // Indexing configuration.
-    props.setProperty("analyzer", MockAnalyzer.class.getName());
-    props.setProperty("content.source", LineDocSource.class.getName());
-    props.setProperty("directory", "RAMDirectory");
-    
-    // Create PerfRunData
-    Config config = new Config(props);
-    PerfRunData runData = new PerfRunData(config);
-
-    TaskSequence tasks = new TaskSequence(runData, "testBzip2", null, false);
-    tasks.addTask(new CreateIndexTask(runData));
-    tasks.addTask(new AddDocTask(runData));
-    tasks.addTask(new CloseIndexTask(runData));
-    tasks.doLogic();
-    
-    IndexSearcher searcher = new IndexSearcher(runData.getDirectory(), true);
-    TopDocs td = searcher.search(new TermQuery(new Term("body", "body")), 10);
-    assertEquals(1, td.totalHits);
-    assertNotNull(td.scoreDocs[0]);
-    searcher.close();
-  }
-  
-  /* Tests LineDocSource with a bzip2 input stream. */
-  public void testBZip2() throws Exception {
-    File file = new File(getWorkDir(), "one-line.bz2");
-    createBZ2LineFile(file);
-    doIndexAndSearchTest(file, true, "true");
-  }
-  
-  public void testBZip2AutoDetect() throws Exception {
-    File file = new File(getWorkDir(), "one-line.bz2");
-    createBZ2LineFile(file);
-    doIndexAndSearchTest(file, false, null);
-  }
-  
-  public void testRegularFile() throws Exception {
-    File file = new File(getWorkDir(), "one-line");
-    createRegularLineFile(file);
-    doIndexAndSearchTest(file, false, null);
-  }
-
-  public void testInvalidFormat() throws Exception {
-    String[] testCases = new String[] {
-      "", // empty line
-      "title", // just title
-      "title" + WriteLineDocTask.SEP, // title + SEP
-      "title" + WriteLineDocTask.SEP + "body", // title + SEP + body
-      // note that title + SEP + body + SEP is a valid line, which results in an
-      // empty body
-    };
-    
-    for (int i = 0; i < testCases.length; i++) {
-      File file = new File(getWorkDir(), "one-line");
-      BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(file), "utf-8"));
-      writer.write(testCases[i]);
-      writer.newLine();
-      writer.close();
-      try {
-        doIndexAndSearchTest(file, false, null);
-        fail("Some exception should have been thrown for: [" + testCases[i] + "]");
-      } catch (Exception e) {
-        // expected.
-      }
-    }
-  }
-  
-}
diff --git a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/TrecContentSourceTest.java b/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/TrecContentSourceTest.java
deleted file mode 100644
index a178c6a..0000000
--- a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/TrecContentSourceTest.java
+++ /dev/null
@@ -1,333 +0,0 @@
-package org.apache.lucene.benchmark.byTask.feeds;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.BufferedReader;
-import java.io.IOException;
-import java.io.StringReader;
-import java.text.ParseException;
-import java.util.Date;
-
-import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.document.DateTools;
-import org.apache.lucene.util.LuceneTestCase;
-
-public class TrecContentSourceTest extends LuceneTestCase {
-
-  /** A TrecDocMaker which works on a String and not files. */
-  private static class StringableTrecSource extends TrecContentSource {
-  
-    private String docs = null;
-    
-    public StringableTrecSource(String docs, boolean forever) {
-      this.docs = docs;
-      this.forever = forever;
-    }
-    
-    @Override
-    void openNextFile() throws NoMoreDataException, IOException {
-      if (reader != null) {
-        if (!forever) {
-          throw new NoMoreDataException();
-        }
-        ++iteration;
-      }
-      
-      reader = new BufferedReader(new StringReader(docs));
-    }
-    
-    @Override
-    public void setConfig(Config config) {
-      htmlParser = new DemoHTMLParser();
-    }
-  }
-  
-  private void assertDocData(DocData dd, String expName, String expTitle,
-                             String expBody, Date expDate)
-      throws ParseException {
-    assertNotNull(dd);
-    assertEquals(expName, dd.getName());
-    assertEquals(expTitle, dd.getTitle());
-    assertTrue(dd.getBody().indexOf(expBody) != -1);
-    Date date = dd.getDate() != null ? DateTools.stringToDate(dd.getDate()) : null;
-    assertEquals(expDate, date);
-  }
-  
-  private void assertNoMoreDataException(StringableTrecSource stdm) throws Exception {
-    boolean thrown = false;
-    try {
-      stdm.getNextDocData(null);
-    } catch (NoMoreDataException e) {
-      thrown = true;
-    }
-    assertTrue("Expecting NoMoreDataException", thrown);
-  }
-  
-  public void testOneDocument() throws Exception {
-    String docs = "<DOC>\r\n" + 
-                  "<DOCNO>TEST-000</DOCNO>\r\n" + 
-                  "<DOCHDR>\r\n" + 
-                  "http://lucene.apache.org.trecdocmaker.test\r\n" + 
-                  "HTTP/1.1 200 OK\r\n" + 
-                  "Date: Sun, 11 Jan 2009 08:00:00 GMT\r\n" + 
-                  "Server: Apache/1.3.27 (Unix)\r\n" + 
-                  "Last-Modified: Sun, 11 Jan 2009 08:00:00 GMT\r\n" + 
-                  "Content-Length: 614\r\n" + 
-                  "Connection: close\r\n" + 
-                  "Content-Type: text/html\r\n" + 
-                  "</DOCHDR>\r\n" + 
-                  "<html>\r\n" + 
-                  "\r\n" + 
-                  "<head>\r\n" + 
-                  "<title>\r\n" + 
-                  "TEST-000 title\r\n" + 
-                  "</title>\r\n" + 
-                  "</head>\r\n" + 
-                  "\r\n" + 
-                  "<body>\r\n" + 
-                  "TEST-000 text\r\n" + 
-                  "\r\n" + 
-                  "</body>\r\n" + 
-                  "\r\n" + 
-                  "</DOC>";
-    StringableTrecSource source = new StringableTrecSource(docs, false);
-    source.setConfig(null);
-
-    DocData dd = source.getNextDocData(new DocData());
-    assertDocData(dd, "TEST-000_0", "TEST-000 title", "TEST-000 text", source
-        .parseDate("Sun, 11 Jan 2009 08:00:00 GMT"));
-    
-    assertNoMoreDataException(source);
-  }
-  
-  public void testTwoDocuments() throws Exception {
-    String docs = "<DOC>\r\n" + 
-                  "<DOCNO>TEST-000</DOCNO>\r\n" + 
-                  "<DOCHDR>\r\n" + 
-                  "http://lucene.apache.org.trecdocmaker.test\r\n" + 
-                  "HTTP/1.1 200 OK\r\n" + 
-                  "Date: Sun, 11 Jan 2009 08:00:00 GMT\r\n" + 
-                  "Server: Apache/1.3.27 (Unix)\r\n" + 
-                  "Last-Modified: Sun, 11 Jan 2009 08:00:00 GMT\r\n" + 
-                  "Content-Length: 614\r\n" + 
-                  "Connection: close\r\n" + 
-                  "Content-Type: text/html\r\n" + 
-                  "</DOCHDR>\r\n" + 
-                  "<html>\r\n" + 
-                  "\r\n" + 
-                  "<head>\r\n" + 
-                  "<title>\r\n" + 
-                  "TEST-000 title\r\n" + 
-                  "</title>\r\n" + 
-                  "</head>\r\n" + 
-                  "\r\n" + 
-                  "<body>\r\n" + 
-                  "TEST-000 text\r\n" + 
-                  "\r\n" + 
-                  "</body>\r\n" + 
-                  "\r\n" + 
-                  "</DOC>\r\n" +
-                  "<DOC>\r\n" + 
-                  "<DOCNO>TEST-001</DOCNO>\r\n" + 
-                  "<DOCHDR>\r\n" + 
-                  "http://lucene.apache.org.trecdocmaker.test\r\n" + 
-                  "HTTP/1.1 200 OK\r\n" + 
-                  "Date: Sun, 11 Jan 2009 08:01:00 GMT\r\n" + 
-                  "Server: Apache/1.3.27 (Unix)\r\n" + 
-                  "Last-Modified: Sun, 11 Jan 2008 08:01:00 GMT\r\n" + 
-                  "Content-Length: 614\r\n" + 
-                  "Connection: close\r\n" + 
-                  "Content-Type: text/html\r\n" + 
-                  "</DOCHDR>\r\n" + 
-                  "<html>\r\n" + 
-                  "\r\n" + 
-                  "<head>\r\n" + 
-                  "<title>\r\n" + 
-                  "TEST-001 title\r\n" + 
-                  "</title>\r\n" + 
-                  "</head>\r\n" + 
-                  "\r\n" + 
-                  "<body>\r\n" + 
-                  "TEST-001 text\r\n" + 
-                  "\r\n" + 
-                  "</body>\r\n" + 
-                  "\r\n" + 
-                  "</DOC>";
-    StringableTrecSource source = new StringableTrecSource(docs, false);
-    source.setConfig(null);
-
-    DocData dd = source.getNextDocData(new DocData());
-    assertDocData(dd, "TEST-000_0", "TEST-000 title", "TEST-000 text", source
-        .parseDate("Sun, 11 Jan 2009 08:00:00 GMT"));
-    
-    dd = source.getNextDocData(dd);
-    assertDocData(dd, "TEST-001_0", "TEST-001 title", "TEST-001 text", source
-        .parseDate("Sun, 11 Jan 2009 08:01:00 GMT"));
-    
-    assertNoMoreDataException(source);
-  }
-
-  // If a Date: attribute is missing, make sure the document is not skipped, but
-  // rather that null Data is assigned.
-  public void testMissingDate() throws Exception {
-    String docs = "<DOC>\r\n" + 
-                  "<DOCNO>TEST-000</DOCNO>\r\n" + 
-                  "<DOCHDR>\r\n" + 
-                  "http://lucene.apache.org.trecdocmaker.test\r\n" + 
-                  "HTTP/1.1 200 OK\r\n" + 
-                  "Server: Apache/1.3.27 (Unix)\r\n" + 
-                  "Last-Modified: Sun, 11 Jan 2009 08:00:00 GMT\r\n" + 
-                  "Content-Length: 614\r\n" + 
-                  "Connection: close\r\n" + 
-                  "Content-Type: text/html\r\n" + 
-                  "</DOCHDR>\r\n" + 
-                  "<html>\r\n" + 
-                  "\r\n" + 
-                  "<head>\r\n" + 
-                  "<title>\r\n" + 
-                  "TEST-000 title\r\n" + 
-                  "</title>\r\n" + 
-                  "</head>\r\n" + 
-                  "\r\n" + 
-                  "<body>\r\n" + 
-                  "TEST-000 text\r\n" + 
-                  "\r\n" + 
-                  "</body>\r\n" + 
-                  "\r\n" + 
-                  "</DOC>\r\n" +
-                  "<DOC>\r\n" + 
-                  "<DOCNO>TEST-001</DOCNO>\r\n" + 
-                  "<DOCHDR>\r\n" + 
-                  "http://lucene.apache.org.trecdocmaker.test\r\n" + 
-                  "HTTP/1.1 200 OK\r\n" + 
-                  "Date: Sun, 11 Jan 2009 08:01:00 GMT\r\n" + 
-                  "Server: Apache/1.3.27 (Unix)\r\n" + 
-                  "Last-Modified: Sun, 11 Jan 2009 08:01:00 GMT\r\n" + 
-                  "Content-Length: 614\r\n" + 
-                  "Connection: close\r\n" + 
-                  "Content-Type: text/html\r\n" + 
-                  "</DOCHDR>\r\n" + 
-                  "<html>\r\n" + 
-                  "\r\n" + 
-                  "<head>\r\n" + 
-                  "<title>\r\n" + 
-                  "TEST-001 title\r\n" + 
-                  "</title>\r\n" + 
-                  "</head>\r\n" + 
-                  "\r\n" + 
-                  "<body>\r\n" + 
-                  "TEST-001 text\r\n" + 
-                  "\r\n" + 
-                  "</body>\r\n" + 
-                  "\r\n" + 
-                  "</DOC>";
-    StringableTrecSource source = new StringableTrecSource(docs, false);
-    source.setConfig(null);
-
-    DocData dd = source.getNextDocData(new DocData());
-    assertDocData(dd, "TEST-000_0", "TEST-000 title", "TEST-000 text", null);
-    
-    dd = source.getNextDocData(dd);
-    assertDocData(dd, "TEST-001_0", "TEST-001 title", "TEST-001 text", source
-        .parseDate("Sun, 11 Jan 2009 08:01:00 GMT"));
-    
-    assertNoMoreDataException(source);
-  }
-
-  // When a 'bad date' is input (unparsable date), make sure the DocData date is
-  // assigned null.
-  public void testBadDate() throws Exception {
-    String docs = "<DOC>\r\n" + 
-                  "<DOCNO>TEST-000</DOCNO>\r\n" + 
-                  "<DOCHDR>\r\n" + 
-                  "http://lucene.apache.org.trecdocmaker.test\r\n" + 
-                  "HTTP/1.1 200 OK\r\n" + 
-                  "Date: Bad Date\r\n" + 
-                  "Server: Apache/1.3.27 (Unix)\r\n" + 
-                  "Last-Modified: Sun, 11 Jan 2009 08:00:00 GMT\r\n" + 
-                  "Content-Length: 614\r\n" + 
-                  "Connection: close\r\n" + 
-                  "Content-Type: text/html\r\n" + 
-                  "</DOCHDR>\r\n" + 
-                  "<html>\r\n" + 
-                  "\r\n" + 
-                  "<head>\r\n" + 
-                  "<title>\r\n" + 
-                  "TEST-000 title\r\n" + 
-                  "</title>\r\n" + 
-                  "</head>\r\n" + 
-                  "\r\n" + 
-                  "<body>\r\n" + 
-                  "TEST-000 text\r\n" + 
-                  "\r\n" + 
-                  "</body>\r\n" + 
-                  "\r\n" + 
-                  "</DOC>";
-    StringableTrecSource source = new StringableTrecSource(docs, false);
-    source.setConfig(null);
-
-    DocData dd = source.getNextDocData(new DocData());
-    assertDocData(dd, "TEST-000_0", "TEST-000 title", "TEST-000 text", null);
-    
-    assertNoMoreDataException(source);
-  }
-
-  public void testForever() throws Exception {
-    String docs = "<DOC>\r\n" + 
-                  "<DOCNO>TEST-000</DOCNO>\r\n" + 
-                  "<DOCHDR>\r\n" + 
-                  "http://lucene.apache.org.trecdocmaker.test\r\n" + 
-                  "HTTP/1.1 200 OK\r\n" + 
-                  "Date: Sun, 11 Jan 2009 08:00:00 GMT\r\n" + 
-                  "Server: Apache/1.3.27 (Unix)\r\n" + 
-                  "Last-Modified: Sun, 11 Jan 2009 08:00:00 GMT\r\n" + 
-                  "Content-Length: 614\r\n" + 
-                  "Connection: close\r\n" + 
-                  "Content-Type: text/html\r\n" + 
-                  "</DOCHDR>\r\n" + 
-                  "<html>\r\n" + 
-                  "\r\n" + 
-                  "<head>\r\n" + 
-                  "<title>\r\n" + 
-                  "TEST-000 title\r\n" + 
-                  "</title>\r\n" + 
-                  "</head>\r\n" + 
-                  "\r\n" + 
-                  "<body>\r\n" + 
-                  "TEST-000 text\r\n" + 
-                  "\r\n" + 
-                  "</body>\r\n" + 
-                  "\r\n" + 
-                  "</DOC>";
-    StringableTrecSource source = new StringableTrecSource(docs, true);
-    source.setConfig(null);
-
-    DocData dd = source.getNextDocData(new DocData());
-    assertDocData(dd, "TEST-000_0", "TEST-000 title", "TEST-000 text", source
-        .parseDate("Sun, 11 Jan 2009 08:00:00 GMT"));
-    
-    // same document, but the second iteration changes the name.
-    dd = source.getNextDocData(dd);
-    assertDocData(dd, "TEST-000_1", "TEST-000 title", "TEST-000 text", source
-        .parseDate("Sun, 11 Jan 2009 08:00:00 GMT"));
-
-    // Don't test that NoMoreDataException is thrown, since the forever flag is
-    // turned on.
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/reuters.first20.lines.txt b/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/reuters.first20.lines.txt
deleted file mode 100644
index 41b04b3..0000000
--- a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/reuters.first20.lines.txt
+++ /dev/null
@@ -1,20 +0,0 @@
-BAHIA COCOA REVIEW	19870226200101	Showers continued throughout the week in the Bahia cocoa zone, alleviating the drought since early January and improving prospects for the coming temporao, although normal humidity levels have not been restored, Comissaria Smith said in its weekly review.     The dry period means the temporao will be late this year.     Arrivals for the week ended February 22 were 155,221 bags of 60 kilos making a cumulative total for the season of 5.93 mln against 5.81 at the same stage last year. Again it seems that cocoa delivered earlier on consignment was included in the arrivals figures.     Comissaria Smith said there is still some doubt as to how much old crop cocoa is still available as harvesting has practically come to an end. With total Bahia crop estimates around 6.4 mln bags and sales standing at almost 6.2 mln there are a few hundred thousand bags still in the hands of farmers, middlemen, exporters and processors.     There are doubts as to how much of this cocoa would be fit for export as shippers are now experiencing dificulties in obtaining +Bahia superior+ certificates.     In view of the lower quality over recent weeks farmers have sold a good part of their cocoa held on consignment.     Comissaria Smith said spot bean prices rose to 340 to 350 cruzados per arroba of 15 kilos.     Bean shippers were reluctant to offer nearby shipment and only limited sales were booked for March shipment at 1,750 to 1,780 dlrs per tonne to ports to be named.     New crop sales were also light and all to open ports with June/July going at 1,850 and 1,880 dlrs and at 35 and 45 dlrs under New York july, Aug/Sept at 1,870, 1,875 and 1,880 dlrs per tonne FOB.     Routine sales of butter were made. March/April sold at 4,340, 4,345 and 4,350 dlrs.     April/May butter went at 2.27 times New York May, June/July at 4,400 and 4,415 dlrs, Aug/Sept at 4,351 to 4,450 dlrs and at 2.27 and 2.28 times New York Sept and Oct/Dec at 4,480 dlrs and 2.27 times New York Dec, Comissaria Smith said.     Destinations were the U.S., Covertible currency areas, Uruguay and open ports.     Cake sales were registered at 785 to 995 dlrs for March/April, 785 dlrs for May, 753 dlrs for Aug and 0.39 times New York Dec for Oct/Dec.     Buyers were the U.S., Argentina, Uruguay and convertible currency areas.     Liquor sales were limited with March/April selling at 2,325 and 2,380 dlrs, June/July at 2,375 dlrs and at 1.25 times New York July, Aug/Sept at 2,400 dlrs and at 1.25 times New York Sept and Oct/Dec at 1.25 times New York Dec, Comissaria Smith said.     Total Bahia sales are currently estimated at 6.13 mln bags against the 1986/87 crop and 1.06 mln bags against the 1987/88 crop.     Final figures for the period to February 28 are expected to be published by the Brazilian Cocoa Trade Commission after carnival which ends midday on February 27.  Reuter &#3;  
-STANDARD OIL <SRD> TO FORM FINANCIAL UNIT	19870226200220	Standard Oil Co and BP North America Inc said they plan to form a venture to manage the money market borrowing and investment activities of both companies.     BP North America is a subsidiary of British Petroleum Co Plc <BP>, which also owns a 55 pct interest in Standard Oil.     The venture will be called BP/Standard Financial Trading and will be operated by Standard Oil under the oversight of a joint management committee.   Reuter &#3;  
-COBANCO INC <CBCO> YEAR NET	19870226201859	Shr 34 cts vs 1.19 dlrs     Net 807,000 vs 2,858,000     Assets 510.2 mln vs 479.7 mln     Deposits 472.3 mln vs 440.3 mln     Loans 299.2 mln vs 327.2 mln     Note: 4th qtr not available. Year includes 1985 extraordinary gain from tax carry forward of 132,000 dlrs, or five cts per shr.  Reuter &#3;  
-WORLD MARKET PRICE FOR UPLAND COTTON - USDA	19870226213846	The U.S. Agriculture Department announced the prevailing world market price, adjusted to U.S. quality and location, for Strict Low Middling, 1-1/16 inch upland cotton at 52.69 cts per lb, to be in effect through midnight March 5.     The adjusted world price is at average U.S. producing locations (near Lubbock, Texas) and will be further adjusted for other qualities and locations. The price will be used in determining First Handler Cotton Certificate payment rates.     Based on data for the week ended February 26, the adjusted world price for upland cotton is determined as follows, in cts per lb --  Northern European Price               66.32       Adjustments --  Average U.S. spot mkt location 10.42   SLM 1-1/16 inch cotton          1.80   Average U.S. location           0.53  Sum of adjustments              12.75  Adjusted world price            53.57  Reuter &#3;  
-SUGAR QUOTA IMPORTS DETAILED -- USDA	19870226213854	The U.S. Agriculture Department said cumulative sugar imports from individual countries during the 1987 quota year, which began January 1, 1987 and ends December 31, 1987 were as follows, with quota allocations for the quota year in short tons, raw value --             CUMULATIVE     QUOTA 1987               IMPORTS     ALLOCATIONS  ARGENTINA        nil          39,130  AUSTRALIA        nil          75,530  BARBADOS         nil           7,500  BELIZE           nil          10,010  BOLIVIA          nil           7,500  BRAZIL           nil         131,950  CANADA           nil          18,876                            QUOTA 1987               IMPORTS     ALLOCATIONS  COLOMBIA         103          21,840  CONGO            nil           7,599  COSTA RICA       nil          17,583  IVORY COAST      nil           7,500  DOM REP        5,848         160,160  ECUADOR          nil          10,010  EL SALVADOR      nil          26,019.8  FIJI             nil          25,190  GABON            nil           7,500                            QUOTA 1987               IMPORTS     ALLOCATIONS  GUATEMALA        nil          43,680  GUYANA           nil          10,920  HAITI            nil           7,500  HONDURAS         nil          15,917.2  INDIA            nil           7,500  JAMAICA          nil          10,010  MADAGASCAR       nil           7,500  MALAWI           nil           9,,100                            QUOTA 1987                IMPORTS    ALLOCATIONS  MAURITIUS         nil         10,920  MEXICO             37          7,500  MOZAMBIQUE        nil         11,830  PANAMA            nil         26,390  PAPUA NEW GUINEA  nil          7,500  PARAGUAY          nil          7,500  PERU              nil         37,310  PHILIPPINES       nil        143,780  ST.CHRISTOPHER-  NEVIS             nil          7,500                           QUOTA 1987                 IMPORTS  ALLOCATIONS  SWAZILAND          nil         14,560  TAIWAN             nil         10,920  THAILAND           nil         12,740  TRINIDAD-TOBAGO    nil          7,500  URUGUAY            nil          7,500  ZIMBABWE           nil         10,920   Reuter &#3;  
-GRAIN SHIPS LOADING AT PORTLAND	19870226213903	There were seven grain ships loading and six ships were waiting to load at Portland, according to the Portland Merchants Exchange.  Reuter &#3;  
-IRAN ANNOUNCES END OF MAJOR OFFENSIVE IN GULF WAR	19870226214000	Iran announced tonight that its major offensive against Iraq in the Gulf war had ended after dealing savage blows against the Baghdad government.     The Iranian news agency IRNA, in a report received in London, said the operation code-named Karbala-5 launched into Iraq on January 9 was now over.     It quoted a joint statewment by the Iranian Army and Revolutionary Guards Corps as saying that their forces had "dealt one of the severest blows on the Iraqi war machine in the history of the Iraq-imposed war."     The statement by the Iranian High Command appeared to herald the close of an assault on the port city of Basra in southern Iraq.     "The operation was launched at a time when the Baghdad government was spreading extensive propaganda on the resistance power of its army...," said the statement quoted by IRNA.     It claimed massive victories in the seven-week offensive and called on supporters of Baghdad to "come to their senses" and discontinue support for what it called the tottering regime in Iraq.     Iran said its forces had "liberated" 155 square kilometers of enemy-occupied territory during the 1987 offensive and taken over islands, townships, rivers and part of a road leading into Basra.     The Iranian forces "are in full control of these areas," the statement said.     It said 81 Iraqi brigades and battalions were totally destroyed, along with 700 tanks and 1,500 other vehicles. The victory list also included 80 warplanes downed, 250 anti- aircraft guns and 400 pieces of military hardware destroyed and the seizure of 220 tanks and armoured personnel carriers.  Reuter &#3;  
-MERIDIAN BANCORP INC <MRDN> SETS REGULAR PAYOUT	19870226214034	Qtly div 25 cts vs 25 cts prior     Pay April one     Record March 15  Reuter &#3;  
-U.S. BANK DISCOUNT BORROWINGS 310 MLN DLRS	19870226214134	U.S. bank discount window borrowings less extended credits averaged 310 mln dlrs in the week to Wednesday February 25, the Federal Reserve said.     The Fed said that overall borrowings in the week fell 131 mln dlrs to 614 mln dlrs, with extended credits up 10 mln dlrs at 304 mln dlrs. The week was the second half of a two-week statement period. Net borrowings in the prior week averaged 451 mln dlrs.     Commenting on the two-week statement period ended February 25, the Fed said that banks had average net free reserves of 644 mln dlrs a day, down from 1.34 billion two weeks earlier.     A Federal Reserve spokesman told a press briefing that there were no large single day net misses in the Fed's reserve projections in the week to Wednesday.     He said that natural float had been "acting a bit strangely" for this time of year, noting that there had been poor weather during the latest week.     The spokesman said that natural float ranged from under 500 mln dlrs on Friday, for which he could give no reason, to nearly one billion dlrs on both Thursday and Wednesday.     The Fed spokeman could give no reason for Thursday's high float, but he said that about 750 mln dlrs of Wednesday's float figure was due to holdover and transportation float at two widely separated Fed districts.     For the week as a whole, he said that float related as of adjustments were "small," adding that they fell to a negative 750 mln dlrs on Tuesday due to a number of corrections for unrelated cash letter errors in six districts around the country.     The spokesman said that on both Tuesday and Wednesday, two different clearing banks had system problems and the securities and Federal funds wires had to be held open until about 2000 or 2100 EST on both days.     However, he said that both problems were cleared up during both afternoons and there was no evidence of any reserve impact.     During the week ended Wednesday, 45 pct of net discount window borrowings were made by the smallest banks, with 30 pct by the 14 large money center banks and 25 pct by large regional institutions.     On Wednesday, 55 pct of the borrowing was accounted for by the money center banks, with 30 pct by the large regionals and 15 pct by the smallest banks.     The Fed spokesman said the banking system had excess reserves on Thursday, Monday and Tuesday and a deficit on Friday and Wedndsday. That produced a small daily average deficit for the week as a whole.     For the two-week period, he said there were relatively high excess reserves on a daily avearge, almost all of which were at the smallest banks.  Reuter &#3;  
-AMERICAN EXPRESS <AXP> SEEN IN POSSIBLE SPINNOFF	19870226214313	American Express Co remained silent on market rumors it would spinoff all or part of its Shearson Lehman Brothers Inc, but some analysts said the company may be considering such a move because it is unhappy with the market value of its stock.     American Express stock got a lift from the rumor, as the market calculated a partially public Shearson may command a good market value, thereby boosting the total value of American Express. The rumor also was accompanied by talk the financial services firm would split its stock and boost its dividend.     American Express closed on the New York Stock Exchange at 72-5/8, up 4-1/8 on heavy volume.     American Express would not comment on the rumors or its stock activity.     Analysts said comments by the company at an analysts' meeting Tuesday helped fuel the rumors as did an announcement yesterday of management changes.     At the meeting, company officials said American Express stock is undervalued and does not fully reflect the performance of Shearson, according to analysts.     Yesterday, Shearson said it was elevating its chief operating officer, Jeffery Lane, to the added position of president, which had been vacant. It also created four new positions for chairmen of its operating divisions.     Analysts speculated a partial spinoff would make most sense, contrary to one variation on market rumors of a total spinoff.     Some analysts, however, disagreed that any spinoff of Shearson would be good since it is a strong profit center for American Express, contributing about 20 pct of earnings last year.     "I think it is highly unlikely that American Express is going to sell shearson," said Perrin Long of Lipper Analytical. He questioned what would be a better investment than "a very profitable securities firm."     Several analysts said American Express is not in need of cash, which might be the only reason to sell a part of a strong asset.     But others believe the company could very well of considered the option of spinning out part of Shearson, and one rumor suggests selling about 20 pct of it in the market.     Larry Eckenfelder of Prudential-Bache Securities said he believes American Express could have considered a partial spinoff in the past.     "Shearson being as profitable as it is would have fetched a big premium in the market place. Shearson's book value is in the 1.4 mln dlr range. Shearson in the market place would probably be worth three to 3.5 bilion dlrs in terms of market capitalization," said Eckenfelder.     Some analysts said American Express could use capital since it plans to expand globally.     "They have enormous internal growth plans that takes capital. You want your stock to reflect realistic valuations to enhance your ability to make all kinds of endeavors down the road," said E.F. Hutton Group analyst Michael Lewis.     "They've outlined the fact that they're investing heavily in the future, which goes heavily into the international arena," said Lewis. "...That does not preclude acquisitions and divestitures along the way," he said.     Lewis said if American Express reduced its exposure to the brokerage business by selling part of shearson, its stock might better reflect other assets, such as the travel related services business.     "It could find its true water mark with a lesser exposure to brokerage. The value of the other components could command a higher multiple because they constitute a higher percentage of the total operating earnings of the company," he said.      Lewis said Shearson contributed 316 mln in after-tax operating earnings, up from about 200 mln dlrs in 1985.      Reuter &#3;  
-OHIO MATTRESS <OMT> MAY HAVE LOWER 1ST QTR NET	19870226201915	Ohio Mattress Co said its first quarter, ending February 28, profits may be below the 2.4 mln dlrs, or 15 cts a share, earned in the first quarter of fiscal 1986.     The company said any decline would be due to expenses related to the acquisitions in the middle of the current quarter of seven licensees of Sealy Inc, as well as 82 pct of the outstanding capital stock of Sealy.     Because of these acquisitions, it said, first quarter sales will be substantially higher than last year's 67.1 mln dlrs.     Noting that it typically reports first quarter results in late march, said the report is likely to be issued in early April this year.     It said the delay is due to administrative considerations, including conducting appraisals, in connection with the acquisitions.  Reuter &#3;  
-U.S. M-1 MONEY SUPPLY ROSE 2.1 BILLION DLRS	19870226214435	U.S. M-1 money supply rose 2.1 billion dlrs to a seasonally adjusted 736.7 billion dlrs in the February 16 week, the Federal Reserve said.     The previous week's M-1 level was revised to 734.6 billion dlrs from 734.2 billion dlrs, while the four-week moving average of M-1 rose to 735.0 billion dlrs from 733.5 billion.     Economists polled by Reuters said that M-1 should be anywhere from down four billion dlrs to up 2.3 billion dlrs. The average forecast called for a 300 mln dlr M-1 rise.  Reuter &#3;  
-GENERAL BINDING <GBND> IN MARKETING AGREEMENT	19870226214508	General Binding Corp said it reached a marketing agreement with Varitronic Systems Inc, a manufacturer and marketer of electronic lettering systems.     Under terms of the agreement, General Binding will carry Varitronics' Merlin Express Presentation Lettering System, a portable, battery-operated lettering system which produces type on adhesive-backed tape.  Reuter &#3;  
-LIBERTY ALL-STAR <USA> SETS INITIAL PAYOUT	19870226214544	Liberty All-Star Equity Fund said it declared an initial dividend of five cts per share, payable April two to shareholders of record March 20.     It said the dividend includes a quarterly dividend of three cts a share and a special payout of two cts a share, which covers the period from November three, 1986, when the fund began operations, to December 31, 1986.     The fund said its quarterly dividend rate may fluctuate in the future.  Reuter &#3;  
-COCA COLA <KO> UNIT AND WORLD FILM IN VENTURE	19870226214745	Coca-Cola Co's Entertainment Business Sector Inc unit said it formed a joint venture with an affiliate of World Film Services to acquire, produce and distribute television programming around the world.     World Film Services was formed by chairman John Heyman in 1963 to produce films.      Reuter &#3;  
-FORD MOTOR CREDIT <F> TO REDEEM DEBENTURES	19870226214753	Ford Motor Co said its Ford Motor Credit Co on April One will redeem 4.0 mln dlrs of its 8.70 pct debentures due April 1, 1999.     It said the debentures are redeemable at a price of 100 pct of the principal. Because April 1, 1987 is an interest payment date on the debentures, no accrued interest will be payable on the redemption date as part of the redemption proceeds.     Debentures will be selected for redemption on a pro rata basis, Ford said.   Reuter &#3;  
-STERLING SOFTWARE <SSW> NOTE HOLDERS OK BUY	19870226214802	Sterling Software Inc said it received consent of a majority of the holders of its eight pct convertible sernior subordinated debentures required to purchase shares of its common.     The company said it may now buy its stock at its discretion depending on market conditions.  Reuter &#3;  
-<SCHULT HOMES CORP> MAKES INITIAL STOCK OFFER	19870226214818	Schult Homes Corp announced an initial public offering of 833,334 units at five dlrs per unit, said Janney Montgomery Scott Inc and Woolcott and Co, managing underwriters of the offering.     They said each unit consists of one common share and one warrant to buy one-half share of common.     The warrant will entitle holders to buy one-half common share at 5.50 dlrs per full share from March one, 1988, to September one, 1989, and thereafter at 6.50 dlrs per full share until March 1991, they said.  Reuter &#3;  
-FLUOR <FLR> UNIT GETS CONSTRUCTION CONTRACT	19870226214826	Fluor Corp said its Fluor Daniel unit received a contract from Union Carbide Corp <UK> covering design, procurement and construction of a 108 megawatt combined cycle cogeneration facility in Seadrift, Texas.     The value of the contract was not disclosed.  Reuter &#3;  
-SUFFIELD FINANCIAL CORP <SFCP> SELLS STOCK	19870226214835	Suffield Financial Corp said   Jon Googel and Benjamin Sisti of Colonial Realty, West Hartford, Conn., purchased 175,900 shares of its stock for 3,416,624.     The company said the purchase equals 5.2 pct of its outstanding shares.  Reuter &#3;  
diff --git a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CountingHighlighterTestTask.java b/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CountingHighlighterTestTask.java
deleted file mode 100644
index 357af06..0000000
--- a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CountingHighlighterTestTask.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.benchmark.byTask.tasks;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.search.highlight.SimpleHTMLFormatter;
-import org.apache.lucene.search.highlight.Highlighter;
-import org.apache.lucene.search.highlight.TextFragment;
-import org.apache.lucene.search.highlight.QueryScorer;
-import org.apache.lucene.search.highlight.TokenSources;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.IndexReader;
-
-import java.io.IOException;
-
-/**
- * Test Search task which counts number of searches.
- */
-public class CountingHighlighterTestTask extends SearchTravRetHighlightTask {
-
-  public static int numHighlightedResults = 0;
-  public static int numDocsRetrieved = 0;
-
-  public CountingHighlighterTestTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  protected Document retrieveDoc(IndexReader ir, int id) throws IOException {
-    Document document = ir.document(id);
-    if (document != null) {
-      numDocsRetrieved++;
-    }
-    return document;
-  }
-
-  @Override
-  public BenchmarkHighlighter getBenchmarkHighlighter(Query q) {
-    highlighter = new Highlighter(new SimpleHTMLFormatter(), new QueryScorer(q));
-    return new BenchmarkHighlighter() {
-      @Override
-      public int doHighlight(IndexReader reader, int doc, String field, Document document, Analyzer analyzer, String text) throws Exception {
-        TokenStream ts = TokenSources.getAnyTokenStream(reader, doc, field, document, analyzer);
-        TextFragment[] frag = highlighter.getBestTextFragments(ts, text, mergeContiguous, maxFrags);
-        numHighlightedResults += frag != null ? frag.length : 0;
-        return frag != null ? frag.length : 0;
-      }
-    };
-  }
-}
diff --git a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CountingSearchTestTask.java b/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CountingSearchTestTask.java
deleted file mode 100755
index 7125723..0000000
--- a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CountingSearchTestTask.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.benchmark.byTask.tasks;
-
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-
-/**
- * Test Search task which counts number of searches.
- */
-public class CountingSearchTestTask extends SearchTask {
-
-  public static int numSearches = 0; 
-  public static long startMillis;
-  public static long lastMillis;
-  public static long prevLastMillis;
-
-  public CountingSearchTestTask(PerfRunData runData) {
-    super(runData);
-  }
-
-  @Override
-  public int doLogic() throws Exception {
-    int res = super.doLogic();
-    incrNumSearches();
-    return res;
-  }
-
-  private static synchronized void incrNumSearches() {
-    prevLastMillis = lastMillis;
-    lastMillis = System.currentTimeMillis();
-    if (0 == numSearches) {
-      startMillis = prevLastMillis = lastMillis;
-    }
-    numSearches++;
-  }
-
-  public long getElapsedMillis() {
-    return lastMillis - startMillis;
-  }
-}
diff --git a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTaskTest.java b/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTaskTest.java
deleted file mode 100644
index 6bab788..0000000
--- a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTaskTest.java
+++ /dev/null
@@ -1,108 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.PrintStream;
-import java.util.Properties;
-
-import org.apache.lucene.benchmark.BenchmarkTestCase;
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.index.NoDeletionPolicy;
-import org.apache.lucene.index.NoMergePolicy;
-import org.apache.lucene.index.NoMergeScheduler;
-import org.apache.lucene.util.Version;
-
-
-/** Tests the functionality of {@link CreateIndexTask}. */
-public class CreateIndexTaskTest extends BenchmarkTestCase {
-
-  private PerfRunData createPerfRunData(String infoStreamValue) throws Exception {
-    Properties props = new Properties();
-    props.setProperty("writer.version", Version.LUCENE_40.toString());
-    props.setProperty("print.props", "false"); // don't print anything
-    props.setProperty("directory", "RAMDirectory");
-    if (infoStreamValue != null) {
-      props.setProperty("writer.info.stream", infoStreamValue);
-    }
-    Config config = new Config(props);
-    return new PerfRunData(config);
-  }
-
-  public void testInfoStream_SystemOutErr() throws Exception {
- 
-    PrintStream curOut = System.out;
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    System.setOut(new PrintStream(baos));
-    try {
-      PerfRunData runData = createPerfRunData("SystemOut");
-      CreateIndexTask cit = new CreateIndexTask(runData);
-      cit.doLogic();
-      new CloseIndexTask(runData).doLogic();
-      assertTrue(baos.size() > 0);
-    } finally {
-      System.setOut(curOut);
-    }
-    
-    PrintStream curErr = System.err;
-    baos.reset();
-    System.setErr(new PrintStream(baos));
-    try {
-      PerfRunData runData = createPerfRunData("SystemErr");
-      CreateIndexTask cit = new CreateIndexTask(runData);
-      cit.doLogic();
-      new CloseIndexTask(runData).doLogic();
-      assertTrue(baos.size() > 0);
-    } finally {
-      System.setErr(curErr);
-    }
-
-  }
-
-  public void testInfoStream_File() throws Exception {
-    
-    File outFile = new File(getWorkDir(), "infoStreamTest");
-    PerfRunData runData = createPerfRunData(outFile.getAbsolutePath());
-    new CreateIndexTask(runData).doLogic();
-    new CloseIndexTask(runData).doLogic();
-    assertTrue(outFile.length() > 0);
-  }
-
-  public void testNoMergePolicy() throws Exception {
-    PerfRunData runData = createPerfRunData(null);
-    runData.getConfig().set("merge.policy", NoMergePolicy.class.getName());
-    new CreateIndexTask(runData).doLogic();
-    new CloseIndexTask(runData).doLogic();
-  }
-  
-  public void testNoMergeScheduler() throws Exception {
-    PerfRunData runData = createPerfRunData(null);
-    runData.getConfig().set("merge.scheduler", NoMergeScheduler.class.getName());
-    new CreateIndexTask(runData).doLogic();
-    new CloseIndexTask(runData).doLogic();
-  }
-
-  public void testNoDeletionPolicy() throws Exception {
-    PerfRunData runData = createPerfRunData(null);
-    runData.getConfig().set("deletion.policy", NoDeletionPolicy.class.getName());
-    new CreateIndexTask(runData).doLogic();
-    new CloseIndexTask(runData).doLogic();
-  }
-}
diff --git a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/PerfTaskTest.java b/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/PerfTaskTest.java
deleted file mode 100644
index 2133faa..0000000
--- a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/PerfTaskTest.java
+++ /dev/null
@@ -1,73 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Properties;
-
-import org.apache.lucene.benchmark.BenchmarkTestCase;
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-
-/** Tests the functionality of the abstract {@link PerfTask}. */
-public class PerfTaskTest extends BenchmarkTestCase {
-
-  private static final class MyPerfTask extends PerfTask {
-
-    public MyPerfTask(PerfRunData runData) {
-      super(runData);
-    }
-
-    @Override
-    public int doLogic() throws Exception {
-      return 0;
-    }
-
-    public int getLogStep() { return logStep; }
-    
-  }
-  
-  private PerfRunData createPerfRunData(boolean setLogStep, int logStepVal,
-      boolean setTaskLogStep, int taskLogStepVal) throws Exception {
-    Properties props = new Properties();
-    if (setLogStep) {
-      props.setProperty("log.step", Integer.toString(logStepVal));
-    }
-    if (setTaskLogStep) {
-      props.setProperty("log.step.MyPerf", Integer.toString(taskLogStepVal));
-    }
-    props.setProperty("directory", "RAMDirectory"); // no accidental FS dir.
-    Config config = new Config(props);
-    return new PerfRunData(config);
-  }
-  
-  private void doLogStepTest(boolean setLogStep, int logStepVal,
-      boolean setTaskLogStep, int taskLogStepVal, int expLogStepValue) throws Exception {
-    PerfRunData runData = createPerfRunData(setLogStep, logStepVal, setTaskLogStep, taskLogStepVal);
-    MyPerfTask mpt = new MyPerfTask(runData);
-    assertEquals(expLogStepValue, mpt.getLogStep());
-  }
-  
-  public void testLogStep() throws Exception {
-    doLogStepTest(false, -1, false, -1, PerfTask.DEFAULT_LOG_STEP);
-    doLogStepTest(true, -1, false, -1, Integer.MAX_VALUE);
-    doLogStepTest(true, 100, false, -1, 100);
-    doLogStepTest(false, -1, true, -1, Integer.MAX_VALUE);
-    doLogStepTest(false, -1, true, 100, 100);
-  }
-  
-}
diff --git a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTaskTest.java b/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTaskTest.java
deleted file mode 100644
index f212116..0000000
--- a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTaskTest.java
+++ /dev/null
@@ -1,290 +0,0 @@
-package org.apache.lucene.benchmark.byTask.tasks;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.util.HashSet;
-import java.util.Properties;
-import java.util.Set;
-
-import org.apache.commons.compress.compressors.CompressorStreamFactory;
-import org.apache.lucene.benchmark.BenchmarkTestCase;
-import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
-import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.Field.Index;
-import org.apache.lucene.document.Field.Store;
-
-/** Tests the functionality of {@link WriteLineDocTask}. */
-public class WriteLineDocTaskTest extends BenchmarkTestCase {
-
-  // class has to be public so that Class.forName.newInstance() will work
-  public static final class WriteLineDocMaker extends DocMaker {
-  
-    @Override
-    public Document makeDocument() throws Exception {
-      Document doc = new Document();
-      doc.add(new Field(BODY_FIELD, "body", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
-      doc.add(new Field(TITLE_FIELD, "title", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
-      doc.add(new Field(DATE_FIELD, "date", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
-      return doc;
-    }
-    
-  }
-  
-  // class has to be public so that Class.forName.newInstance() will work
-  public static final class NewLinesDocMaker extends DocMaker {
-  
-    @Override
-    public Document makeDocument() throws Exception {
-      Document doc = new Document();
-      doc.add(new Field(BODY_FIELD, "body\r\ntext\ttwo", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
-      doc.add(new Field(TITLE_FIELD, "title\r\ntext", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
-      doc.add(new Field(DATE_FIELD, "date\r\ntext", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
-      return doc;
-    }
-    
-  }
-  
-  // class has to be public so that Class.forName.newInstance() will work
-  public static final class NoBodyDocMaker extends DocMaker {
-    @Override
-    public Document makeDocument() throws Exception {
-      Document doc = new Document();
-      doc.add(new Field(TITLE_FIELD, "title", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
-      doc.add(new Field(DATE_FIELD, "date", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
-      return doc;
-    }
-  }
-  
-  // class has to be public so that Class.forName.newInstance() will work
-  public static final class NoTitleDocMaker extends DocMaker {
-    @Override
-    public Document makeDocument() throws Exception {
-      Document doc = new Document();
-      doc.add(new Field(BODY_FIELD, "body", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
-      doc.add(new Field(DATE_FIELD, "date", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
-      return doc;
-    }
-  }
-  
-  // class has to be public so that Class.forName.newInstance() will work
-  public static final class JustDateDocMaker extends DocMaker {
-    @Override
-    public Document makeDocument() throws Exception {
-      Document doc = new Document();
-      doc.add(new Field(DATE_FIELD, "date", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
-      return doc;
-    }
-  }
-  
-  // class has to be public so that Class.forName.newInstance() will work
-  public static final class ThreadingDocMaker extends DocMaker {
-  
-    @Override
-    public Document makeDocument() throws Exception {
-      Document doc = new Document();
-      String name = Thread.currentThread().getName();
-      doc.add(new Field(BODY_FIELD, "body_" + name, Store.NO, Index.NOT_ANALYZED_NO_NORMS));
-      doc.add(new Field(TITLE_FIELD, "title_" + name, Store.NO, Index.NOT_ANALYZED_NO_NORMS));
-      doc.add(new Field(DATE_FIELD, "date_" + name, Store.NO, Index.NOT_ANALYZED_NO_NORMS));
-      return doc;
-    }
-    
-  }
-
-  private static final CompressorStreamFactory csFactory = new CompressorStreamFactory();
-
-  private PerfRunData createPerfRunData(File file, boolean setBZCompress,
-                                        String bz2CompressVal,
-                                        String docMakerName) throws Exception {
-    Properties props = new Properties();
-    props.setProperty("doc.maker", docMakerName);
-    props.setProperty("line.file.out", file.getAbsolutePath());
-    if (setBZCompress) {
-      props.setProperty("bzip.compression", bz2CompressVal);
-    }
-    props.setProperty("directory", "RAMDirectory"); // no accidental FS dir.
-    Config config = new Config(props);
-    return new PerfRunData(config);
-  }
-  
-  private void doReadTest(File file, boolean bz2File, String expTitle,
-                          String expDate, String expBody) throws Exception {
-    InputStream in = new FileInputStream(file);
-    if (bz2File) {
-      in = csFactory.createCompressorInputStream("bzip2", in);
-    }
-    BufferedReader br = new BufferedReader(new InputStreamReader(in, "utf-8"));
-    try {
-      String line = br.readLine();
-      assertNotNull(line);
-      String[] parts = line.split(Character.toString(WriteLineDocTask.SEP));
-      int numExpParts = expBody == null ? 2 : 3;
-      assertEquals(numExpParts, parts.length);
-      assertEquals(expTitle, parts[0]);
-      assertEquals(expDate, parts[1]);
-      if (expBody != null) {
-        assertEquals(expBody, parts[2]);
-      }
-      assertNull(br.readLine());
-    } finally {
-      br.close();
-    }
-  }
-  
-  /* Tests WriteLineDocTask with a bzip2 format. */
-  public void testBZip2() throws Exception {
-    
-    // Create a document in bz2 format.
-    File file = new File(getWorkDir(), "one-line.bz2");
-    PerfRunData runData = createPerfRunData(file, true, "true", WriteLineDocMaker.class.getName());
-    WriteLineDocTask wldt = new WriteLineDocTask(runData);
-    wldt.doLogic();
-    wldt.close();
-    
-    doReadTest(file, true, "title", "date", "body");
-  }
-  
-  public void testBZip2AutoDetect() throws Exception {
-    
-    // Create a document in bz2 format.
-    File file = new File(getWorkDir(), "one-line.bz2");
-    PerfRunData runData = createPerfRunData(file, false, null, WriteLineDocMaker.class.getName());
-    WriteLineDocTask wldt = new WriteLineDocTask(runData);
-    wldt.doLogic();
-    wldt.close();
-    
-    doReadTest(file, true, "title", "date", "body");
-  }
-  
-  public void testRegularFile() throws Exception {
-    
-    // Create a document in regular format.
-    File file = new File(getWorkDir(), "one-line");
-    PerfRunData runData = createPerfRunData(file, true, "false", WriteLineDocMaker.class.getName());
-    WriteLineDocTask wldt = new WriteLineDocTask(runData);
-    wldt.doLogic();
-    wldt.close();
-    
-    doReadTest(file, false, "title", "date", "body");
-  }
-
-  public void testCharsReplace() throws Exception {
-    // WriteLineDocTask replaced only \t characters w/ a space, since that's its
-    // separator char. However, it didn't replace newline characters, which
-    // resulted in errors in LineDocSource.
-    File file = new File(getWorkDir(), "one-line");
-    PerfRunData runData = createPerfRunData(file, false, null, NewLinesDocMaker.class.getName());
-    WriteLineDocTask wldt = new WriteLineDocTask(runData);
-    wldt.doLogic();
-    wldt.close();
-    
-    doReadTest(file, false, "title text", "date text", "body text two");
-  }
-  
-  public void testEmptyBody() throws Exception {
-    // WriteLineDocTask threw away documents w/ no BODY element, even if they
-    // had a TITLE element (LUCENE-1755). It should throw away documents if they
-    // don't have BODY nor TITLE
-    File file = new File(getWorkDir(), "one-line");
-    PerfRunData runData = createPerfRunData(file, false, null, NoBodyDocMaker.class.getName());
-    WriteLineDocTask wldt = new WriteLineDocTask(runData);
-    wldt.doLogic();
-    wldt.close();
-    
-    doReadTest(file, false, "title", "date", null);
-  }
-  
-  public void testEmptyTitle() throws Exception {
-    File file = new File(getWorkDir(), "one-line");
-    PerfRunData runData = createPerfRunData(file, false, null, NoTitleDocMaker.class.getName());
-    WriteLineDocTask wldt = new WriteLineDocTask(runData);
-    wldt.doLogic();
-    wldt.close();
-    
-    doReadTest(file, false, "", "date", "body");
-  }
-  
-  public void testJustDate() throws Exception {
-    File file = new File(getWorkDir(), "one-line");
-    PerfRunData runData = createPerfRunData(file, false, null, JustDateDocMaker.class.getName());
-    WriteLineDocTask wldt = new WriteLineDocTask(runData);
-    wldt.doLogic();
-    wldt.close();
-    
-    BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(file), "utf-8"));
-    try {
-      String line = br.readLine();
-      assertNull(line);
-    } finally {
-      br.close();
-    }
-  }
-
-  public void testMultiThreaded() throws Exception {
-    File file = new File(getWorkDir(), "one-line");
-    PerfRunData runData = createPerfRunData(file, false, null, ThreadingDocMaker.class.getName());
-    final WriteLineDocTask wldt = new WriteLineDocTask(runData);
-    Thread[] threads = new Thread[10];
-    for (int i = 0; i < threads.length; i++) {
-      threads[i] = new Thread("t" + i) {
-        @Override
-        public void run() {
-          try {
-            wldt.doLogic();
-          } catch (Exception e) {
-            throw new RuntimeException(e);
-          }
-        }
-      };
-    }
-    
-    for (Thread t : threads) t.start();
-    for (Thread t : threads) t.join();
-    
-    wldt.close();
-    
-    Set<String> ids = new HashSet<String>();
-    BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(file), "utf-8"));
-    try {
-      for (int i = 0; i < threads.length; i++) {
-        String line = br.readLine();
-        String[] parts = line.split(Character.toString(WriteLineDocTask.SEP));
-        assertEquals(3, parts.length);
-        // check that all thread names written are the same in the same line
-        String tname = parts[0].substring(parts[0].indexOf('_'));
-        ids.add(tname);
-        assertEquals(tname, parts[1].substring(parts[1].indexOf('_')));
-        assertEquals(tname, parts[2].substring(parts[2].indexOf('_')));
-      }
-      // only threads.length lines should exist
-      assertNull(br.readLine());
-      assertEquals(threads.length, ids.size());
-    } finally {
-      br.close();
-    }
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/utils/TestConfig.java b/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/utils/TestConfig.java
deleted file mode 100644
index 939f74d..0000000
--- a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/utils/TestConfig.java
+++ /dev/null
@@ -1,37 +0,0 @@
-package org.apache.lucene.benchmark.byTask.utils;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Properties;
-
-import org.apache.lucene.util.LuceneTestCase;
-import org.junit.Test;
-
-public class TestConfig extends LuceneTestCase {
-
-  @Test
-  public void testAbsolutePathNamesWindows() throws Exception {
-    Properties props = new Properties();
-    props.setProperty("work.dir1", "c:\\temp");
-    props.setProperty("work.dir2", "c:/temp");
-    Config conf = new Config(props);
-    assertEquals("c:\\temp", conf.get("work.dir1", ""));
-    assertEquals("c:/temp", conf.get("work.dir2", ""));
-  }
-
-}
diff --git a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/TestQualityRun.java b/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/TestQualityRun.java
deleted file mode 100644
index 4cee276..0000000
--- a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/TestQualityRun.java
+++ /dev/null
@@ -1,195 +0,0 @@
-package org.apache.lucene.benchmark.quality;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.benchmark.BenchmarkTestCase;
-import org.apache.lucene.benchmark.quality.trec.TrecJudge;
-import org.apache.lucene.benchmark.quality.trec.TrecTopicsReader;
-import org.apache.lucene.benchmark.quality.utils.SimpleQQParser;
-import org.apache.lucene.benchmark.quality.utils.SubmissionReport;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.store.Directory;
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.io.PrintWriter;
-
-/**
- * Test that quality run does its job.
- * <p>
- * NOTE: if the default scoring or StandardAnalyzer is changed, then
- * this test will not work correctly, as it does not dynamically
- * generate its test trec topics/qrels!
- */
-public class TestQualityRun extends BenchmarkTestCase {
-  
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    copyToWorkDir("reuters.578.lines.txt.bz2");
-  }
-
-  public void testTrecQuality() throws Exception {
-    // first create the partial reuters index
-    createReutersIndex();
-    
-    int maxResults = 1000;
-    String docNameField = "doctitle"; // orig docID is in the linedoc format title 
-    
-    PrintWriter logger = VERBOSE ? new PrintWriter(System.out,true) : null;
-   
-    // prepare topics
-    InputStream topics = getClass().getResourceAsStream("trecTopics.txt");
-    TrecTopicsReader qReader = new TrecTopicsReader();
-    QualityQuery qqs[] = qReader.readQueries(new BufferedReader(new InputStreamReader(topics, "UTF-8")));
-    
-    // prepare judge
-    InputStream qrels = getClass().getResourceAsStream("trecQRels.txt");
-    Judge judge = new TrecJudge(new BufferedReader(new InputStreamReader(qrels, "UTF-8")));
-    
-    // validate topics & judgments match each other
-    judge.validateData(qqs, logger);
-    
-    Directory dir = newFSDirectory(new File(getWorkDir(),"index"));
-    IndexSearcher searcher = new IndexSearcher(dir, true);
-
-    QualityQueryParser qqParser = new SimpleQQParser("title","body");
-    QualityBenchmark qrun = new QualityBenchmark(qqs, qqParser, searcher, docNameField);
-    
-    SubmissionReport submitLog = VERBOSE ? new SubmissionReport(logger, "TestRun") : null;
-    qrun.setMaxResults(maxResults);
-    QualityStats stats[] = qrun.execute(judge, submitLog, logger);
-    
-    // --------- verify by the way judgments were altered for this test:
-    // for some queries, depending on m = qnum % 8
-    // m==0: avg_precision and recall are hurt, by marking fake docs as relevant
-    // m==1: precision_at_n and avg_precision are hurt, by unmarking relevant docs
-    // m==2: all precision, precision_at_n and recall are hurt.
-    // m>=3: these queries remain perfect
-    for (int i = 0; i < stats.length; i++) {
-      QualityStats s = stats[i];
-      switch (i%8) {
-
-      case 0:
-        assertTrue("avg-p should be hurt: "+s.getAvp(), 1.0 > s.getAvp());
-        assertTrue("recall should be hurt: "+s.getRecall(), 1.0 > s.getRecall());
-        for (int j = 1; j <= QualityStats.MAX_POINTS; j++) {
-          assertEquals("p_at_"+j+" should be perfect: "+s.getPrecisionAt(j), 1.0, s.getPrecisionAt(j), 1E-2);
-        }
-        break;
-      
-      case 1:
-        assertTrue("avg-p should be hurt", 1.0 > s.getAvp());
-        assertEquals("recall should be perfect: "+s.getRecall(), 1.0, s.getRecall(), 1E-2);
-        for (int j = 1; j <= QualityStats.MAX_POINTS; j++) {
-          assertTrue("p_at_"+j+" should be hurt: "+s.getPrecisionAt(j), 1.0 > s.getPrecisionAt(j));
-        }
-        break;
-
-      case 2:
-        assertTrue("avg-p should be hurt: "+s.getAvp(), 1.0 > s.getAvp());
-        assertTrue("recall should be hurt: "+s.getRecall(), 1.0 > s.getRecall());
-        for (int j = 1; j <= QualityStats.MAX_POINTS; j++) {
-          assertTrue("p_at_"+j+" should be hurt: "+s.getPrecisionAt(j), 1.0 > s.getPrecisionAt(j));
-        }
-        break;
-
-      default: {
-        assertEquals("avg-p should be perfect: "+s.getAvp(), 1.0, s.getAvp(), 1E-2);
-        assertEquals("recall should be perfect: "+s.getRecall(), 1.0, s.getRecall(), 1E-2);
-        for (int j = 1; j <= QualityStats.MAX_POINTS; j++) {
-          assertEquals("p_at_"+j+" should be perfect: "+s.getPrecisionAt(j), 1.0, s.getPrecisionAt(j), 1E-2);
-        }
-      }
-      
-      }
-    }
-    
-    QualityStats avg = QualityStats.average(stats);
-    if (logger!=null) {
-      avg.log("Average statistis:",1,logger,"  ");
-    }
-    
-    assertTrue("mean avg-p should be hurt: "+avg.getAvp(), 1.0 > avg.getAvp());
-    assertTrue("avg recall should be hurt: "+avg.getRecall(), 1.0 > avg.getRecall());
-    for (int j = 1; j <= QualityStats.MAX_POINTS; j++) {
-      assertTrue("avg p_at_"+j+" should be hurt: "+avg.getPrecisionAt(j), 1.0 > avg.getPrecisionAt(j));
-    }
-    
-    searcher.close();
-    dir.close();
-  }
-  
-  public void testTrecTopicsReader() throws Exception {    
-    // prepare topics
-    InputStream topicsFile = getClass().getResourceAsStream("trecTopics.txt");
-    TrecTopicsReader qReader = new TrecTopicsReader();
-    QualityQuery qqs[] = qReader.readQueries(
-        new BufferedReader(new InputStreamReader(topicsFile, "UTF-8")));
-    
-    assertEquals(20, qqs.length);
-    
-    QualityQuery qq = qqs[0];
-    assertEquals("statement months  total 1987", qq.getValue("title"));
-    assertEquals("Topic 0 Description Line 1 Topic 0 Description Line 2", 
-        qq.getValue("description"));
-    assertEquals("Topic 0 Narrative Line 1 Topic 0 Narrative Line 2", 
-        qq.getValue("narrative"));
-    
-    qq = qqs[1];
-    assertEquals("agreed 15  against five", qq.getValue("title"));
-    assertEquals("Topic 1 Description Line 1 Topic 1 Description Line 2", 
-        qq.getValue("description"));
-    assertEquals("Topic 1 Narrative Line 1 Topic 1 Narrative Line 2", 
-        qq.getValue("narrative"));
-    
-    qq = qqs[19];
-    assertEquals("20 while  common week", qq.getValue("title"));
-    assertEquals("Topic 19 Description Line 1 Topic 19 Description Line 2", 
-        qq.getValue("description"));
-    assertEquals("Topic 19 Narrative Line 1 Topic 19 Narrative Line 2", 
-        qq.getValue("narrative"));
-  }
-
-  // use benchmark logic to create the mini Reuters index
-  private void createReutersIndex() throws Exception {
-    // 1. alg definition
-    String algLines[] = {
-        "# ----- properties ",
-        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
-        "analyzer=org.apache.lucene.analysis.standard.ClassicAnalyzer",
-        "docs.file=" + getWorkDirResourcePath("reuters.578.lines.txt.bz2"),
-        "content.source.log.step=2500",
-        "doc.term.vector=false",
-        "content.source.forever=false",
-        "directory=FSDirectory",
-        "doc.stored=true",
-        "doc.tokenized=true",
-        "# ----- alg ",
-        "ResetSystemErase",
-        "CreateIndex",
-        "{ AddDoc } : *",
-        "CloseIndex",
-    };
-    
-    // 2. execute the algorithm  (required in every "logic" test)
-    execBenchmark(algLines);
-  }
-}
diff --git a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/reuters.578.lines.txt.bz2 b/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/reuters.578.lines.txt.bz2
deleted file mode 100644
index 1fd8d54..0000000
Binary files a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/reuters.578.lines.txt.bz2 and /dev/null differ
diff --git a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/trecQRels.txt b/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/trecQRels.txt
deleted file mode 100755
index 13c2d77..0000000
--- a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/trecQRels.txt
+++ /dev/null
@@ -1,723 +0,0 @@
-# -----------------------------------------------------------------------
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-# 
-#     http://www.apache.org/licenses/LICENSE-2.0
-# 
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# -----------------------------------------------------------------------
-
-# ------------------------------------------------------------
-# Format:
-#
-#       qnum   0   doc-name     is-relevant
-#
-#
-# The origin of this file was created using 
-# utils.QualityQueriesFinder, so all queries 
-# would have perfect 1.0 for all meassures.
-#
-# To make it suitable for testing it was modified
-# for some queries, depending on m = qnum % 8
-# m==0: avg_precision and recall are hurt, by marking fake docs as relevant
-# m==1: precision_at_n and avg_precision are hurt, by unmarking relevant docs
-# m==2: all precision, precision_at_n and recall are hurt.
-# m>=3: these queries remain perfect
-# ------------------------------------------------------------
-
-# --- m==0: avg_precision and recall are hurt, by marking fake docs as relevant
-
-0 	 0 	 fakedoc1             	 1
-0 	 0 	 fakedoc2             	 1
-0 	 0 	 fakedoc3             	 1
-0 	 0 	 fakedoc4             	 1
-
-0 	 0 	 doc18211             	 1
-0 	 0 	 doc20192             	 1
-0 	 0 	 doc7401              	 1
-0 	 0 	 doc11285             	 1
-0 	 0 	 doc20647             	 1
-0 	 0 	 doc3057              	 1
-0 	 0 	 doc12431             	 1
-0 	 0 	 doc4989              	 1
-0 	 0 	 doc17324             	 1
-0 	 0 	 doc4030              	 1
-0 	 0 	 doc4290              	 1
-0 	 0 	 doc3462              	 1
-0 	 0 	 doc15313             	 1
-0 	 0 	 doc10303             	 1
-0 	 0 	 doc1893              	 1
-0 	 0 	 doc5008              	 1
-0 	 0 	 doc14634             	 1
-0 	 0 	 doc5471              	 1
-0 	 0 	 doc17904             	 1
-0 	 0 	 doc7168              	 1
-0 	 0 	 doc21275             	 1
-0 	 0 	 doc9011              	 1
-0 	 0 	 doc17546             	 1
-0 	 0 	 doc9102              	 1
-0 	 0 	 doc13199             	 1
-
-# --- m==1: precision_at_n and avg_precision are hurt, by unmarking relevant docs
-
-1 	 0 	 doc9857              	 0
-1 	 0 	 doc16846             	 1
-1 	 0 	 doc4320              	 1
-1 	 0 	 doc9501              	 0
-1 	 0 	 doc10159             	 1
-1 	 0 	 doc16642             	 1
-1 	 0 	 doc17536             	 0
-1 	 0 	 doc17571             	 1
-1 	 0 	 doc18728             	 1
-1 	 0 	 doc18828             	 1
-1 	 0 	 doc19108             	 0
-1 	 0 	 doc9940              	 1
-1 	 0 	 doc11852             	 1
-1 	 0 	 doc7430              	 0
-1 	 0 	 doc19162             	 1
-1 	 0 	 doc1743              	 1
-1 	 0 	 doc2137              	 1
-1 	 0 	 doc7611              	 1
-1 	 0 	 doc8072              	 1
-1 	 0 	 doc12764             	 1
-1 	 0 	 doc2593              	 1
-1 	 0 	 doc11088             	 1
-1 	 0 	 doc931               	 1
-1 	 0 	 doc7673              	 1
-1 	 0 	 doc12941             	 1
-1 	 0 	 doc11797             	 1
-1 	 0 	 doc11831             	 1
-1 	 0 	 doc13162             	 1
-1 	 0 	 doc4423              	 1
-1 	 0 	 doc5217              	 1
-
-# ---- m==2: all precision, precision_at_n and recall are hurt.
-
-2 	 0 	 fakedoc1             	 1
-2 	 0 	 fakedoc2             	 1
-2 	 0 	 fakedoc3             	 1
-2 	 0 	 fakedoc4             	 1
-
-2 	 0 	 doc3137              	 0
-2 	 0 	 doc7142              	 0
-2 	 0 	 doc13667             	 0
-2 	 0 	 doc13171             	 0
-2 	 0 	 doc13372             	 1
-2 	 0 	 doc21415             	 1
-2 	 0 	 doc16298             	 1
-2 	 0 	 doc14957             	 1
-2 	 0 	 doc153               	 1
-2 	 0 	 doc16092             	 1
-2 	 0 	 doc16096             	 1
-2 	 0 	 doc21303             	 1
-2 	 0 	 doc18681             	 1
-2 	 0 	 doc20756             	 1
-2 	 0 	 doc355               	 1
-2 	 0 	 doc13395             	 1
-2 	 0 	 doc5009              	 1
-2 	 0 	 doc17164             	 1
-2 	 0 	 doc13162             	 1
-2 	 0 	 doc11757             	 1
-2 	 0 	 doc9637              	 1
-2 	 0 	 doc18087             	 1
-2 	 0 	 doc4593              	 1
-2 	 0 	 doc4677              	 1
-2 	 0 	 doc20865             	 1
-2 	 0 	 doc8556              	 1
-2 	 0 	 doc2578              	 1
-2 	 0 	 doc1163              	 1
-2 	 0 	 doc3797              	 1
-2 	 0 	 doc11094             	 1
-
-
-3 	 0 	 doc19578             	 1
-3 	 0 	 doc14860             	 1
-3 	 0 	 doc7235              	 1
-3 	 0 	 doc20590             	 1
-3 	 0 	 doc17933             	 1
-3 	 0 	 doc9384              	 1
-3 	 0 	 doc10783             	 1
-3 	 0 	 doc1963              	 1
-3 	 0 	 doc18356             	 1
-3 	 0 	 doc13254             	 1
-3 	 0 	 doc18402             	 1
-3 	 0 	 doc15241             	 1
-3 	 0 	 doc3303              	 1
-3 	 0 	 doc8868              	 1
-3 	 0 	 doc18520             	 1
-3 	 0 	 doc4650              	 1
-3 	 0 	 doc4727              	 1
-3 	 0 	 doc21518             	 1
-3 	 0 	 doc5060              	 1
-3 	 0 	 doc7587              	 1
-3 	 0 	 doc2990              	 1
-3 	 0 	 doc8042              	 1
-3 	 0 	 doc6304              	 1
-3 	 0 	 doc13223             	 1
-3 	 0 	 doc1964              	 1
-3 	 0 	 doc10597             	 1
-3 	 0 	 doc21023             	 1
-3 	 0 	 doc19057             	 1
-3 	 0 	 doc14948             	 1
-3 	 0 	 doc9692              	 1
-
-
-4 	 0 	 doc2534              	 1
-4 	 0 	 doc21388             	 1
-4 	 0 	 doc20923             	 1
-4 	 0 	 doc11547             	 1
-4 	 0 	 doc19755             	 1
-4 	 0 	 doc3793              	 1
-4 	 0 	 doc6714              	 1
-4 	 0 	 doc12722             	 1
-4 	 0 	 doc5552              	 1
-4 	 0 	 doc6810              	 1
-4 	 0 	 doc16953             	 1
-4 	 0 	 doc2527              	 1
-4 	 0 	 doc5361              	 1
-4 	 0 	 doc12353             	 1
-4 	 0 	 doc7308              	 1
-4 	 0 	 doc3836              	 1
-4 	 0 	 doc2293              	 1
-4 	 0 	 doc7348              	 1
-4 	 0 	 doc17119             	 1
-4 	 0 	 doc19331             	 1
-4 	 0 	 doc3411              	 1
-4 	 0 	 doc14643             	 1
-4 	 0 	 doc9058              	 1
-4 	 0 	 doc11099             	 1
-4 	 0 	 doc12485             	 1
-4 	 0 	 doc16432             	 1
-4 	 0 	 doc10047             	 1
-4 	 0 	 doc13788             	 1
-4 	 0 	 doc117               	 1
-4 	 0 	 doc638               	 1
-
-
-
-5 	 0 	 doc169               	 1
-5 	 0 	 doc13181             	 1
-5 	 0 	 doc4350              	 1
-5 	 0 	 doc10242             	 1
-5 	 0 	 doc955               	 1
-5 	 0 	 doc5389              	 1
-5 	 0 	 doc17122             	 1
-5 	 0 	 doc17417             	 1
-5 	 0 	 doc12199             	 1
-5 	 0 	 doc6918              	 1
-5 	 0 	 doc3857              	 1
-5 	 0 	 doc2981              	 1
-5 	 0 	 doc10639             	 1
-5 	 0 	 doc10478             	 1
-5 	 0 	 doc8573              	 1
-5 	 0 	 doc9197              	 1
-5 	 0 	 doc9298              	 1
-5 	 0 	 doc2492              	 1
-5 	 0 	 doc10262             	 1
-5 	 0 	 doc5180              	 1
-5 	 0 	 doc11758             	 1
-5 	 0 	 doc4065              	 1
-5 	 0 	 doc9124              	 1
-5 	 0 	 doc11528             	 1
-5 	 0 	 doc18879             	 1
-5 	 0 	 doc17864             	 1
-5 	 0 	 doc3204              	 1
-5 	 0 	 doc12157             	 1
-5 	 0 	 doc4496              	 1
-5 	 0 	 doc20190             	 1
-
-
-
-6 	 0 	 doc9507              	 1
-6 	 0 	 doc15630             	 1
-6 	 0 	 doc8469              	 1
-6 	 0 	 doc11918             	 1
-6 	 0 	 doc20482             	 1
-6 	 0 	 doc20158             	 1
-6 	 0 	 doc19831             	 1
-6 	 0 	 doc8296              	 1
-6 	 0 	 doc8930              	 1
-6 	 0 	 doc16460             	 1
-6 	 0 	 doc2577              	 1
-6 	 0 	 doc15476             	 1
-6 	 0 	 doc1767              	 1
-6 	 0 	 doc689               	 1
-6 	 0 	 doc16606             	 1
-6 	 0 	 doc6149              	 1
-6 	 0 	 doc18691             	 1
-6 	 0 	 doc2208              	 1
-6 	 0 	 doc3592              	 1
-6 	 0 	 doc11199             	 1
-6 	 0 	 doc16329             	 1
-6 	 0 	 doc6007              	 1
-6 	 0 	 doc15231             	 1
-6 	 0 	 doc20622             	 1
-6 	 0 	 doc21468             	 1
-6 	 0 	 doc12230             	 1
-6 	 0 	 doc5723              	 1
-6 	 0 	 doc8120              	 1
-6 	 0 	 doc8668              	 1
-6 	 0 	 doc303               	 1
-
-
-
-
-7 	 0 	 doc7728              	 1
-7 	 0 	 doc7693              	 1
-7 	 0 	 doc21088             	 1
-7 	 0 	 doc5017              	 1
-7 	 0 	 doc10807             	 1
-7 	 0 	 doc16204             	 1
-7 	 0 	 doc2233              	 1
-7 	 0 	 doc3632              	 1
-7 	 0 	 doc4719              	 1
-7 	 0 	 doc6477              	 1
-7 	 0 	 doc6502              	 1
-7 	 0 	 doc6709              	 1
-7 	 0 	 doc7710              	 1
-7 	 0 	 doc9193              	 1
-7 	 0 	 doc9309              	 1
-7 	 0 	 doc9789              	 1
-7 	 0 	 doc10971             	 1
-7 	 0 	 doc18059             	 1
-7 	 0 	 doc19906             	 1
-7 	 0 	 doc20089             	 1
-7 	 0 	 doc20102             	 1
-7 	 0 	 doc21040             	 1
-7 	 0 	 doc21153             	 1
-7 	 0 	 doc9147              	 1
-7 	 0 	 doc9930              	 1
-7 	 0 	 doc19763             	 1
-7 	 0 	 doc1559              	 1
-7 	 0 	 doc21248             	 1
-7 	 0 	 doc17945             	 1
-7 	 0 	 doc526               	 1
-
-
-# --- m==0: avg_precision and recall are hurt, by marking fake docs as relevant
-
-8 	 0 	 fakedoc1             	 1
-8 	 0 	 fakedoc2             	 1
-8 	 0 	 fakedoc3             	 1
-8 	 0 	 fakedoc4             	 1
-
-8 	 0 	 doc16299             	 1
-8 	 0 	 doc1662              	 1
-8 	 0 	 doc4585              	 1
-8 	 0 	 doc12315             	 1
-8 	 0 	 doc16266             	 1
-8 	 0 	 doc13136             	 1
-8 	 0 	 doc19212             	 1
-8 	 0 	 doc7086              	 1
-8 	 0 	 doc7062              	 1
-8 	 0 	 doc6134              	 1
-8 	 0 	 doc13953             	 1
-8 	 0 	 doc16264             	 1
-8 	 0 	 doc2494              	 1
-8 	 0 	 doc10636             	 1
-8 	 0 	 doc10894             	 1
-8 	 0 	 doc6844              	 1
-8 	 0 	 doc674               	 1
-8 	 0 	 doc13520             	 1
-8 	 0 	 doc344               	 1
-8 	 0 	 doc2896              	 1
-8 	 0 	 doc11871             	 1
-8 	 0 	 doc1862              	 1
-8 	 0 	 doc16728             	 1
-8 	 0 	 doc10308             	 1
-8 	 0 	 doc2227              	 1
-8 	 0 	 doc13167             	 1
-8 	 0 	 doc20607             	 1
-8 	 0 	 doc9670              	 1
-8 	 0 	 doc1566              	 1
-8 	 0 	 doc17885             	 1
-
-
-# ---- m==1: precision_at_n and avg_precision are hurt, by unmarking relevant docs
-
-
-9 	 0 	 doc1990              	 0
-9 	 0 	 doc9342              	 1
-9 	 0 	 doc19427             	 1
-9 	 0 	 doc12432             	 0
-9 	 0 	 doc13480             	 1
-9 	 0 	 doc3322              	 1
-9 	 0 	 doc16044             	 1
-9 	 0 	 doc266               	 0
-9 	 0 	 doc3437              	 1
-9 	 0 	 doc5370              	 1
-9 	 0 	 doc10314             	 1
-9 	 0 	 doc4892              	 1
-9 	 0 	 doc5763              	 0
-9 	 0 	 doc14045             	 1
-9 	 0 	 doc1090              	 1
-9 	 0 	 doc7437              	 1
-9 	 0 	 doc5822              	 1
-9 	 0 	 doc4285              	 1
-9 	 0 	 doc17119             	 1
-9 	 0 	 doc21001             	 1
-9 	 0 	 doc4337              	 1
-9 	 0 	 doc5967              	 1
-9 	 0 	 doc10214             	 1
-9 	 0 	 doc12001             	 1
-9 	 0 	 doc18553             	 1
-9 	 0 	 doc12116             	 1
-9 	 0 	 doc5064              	 1
-9 	 0 	 doc5018              	 1
-9 	 0 	 doc5037              	 1
-9 	 0 	 doc8025              	 1
-
-
-# ---- m==2: all precision, precision_at_n and recall are hurt.
-
-10 	 0 	 fakedoc1             	 1
-10 	 0 	 fakedoc2             	 1
-10 	 0 	 fakedoc3             	 1
-10 	 0 	 fakedoc4             	 1
-
-10 	 0 	 doc17218             	 0
-10 	 0 	 doc10270             	 0
-10 	 0 	 doc5958              	 0
-10 	 0 	 doc19943             	 0
-10 	 0 	 doc6510              	 1
-10 	 0 	 doc16087             	 1
-10 	 0 	 doc14893             	 1
-10 	 0 	 doc8933              	 1
-10 	 0 	 doc4354              	 1
-10 	 0 	 doc16729             	 1
-10 	 0 	 doc16761             	 1
-10 	 0 	 doc6964              	 1
-10 	 0 	 doc16743             	 1
-10 	 0 	 doc7357              	 1
-10 	 0 	 doc2534              	 1
-10 	 0 	 doc18321             	 1
-10 	 0 	 doc18497             	 1
-10 	 0 	 doc11214             	 1
-10 	 0 	 doc11819             	 1
-10 	 0 	 doc10818             	 1
-10 	 0 	 doc15769             	 1
-10 	 0 	 doc5348              	 1
-10 	 0 	 doc14948             	 1
-10 	 0 	 doc7891              	 1
-10 	 0 	 doc9897              	 1
-10 	 0 	 doc15559             	 1
-10 	 0 	 doc14935             	 1
-10 	 0 	 doc14954             	 1
-10 	 0 	 doc6621              	 1
-10 	 0 	 doc6930              	 1
-
-
-11 	 0 	 doc11943             	 1
-11 	 0 	 doc286               	 1
-11 	 0 	 doc1574              	 1
-11 	 0 	 doc17916             	 1
-11 	 0 	 doc17918             	 1
-11 	 0 	 doc19213             	 1
-11 	 0 	 doc9337              	 1
-11 	 0 	 doc8593              	 1
-11 	 0 	 doc8800              	 1
-11 	 0 	 doc18580             	 1
-11 	 0 	 doc209               	 1
-11 	 0 	 doc1893              	 1
-11 	 0 	 doc11189             	 1
-11 	 0 	 doc17702             	 1
-11 	 0 	 doc10180             	 1
-11 	 0 	 doc11869             	 1
-11 	 0 	 doc9705              	 1
-11 	 0 	 doc8715              	 1
-11 	 0 	 doc12753             	 1
-11 	 0 	 doc10195             	 1
-11 	 0 	 doc3552              	 1
-11 	 0 	 doc16030             	 1
-11 	 0 	 doc4623              	 1
-11 	 0 	 doc3188              	 1
-11 	 0 	 doc8735              	 1
-11 	 0 	 doc151               	 1
-11 	 0 	 doc5792              	 1
-11 	 0 	 doc5194              	 1
-11 	 0 	 doc3393              	 1
-11 	 0 	 doc19027             	 1
-
-
-
-12 	 0 	 doc18198             	 1
-12 	 0 	 doc2444              	 1
-12 	 0 	 doc4305              	 1
-12 	 0 	 doc6544              	 1
-12 	 0 	 doc11639             	 1
-12 	 0 	 doc10640             	 1
-12 	 0 	 doc12192             	 1
-12 	 0 	 doc128               	 1
-12 	 0 	 doc10760             	 1
-12 	 0 	 doc10881             	 1
-12 	 0 	 doc2698              	 1
-12 	 0 	 doc3552              	 1
-12 	 0 	 doc20524             	 1
-12 	 0 	 doc1884              	 1
-12 	 0 	 doc9187              	 1
-12 	 0 	 doc3131              	 1
-12 	 0 	 doc2911              	 1
-12 	 0 	 doc2589              	 1
-12 	 0 	 doc3747              	 1
-12 	 0 	 doc3813              	 1
-12 	 0 	 doc5222              	 1
-12 	 0 	 doc6023              	 1
-12 	 0 	 doc6624              	 1
-12 	 0 	 doc7655              	 1
-12 	 0 	 doc9205              	 1
-12 	 0 	 doc12062             	 1
-12 	 0 	 doc15504             	 1
-12 	 0 	 doc13625             	 1
-12 	 0 	 doc18704             	 1
-12 	 0 	 doc2277              	 1
-
-
-
-13 	 0 	 doc4948              	 1
-13 	 0 	 doc21565             	 1
-13 	 0 	 doc17135             	 1
-13 	 0 	 doc1866              	 1
-13 	 0 	 doc13989             	 1
-13 	 0 	 doc5605              	 1
-13 	 0 	 doc13431             	 1
-13 	 0 	 doc2100              	 1
-13 	 0 	 doc16347             	 1
-13 	 0 	 doc16894             	 1
-13 	 0 	 doc6764              	 1
-13 	 0 	 doc8554              	 1
-13 	 0 	 doc8695              	 1
-13 	 0 	 doc8977              	 1
-13 	 0 	 doc19478             	 1
-13 	 0 	 doc14595             	 1
-13 	 0 	 doc2408              	 1
-13 	 0 	 doc2592              	 1
-13 	 0 	 doc10947             	 1
-13 	 0 	 doc15794             	 1
-13 	 0 	 doc5236              	 1
-13 	 0 	 doc14847             	 1
-13 	 0 	 doc3980              	 1
-13 	 0 	 doc1844              	 1
-13 	 0 	 doc42                	 1
-13 	 0 	 doc7783              	 1
-13 	 0 	 doc4557              	 1
-13 	 0 	 doc16423             	 1
-13 	 0 	 doc17170             	 1
-13 	 0 	 doc5822              	 1
-
-
-
-14 	 0 	 doc17172             	 1
-14 	 0 	 doc17210             	 1
-14 	 0 	 doc5044              	 1
-14 	 0 	 doc4627              	 1
-14 	 0 	 doc4683              	 1
-14 	 0 	 doc15126             	 1
-14 	 0 	 doc4538              	 1
-14 	 0 	 doc273               	 1
-14 	 0 	 doc19585             	 1
-14 	 0 	 doc16078             	 1
-14 	 0 	 doc4529              	 1
-14 	 0 	 doc4186              	 1
-14 	 0 	 doc12961             	 1
-14 	 0 	 doc19217             	 1
-14 	 0 	 doc5670              	 1
-14 	 0 	 doc1699              	 1
-14 	 0 	 doc4716              	 1
-14 	 0 	 doc12644             	 1
-14 	 0 	 doc18387             	 1
-14 	 0 	 doc336               	 1
-14 	 0 	 doc16130             	 1
-14 	 0 	 doc18718             	 1
-14 	 0 	 doc12527             	 1
-14 	 0 	 doc11797             	 1
-14 	 0 	 doc11831             	 1
-14 	 0 	 doc7538              	 1
-14 	 0 	 doc17259             	 1
-14 	 0 	 doc18724             	 1
-14 	 0 	 doc19330             	 1
-14 	 0 	 doc19206             	 1
-
-
-
-15 	 0 	 doc12198             	 1
-15 	 0 	 doc20371             	 1
-15 	 0 	 doc2947              	 1
-15 	 0 	 doc10750             	 1
-15 	 0 	 doc7239              	 1
-15 	 0 	 doc14189             	 1
-15 	 0 	 doc19474             	 1
-15 	 0 	 doc14776             	 1
-15 	 0 	 doc21270             	 1
-15 	 0 	 doc6387              	 1
-15 	 0 	 doc12908             	 1
-15 	 0 	 doc9573              	 1
-15 	 0 	 doc17102             	 1
-15 	 0 	 doc21482             	 1
-15 	 0 	 doc6524              	 1
-15 	 0 	 doc18034             	 1
-15 	 0 	 doc1358              	 1
-15 	 0 	 doc13147             	 1
-15 	 0 	 doc17731             	 1
-15 	 0 	 doc12890             	 1
-15 	 0 	 doc20887             	 1
-15 	 0 	 doc19508             	 1
-15 	 0 	 doc18498             	 1
-15 	 0 	 doc20642             	 1
-15 	 0 	 doc19878             	 1
-15 	 0 	 doc6556              	 1
-15 	 0 	 doc10272             	 1
-15 	 0 	 doc5720              	 1
-15 	 0 	 doc17578             	 1
-15 	 0 	 doc17164             	 1
-
-
-# --- m==0: avg_precision and recall are hurt, by marking fake docs as relevant
-
-16 	 0 	 fakedoc1             	 1
-16 	 0 	 fakedoc2             	 1
-16 	 0 	 fakedoc3             	 1
-16 	 0 	 fakedoc4             	 1
-
-16 	 0 	 doc4043              	 1
-16 	 0 	 doc14985             	 1
-16 	 0 	 doc15370             	 1
-16 	 0 	 doc15426             	 1
-16 	 0 	 doc1702              	 1
-16 	 0 	 doc3062              	 1
-16 	 0 	 doc16134             	 1
-16 	 0 	 doc15037             	 1
-16 	 0 	 doc8224              	 1
-16 	 0 	 doc5044              	 1
-16 	 0 	 doc8545              	 1
-16 	 0 	 doc7228              	 1
-16 	 0 	 doc12686             	 1
-16 	 0 	 doc16609             	 1
-16 	 0 	 doc13161             	 1
-16 	 0 	 doc3446              	 1
-16 	 0 	 doc16493             	 1
-16 	 0 	 doc19297             	 1
-16 	 0 	 doc13619             	 1
-16 	 0 	 doc3281              	 1
-16 	 0 	 doc15499             	 1
-16 	 0 	 doc7373              	 1
-16 	 0 	 doc9064              	 1
-16 	 0 	 doc1710              	 1
-16 	 0 	 doc15411             	 1
-16 	 0 	 doc10890             	 1
-16 	 0 	 doc3166              	 1
-16 	 0 	 doc17894             	 1
-16 	 0 	 doc4560              	 1
-16 	 0 	 doc12766             	 1
-
-
-# --- m==1: precision_at_n and avg_precision are hurt, by unmarking relevant docs
-
-17 	 0 	 doc3117              	 0
-17 	 0 	 doc7477              	 0
-17 	 0 	 doc7569              	 0
-17 	 0 	 doc20667             	 0
-17 	 0 	 doc20260             	 1
-17 	 0 	 doc17355             	 1
-17 	 0 	 doc11021             	 1
-17 	 0 	 doc20934             	 1
-17 	 0 	 doc552               	 1
-17 	 0 	 doc20856             	 1
-17 	 0 	 doc3524              	 1
-17 	 0 	 doc17343             	 1
-17 	 0 	 doc21055             	 1
-17 	 0 	 doc19032             	 1
-17 	 0 	 doc19786             	 1
-17 	 0 	 doc9281              	 1
-17 	 0 	 doc1695              	 1
-17 	 0 	 doc15940             	 1
-17 	 0 	 doc9215              	 1
-17 	 0 	 doc8335              	 1
-17 	 0 	 doc20936             	 1
-17 	 0 	 doc6914              	 1
-17 	 0 	 doc12122             	 1
-17 	 0 	 doc6618              	 1
-17 	 0 	 doc5049              	 1
-17 	 0 	 doc450               	 1
-17 	 0 	 doc19206             	 1
-17 	 0 	 doc18823             	 1
-17 	 0 	 doc5307              	 1
-17 	 0 	 doc17295             	 1
-
-
-# ---- m==2: all precision, precision_at_n and recall are hurt.
-
-18 	 0 	 fakedoc1             	 1
-18 	 0 	 fakedoc2             	 1
-18 	 0 	 fakedoc3             	 1
-18 	 0 	 fakedoc4             	 1
-
-18 	 0 	 doc8064              	 0
-18 	 0 	 doc18142             	 0
-18 	 0 	 doc19383             	 0
-18 	 0 	 doc21151             	 0
-18 	 0 	 doc4665              	 1
-18 	 0 	 doc2897              	 1
-18 	 0 	 doc6878              	 1
-18 	 0 	 doc14507             	 1
-18 	 0 	 doc2976              	 1
-18 	 0 	 doc11757             	 1
-18 	 0 	 doc12625             	 1
-18 	 0 	 doc14908             	 1
-18 	 0 	 doc12790             	 1
-18 	 0 	 doc17915             	 1
-18 	 0 	 doc11804             	 1
-18 	 0 	 doc12935             	 1
-18 	 0 	 doc8225              	 1
-18 	 0 	 doc18011             	 1
-18 	 0 	 doc10493             	 1
-18 	 0 	 doc17922             	 1
-18 	 0 	 doc1902              	 1
-18 	 0 	 doc14049             	 1
-18 	 0 	 doc1334              	 1
-18 	 0 	 doc1168              	 1
-18 	 0 	 doc4859              	 1
-18 	 0 	 doc7124              	 1
-18 	 0 	 doc9692              	 1
-18 	 0 	 doc18402             	 1
-18 	 0 	 doc9089              	 1
-18 	 0 	 doc15375             	 1
-
-
-19 	 0 	 doc5267              	 1
-19 	 0 	 doc2310              	 1
-19 	 0 	 doc11435             	 1
-19 	 0 	 doc15666             	 1
-19 	 0 	 doc12733             	 1
-19 	 0 	 doc7925              	 1
-19 	 0 	 doc2444              	 1
-19 	 0 	 doc4900              	 1
-19 	 0 	 doc10803             	 1
-19 	 0 	 doc8869              	 1
-19 	 0 	 doc5051              	 1
-19 	 0 	 doc9163              	 1
-19 	 0 	 doc529               	 1
-19 	 0 	 doc19546             	 1
-19 	 0 	 doc18561             	 1
-19 	 0 	 doc10634             	 1
-19 	 0 	 doc3979              	 1
-19 	 0 	 doc8833              	 1
-19 	 0 	 doc7652              	 1
-19 	 0 	 doc4804              	 1
-19 	 0 	 doc12616             	 1
-19 	 0 	 doc8419              	 1
-19 	 0 	 doc9431              	 1
-19 	 0 	 doc16235             	 1
-19 	 0 	 doc732               	 1
-19 	 0 	 doc2515              	 1
-19 	 0 	 doc7194              	 1
-19 	 0 	 doc16301             	 1
-19 	 0 	 doc4494              	 1
-19 	 0 	 doc4496              	 1
diff --git a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/trecTopics.txt b/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/trecTopics.txt
deleted file mode 100755
index 2f3ada2..0000000
--- a/lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/trecTopics.txt
+++ /dev/null
@@ -1,287 +0,0 @@
-# -----------------------------------------------------------------------
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-# 
-#     http://www.apache.org/licenses/LICENSE-2.0
-# 
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# -----------------------------------------------------------------------
-
-# ------------------------------------------------------------
-# This file was created using utils.QualityQueriesFinder.
-# See also TrecQRels.txt.
-# ------------------------------------------------------------
-
-<top>
-<num> Number: 0
-
-<title> statement months  total 1987
-
-<desc> Description:
-Topic 0 Description Line 1
-Topic 0 Description Line 2
-
-<narr> Narrative:
-Topic 0 Narrative Line 1
-Topic 0 Narrative Line 2
-
-</top>
-
-<top>
-<num> Number: 1
-
-<title> agreed 15  against five
-
-<desc> Description:
-Topic 1 Description Line 1
-Topic 1 Description Line 2
-
-<narr> Narrative:
-Topic 1 Narrative Line 1
-Topic 1 Narrative Line 2
-
-</top>
-
-<top>
-<num> Number: 2
-
-<title> nine only  month international
-
-<desc> Description:
-
-
-<narr> Narrative:
-
-
-</top>
-
-<top>
-<num> Number: 3
-
-<title> finance any  10 government
-
-<desc> Description:
-
-
-<narr> Narrative:
-
-
-</top>
-
-<top>
-<num> Number: 4
-
-<title> issue next  years all
-
-<desc> Description:
-
-
-<narr> Narrative:
-
-
-</top>
-
-<top>
-<num> Number: 5
-
-<title> who major  ltd today
-
-<desc> Description:
-
-
-<narr> Narrative:
-
-
-</top>
-
-<top>
-<num> Number: 6
-
-<title> business revs  securities per
-
-<desc> Description:
-
-
-<narr> Narrative:
-
-
-</top>
-
-<top>
-<num> Number: 7
-
-<title> quarter time  note sales
-
-<desc> Description:
-
-
-<narr> Narrative:
-
-
-</top>
-
-<top>
-<num> Number: 8
-
-<title> february earlier  loss group
-
-<desc> Description:
-
-
-<narr> Narrative:
-
-
-</top>
-
-<top>
-<num> Number: 9
-
-<title> out end  made some
-
-<desc> Description:
-
-
-<narr> Narrative:
-
-
-</top>
-
-<top>
-<num> Number: 10
-
-<title> spokesman financial  30 expected
-
-<desc> Description:
-
-
-<narr> Narrative:
-
-
-</top>
-
-<top>
-<num> Number: 11
-
-<title> 1985 now  prices due
-
-<desc> Description:
-
-
-<narr> Narrative:
-
-
-</top>
-
-<top>
-<num> Number: 12
-
-<title> before board  record could
-
-<desc> Description:
-
-
-<narr> Narrative:
-
-
-</top>
-
-<top>
-<num> Number: 13
-
-<title> pay debt  because trade
-
-<desc> Description:
-
-
-<narr> Narrative:
-
-
-</top>
-
-<top>
-<num> Number: 14
-
-<title> meeting increase  four price
-
-<desc> Description:
-
-
-<narr> Narrative:
-
-
-</top>
-
-<top>
-<num> Number: 15
-
-<title> chairman rate  six interest
-
-<desc> Description:
-
-
-<narr> Narrative:
-
-
-</top>
-
-<top>
-<num> Number: 16
-
-<title> since current  between agreement
-
-<desc> Description:
-
-
-<narr> Narrative:
-
-
-</top>
-
-<top>
-<num> Number: 17
-
-<title> oil we  when president
-
-<desc> Description:
-
-
-<narr> Narrative:
-
-
-</top>
-
-<top>
-<num> Number: 18
-
-<title> capital through  foreign added
-
-<desc> Description:
-
-
-<narr> Narrative:
-
-
-</top>
-
-<top>
-<num> Number: 19
-
-<title> 20 while  common week
-
-<desc> Description:
-Topic 19 Description Line 1
-Topic 19 Description Line 2
-
-<narr> Narrative:
-Topic 19 Narrative Line 1
-Topic 19 Narrative Line 2
-
-</top>
diff --git a/modules/benchmark/.rsync-filter b/modules/benchmark/.rsync-filter
new file mode 100644
index 0000000..48f861a
--- /dev/null
+++ b/modules/benchmark/.rsync-filter
@@ -0,0 +1,2 @@
+- /work
+- /temp
diff --git a/modules/benchmark/CHANGES.txt b/modules/benchmark/CHANGES.txt
new file mode 100644
index 0000000..58f0f70
--- /dev/null
+++ b/modules/benchmark/CHANGES.txt
@@ -0,0 +1,359 @@
+Lucene Benchmark Contrib Change Log
+
+The Benchmark contrib package contains code for benchmarking Lucene in a variety of ways.
+
+10/10/2010
+  The locally built patched version of the Xerces-J jar introduced
+  as part of LUCENE-1591 is no longer required, because Xerces
+  2.10.0, which contains a fix for XERCESJ-1257 (see
+  http://svn.apache.org/viewvc?view=revision&revision=554069),
+  was released earlier this year.  Upgraded
+  xerces-2.9.1-patched-XERCESJ-1257.jar and xml-apis-2.9.0.jar
+  to xercesImpl-2.10.0.jar and xml-apis-2.10.0.jar. (Steven Rowe)
+
+8/2/2010
+  LUCENE-2582: You can now specify the default codec to use for
+  writing new segments by adding default.codec = Pulsing (for
+  example), in the alg file.  (Mike McCandless)
+
+4/27/2010: WriteLineDocTask now supports multi-threading. Also, 
+  StringBufferReader was renamed to StringBuilderReader and works on 
+  StringBuilder now. In addition, LongToEnglishCountentSource starts from 0
+  (instead of Long.MIN_VAL+10) and wraps around to MIN_VAL (if you ever hit 
+  Long.MAX_VAL). (Shai Erera)
+
+4/07/2010
+  LUCENE-2377: Enable the use of NoMergePolicy and NoMergeScheduler by 
+  CreateIndexTask. (Shai Erera)
+  
+3/28/2010
+  LUCENE-2353: Fixed bug in Config where Windows absolute path property values 
+  were incorrectly handled (Shai Erera)
+  
+3/24/2010
+  LUCENE-2343: Added support for benchmarking collectors. (Grant Ingersoll, Shai Erera)
+
+2/21/2010
+  LUCENE-2254: Add support to the quality package for running
+  experiments with any combination of Title, Description, and Narrative.
+  (Robert Muir)
+
+1/28/2010
+  LUCENE-2223: Add a benchmark for ShingleFilter. You can wrap any
+  analyzer with ShingleAnalyzerWrapper and specify shingle parameters
+  with the NewShingleAnalyzer task.  (Steven Rowe via Robert Muir)
+
+1/14/2010
+  LUCENE-2210: TrecTopicsReader now properly reads descriptions and
+  narratives from trec topics files.  (Robert Muir)
+
+1/11/2010
+  LUCENE-2181: Add a benchmark for collation. This adds NewLocaleTask,
+  which sets a Locale in the run data for collation to use, and can be
+  used in the future for benchmarking localized range queries and sorts.
+  Also add NewCollationAnalyzerTask, which works with both JDK and ICU
+  Collator implementations. Fix ReadTokensTask to not tokenize fields
+  unless they should be tokenized according to DocMaker config. The 
+  easiest way to run the benchmark is to run 'ant collation'
+  (Steven Rowe via Robert Muir)
+
+12/22/2009
+  LUCENE-2178: Allow multiple locations to add to the class path with
+  -Dbenchmark.ext.classpath=... when running "ant run-task" (Steven
+  Rowe via Mike McCandless)
+
+12/17/2009
+  LUCENE-2168: Allow negative relative thread priority for BG tasks
+  (Mike McCandless)
+
+12/07/2009
+  LUCENE-2106: ReadTask does not close its Reader when 
+  OpenReader/CloseReader are not used. (Mark Miller)
+
+11/17/2009
+  LUCENE-2079: Allow specifying delta thread priority after the "&";
+  added log.time.step.msec to print per-time-period counts; fixed
+  NearRealTimeTask to print reopen times (in msec) of each reopen, at
+  the end.  (Mike McCandless)
+
+11/13/2009
+  LUCENE-2050: Added ability to run tasks within a serial sequence in
+  the background, by appending "&".  The tasks are stopped & joined at
+  the end of the sequence.  Also added Wait and RollbackIndex tasks.
+  Genericized NearRealTimeReaderTask to only reopen the reader
+  (previously it spawned its own thread, and also did searching).
+  Also changed the API of PerfRunData.getIndexReader: it now returns a
+  reference, and it's your job to decRef the reader when you're done
+  using it.  (Mike McCandless)
+
+11/12/2009
+  LUCENE-2059: allow TrecContentSource not to change the docname.
+  Previously, it would always append the iteration # to the docname.
+  With the new option content.source.excludeIteration, you can disable this.
+  The resulting index can then be used with the quality package to measure
+  relevance. (Robert Muir)
+  
+11/12/2009
+  LUCENE-2058: specify trec_eval submission output from the command line.
+  Previously, 4 arguments were required, but the third was unused. The 
+  third argument is now the desired location of submission.txt  (Robert Muir)
+
+11/08/2009
+  LUCENE-2044: Added delete.percent.rand.seed to seed the Random instance
+  used by DeleteByPercentTask.  (Mike McCandless)
+
+11/07/2009
+  LUCENE-2043: Fix CommitIndexTask to also commit pending IndexReader
+  changes (Mike McCandless)
+
+11/07/2009
+  LUCENE-2042: Added print.hits.field, to print each hit from the
+  Search* tasks.  (Mike McCandless)
+
+11/04/2009
+  LUCENE-2029: Added doc.body.stored and doc.body.tokenized; each
+  falls back to the non-body variant as its default.  (Mike McCandless)
+
+10/28/2009
+  LUCENE-1994: Fix thread safety of EnwikiContentSource and DocMaker
+  when doc.reuse.fields is false.  Also made docs.reuse.fields=true
+  thread safe.  (Mark Miller, Shai Erera, Mike McCandless)
+
+8/4/2009
+  LUCENE-1770: Add EnwikiQueryMaker (Mark Miller)
+
+8/04/2009
+  LUCENE-1773: Add FastVectorHighlighter tasks.  This change is a
+  non-backwards compatible change in how subclasses of ReadTask define
+  a highlighter.  The methods doHighlight, isMergeContiguousFragments,
+  maxNumFragments and getHighlighter are no longer used and have been
+  mark deprecated and package protected private so there's a compile
+  time error.  Instead, the new getBenchmarkHighlighter method should
+  return an appropriate highlighter for the task. The configuration of
+  the highlighter tasks (maxFrags, mergeContiguous, etc.) is now
+  accepted as params to the task.  (Koji Sekiguchi via Mike McCandless)
+
+8/03/2009
+  LUCENE-1778: Add support for log.step setting per task type. Perviously, if
+  you included a log.step line in the .alg file, it had been applied to all
+  tasks. Now, you can include a log.step.AddDoc, or log.step.DeleteDoc (for 
+  example) to control logging for just these tasks. If you want to ommit logging
+  for any other task, include log.step=-1. The syntax is "log.step." together
+  with the Task's 'short' name (i.e., without the 'Task' part).
+  (Shai Erera via Mark Miller)
+
+7/24/2009
+  LUCENE-1595: Deprecate LineDocMaker and EnwikiDocMaker in favor of
+  using DocMaker directly, with content.source = LineDocSource or
+  EnwikiContentSource.  NOTE: with this change, the "id" field from
+  the Wikipedia XML export is now indexed as the "docname" field
+  (previously it was indexed as "docid").  Additionaly, the
+  SearchWithSort task now accepts all types that SortField can accept
+  and no longer falls back to SortField.AUTO, which has been
+  deprecated. (Mike McCandless)
+
+7/20/2009
+  LUCENE-1755: Fix WriteLineDocTask to output a document if it contains either 
+  a title or body (or both).  (Shai Erera via Mark Miller)
+
+7/14/2009
+  LUCENE-1725: Fix the example Sort algorithm - auto is now deprecated and no longer works
+  with Benchmark. Benchmark will now throw an exception if you specify sort fields without
+  a type. The example sort algorithm is now typed.  (Mark Miller)
+
+7/6/2009
+  LUCENE-1730: Fix TrecContentSource to use ISO-8859-1 when reading the TREC files, 
+  unless a different encoding is specified. Additionally, ContentSource now supports 
+  a content.source.encoding parameter in the configuration file. 
+  (Shai Erera via Mark Miller)
+
+6/26/2009
+  LUCENE-1716: Added the following support: 
+  doc.tokenized.norms: specifies whether to store norms
+  doc.body.tokenized.norms: special attribute for the body field
+  doc.index.props: specifies whether DocMaker should index the properties set on
+  DocData
+  writer.info.stream: specifies the info stream to set on IndexWriter (supported
+  values are: SystemOut, SystemErr and a file name). (Shai Erera via Mike McCandless)
+  
+6/23/09
+  LUCENE-1714: WriteLineDocTask incorrectly  normalized text, by replacing only 
+  occurrences of "\t" with a space. It now replaces "\r\n" in addition to that, 
+  so that LineDocMaker won't fail. (Shai Erera via Michael McCandless)
+  
+6/17/09 
+  LUCENE-1595: This issue breaks previous external algorithms. DocMaker has been 
+  replaced with a concrete class which accepts a ContentSource for iterating over 
+  a content source's documents. Most of the old DocMakers were changed to a 
+  ContentSource implementation, and DocMaker is now a default document creation impl
+  that provides an easy way for reusing fields. When [doc.maker] is not defined in 
+  an algorithm, the new DocMaker is the default. If you have .alg files which 
+  specify a DocMaker (like ReutersDocMaker), you should change the [doc.maker] line to: 
+  [content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource]
+  
+  i.e.
+  doc.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersDocMaker
+  becomes
+  content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+  
+  doc.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleDocMaker
+  becomes
+  content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
+ 	
+  Also, PerfTask now logs a message in tearDown() rather than each Task doing its
+  own logging. A new setting called [log.step] is consulted to determine how often 
+  to log. [doc.add.log.step] is no longer a valid setting. For easy migration of 
+  current .alg files, rename [doc.add.log.step] to [log.step] and [doc.delete.log.step] 
+  to [delete.log.step]. 
+  
+  Additionally, [doc.maker.forever] should be changed to [content.source.forever].
+  (Shai Erera via Mark Miller)
+
+6/12/09 
+  LUCENE-1539: Added DeleteByPercentTask which enables deleting a
+  percentage of documents and searching on them.  Changed CommitIndex
+  to optionally accept a label (recorded as userData=<label> in the
+  commit point).  Added FlushReaderTask, and modified OpenReaderTask
+  to also optionally take a label referencing a commit point to open.
+  Also changed default autoCommit (when IndexWriter is opened) to
+  true. (Jason Rutherglen via Mike McCandless)
+
+12/20/08
+  LUCENE-1495: Allow task sequence to run for specfied number of seconds by adding ": 2.7s" (for example).
+
+12/16/08
+  LUCENE-1493: Stop using deprecated Hits API for searching; add new
+  param search.num.hits to set top N docs to collect.
+
+12/16/08
+  LUCENE-1492: Added optional readOnly param (default true) to OpenReader task.
+
+9/9/08
+ LUCENE-1243: Added new sorting benchmark capabilities.  Also Reopen and commit tasks.  (Mark Miller via Grant Ingersoll)
+
+5/10/08
+  LUCENE-1090: remove relative paths assumptions from benchmark code.
+  Only build.xml was modified: work-dir definition must remain so  
+  benchmark tests can run from both trunk-home and benchmark-home.  
+  
+3/9/08
+  LUCENE-1209: Fixed DocMaker settings by round. Prior to this fix, DocMaker settings of 
+  first round were used in all rounds.  (E.g. term vectors.)
+  (Mark Miller via Doron Cohen) 
+
+1/30/08
+  LUCENE-1156: Fixed redirect problem in EnwikiDocMaker.  Refactored ExtractWikipedia to use EnwikiDocMaker.  Added property to EnwikiDocMaker to allow
+  for skipping image only documents.
+
+1/24/2008
+  LUCENE-1136: add ability to not count sub-task doLogic increment
+  
+1/23/2008
+  LUCENE-1129: ReadTask properly uses the traversalSize value
+  LUCENE-1128: Added support for benchmarking the highlighter
+
+01/20/08
+  LUCENE-1139: various fixes
+  - add merge.scheduler, merge.policy config properties
+  - refactor Open/CreateIndexTask to share setting config on IndexWriter
+  - added doc.reuse.fields=true|false for LineDocMaker
+  - OptimizeTask now takes int param to call optimize(int maxNumSegments)
+  - CloseIndexTask now takes bool param to call close(false) (abort running merges)
+
+
+01/03/08
+  LUCENE-1116: quality package improvements:
+  - add MRR computation; 
+  - allow control of max #queries to run;
+  - verify log & report are flushed.
+  - add TREC query reader for the 1MQ track.  
+      
+12/31/07
+  LUCENE-1102: EnwikiDocMaker now indexes the docid field, so results might not be comparable with results prior to this change, although
+  it is doubted that this one small field makes much difference.
+  
+12/13/07
+  LUCENE-1086: DocMakers setup for the "docs.dir" property
+  fixed to properly handle absolute paths. (Shai Erera via Doron Cohen)
+  
+9/18/07
+  LUCENE-941: infinite loop for alg: {[AddDoc(4000)]: 4} : *
+  ResetInputsTask fixed to work also after exhaustion.
+  All Reset Tasks now subclas ResetInputsTask.
+
+8/9/07
+  LUCENE-971: Change enwiki tasks to a doc maker (extending
+  LineDocMaker) that directly processes the Wikipedia XML and produces
+  documents.  Intermediate files (one per document) are no longer
+  created.
+
+8/1/07
+  LUCENE-967: Add "ReadTokensTask" to allow for benchmarking just tokenization.
+
+7/27/07
+  LUCENE-836: Add support for search quality benchmarking, running 
+  a set of queries against a searcher, and, optionally produce a submission
+  report, and, if query judgements are available, compute quality measures:
+  recall, precision_at_N, average_precision, MAP. TREC specific Judge (based 
+  on TREC QRels) and TREC Topics reader are included in o.a.l.benchmark.quality.trec
+  but any other format of queries and judgements can be implemented and used.
+  
+7/24/07
+  LUCENE-947: Add support for creating and index "one document per
+  line" from a large text file, which reduces per-document overhead of
+  opening a single file for each document.
+
+6/30/07
+  LUCENE-848: Added support for Wikipedia benchmarking.
+
+6/25/07
+- LUCENE-940: Multi-threaded issues fixed: SimpleDateFormat; logging for addDoc/deleteDoc tasks.
+- LUCENE-945: tests fail to find data dirs. Added sys-prop benchmark.work.dir and cfg-prop work.dir.
+(Doron Cohen)
+
+4/17/07
+- LUCENE-863: Deprecated StandardBenchmarker in favour of byTask code.
+  (Otis Gospodnetic)
+
+4/13/07
+
+Better error handling and javadocs around "exhaustive" doc making.
+
+3/25/07
+
+LUCENE-849: 
+1. which HTML Parser is used is configurable with html.parser property.
+2. External classes added to classpath with -Dbenchmark.ext.classpath=path.
+3. '*' as repeating number now means "exhaust doc maker - no repetitions".
+
+3/22/07
+
+-Moved withRetrieve() call out of the loop in ReadTask
+-Added SearchTravRetLoadFieldSelectorTask to help benchmark some of the FieldSelector capabilities
+-Added options to store content bytes on the Reuters Doc (and others, but Reuters is the only one w/ it enabled)
+
+3/21/07
+
+Tests (for benchmarking code correctness) were added - LUCENE-840.
+To be invoked by "ant test" from contrib/benchmark. (Doron Cohen)
+
+3/19/07
+
+1. Introduced an AbstractQueryMaker to hold common QueryMaker code. (GSI)
+2. Added traversalSize parameter to SearchTravRetTask and SearchTravTask.  Changed SearchTravRetTask to extend SearchTravTask. (GSI)
+3. Added FileBasedQueryMaker to run queries from a File or resource. (GSI)
+4. Modified query-maker generation for read related tasks to make further read tasks addition simpler and safer. (DC)
+5. Changed Taks' setParams() to throw UnsupportedOperationException if that task does not suppot command line param. (DC)
+6. Improved javadoc to specify all properties command line params currently supported. (DC)
+7. Refactored ReportTasks so that it is easy/possible now to create new report tasks. (DC)
+
+01/09/07
+
+1. Committed Doron Cohen's benchmarking contribution, which provides an easily expandable task based approach to benchmarking.  See the javadocs for information. (Doron Cohen via Grant Ingersoll)
+
+2. Added this file.
+
+3. 2/11/07: LUCENE-790 and 788:  Fixed Locale issue with date formatter. Fixed some minor issues with benchmarking by task.  Added a dependency
+ on the Lucene demo to the build classpath.  (Doron Cohen, Grant Ingersoll)
+
+4. 2/13/07: LUCENE-801: build.xml now builds Lucene core and Demo first and has classpath dependencies on the output of that build.  (Doron Cohen, Grant Ingersoll)
diff --git a/modules/benchmark/README.enwiki b/modules/benchmark/README.enwiki
new file mode 100644
index 0000000..f9d4930
--- /dev/null
+++ b/modules/benchmark/README.enwiki
@@ -0,0 +1,22 @@
+Support exists for downloading, parsing, and loading the English
+version of wikipedia (enwiki).
+
+The build file can automatically try to download the most current
+enwiki dataset (pages-articles.xml.bz2) from the "latest" directory,
+http://download.wikimedia.org/enwiki/latest/. However, this file
+doesn't always exist, depending on where wikipedia is in the dump
+process and whether prior dumps have succeeded. If this file doesn't
+exist, you can sometimes find an older or in progress version by
+looking in the dated directories under
+http://download.wikimedia.org/enwiki/. For example, as of this
+writing, there is a page file in
+http://download.wikimedia.org/enwiki/20070402/. You can download this
+file manually and put it in temp. Note that the file you download will
+probably have the date in the name, e.g.,
+http://download.wikimedia.org/enwiki/20070402/enwiki-20070402-pages-articles.xml.bz2. When
+you put it in temp, rename it to enwiki-latest-pages-articles.xml.bz2.
+
+After that, ant enwiki should process the data set and run a load
+test. Ant targets get-enwiki, expand-enwiki, and extract-enwiki can
+also be used to download, decompress, and extract (to individual files
+in work/enwiki) the dataset, respectively.
diff --git a/modules/benchmark/build.xml b/modules/benchmark/build.xml
new file mode 100644
index 0000000..3bccf56
--- /dev/null
+++ b/modules/benchmark/build.xml
@@ -0,0 +1,264 @@
+<?xml version="1.0"?>
+<project name="benchmark" default="default">
+
+    <description>
+        Lucene Benchmarking Contributions
+    </description>
+
+    <property name="build.dir" location="build/" />
+    <property name="dist.dir" location="dist/" />
+    <property name="maven.dist.dir" location="dist/maven" />
+
+    <import file="../../lucene/contrib/contrib-build.xml"/>
+    <property name="working.dir" location="work"/>
+
+    <!-- the tests have some parallel problems -->
+    <property name="tests.threadspercpu" value="0"/>
+
+    <contrib-uptodate name="highlighter" property="highlighter.uptodate" classpath.property="highlighter.jar"/>
+    <module-uptodate name="analysis/icu" jarfile="${common.dir}/../modules/analysis/build/icu/lucene-analyzers-icu-${version}.jar"
+      property="analyzers-icu.uptodate" classpath.property="analyzers-icu.jar"/>
+    <!-- analyzers common needs a hack for the jar file: -->
+    <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
+      property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+    <contrib-uptodate name="memory" property="memory.uptodate" classpath.property="memory.jar"/>
+    <contrib-uptodate name="demo" property="demo.uptodate" classpath.property="demo.jar"/>
+
+    <target name="check-files">
+        <available file="temp/news20.tar.gz" property="news20.exists"/>
+
+        <available file="${working.dir}/20_newsgroup" property="news20.expanded"/>
+
+        <available file="temp/reuters21578.tar.gz" property="reuters.exists"/>
+        <available file="${working.dir}/reuters" property="reuters.expanded"/>
+        <available file="${working.dir}/reuters-out" property="reuters.extracted"/>
+        <available file="temp/20news-18828.tar.gz" property="20news-18828.exists"/>
+        <available file="${working.dir}/20news-18828" property="20news-18828.expanded"/>
+        <available file="${working.dir}/mini_newsgroups" property="mini.expanded"/>
+        
+        <available file="temp/enwiki-20070527-pages-articles.xml.bz2" property="enwiki.exists"/>
+        <available file="temp/enwiki-20070527-pages-articles.xml" property="enwiki.expanded"/>
+        <available file="${working.dir}/enwiki.txt" property="enwiki.extracted"/>
+    	<available file="temp/${top.100k.words.archive.filename}"
+                   property="top.100k.words.archive.present"/>
+    	<available file="${working.dir}/top100k-out" 
+                   property="top.100k.word.files.expanded"/>
+    </target>
+
+    <target name="enwiki-files" depends="check-files">
+        <mkdir dir="temp"/>
+        <antcall target="get-enwiki"/>
+        <antcall target="expand-enwiki"/>
+    </target>
+
+    <target name="get-enwiki" unless="enwiki.exists">
+        <get src="http://people.apache.org/~gsingers/wikipedia/enwiki-20070527-pages-articles.xml.bz2"
+             dest="temp/enwiki-20070527-pages-articles.xml.bz2"/>
+    </target>
+
+    <target name="expand-enwiki"  unless="enwiki.expanded">
+        <bunzip2 src="temp/enwiki-20070527-pages-articles.xml.bz2" dest="temp"/>
+    </target>
+
+    <target name="get-news-20" unless="20news-18828.exists">
+        <get src="http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz"
+             dest="temp/news20.tar.gz"/>
+
+    </target>
+    <target name="get-reuters" unless="reuters.exists">
+
+        <get src="http://www.daviddlewis.com/resources/testcollections/reuters21578/reuters21578.tar.gz"
+            dest="temp/reuters21578.tar.gz"/>
+    </target>
+
+    <target name="expand-news-20"  unless="news20.expanded">
+        <gunzip src="temp/news20.tar.gz" dest="temp"/>
+        <untar src="temp/news20.tar" dest="${working.dir}"/>
+    </target>
+    <target name="expand-reuters" unless="reuters.expanded">
+        <gunzip src="temp/reuters21578.tar.gz" dest="temp"/>
+        <mkdir dir="${working.dir}/reuters"/>
+        <untar src="temp/reuters21578.tar" dest="${working.dir}/reuters"/>
+        <delete >
+            <fileset dir="${working.dir}/reuters">
+                <include name="*.txt"/>
+            </fileset>
+        </delete>
+
+    </target>
+    <target name="extract-reuters" depends="check-files" unless="reuters.extracted">
+        <mkdir dir="${working.dir}/reuters-out"/>
+        <java classname="org.apache.lucene.benchmark.utils.ExtractReuters" maxmemory="1024M" fork="true">
+            <classpath refid="run.classpath"/>
+            <arg file="${working.dir}/reuters"/>
+            <arg file="${working.dir}/reuters-out"/>
+        </java>
+    </target>
+    <target name="get-20news-18828" unless="20news-18828.exists">
+        <get src="http://people.csail.mit.edu/u/j/jrennie/public_html/20Newsgroups/20news-18828.tar.gz"
+             dest="temp/20news-18828.tar.gz"/>
+
+    </target>
+    <target name="expand-20news-18828" unless="20news-18828.expanded">
+        <gunzip src="temp/20news-18828.tar.gz" dest="temp"/>
+        <untar src="temp/20news-18828.tar" dest="${working.dir}"/>
+    </target>
+    <target name="get-mini-news" unless="mini.exists">
+        <get src="http://kdd.ics.uci.edu/databases/20newsgroups/mini_newsgroups.tar.gz"
+             dest="temp/mini_newsgroups.tar.gz"/>
+    </target>
+    <target name="expand-mini-news" unless="mini.expanded">
+        <gunzip src="temp/mini_newsgroups.tar.gz" dest="temp"/>
+        <untar src="temp/mini_newsgroups.tar" dest="${working.dir}"/>
+    </target>
+
+	<property name="top.100k.words.archive.filename" 
+	          value="top.100k.words.de.en.fr.uk.wikipedia.2009-11.tar.bz2"/>
+	<property name="top.100k.words.archive.base.url"
+	          value="http://people.apache.org/~rmuir/wikipedia"/>
+	<target name="get-top-100k-words-archive" unless="top.100k.words.archive.present">
+		<mkdir dir="temp"/>
+	    <get src="${top.100k.words.archive.base.url}/${top.100k.words.archive.filename}"
+	         dest="temp/${top.100k.words.archive.filename}"/>
+	</target>
+	<target name="expand-top-100k-word-files" unless="top.100k.word.files.expanded">
+		<mkdir dir="${working.dir}/top100k-out"/>
+	    <untar src="temp/${top.100k.words.archive.filename}"
+	           overwrite="true" compression="bzip2" dest="${working.dir}/top100k-out"/>
+	</target>
+	
+	<target name="top-100k-wiki-word-files" depends="check-files">
+	  <mkdir dir="${working.dir}"/>
+	  <antcall target="get-top-100k-words-archive"/>
+	  <antcall target="expand-top-100k-word-files"/>
+	</target>
+	
+    <target name="get-files" depends="check-files">
+        <mkdir dir="temp"/>
+        <antcall target="get-reuters"/>
+        <antcall target="expand-reuters"/>
+        <antcall target="extract-reuters"/>
+    </target>
+
+    <path id="classpath">
+      <pathelement path="${memory.jar}"/>
+      <pathelement path="${highlighter.jar}"/>
+      <pathelement path="${analyzers-common.jar}"/>
+      <pathelement path="${demo.jar}"/>
+      <path refid="base.classpath"/>
+    	<fileset dir="lib">
+    		<include name="**/*.jar"/>
+    	</fileset>
+    </path>
+    <path id="run.classpath">
+        <path refid="classpath"/>
+        <pathelement location="${build.dir}/classes/java"/>
+        <pathelement path="${benchmark.ext.classpath}"/>
+    </path>
+
+    <property name="task.alg" location="conf/micro-standard.alg"/>
+    <property name="task.mem" value="140M"/>
+
+    <target name="run-task" depends="compile,check-files,get-files" 
+     description="Run compound penalty perf test (optional: -Dtask.alg=your-algorithm-file -Dtask.mem=java-max-mem)">
+        <echo>Working Directory: ${working.dir}</echo>
+        <java classname="org.apache.lucene.benchmark.byTask.Benchmark" maxmemory="${task.mem}" fork="true">
+            <classpath refid="run.classpath"/>
+            <arg file="${task.alg}"/>
+        </java>
+    </target>
+
+    <target name="enwiki" depends="compile,check-files,enwiki-files">
+        <echo>Working Directory: ${working.dir}</echo>
+        <java classname="org.apache.lucene.benchmark.byTask.Benchmark" maxmemory="1024M" fork="true">
+            <assertions>
+              <enable/>
+            </assertions>
+            <classpath refid="run.classpath"/>
+            <arg file="conf/extractWikipedia.alg"/>
+        </java>
+    </target>
+
+	<property name="collation.alg.file" location="conf/collation.alg"/>
+	<property name="collation.output.file" 
+	          value="${working.dir}/collation.benchmark.output.txt"/>
+	<property name="collation.jira.output.file" 
+	          value="${working.dir}/collation.bm2jira.output.txt"/>
+	
+	<path id="collation.runtime.classpath">
+	  <path refid="run.classpath"/>
+    <pathelement path="${analyzers-icu.jar}"/>
+    <fileset dir="${common.dir}/../modules/analysis/icu/lib" includes="icu4j*.jar"/>
+	</path>
+	
+	<target name="collation" depends="compile,compile-analyzers-icu,top-100k-wiki-word-files">
+	    <echo>Running contrib/benchmark with alg file: ${collation.alg.file}</echo>
+	    <java fork="true" classname="org.apache.lucene.benchmark.byTask.Benchmark" 
+	          maxmemory="${task.mem}" output="${collation.output.file}">
+	      <classpath refid="collation.runtime.classpath"/>
+	      <arg file="${collation.alg.file}"/>
+	    </java>
+	    <echo>Benchmark output is in file: ${collation.output.file}</echo>
+	    <echo>Converting to JIRA table format...</echo>
+	    <exec executable="perl" output="${collation.jira.output.file}" failonerror="true">
+	      <arg value="scripts/collation.bm2jira.pl"/>
+	      <arg value="${collation.output.file}"/>
+	    </exec>
+	    <echo>Benchmark output in JIRA table format is in file: ${collation.jira.output.file}</echo>
+	</target>
+	
+    <property name="shingle.alg.file" location="conf/shingle.alg"/>
+    <property name="shingle.output.file" 
+              value="${working.dir}/shingle.benchmark.output.txt"/>
+    <property name="shingle.jira.output.file" 
+              value="${working.dir}/shingle.bm2jira.output.txt"/>
+	
+    <path id="shingle.runtime.classpath">
+      <path refid="run.classpath"/>
+    </path>
+	
+    <target name="shingle" depends="compile,get-files">
+      <echo>Running contrib/benchmark with alg file: ${shingle.alg.file}</echo>
+      <java fork="true" classname="org.apache.lucene.benchmark.byTask.Benchmark" 
+            maxmemory="${task.mem}" output="${shingle.output.file}">
+        <classpath refid="run.classpath"/>
+        <arg file="${shingle.alg.file}"/>
+      </java>
+      <echo>Benchmark output is in file: ${shingle.output.file}</echo>
+      <echo>Converting to JIRA table format...</echo>
+      <exec executable="perl" output="${shingle.jira.output.file}" failonerror="true">
+        <arg value="scripts/shingle.bm2jira.pl"/>
+        <arg value="${shingle.output.file}"/>
+      </exec>
+      <echo>Benchmark output in JIRA table format is in file: ${shingle.jira.output.file}</echo>
+    </target>
+
+    <target name="compile-demo" unless="demo.uptodate">
+      <subant target="default">
+         <fileset dir="${common.dir}/contrib/demo" includes="build.xml"/>
+      </subant>
+    </target>
+    <target name="compile-highlighter" unless="highlighter.uptodate">
+      <subant target="default">
+         <fileset dir="${common.dir}/contrib/highlighter" includes="build.xml"/>
+      </subant>
+    </target>
+    <target name="compile-analyzers-icu" unless="analyzers-icu.uptodate">
+      <subant target="default">
+         <fileset dir="${common.dir}/../modules/analysis/icu" includes="build.xml"/>
+      </subant>
+    </target>
+    <target name="compile-analyzers-common" unless="analyzers-common.uptodate">
+      <subant target="default">
+        <fileset dir="${common.dir}/../modules/analysis/common" includes="build.xml"/>
+      </subant>
+    </target>
+    <target name="compile-memory" unless="memory.uptodate">
+      <subant target="default">
+         <fileset dir="${common.dir}/contrib/memory" includes="build.xml"/>
+      </subant>
+    </target>
+
+    <target name="init" depends="contrib-build.init,compile-demo,compile-memory,compile-highlighter,compile-analyzers-common"/>
+    
+</project>
diff --git a/modules/benchmark/conf/analyzer.alg b/modules/benchmark/conf/analyzer.alg
new file mode 100644
index 0000000..1a1ec4c
--- /dev/null
+++ b/modules/benchmark/conf/analyzer.alg
@@ -0,0 +1,79 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+merge.factor=mrg:10
+#:100:10:100
+max.buffered=buf:10
+#:10:100:100
+compound=true
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+#directory=RamDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=500
+
+docs.dir=reuters-out
+#docs.dir=reuters-111
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+
+    ResetSystemErase
+#If the analyzer is in o.a.l.analysis, then just the classname can be used, otherwise the FQN must be used
+#Standard Analyzer can be shortened to standard.StandardAnalyzer
+    {"NewAnalyzer" NewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer) > 
+    { "Populate"
+        CreateIndex
+        { "MAddDocs" AddDoc > : 2000
+        Optimize
+        CloseIndex
+    }
+
+    OpenReader  
+    { "SearchSameRdr" Search > : 5000
+    CloseReader 
+                
+    { "WarmNewRdr" Warm > : 50
+                
+    { "SrchNewRdr" Search > : 500
+                
+    { "SrchTrvNewRdr" SearchTrav(1000) > : 300
+                
+    { "SrchTrvRetNewRdr" SearchTravRet(2000) > : 100
+                
+    NewRound
+
+} : 4
+
+RepSumByName
+RepSumByPrefRound MAddDocs
diff --git a/modules/benchmark/conf/basicNRT.alg b/modules/benchmark/conf/basicNRT.alg
new file mode 100644
index 0000000..259b613
--- /dev/null
+++ b/modules/benchmark/conf/basicNRT.alg
@@ -0,0 +1,80 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+#
+# based on micro-standard
+#
+# modified to use wikipedia sources and index entire docs
+# currently just used to measure ingest rate
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+
+work.dir = /x/lucene/wiki.5M
+
+doc.stored=true
+doc.body.stored=false
+doc.tokenized=false
+doc.body.tokenized=true
+doc.term.vector=false
+log.step.AddDoc = 10000
+log.step.Search = 10000
+compound = false
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
+content.source.forever = false
+file.query.maker.file = queries.txt
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker
+docs.file = /x/lucene/enwiki-20090306-lines-1k-fixed.txt
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+
+# -------------------------------------------------------------------------------------
+
+# Open a writer
+OpenIndex
+{
+  # Get a new near-real-time reader, once per second:
+  NearRealtimeReader(1.0) &
+
+  # Warm
+  Search
+
+  # Index with 2 threads, each adding 100 docs per sec
+  [ "Indexing" { AddDoc > : * : 100/sec ] : 2 &
+
+  # Redline search (from queries.txt) with 4 threads
+  [ "Searching" { Search > : * ] : 4 &
+
+  # Wait 60 sec, then wrap up
+  Wait(5.0)
+}
+CloseReader
+
+# Don't keep any changes, so we can re-test on the same index again
+RollbackIndex
+
+RepSumByPref Indexing
+RepSumByPref Searching
+RepSumByPref NearRealtimeReader
+
+
diff --git a/modules/benchmark/conf/collation.alg b/modules/benchmark/conf/collation.alg
new file mode 100644
index 0000000..798befc
--- /dev/null
+++ b/modules/benchmark/conf/collation.alg
@@ -0,0 +1,97 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+# 
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
+content.source.encoding=UTF-8
+doc.tokenized=false
+doc.body.tokenized=true
+docs.file=work/top100k-out/top.fr.wikipedia.words.txt
+content.source.forever=false
+log.step=100000
+
+{ "Rounds"
+    -NewAnalyzer(KeywordAnalyzer)
+    -SetProp(docs.file,work/top100k-out/top.fr.wikipedia.words.txt)
+    ResetInputs
+    { "FrenchKeyword" { ReadTokens > : * ResetInputs } : 10
+
+    -NewAnalyzer(KeywordAnalyzer)
+    -SetProp(docs.file,work/top100k-out/top.de.wikipedia.words.txt)
+    ResetInputs
+    { "GermanKeyword" { ReadTokens > : * ResetInputs } : 10
+
+    -NewAnalyzer(KeywordAnalyzer)
+    -SetProp(docs.file,work/top100k-out/top.uk.wikipedia.words.txt)
+    ResetInputs
+    { "UkrainianKeyword" { ReadTokens > : * ResetInputs } : 10
+ 
+    -NewAnalyzer(KeywordAnalyzer)
+    -SetProp(docs.file,work/top100k-out/top.en.wikipedia.words.txt)
+    ResetInputs
+    { "EnglishKeyword" { ReadTokens > : * ResetInputs } : 10
+ 
+    -NewLocale(fr)
+    -NewCollationAnalyzer
+    -SetProp(docs.file,work/top100k-out/top.fr.wikipedia.words.txt)
+    ResetInputs
+    { "FrenchJDK" { ReadTokens > : * ResetInputs } : 10
+
+    -NewLocale(de)
+    -NewCollationAnalyzer
+    -SetProp(docs.file,work/top100k-out/top.de.wikipedia.words.txt)
+    ResetInputs
+    { "GermanJDK" { ReadTokens > : * ResetInputs } : 10
+
+    -NewLocale(uk)
+    -NewCollationAnalyzer
+    -SetProp(docs.file,work/top100k-out/top.uk.wikipedia.words.txt)
+    ResetInputs
+    { "UkrainianJDK" { ReadTokens > : * ResetInputs } : 10
+
+    -NewLocale(en)
+    -NewCollationAnalyzer
+    -SetProp(docs.file,work/top100k-out/top.en.wikipedia.words.txt)
+    ResetInputs
+    { "EnglishJDK" { ReadTokens > : * ResetInputs } : 10
+
+    -NewLocale(fr)
+    -NewCollationAnalyzer(impl:icu)
+    -SetProp(docs.file,work/top100k-out/top.fr.wikipedia.words.txt)
+    ResetInputs
+    { "FrenchICU" { ReadTokens > : * ResetInputs } : 10
+
+    -NewLocale(de)
+    -NewCollationAnalyzer(impl:icu)
+    -SetProp(docs.file,work/top100k-out/top.de.wikipedia.words.txt)
+    ResetInputs
+    { "GermanICU" { ReadTokens > : * ResetInputs } : 10
+
+    -NewLocale(uk)
+    -NewCollationAnalyzer(impl:icu)
+    -SetProp(docs.file,work/top100k-out/top.uk.wikipedia.words.txt)
+    ResetInputs
+    { "UkrainianICU" { ReadTokens > : * ResetInputs } : 10
+
+    -NewLocale(en)
+    -NewCollationAnalyzer(impl:icu)
+    -SetProp(docs.file,work/top100k-out/top.en.wikipedia.words.txt)
+    ResetInputs
+    { "EnglishICU" { ReadTokens > : * ResetInputs } : 10
+
+    NewRound
+
+} : 5
+
+RepSumByNameRound
diff --git a/modules/benchmark/conf/collector-small.alg b/modules/benchmark/conf/collector-small.alg
new file mode 100644
index 0000000..c67cab9
--- /dev/null
+++ b/modules/benchmark/conf/collector-small.alg
@@ -0,0 +1,91 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+# collector.class can be:
+#    Fully Qualified Class Name of a Collector with a empty constructor
+#    topScoreDocOrdered - Creates a TopScoreDocCollector that requires in order docs
+#    topScoreDocUnordered - Like above, but allows out of order
+collector.class=coll:topScoreDocOrdered:topScoreDocUnordered:topScoreDocOrdered:topScoreDocUnordered
+
+analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer
+directory=FSDirectory
+#directory=RamDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=100000
+
+search.num.hits=100000
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.LongToEnglishContentSource
+
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.LongToEnglishQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+
+    ResetSystemErase
+
+    { "Populate"
+        CreateIndex
+        { "MAddDocs" AddDoc } : 200000
+        Optimize
+        CloseIndex
+    }
+
+    OpenReader
+    { "topDocs" SearchWithCollector > : 10
+    CloseReader
+
+#    OpenReader
+#uses an array of search.num.hits size, but can also take in a parameter
+#    { "psc" SearchWithPostSortCollector > : 10
+#    { "psc100" SearchWithPostSortCollector(100) > : 10
+#    { "psc1000" SearchWithPostSortCollector(1000) > : 10
+#    { "psc10000" SearchWithPostSortCollector(10000) > : 10
+#    { "psc50000" SearchWithPostSortCollector(50000) > : 10
+#    CloseReader
+
+    RepSumByPref topDocs
+#    RepSumByPref psc
+#    RepSumByPref psc100
+#    RepSumByPref psc1000
+#    RepSumByPref psc10000
+#    RepSumByPref psc50000
+
+    NewRound
+
+} : 4
+
+#RepSumByNameRound
+#RepSumByName
+#RepSumByPrefRound topDocs
+#RepSumByPrefRound psc
+#RepSumByPrefRound psc100
+#RepSumByPrefRound psc1000
+#RepSumByPrefRound psc10000
+#RepSumByPrefRound psc50000
+
diff --git a/modules/benchmark/conf/collector.alg b/modules/benchmark/conf/collector.alg
new file mode 100644
index 0000000..a80d564
--- /dev/null
+++ b/modules/benchmark/conf/collector.alg
@@ -0,0 +1,91 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+# collector.class can be:
+#    Fully Qualified Class Name of a Collector with a empty constructor
+#    topScoreDocOrdered - Creates a TopScoreDocCollector that requires in order docs
+#    topScoreDocUnordered - Like above, but allows out of order
+collector.class=coll:topScoreDocOrdered:topScoreDocUnordered:topScoreDocOrdered:topScoreDocUnordered
+
+analyzer=org.apache.lucene.analysis.core.WhitespaceAnalyzer
+directory=FSDirectory
+#directory=RamDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=100000
+
+search.num.hits=1000000
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.LongToEnglishContentSource
+
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.LongToEnglishQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+
+    ResetSystemErase
+
+    { "Populate"
+        CreateIndex
+        { "MAddDocs" AddDoc } : 2000000
+        Optimize
+        CloseIndex
+    }
+
+    OpenReader
+    { "topDocs" SearchWithCollector > : 10
+    CloseReader
+
+#    OpenReader
+#uses an array of search.num.hits size, but can also take in a parameter
+#    { "psc" SearchWithPostSortCollector > : 10
+#    { "psc100" SearchWithPostSortCollector(100) > : 10
+#    { "psc1000" SearchWithPostSortCollector(1000) > : 10
+#    { "psc10000" SearchWithPostSortCollector(10000) > : 10
+#    { "psc50000" SearchWithPostSortCollector(50000) > : 10
+#    CloseReader
+
+    RepSumByPref topDocs
+#    RepSumByPref psc
+#    RepSumByPref psc100
+#    RepSumByPref psc1000
+#    RepSumByPref psc10000
+#    RepSumByPref psc50000
+
+    NewRound
+
+} : 4
+
+#RepSumByNameRound
+#RepSumByName
+#RepSumByPrefRound topDocs
+#RepSumByPrefRound psc
+#RepSumByPrefRound psc100
+#RepSumByPrefRound psc1000
+#RepSumByPrefRound psc10000
+#RepSumByPrefRound psc50000
+
diff --git a/modules/benchmark/conf/compound-penalty.alg b/modules/benchmark/conf/compound-penalty.alg
new file mode 100644
index 0000000..1291198
--- /dev/null
+++ b/modules/benchmark/conf/compound-penalty.alg
@@ -0,0 +1,92 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+
+# --------------------------------------------------------
+# Compound: what is the cost of compound format in indexing?
+# It does twice as much IO, is it twice slower? (no)
+# --------------------------------------------------------
+
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+merge.factor=mrg:10
+max.buffered=buf:10
+compound=compnd:true:false
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+#directory=RamDirectory
+
+doc.stored=stored:true:true:false:false
+doc.tokenized=true
+doc.term.vector=vector:true:true:false:false
+log.step=500
+log.step.DeleteDoc=100
+
+docs.dir=reuters-out
+#docs.dir=reuters-111
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=1
+
+log.queries=false
+# -------------------------------------------------------------------------------------
+
+ResetSystemErase
+
+{ "Round"
+  CreateIndex
+  { "AddDocs" AddDoc > : 10000
+  CloseIndex
+
+  OpenReader  
+  { "SearchSameRdr" Search > : 500
+  CloseReader 
+              
+  { "WarmNewRdr" Warm > : 50
+              
+  { "SrchNewRdr" Search > : 500
+              
+  { "SrchTrvNewRdr" SearchTrav > : 300
+              
+  { "SrchTrvRetNewRdr" SearchTravRet > : 100
+
+  [ "WarmNewRdr" Warm > : 50
+              
+  [ "SrchNewRdr" Search > : 500
+              
+  [ "SrchTrvNewRdr" SearchTrav > : 300
+              
+  [ "SrchTrvRetNewRdr" SearchTravRet > : 100
+
+  ResetInputs
+  RepSumByName
+  NewRound
+} : 4
+            
+RepSumByName
+RepSumByNameRound
+RepSumByPrefRound AddDocs
+RepSumByPrefRound SearchSameRdr
+RepSumByPrefRound WarmNewRdr
+RepSumByPrefRound SrchTrvNewRdr
+RepSumByPrefRound SrchTrvRetNewRdr
diff --git a/modules/benchmark/conf/createLineFile.alg b/modules/benchmark/conf/createLineFile.alg
new file mode 100644
index 0000000..969f307
--- /dev/null
+++ b/modules/benchmark/conf/createLineFile.alg
@@ -0,0 +1,43 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+
+#
+# This alg will process the Reuters documents feed to produce a
+# single file that contains all documents, one per line.
+#
+# To use this, first cd to contrib/benchmark and then run:
+#
+#   ant run-task -Dtask.alg=conf/createLineFile.alg
+#
+# Then, to index the documents in the line file, see
+# indexLineFile.alg.
+#
+
+# Where to get documents from:
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+# Where to write the line file output:
+line.file.out=work/reuters.lines.txt
+
+# Stop after processing the document feed once:
+content.source.forever=false
+
+# -------------------------------------------------------------------------------------
+
+# Process all documents, appending each one to the line file:
+{WriteLineDoc()}: * 
diff --git a/modules/benchmark/conf/deletepercent.alg b/modules/benchmark/conf/deletepercent.alg
new file mode 100644
index 0000000..59d1672
--- /dev/null
+++ b/modules/benchmark/conf/deletepercent.alg
@@ -0,0 +1,105 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+#directory=RamDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=500
+
+docs.dir=reuters-out
+#docs.dir=reuters-111
+
+#doc.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleDocMaker
+#doc.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersDocMaker
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+deletion.policy=org.apache.lucene.index.NoDeletionPolicy
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+
+    ResetSystemErase
+
+    { "Populate"
+        -CreateIndex
+        { "MAddDocs" AddDoc > : 1000
+        CommitIndex(original)
+        CloseIndex
+    }
+
+    OpenReader(false,original)
+    DeleteByPercent(5)
+    { "SearchSameRdr5" Search > : 500
+    FlushReader(5%)
+    CloseReader 
+    PrintReader(5%)
+
+    OpenReader(false,5%)
+    DeleteByPercent(10)
+    { "SearchSameRdr10" Search > : 500
+    FlushReader(10%)
+    CloseReader 
+    PrintReader(10%)
+
+    OpenReader(false,10%)
+    DeleteByPercent(20)
+    { "SearchSameRdr20" Search > : 500
+    FlushReader(20%)
+    CloseReader 
+    PrintReader(20%)
+    
+    OpenReader(false,20%)
+    DeleteByPercent(60)
+    { "SearchSameRdr60" Search > : 500
+    FlushReader(60%)
+    CloseReader 
+    PrintReader(60%)
+    
+    OpenReader(false,60%)
+    DeleteByPercent(75)
+    { "SearchSameRdr75" Search > : 500
+    FlushReader(75%)
+    CloseReader 
+    PrintReader(75%)
+
+    # Test lower percentage of deletes (so undeleteAll is used)
+    OpenReader(false,75%)
+    DeleteByPercent(7)
+    { "SearchSameRdr7" Search > : 500
+    FlushReader(7%)
+    CloseReader 
+    PrintReader(7%)
+
+    NewRound
+
+} : 1
+
+RepSumByName
+RepSumByPrefRound MAddDocs
diff --git a/modules/benchmark/conf/deletes.alg b/modules/benchmark/conf/deletes.alg
new file mode 100644
index 0000000..a54d4f8
--- /dev/null
+++ b/modules/benchmark/conf/deletes.alg
@@ -0,0 +1,70 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# --------------------------------------------------------
+# Deletes: what is the cost of deleting documents?
+# --------------------------------------------------------
+
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+merge.factor=mrg:10
+max.buffered=buf:100
+compound=true
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+#directory=RamDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=10000
+log.step.DeleteDoc=100
+
+docs.dir=reuters-out
+#docs.dir=reuters-111
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
+#content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
+#query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=1
+
+log.queries=false
+# -------------------------------------------------------------------------------------
+
+ResetSystemErase
+
+CreateIndex
+CloseIndex
+
+{ "Populate"
+    OpenIndex
+    { AddDoc(10) > : 200000
+    Optimize
+    CloseIndex
+> 
+
+{ "Deletions"
+   OpenReader(false)  DeleteDoc   CloseReader
+} : 4000
+
+RepSumByName
+
diff --git a/modules/benchmark/conf/extractWikipedia.alg b/modules/benchmark/conf/extractWikipedia.alg
new file mode 100644
index 0000000..f0df54d
--- /dev/null
+++ b/modules/benchmark/conf/extractWikipedia.alg
@@ -0,0 +1,44 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+
+#
+# This alg will process the Wikipedia documents feed to produce a
+# single file that contains all documents, one per line.
+#
+# To use this, first cd to contrib/benchmark and then run:
+#
+#   ant run-task -Dtask.alg=conf/extractWikipedia.alg
+#
+# Then, to index the documents in the line file, see
+# indexLineFile.alg.
+#
+
+# Where to get documents from:
+content.source=org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource
+docs.file=temp/enwiki-20070527-pages-articles.xml
+
+# Where to write the line file output:
+line.file.out=work/enwiki.txt
+
+# Stop after processing the document feed once:
+content.source.forever=false
+
+# -------------------------------------------------------------------------------------
+
+# Process all documents, appending each one to the line file:
+{WriteLineDoc() > : *
diff --git a/modules/benchmark/conf/highlight-profile.alg b/modules/benchmark/conf/highlight-profile.alg
new file mode 100644
index 0000000..234ebb1
--- /dev/null
+++ b/modules/benchmark/conf/highlight-profile.alg
@@ -0,0 +1,68 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+ram.flush.mb=flush:32:32
+compound=cmpnd:true:false
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=true
+doc.term.vector.offsets=true
+doc.term.vector.positions=true
+log.step=2000
+
+docs.dir=reuters-out
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+{ "Populate"
+        CreateIndex
+        { "MAddDocs" AddDoc } : 20000
+        Optimize
+        CloseIndex
+    }
+{ "Rounds"
+
+    ResetSystemSoft
+
+
+    OpenReader
+      { "SearchHlgtSameRdr" SearchTravRetHighlight(maxFrags[10],fields[body]) > : 1000
+
+    CloseReader
+
+    RepSumByPref MAddDocs
+
+    NewRound
+
+} : 4
+
+RepSumByNameRound
+RepSumByName
+RepSumByPrefRound MAddDocs
diff --git a/modules/benchmark/conf/highlight-vs-vector-highlight.alg b/modules/benchmark/conf/highlight-vs-vector-highlight.alg
new file mode 100644
index 0000000..a98e321
--- /dev/null
+++ b/modules/benchmark/conf/highlight-vs-vector-highlight.alg
@@ -0,0 +1,80 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+
+ram.flush.mb=flush:32:32
+compound=cmpnd:true:false
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=true
+doc.term.vector.offsets=true
+doc.term.vector.positions=true
+log.step=2000
+
+docs.dir=reuters-out
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource
+docs.file=temp/enwiki-20070527-pages-articles.xml
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.EnwikiQueryMaker
+enwikiQueryMaker.disableSpanQueries=true
+
+max.field.length=2147483647
+highlighter.maxDocCharsToAnalyze=2147483647
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+{ "Populate"
+        CreateIndex
+        { "MAddDocs" AddDoc } : 20000
+        Optimize
+        CloseIndex
+}
+{
+        OpenReader
+          { "WarmTV" SearchTravRetVectorHighlight(maxFrags[3],fields[body]) > : 100
+        CloseReader
+}
+{
+	"Rounds"
+
+        ResetSystemSoft
+
+        OpenReader
+          { "SearchVecHlgtSameRdr" SearchTravRetVectorHighlight(maxFrags[3],fields[body]) > : 200
+        CloseReader
+
+        ResetSystemSoft
+
+        OpenReader
+          { "SearchHlgtSameRdr" SearchTravRetHighlight(maxFrags[3],fields[body]) > : 200
+        CloseReader
+
+        RepSumByPref Search
+
+        NewRound
+} : 4
+
+RepSumByNameRound
+RepSumByName
diff --git a/modules/benchmark/conf/indexLineFile.alg b/modules/benchmark/conf/indexLineFile.alg
new file mode 100644
index 0000000..bcb9922
--- /dev/null
+++ b/modules/benchmark/conf/indexLineFile.alg
@@ -0,0 +1,53 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+
+#
+# This file indexes documents contained in a single text file, one per
+# line.  See createLineFile.alg for how to create this file.  The
+# benefit of this is it removes the IO cost of opening one file per
+# document to let you more accurately measure time spent analyzing and
+# indexing your documents vs time spent creating the documents.
+#
+# To use this, you must first run the createLineFile.alg, then cd to
+# contrib/benchmark and then run:
+#
+#   ant run-task -Dtask.alg=conf/indexLineFile.alg
+#
+
+analyzer=org.apache.lucene.analysis.core.SimpleAnalyzer
+
+# Feed that knows how to process the line file format:
+content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
+
+# File that contains one document per line:
+docs.file=work/reuters.lines.txt
+
+# Process documents only once:
+content.source.forever=false
+
+# -------------------------------------------------------------------------------------
+
+# Reset the system, create a new index, index all docs from the line
+# file, close the index, produce a report.
+
+ResetSystemErase
+CreateIndex
+{AddDoc}: *
+CloseIndex
+
+RepSumByPref AddDoc 
diff --git a/modules/benchmark/conf/indexing-flush-by-RAM-multithreaded.alg b/modules/benchmark/conf/indexing-flush-by-RAM-multithreaded.alg
new file mode 100644
index 0000000..58028f9
--- /dev/null
+++ b/modules/benchmark/conf/indexing-flush-by-RAM-multithreaded.alg
@@ -0,0 +1,70 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+writer.version=LUCENE_40
+#merge.factor=mrg:10:100:10:100:10:100:10:100
+#max.buffered=buf:10:10:100:100:10:10:100:100
+ram.flush.mb=flush:32:40:48:56:32:40:48:56
+compound=cmpnd:true:true:true:true:false:false:false:false
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+#directory=RamDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=2000
+
+docs.dir=reuters-out
+#docs.dir=reuters-111
+
+#content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+
+    ResetSystemErase
+
+    { "Populate"
+        CreateIndex
+        [{ "MAddDocs" AddDoc } : 5000] : 4
+        Optimize
+        CloseIndex
+    }
+
+
+    RepSumByPref MAddDocs
+
+    NewRound
+
+} : 8
+
+RepSumByNameRound
+RepSumByName
+RepSumByPrefRound MAddDocs
diff --git a/modules/benchmark/conf/indexing-flush-by-RAM.alg b/modules/benchmark/conf/indexing-flush-by-RAM.alg
new file mode 100644
index 0000000..be88a1f
--- /dev/null
+++ b/modules/benchmark/conf/indexing-flush-by-RAM.alg
@@ -0,0 +1,70 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+writer.version=LUCENE_40
+#merge.factor=mrg:10:100:10:100:10:100:10:100
+#max.buffered=buf:10:10:100:100:10:10:100:100
+ram.flush.mb=flush:32:40:48:56:32:40:48:56
+compound=cmpnd:true:true:true:true:false:false:false:false
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+#directory=RamDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=2000
+
+docs.dir=reuters-out
+#docs.dir=reuters-111
+
+#content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+
+    ResetSystemErase
+
+    { "Populate"
+        CreateIndex
+        { "MAddDocs" AddDoc } : 20000
+        Optimize
+        CloseIndex
+    }
+
+
+    RepSumByPref MAddDocs
+
+    NewRound
+
+} : 8
+
+RepSumByNameRound
+RepSumByName
+RepSumByPrefRound MAddDocs
diff --git a/modules/benchmark/conf/indexing-multithreaded.alg b/modules/benchmark/conf/indexing-multithreaded.alg
new file mode 100644
index 0000000..261cdb3
--- /dev/null
+++ b/modules/benchmark/conf/indexing-multithreaded.alg
@@ -0,0 +1,71 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+writer.version=LUCENE_40
+merge.factor=mrg:10:100:10:100:10:100:10:100
+max.buffered=buf:10:10:100:100:10:10:100:100
+#ram.flush.mb=flush:32:40:48:56:32:40:48:56
+compound=cmpnd:true:true:true:true:false:false:false:false
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+#directory=RamDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=2000
+
+docs.dir=reuters-out
+#docs.dir=reuters-111
+
+#content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+
+    ResetSystemErase
+
+    { "Populate"
+        CreateIndex
+        [{ "MAddDocs" AddDoc } : 5000] : 4
+        Optimize
+        CommitIndex(commit1)
+        CloseIndex
+    }
+
+
+    RepSumByPref MAddDocs
+
+    NewRound
+
+} : 8
+
+RepSumByNameRound
+RepSumByName
+RepSumByPrefRound MAddDocs
diff --git a/modules/benchmark/conf/indexing.alg b/modules/benchmark/conf/indexing.alg
new file mode 100644
index 0000000..7c8673b
--- /dev/null
+++ b/modules/benchmark/conf/indexing.alg
@@ -0,0 +1,70 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+writer.version=LUCENE_40
+merge.factor=mrg:10:100:10:100:10:100:10:100
+max.buffered=buf:10:10:100:100:10:10:100:100
+#ram.flush.mb=flush:32:40:48:56:32:40:48:56
+compound=cmpnd:true:true:true:true:false:false:false:false
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+#directory=RamDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=2000
+
+docs.dir=reuters-out
+#docs.dir=reuters-111
+
+#content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+
+    ResetSystemErase
+
+    { "Populate"
+        CreateIndex
+        { "MAddDocs" AddDoc } : 20000
+        Optimize
+        CloseIndex
+    }
+
+
+    RepSumByPref MAddDocs
+
+    NewRound
+
+} : 8
+
+RepSumByNameRound
+RepSumByName
+RepSumByPrefRound MAddDocs
diff --git a/modules/benchmark/conf/micro-standard-flush-by-ram.alg b/modules/benchmark/conf/micro-standard-flush-by-ram.alg
new file mode 100644
index 0000000..0d2c685
--- /dev/null
+++ b/modules/benchmark/conf/micro-standard-flush-by-ram.alg
@@ -0,0 +1,77 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+#merge.factor=mrg:10:100:10:100
+#max.buffered=buf:10:10:100:100
+ram.flush.mb=flush:32:40:48:56
+compound=true
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+#directory=RamDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=500
+
+docs.dir=reuters-out
+#docs.dir=reuters-111
+
+#content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+
+    ResetSystemErase
+
+    { "Populate"
+        CreateIndex
+        { "MAddDocs" AddDoc > : 2000
+        Optimize
+        CloseIndex
+    }
+
+    OpenReader  
+    { "SearchSameRdr" Search > : 5000
+    CloseReader 
+                
+    { "WarmNewRdr" Warm > : 50
+                
+    { "SrchNewRdr" Search > : 500
+                
+    { "SrchTrvNewRdr" SearchTrav(1000) > : 300
+                
+    { "SrchTrvRetNewRdr" SearchTravRet(2000) > : 100
+                
+    NewRound
+
+} : 4
+
+RepSumByName
+RepSumByPrefRound MAddDocs
diff --git a/modules/benchmark/conf/micro-standard.alg b/modules/benchmark/conf/micro-standard.alg
new file mode 100644
index 0000000..e0a554a
--- /dev/null
+++ b/modules/benchmark/conf/micro-standard.alg
@@ -0,0 +1,76 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+merge.factor=mrg:10:100:10:100
+max.buffered=buf:10:10:100:100
+compound=true
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+#directory=RamDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=500
+
+docs.dir=reuters-out
+#docs.dir=reuters-111
+
+#content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+
+    ResetSystemErase
+
+    { "Populate"
+        -CreateIndex
+        { "MAddDocs" AddDoc > : 2000
+        -Optimize
+        -CloseIndex
+    }
+
+    OpenReader  
+    { "SearchSameRdr" Search > : 5000
+    CloseReader 
+                
+    { "WarmNewRdr" Warm > : 50
+                
+    { "SrchNewRdr" Search > : 500
+                
+    { "SrchTrvNewRdr" SearchTrav(1000) > : 300
+                
+    { "SrchTrvRetNewRdr" SearchTravRet(2000) > : 100
+                
+    NewRound
+
+} : 4
+
+RepSumByName
+RepSumByPrefRound MAddDocs
diff --git a/modules/benchmark/conf/readContentSource.alg b/modules/benchmark/conf/readContentSource.alg
new file mode 100644
index 0000000..9923af0
--- /dev/null
+++ b/modules/benchmark/conf/readContentSource.alg
@@ -0,0 +1,45 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+
+#
+# This alg reads the information from a ContentSoruce. It is useful for 
+# measuring the performance of a particular ContentSource implementation, or 
+# gather baselines for operations like indexing (if reading from the content 
+# source takes 'X' time, we cannot index faster).
+#
+# To use this, first cd to contrib/benchmark and then run:
+#
+#   ant run-task -Dtask.alg=conf/readContentSource.alg
+#
+
+# Where to get documents from:
+content.source=org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource
+docs.file=temp/enwiki-20070527-pages-articles.xml.bz2
+
+# Stop after processing the document feed once:
+content.source.forever=false
+
+# Log messages every:
+log.step=100000
+
+# -------------------------------------------------------------------------------------
+
+# Process all documents, appending each one to the line file:
+{ ConsumeContentSource } : *
+
+RepSumByPref ConsumeContentSource
diff --git a/modules/benchmark/conf/sample.alg b/modules/benchmark/conf/sample.alg
new file mode 100644
index 0000000..c7b9f25
--- /dev/null
+++ b/modules/benchmark/conf/sample.alg
@@ -0,0 +1,85 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# --------------------------------------------------------
+# 
+# Sample: what is the effect of doc size on indexing time?
+#
+# There are two parts in this test:
+# - PopulateShort adds 2N documents of length  L
+# - PopulateLong  adds  N documents of length 2L
+# Which one would be faster?
+# The comparison is done twice.
+#
+# --------------------------------------------------------
+
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+merge.factor=mrg:10:20
+max.buffered=buf:100:1000
+compound=true
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+#directory=RamDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=500
+
+docs.dir=reuters-out
+#docs.dir=reuters-111
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
+#content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
+#query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=false
+# -------------------------------------------------------------------------------------
+
+{
+
+    { "PopulateShort"
+        CreateIndex
+        { AddDoc(4000) > : 20000
+        Optimize
+        CloseIndex
+    >
+
+    ResetSystemErase
+    
+    { "PopulateLong"
+        CreateIndex
+        { AddDoc(8000) > : 10000
+        Optimize
+        CloseIndex
+    >
+
+    ResetSystemErase
+
+    NewRound
+
+} : 2
+
+RepSumByName
+RepSelectByPref Populate
diff --git a/modules/benchmark/conf/shingle.alg b/modules/benchmark/conf/shingle.alg
new file mode 100644
index 0000000..5fb6876
--- /dev/null
+++ b/modules/benchmark/conf/shingle.alg
@@ -0,0 +1,48 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+# 
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+doc.tokenized=false
+doc.body.tokenized=true
+docs.dir=reuters-out
+log.step=1000
+
+{ "Rounds"
+
+    -NewShingleAnalyzer(maxShingleSize:2,outputUnigrams:true)
+    -ResetInputs
+    { "BigramsAndUnigrams" { ReadTokens > : 10000 }
+
+    -NewShingleAnalyzer(maxShingleSize:2,outputUnigrams:false)
+    -ResetInputs
+    { "BigramsOnly" { ReadTokens > : 10000 }
+
+    -NewShingleAnalyzer(maxShingleSize:4,outputUnigrams:true)
+    -ResetInputs
+    { "FourgramsAndUnigrams" { ReadTokens > : 10000 }
+
+    -NewShingleAnalyzer(maxShingleSize:4,outputUnigrams:false)
+    -ResetInputs
+    { "FourgramsOnly" { ReadTokens > : 10000 }
+
+    -NewAnalyzer(standard.StandardAnalyzer)
+    -ResetInputs
+    { "UnigramsOnly" { ReadTokens > : 10000 }
+
+    NewRound
+
+} : 5
+
+RepSumByNameRound
diff --git a/modules/benchmark/conf/sloppy-phrase.alg b/modules/benchmark/conf/sloppy-phrase.alg
new file mode 100644
index 0000000..f0caad7
--- /dev/null
+++ b/modules/benchmark/conf/sloppy-phrase.alg
@@ -0,0 +1,74 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+max.buffered=100
+merge.factor=10
+compound=true
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+#directory=RamDirectory
+
+doc.stored=false
+doc.tokenized=true
+doc.term.vector=false
+log.step=500
+
+docs.dir=reuters-out
+#docs.dir=reuters-111
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
+#content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleSloppyPhraseQueryMaker
+#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
+#query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=1
+
+log.queries=false
+# -------------------------------------------------------------------------------------
+
+
+ResetSystemErase
+
+{ "Populate"
+    CreateIndex
+    { "MAddDocs" AddDoc(2000) > : 20000     
+    Optimize
+    CloseIndex
+}
+
+
+{ "Round"
+
+  OpenReader  
+  { "SearchSameRdr" Search > : 6000
+  CloseReader 
+
+  ResetInputs
+  RepSumByName
+  NewRound
+} : 4
+            
+RepSumByPrefRound MAddDocs
+
+RepSumByName
+RepSumByPrefRound Search
diff --git a/modules/benchmark/conf/sort-standard.alg b/modules/benchmark/conf/sort-standard.alg
new file mode 100644
index 0000000..c7413fc
--- /dev/null
+++ b/modules/benchmark/conf/sort-standard.alg
@@ -0,0 +1,71 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+merge.factor=mrg:50
+compound=false
+
+sort.rng=20000:10000:20000:10000
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+#directory=RamDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=100000
+
+docs.dir=reuters-out
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.SortableSingleDocSource
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+	{ "Run"
+      ResetSystemErase
+
+      { "Populate"
+        -CreateIndex
+        { "MAddDocs" AddDoc(100) > : 500000
+        -Optimize
+        -CloseIndex
+      }
+    
+      { "TestSortSpeed"
+        OpenReader  
+        { "LoadFieldCacheAndSearch" SearchWithSort(sort_field:int) > : 1 
+        { "SearchWithSort" SearchWithSort(sort_field:int) > : 5000
+        CloseReader 
+      
+      }
+    
+      NewRound
+     } : 4
+
+} 
+
+RepSumByName
+
diff --git a/modules/benchmark/conf/standard-flush-by-RAM.alg b/modules/benchmark/conf/standard-flush-by-RAM.alg
new file mode 100644
index 0000000..ba60ac8
--- /dev/null
+++ b/modules/benchmark/conf/standard-flush-by-RAM.alg
@@ -0,0 +1,92 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+#merge.factor=mrg:10:100:10:100:10:100:10:100
+#max.buffered=buf:10:10:100:100:10:10:100:100
+ram.flush.mb=flush:32:40:48:56:32:40:48:56
+compound=cmpnd:true:true:true:true:false:false:false:false
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+#directory=RamDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=2000
+
+docs.dir=reuters-out
+#docs.dir=reuters-111
+
+#content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+
+    ResetSystemErase
+
+    { "Populate"
+        CreateIndex
+        { "MAddDocs" AddDoc } : 20000
+        Optimize
+        CloseIndex
+    }
+
+    OpenReader  
+    { "SearchSameRdr" Search > : 5000
+    CloseReader 
+                
+    { "WarmNewRdr" Warm > : 50
+                
+    { "SrchNewRdr" Search > : 500
+                
+    { "SrchTrvNewRdr" SearchTrav > : 300
+                
+    { "SrchTrvRetNewRdr" SearchTravRet > : 100
+                
+    OpenReader  
+    [ "SearchSameRdr" Search > : 5000 : 2500
+    CloseReader 
+                
+    [ "WarmNewRdr" Warm > : 50 : 25
+                
+    [ "SrchNewRdr" Search > : 50 : 25
+                
+    [ "SrchTrvNewRdr" SearchTrav > : 300 : 150
+                
+    [ "SrchTrvRetNewRdr" SearchTravRet > : 100 : 50
+
+    RepSumByPref MAddDocs
+
+    NewRound
+
+} : 8
+
+RepSumByNameRound
+RepSumByName
+RepSumByPrefRound MAddDocs
diff --git a/modules/benchmark/conf/standard-highlights-notv.alg b/modules/benchmark/conf/standard-highlights-notv.alg
new file mode 100644
index 0000000..889f5d7
--- /dev/null
+++ b/modules/benchmark/conf/standard-highlights-notv.alg
@@ -0,0 +1,69 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+ram.flush.mb=flush:32:32
+compound=cmpnd:true:false
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+doc.term.vector.offsets=false
+doc.term.vector.positions=false
+log.step=2000
+
+docs.dir=reuters-out
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+{ "Populate"
+        CreateIndex
+        { "MAddDocs" AddDoc } : 20000
+        Optimize
+        CloseIndex
+}
+{ "Rounds"
+
+    ResetSystemSoft
+    OpenReader
+      { "SrchTrvRetNewRdr" SearchTravRet(10) > : 1000
+    CloseReader
+    OpenReader
+      { "SearchHlgtSameRdr" SearchTravRetHighlight(size[10],highlight[10],mergeContiguous[true],maxFrags[3],fields[body]) > : 1000
+
+    CloseReader
+
+    RepSumByPref SearchHlgtSameRdr
+
+    NewRound
+
+} : 2
+
+RepSumByNameRound
+RepSumByName
+RepSumByPrefRound MAddDocs
diff --git a/modules/benchmark/conf/standard-highlights-tv.alg b/modules/benchmark/conf/standard-highlights-tv.alg
new file mode 100644
index 0000000..8c7f533
--- /dev/null
+++ b/modules/benchmark/conf/standard-highlights-tv.alg
@@ -0,0 +1,69 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+ram.flush.mb=flush:32:32
+compound=cmpnd:true:false
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=true
+doc.term.vector.offsets=true
+doc.term.vector.positions=true
+log.step=2000
+
+docs.dir=reuters-out
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+{ "Populate"
+        CreateIndex
+        { "MAddDocs" AddDoc } : 20000
+        Optimize
+        CloseIndex
+}
+{ "Rounds"
+
+    ResetSystemSoft
+    OpenReader
+      { "SrchTrvRetNewRdr" SearchTravRet(10) > : 1000
+    CloseReader
+    OpenReader
+      { "SearchHlgtSameRdr" SearchTravRetHighlight(size[10],highlight[10],mergeContiguous[true],maxFrags[3],fields[body]) > : 1000
+
+    CloseReader
+
+    RepSumByPref SearchHlgtSameRdr
+
+    NewRound
+
+} : 2
+
+RepSumByNameRound
+RepSumByName
+RepSumByPrefRound MAddDocs
diff --git a/modules/benchmark/conf/standard.alg b/modules/benchmark/conf/standard.alg
new file mode 100644
index 0000000..66e66ef
--- /dev/null
+++ b/modules/benchmark/conf/standard.alg
@@ -0,0 +1,92 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+writer.version=LUCENE_40
+merge.factor=mrg:10:100:10:100:10:100:10:100
+max.buffered=buf:10:10:100:100:10:10:100:100
+compound=cmpnd:true:true:true:true:false:false:false:false
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+#directory=RamDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=2000
+
+docs.dir=reuters-out
+#docs.dir=reuters-111
+
+#content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+#query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+
+    ResetSystemErase
+
+    { "Populate"
+        CreateIndex
+        { "MAddDocs" AddDoc } : 20000
+        Optimize
+        CloseIndex
+    }
+
+    OpenReader  
+    { "SearchSameRdr" Search > : 5000
+    CloseReader 
+                
+    { "WarmNewRdr" Warm > : 50
+                
+    { "SrchNewRdr" Search > : 500
+                
+    { "SrchTrvNewRdr" SearchTrav > : 300
+                
+    { "SrchTrvRetNewRdr" SearchTravRet > : 100
+                
+    OpenReader  
+    [ "SearchSameRdr" Search > : 5000 : 2500
+    CloseReader 
+                
+    [ "WarmNewRdr" Warm > : 50 : 25
+                
+    [ "SrchNewRdr" Search > : 50 : 25
+                
+    [ "SrchTrvNewRdr" SearchTrav > : 300 : 150
+                
+    [ "SrchTrvRetNewRdr" SearchTravRet > : 100 : 50
+
+    RepSumByPref MAddDocs
+
+    NewRound
+
+} : 8
+
+RepSumByNameRound
+RepSumByName
+RepSumByPrefRound MAddDocs
diff --git a/modules/benchmark/conf/tokenize.alg b/modules/benchmark/conf/tokenize.alg
new file mode 100644
index 0000000..57951ff
--- /dev/null
+++ b/modules/benchmark/conf/tokenize.alg
@@ -0,0 +1,36 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+
+#
+# This alg reads all tokens out of a document but does not index them.
+# This is useful for benchmarking tokenizers.
+#
+# To use this, cd to contrib/benchmark and then run:
+#
+#   ant run-task -Dtask.alg=conf/tokenize.alg
+#
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+content.source.forever=false
+
+
+#
+-------------------------------------------------------------------------------------
+
+{ReadTokens > : *
+RepSumByName
diff --git a/modules/benchmark/conf/vector-highlight-profile.alg b/modules/benchmark/conf/vector-highlight-profile.alg
new file mode 100644
index 0000000..6b456df
--- /dev/null
+++ b/modules/benchmark/conf/vector-highlight-profile.alg
@@ -0,0 +1,68 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+
+ram.flush.mb=flush:32:32
+compound=cmpnd:true:false
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=true
+doc.term.vector.offsets=true
+doc.term.vector.positions=true
+log.step=2000
+
+docs.dir=reuters-out
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+# -------------------------------------------------------------------------------------
+{ "Populate"
+        CreateIndex
+        { "MAddDocs" AddDoc } : 20000
+        Optimize
+        CloseIndex
+    }
+{ "Rounds"
+
+    ResetSystemSoft
+
+
+    OpenReader
+      { "SearchVecHlgtSameRdr" SearchTravRetVectorHighlight(maxFrags[10],fields[body]) > : 1000
+
+    CloseReader
+
+    RepSumByPref MAddDocs
+
+    NewRound
+
+} : 4
+
+RepSumByNameRound
+RepSumByName
+RepSumByPrefRound MAddDocs
diff --git a/modules/benchmark/conf/wikipedia-flush-by-RAM.alg b/modules/benchmark/conf/wikipedia-flush-by-RAM.alg
new file mode 100644
index 0000000..5bedfb3
--- /dev/null
+++ b/modules/benchmark/conf/wikipedia-flush-by-RAM.alg
@@ -0,0 +1,69 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+#
+# based on micro-standard
+#
+# modified to use wikipedia sources and index entire docs
+# currently just used to measure ingest rate
+
+#merge.factor=mrg:10:100:10:100
+#max.buffered=buf:10:10:100:100
+ram.flush.mb=ram:32:40:48:56
+
+max.field.length=2147483647
+
+
+compound=true
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=5000
+
+docs.file=temp/enwiki-20070527-pages-articles.xml
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=false
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+
+    ResetSystemErase
+
+    { "Populate"
+        CreateIndex
+        { "MAddDocs" AddDoc > : 200000
+        CloseIndex
+    }
+
+    NewRound
+
+} : 4
+
+RepSumByName
+RepSumByPrefRound MAddDocs
diff --git a/modules/benchmark/conf/wikipedia.alg b/modules/benchmark/conf/wikipedia.alg
new file mode 100644
index 0000000..1417f24
--- /dev/null
+++ b/modules/benchmark/conf/wikipedia.alg
@@ -0,0 +1,65 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+#
+# based on micro-standard
+#
+# modified to use wikipedia sources and index entire docs
+# currently just used to measure ingest rate
+
+merge.factor=mrg:10:100:10:100
+max.field.length=2147483647
+max.buffered=buf:10:10:100:100
+compound=true
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=5000
+
+docs.file=temp/enwiki-20070527-pages-articles.xml
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=false
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+
+    ResetSystemErase
+
+    { "Populate"
+        CreateIndex
+        { "MAddDocs" AddDoc > : 200000
+        CloseIndex
+    }
+
+    NewRound
+
+} : 4
+
+RepSumByName
+RepSumByPrefRound MAddDocs
diff --git a/modules/benchmark/conf/wikipediaOneRound.alg b/modules/benchmark/conf/wikipediaOneRound.alg
new file mode 100644
index 0000000..e2aaa74
--- /dev/null
+++ b/modules/benchmark/conf/wikipediaOneRound.alg
@@ -0,0 +1,65 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+#
+# based on micro-standard
+#
+# modified to use wikipedia sources and index entire docs
+# currently just used to measure ingest rate
+
+merge.factor=mrg:10:100:10:100
+max.field.length=2147483647
+max.buffered=buf:10:10:100:100
+compound=true
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+log.step=5000
+
+docs.file=temp/enwiki-20070527-pages-articles.xml
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=false
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+
+    ResetSystemErase
+
+    { "Populate"
+        CreateIndex
+        { "MAddDocs" AddDoc > : 200000
+        CloseIndex
+    }
+
+    NewRound
+
+} : 1
+
+RepSumByName
+RepSumByPrefRound MAddDocs
diff --git a/modules/benchmark/lib/commons-beanutils-1.7.0.jar b/modules/benchmark/lib/commons-beanutils-1.7.0.jar
new file mode 100644
index 0000000..e211356
--- /dev/null
+++ b/modules/benchmark/lib/commons-beanutils-1.7.0.jar
@@ -0,0 +1,2 @@
+AnyObjectId[b1b89c9c921f16af22a88db3ff28975a8e40d886] was removed in git history.
+Apache SVN contains full history.
\ No newline at end of file
diff --git a/modules/benchmark/lib/commons-collections-3.1.jar b/modules/benchmark/lib/commons-collections-3.1.jar
new file mode 100644
index 0000000..6e5f877
--- /dev/null
+++ b/modules/benchmark/lib/commons-collections-3.1.jar
@@ -0,0 +1,2 @@
+AnyObjectId[41e230feeaa53618b6ac5f8d11792c2eecf4d4fd] was removed in git history.
+Apache SVN contains full history.
\ No newline at end of file
diff --git a/modules/benchmark/lib/commons-compress-1.0.jar b/modules/benchmark/lib/commons-compress-1.0.jar
new file mode 100644
index 0000000..473e2bf
--- /dev/null
+++ b/modules/benchmark/lib/commons-compress-1.0.jar
@@ -0,0 +1,2 @@
+AnyObjectId[78d832c11c42023d4bc12077a1d9b7b5025217bc] was removed in git history.
+Apache SVN contains full history.
\ No newline at end of file
diff --git a/modules/benchmark/lib/commons-digester-1.7.jar b/modules/benchmark/lib/commons-digester-1.7.jar
new file mode 100644
index 0000000..97d5d05
--- /dev/null
+++ b/modules/benchmark/lib/commons-digester-1.7.jar
@@ -0,0 +1,2 @@
+AnyObjectId[1783dbea232ced6db122268f8faa5ce773c7ea42] was removed in git history.
+Apache SVN contains full history.
\ No newline at end of file
diff --git a/modules/benchmark/lib/commons-logging-1.0.4.jar b/modules/benchmark/lib/commons-logging-1.0.4.jar
new file mode 100644
index 0000000..f330fde
--- /dev/null
+++ b/modules/benchmark/lib/commons-logging-1.0.4.jar
@@ -0,0 +1,2 @@
+AnyObjectId[b73a80fab641131e6fbe3ae833549efb3c540d17] was removed in git history.
+Apache SVN contains full history.
\ No newline at end of file
diff --git a/modules/benchmark/lib/xercesImpl-2.10.0.jar b/modules/benchmark/lib/xercesImpl-2.10.0.jar
new file mode 100644
index 0000000..11b416c
--- /dev/null
+++ b/modules/benchmark/lib/xercesImpl-2.10.0.jar
@@ -0,0 +1,2 @@
+AnyObjectId[9dcd8c38196b24e51f78d8e1b0a42d1ffef60acb] was removed in git history.
+Apache SVN contains full history.
\ No newline at end of file
diff --git a/modules/benchmark/lib/xml-apis-2.10.0.jar b/modules/benchmark/lib/xml-apis-2.10.0.jar
new file mode 100644
index 0000000..c59f0f1
--- /dev/null
+++ b/modules/benchmark/lib/xml-apis-2.10.0.jar
@@ -0,0 +1,2 @@
+AnyObjectId[46733464fc746776c331ecc51061f3a05e662fd1] was removed in git history.
+Apache SVN contains full history.
\ No newline at end of file
diff --git a/modules/benchmark/pom.xml.template b/modules/benchmark/pom.xml.template
new file mode 100644
index 0000000..9e6a1ec
--- /dev/null
+++ b/modules/benchmark/pom.xml.template
@@ -0,0 +1,67 @@
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+
+  <!--
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+  -->
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <groupId>org.apache.lucene</groupId>
+    <artifactId>lucene-contrib</artifactId>
+    <version>@version@</version>
+  </parent>
+  <groupId>org.apache.lucene</groupId>
+  <artifactId>lucene-benchmark</artifactId>
+  <name>Lucene Benchmark</name>
+  <version>@version@</version>
+  <description>Lucene Benchmarking Contributions</description>
+  <packaging>jar</packaging>
+  <dependencies>
+    <dependency>
+      <groupId>org.apache.lucene</groupId>
+      <artifactId>lucene-demos</artifactId>
+      <version>@version@</version>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.lucene</groupId>
+      <artifactId>lucene-highlighter</artifactId>
+      <version>@version@</version>
+    </dependency>
+    <dependency>
+      <groupId>commons-beanutils</groupId>
+      <artifactId>commons-beanutils</artifactId>
+      <version>${commons-beanutils-version}</version>
+    </dependency>
+    <dependency>
+      <groupId>commons-collections</groupId>
+      <artifactId>commons-collections</artifactId>
+      <version>${commons-collections-version}</version>
+    </dependency>
+    <dependency>
+      <groupId>commons-digester</groupId>
+      <artifactId>commons-digester</artifactId>
+      <version>${commons-digester-version}</version>
+    </dependency>
+    <dependency>
+      <groupId>commons-logging</groupId>
+      <artifactId>commons-logging</artifactId>
+      <version>${commons-logging-version}</version>
+    </dependency>
+  </dependencies>
+</project>
diff --git a/modules/benchmark/scripts/collation.bm2jira.pl b/modules/benchmark/scripts/collation.bm2jira.pl
new file mode 100644
index 0000000..b423f75
--- /dev/null
+++ b/modules/benchmark/scripts/collation.bm2jira.pl
@@ -0,0 +1,63 @@
+#!/usr/bin/perl
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+# 
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+# ----------
+# bm2jira.pl
+#
+# Converts Lucene contrib-benchmark output produced using the 
+# benchmark.collation.alg file into a JIRA-formatted table.
+#
+
+use strict;
+use warnings;
+
+my %min_elapsed = ();
+
+while (<>) {
+  if (/(\S+)(Keyword|JDK|ICU)_\d+\s*([^\s{].*)/) {
+    my $lang = $1;
+    my $analyzer = $2;
+    my $stats = $3;
+    my ($elapsed) = $stats =~ /(?:[\d,.]+[-\s]*){4}([.\d]+)/;
+    $min_elapsed{$analyzer}{$lang} = $elapsed
+      unless (defined($min_elapsed{$analyzer}{$lang})
+              && $elapsed >= $min_elapsed{$analyzer}{$lang});
+  }
+}
+
+# Print out platform info
+print "JAVA:\n", `java -version 2>&1`, "\nOS:\n";
+if ($^O =~ /win/i) {
+  print "$^O\n";
+  eval {
+    require Win32;
+    print Win32::GetOSName(), "\n", Win32::GetOSVersion(), "\n";
+  };
+  die "Error loading Win32: $@" if ($@);
+} else {
+  print `uname -a 2>&1`;
+}
+
+print "\n||Language||java.text||ICU4J||KeywordAnalyzer||ICU4J Improvement||\n";
+
+for my $lang (sort keys %{$min_elapsed{ICU}}) {
+  my $ICU = $min_elapsed{ICU}{$lang};
+  my $JDK = $min_elapsed{JDK}{$lang};
+  my $keyword = $min_elapsed{Keyword}{$lang};
+  my $improved = int(100 * ($JDK - $ICU) / ($ICU - $keyword) + 0.5);
+  printf "|$lang|${JDK}s|${ICU}s|${keyword}s|\%d%%|\n", $improved;
+}
diff --git a/modules/benchmark/scripts/compare.collation.benchmark.tables.pl b/modules/benchmark/scripts/compare.collation.benchmark.tables.pl
new file mode 100644
index 0000000..bd94176
--- /dev/null
+++ b/modules/benchmark/scripts/compare.collation.benchmark.tables.pl
@@ -0,0 +1,91 @@
+#!/usr/bin/perl
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+# 
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+# ------------------------------------------
+# compare.collation.benchmark.jira.tables.pl
+#
+# Takes as cmdline parameters two JIRA-formatted benchmark results, as produced
+# by bm2jira.pl (located in the same directory as this script), and outputs a
+# third JIRA-formatted comparison table, showing the differences between two
+# benchmarking runs' java.text and ICU4J columns, after accounting for the
+# KeywordAnalyzer column; the "ICU4J Improvement" column is ignored.
+#
+# The difference is calculated as a percentage:
+#
+#   100 * (patched-rate - unpatched-rate / unpatched-rate)
+#
+# where the (un)patched-rate is:
+#
+#   1 / ( elapsed-(un)patched-time - elapsed-KeywordAnalyzer-time)
+#
+
+use strict;
+use warnings;
+
+my $usage = "Usage: $0 <unpatched-file> <patched-file>\n";
+
+die $usage unless ($#ARGV == 1 && -f $ARGV[0] && -f $ARGV[1]);
+
+my %stats = ();
+
+open UNPATCHED, "<$ARGV[0]" || die "ERROR opening '$ARGV[0]': $!";
+while (<UNPATCHED>) {
+  # ||Language||java.text||ICU4J||KeywordAnalyzer||ICU4J Improvement||
+  # |English|4.51s|2.47s|1.47s|204%|
+  next unless (/^\|([^|]+)\|([^|s]+)s\|([^|s]+)s\|([^|s]+)s/);
+  my ($lang, $jdk_elapsed, $icu_elapsed, $keyword_analyzer_elapsed)
+    = ($1, $2, $3, $4);
+  $stats{unpatched}{$lang}{jdk} = $jdk_elapsed;
+  $stats{unpatched}{$lang}{icu} = $icu_elapsed;
+  $stats{unpatched}{$lang}{keyword_analyzer} = $keyword_analyzer_elapsed;
+}
+close UNPATCHED;
+
+open PATCHED, "<$ARGV[1]" || die "ERROR opening '$ARGV[1]': $!";
+while (<PATCHED>) {
+  # ||Language||java.text||ICU4J||KeywordAnalyzer||ICU4J Improvement||
+  # |English|4.51s|2.47s|1.47s|204%|
+  next unless (/^\|([^|]+)\|([^|s]+)s\|([^|s]+)s\|([^|s]+)s/);
+  my ($lang, $jdk_elapsed, $icu_elapsed, $keyword_analyzer_elapsed)
+    = ($1, $2, $3, $4);
+  $stats{patched}{$lang}{jdk} = $jdk_elapsed;
+  $stats{patched}{$lang}{icu} = $icu_elapsed;
+  $stats{patched}{$lang}{keyword_analyzer} = $keyword_analyzer_elapsed;
+}
+close PATCHED;
+
+print "||Language||java.text improvement||ICU4J improvement||\n";
+for my $lang (sort keys %{$stats{unpatched}}) {
+  my $keyword_analyzer1 = $stats{unpatched}{$lang}{keyword_analyzer};
+  my $jdk1 = $stats{unpatched}{$lang}{jdk};
+  my $jdk_diff1 = $jdk1 - $keyword_analyzer1;
+  my $icu1 = $stats{unpatched}{$lang}{icu};
+  my $icu_diff1 = $icu1 - $keyword_analyzer1;
+
+  my $keyword_analyzer2 = $stats{patched}{$lang}{keyword_analyzer};
+  my $jdk2 = $stats{patched}{$lang}{jdk};
+  my $jdk_diff2 = $jdk2 - $keyword_analyzer2;
+  my $icu2 = $stats{patched}{$lang}{icu};
+  my $icu_diff2 = $icu2 - $keyword_analyzer2;
+
+  my $jdk_impr 
+    = int((1./$jdk_diff2 - 1./$jdk_diff1) / (1./$jdk_diff1) * 1000 + 5) / 10;
+  my $icu_impr
+    = int((1./$icu_diff2 - 1./$icu_diff1) / (1./$icu_diff1) * 1000 + 5) / 10;
+
+  printf "|$lang|%2.1f%%|%2.1f%%|\n", $jdk_impr, $icu_impr;
+}
diff --git a/modules/benchmark/scripts/compare.shingle.benchmark.tables.pl b/modules/benchmark/scripts/compare.shingle.benchmark.tables.pl
new file mode 100644
index 0000000..3af2c78
--- /dev/null
+++ b/modules/benchmark/scripts/compare.shingle.benchmark.tables.pl
@@ -0,0 +1,116 @@
+#!/usr/bin/perl
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+# 
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+# ------------------------------------------
+# compare.shingle.benchmark.jira.tables.pl
+#
+# Takes as cmdline parameters two JIRA-formatted benchmark results, as produced
+# by shingle.bm2jira.pl (located in the same directory as this script), and
+# outputs a third JIRA-formatted comparison table.
+#
+# The difference is calculated as a percentage:
+#
+#   100 * (unpatched-elapsed - patched-elapsed / patched-elapsed)
+#
+# where (un)patched-elapsed values have had the no-shingle-filter 
+# (StandardAnalyzer) elapsed time subtracted from them.
+#
+#
+# Example shingle.bm2jira.pl output:
+# ----------------------------------
+# JAVA:
+# java version "1.5.0_15"
+# Java(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_15-b04)
+# Java HotSpot(TM) 64-Bit Server VM (build 1.5.0_15-b04, mixed mode)
+#
+# OS:
+# cygwin
+# WinVistaService Pack 2
+# Service Pack 26060022202561
+#
+# ||Max Shingle Size||Unigrams?||Elapsed||
+# |1 (Unigrams)|yes|2.19s|
+# |2|no|4.74s|
+# |2|yes|4.90s|
+# |4|no|5.82s|
+# |4|yes|5.97s|
+
+use strict;
+use warnings;
+
+my $usage = "Usage: $0 <unpatched-file> <patched-file>\n";
+
+die $usage unless ($#ARGV == 1 && -f $ARGV[0] && -f $ARGV[1]);
+
+my %stats = ();
+
+open UNPATCHED, "<$ARGV[0]" || die "ERROR opening '$ARGV[0]': $!";
+my $table_encountered = 0;
+my $standard_analyzer_elapsed = 0;
+my %unpatched_stats = ();
+my %patched_stats = ();
+while (<UNPATCHED>) {
+  unless ($table_encountered) {
+    if (/\Q||Max Shingle Size||Unigrams?||Elapsed||\E/) {
+      $table_encountered = 1;
+    } else {
+      print;
+    }
+  } elsif (/\|([^|]+)\|([^|]+)\|([\d.]+)s\|/) {
+    my $max_shingle_size = $1;
+    my $output_unigrams = $2;
+    my $elapsed = $3;
+    if ($max_shingle_size =~ /Unigrams/) {
+      $standard_analyzer_elapsed = $elapsed;
+    } else {
+      $unpatched_stats{$max_shingle_size}{$output_unigrams} = $elapsed;
+    }
+  }
+}
+close UNPATCHED;
+
+open PATCHED, "<$ARGV[1]" || die "ERROR opening '$ARGV[1]': $!";
+while (<PATCHED>) {
+  if (/\|([^|]+)\|([^|]+)\|([\d.]+)s\|/) {
+    my $max_shingle_size = $1;
+    my $output_unigrams = $2;
+    my $elapsed = $3;
+    if ($max_shingle_size =~ /Unigrams/) {
+      $standard_analyzer_elapsed = $elapsed
+         if ($elapsed < $standard_analyzer_elapsed);
+    } else {
+      $patched_stats{$max_shingle_size}{$output_unigrams} = $elapsed;
+    }
+  }
+}
+close PATCHED;
+
+print "||Max Shingle Size||Unigrams?||Unpatched||Patched||StandardAnalyzer||Improvement||\n";
+for my $max_shingle_size (sort { $a <=> $b } keys %unpatched_stats) {
+  for my $output_unigrams (sort keys %{$unpatched_stats{$max_shingle_size}}) {
+    my $improvement 
+      = ( $unpatched_stats{$max_shingle_size}{$output_unigrams}
+        - $patched_stats{$max_shingle_size}{$output_unigrams})
+      / ( $patched_stats{$max_shingle_size}{$output_unigrams}
+        - $standard_analyzer_elapsed);
+    $improvement = int($improvement * 1000 + .5) / 10; # Round and truncate
+    printf "|$max_shingle_size|$output_unigrams"
+          ."|$unpatched_stats{$max_shingle_size}{$output_unigrams}s"
+          ."|$patched_stats{$max_shingle_size}{$output_unigrams}s"
+          ."|${standard_analyzer_elapsed}s|%2.1f%%|\n", $improvement;
+  }
+}
diff --git a/modules/benchmark/scripts/shingle.bm2jira.pl b/modules/benchmark/scripts/shingle.bm2jira.pl
new file mode 100644
index 0000000..ce6d193
--- /dev/null
+++ b/modules/benchmark/scripts/shingle.bm2jira.pl
@@ -0,0 +1,73 @@
+#!/usr/bin/perl
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+# 
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+# ----------
+# shingle.bm2jira.pl
+#
+# Converts Lucene contrib-benchmark output produced using the 
+# conf/shingle.alg file into a JIRA-formatted table.
+#
+
+use strict;
+use warnings;
+
+my %min_elapsed = ();
+
+#Operation           round  runCnt  recsPerRun  rec/s      elapsedSec  avgUsedMem  avgTotalMem
+#BigramsAndUnigrams  0      1       255691      21,147.22  12.09       15,501,840  35,061,760
+#BigramsOnly   -  -  0 -  - 1 -  -  127383   -  16,871.92  7.55    -   31,725,312  41,746,432
+#FourgramsAndUnigrams
+#FourgramsOnly
+#UnigramsOnly
+
+while (<>) {
+  if (/^((?:Uni|Bi|Four)grams\S+)[-\s]*([^\s{].*)/) {
+    my $operation = $1;
+    my $stats = $2;
+    my $max_shingle_size 
+    = ($operation =~ /^Bigrams/ ? 2 : $operation =~ /^Unigrams/ ? 1 : 4);
+    my $output_unigrams 
+      = ($operation =~ /(?:AndUnigrams|UnigramsOnly)$/ ? 'yes' : 'no'); 
+    my ($elapsed) = $stats =~ /(?:[\d,.]+[-\s]*){4}([.\d]+)/;
+    $min_elapsed{$max_shingle_size}{$output_unigrams} = $elapsed
+      unless (defined($min_elapsed{$max_shingle_size}{$output_unigrams})
+              && $elapsed >= $min_elapsed{$max_shingle_size}{$output_unigrams});
+  }
+}
+
+# Print out platform info
+print "JAVA:\n", `java -version 2>&1`, "\nOS:\n";
+if ($^O =~ /win/i) {
+  print "$^O\n";
+  eval {
+    require Win32;
+    print Win32::GetOSName(), "\n", Win32::GetOSVersion(), "\n";
+  };
+  die "Error loading Win32: $@" if ($@);
+} else {
+  print `uname -a 2>&1`;
+}
+
+print "\n||Max Shingle Size||Unigrams?||Elapsed||\n";
+
+for my $max_shingle_size (sort { $a <=> $b } keys %min_elapsed) {
+  for my $output_unigrams (sort keys %{$min_elapsed{$max_shingle_size}}) {
+    my $size = (1 == $max_shingle_size ? '1 (Unigrams)' : $max_shingle_size);   
+    printf "|$size|$output_unigrams|\%2.2fs|\n",
+           $min_elapsed{$max_shingle_size}{$output_unigrams};
+  }
+}
diff --git a/modules/benchmark/sortBench.py b/modules/benchmark/sortBench.py
new file mode 100644
index 0000000..f027bf2
--- /dev/null
+++ b/modules/benchmark/sortBench.py
@@ -0,0 +1,553 @@
+import types
+import re
+import time
+import os
+import shutil
+import sys
+import cPickle
+import datetime
+
+# TODO
+#   - build wiki/random index as needed (balanced or not, varying # segs, docs)
+#   - verify step
+#   - run searches
+#   - get all docs query in here
+
+if sys.platform.lower().find('darwin') != -1:
+  osName = 'osx'
+elif sys.platform.lower().find('win') != -1:
+  osName = 'windows'
+elif sys.platform.lower().find('linux') != -1:
+  osName = 'linux'
+else:
+  osName = 'unix'
+
+TRUNK_DIR = '/lucene/clean'
+FLEX_DIR = '/lucene/flex.branch'
+
+DEBUG = False
+
+# let shell find it:
+JAVA_COMMAND = 'java -Xms2048M -Xmx2048M -Xbatch -server'
+#JAVA_COMMAND = 'java -Xms1024M -Xmx1024M -Xbatch -server -XX:+AggressiveOpts -XX:CompileThreshold=100 -XX:+UseFastAccessorMethods'
+
+INDEX_NUM_THREADS = 1
+
+INDEX_NUM_DOCS = 5000000
+
+LOG_DIR = 'logs'
+
+DO_BALANCED = False
+
+if osName == 'osx':
+  WIKI_FILE = '/x/lucene/enwiki-20090724-pages-articles.xml.bz2'
+  INDEX_DIR_BASE = '/lucene'
+else:
+  WIKI_FILE = '/x/lucene/enwiki-20090724-pages-articles.xml.bz2'
+  INDEX_DIR_BASE = '/x/lucene'
+
+if DEBUG:
+  NUM_ROUND = 0
+else:
+  NUM_ROUND = 7
+
+if 0:
+  print 'compile...'
+  if '-nocompile' not in sys.argv:
+    if os.system('ant compile > compile.log 2>&1') != 0:
+      raise RuntimeError('compile failed (see compile.log)')
+
+BASE_SEARCH_ALG = '''
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+work.dir = $INDEX$
+search.num.hits = $NUM_HITS$
+query.maker=org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker
+file.query.maker.file = queries.txt
+print.hits.field = $PRINT_FIELD$
+log.queries=true
+log.step=100000
+
+$OPENREADER$
+{"XSearchWarm" $SEARCH$}
+
+# Turn off printing, after warming:
+SetProp(print.hits.field,)
+
+$ROUNDS$
+CloseReader 
+RepSumByPrefRound XSearch
+'''
+
+BASE_INDEX_ALG = '''
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+
+$OTHER$
+deletion.policy = org.apache.lucene.index.NoDeletionPolicy
+doc.tokenized = false
+doc.body.tokenized = true
+doc.stored = true
+doc.body.stored = false
+doc.term.vector = false
+log.step.AddDoc=10000
+
+directory=FSDirectory
+autocommit=false
+compound=false
+
+work.dir=$WORKDIR$
+
+{ "BuildIndex"
+  - CreateIndex
+  $INDEX_LINE$
+  - CommitIndex(dp0)
+  - CloseIndex
+  $DELETIONS$
+}
+
+RepSumByPrefRound BuildIndex
+'''
+
+class RunAlgs:
+
+  def __init__(self, resultsPrefix):
+    self.counter = 0
+    self.results = []
+    self.fOut = open('%s.txt' % resultsPrefix, 'wb')
+    
+  def makeIndex(self, label, dir, source, numDocs, balancedNumSegs=None, deletePcts=None):
+
+    if source not in ('wiki', 'random'):
+      raise RuntimeError('source must be wiki or random')
+
+    if dir is not None:
+      fullDir = '%s/contrib/benchmark' % dir
+      if DEBUG:
+        print '  chdir %s' % fullDir
+      os.chdir(fullDir)
+
+    indexName = '%s.%s.nd%gM' % (source, label, numDocs/1000000.0)
+    if balancedNumSegs is not None:
+      indexName += '_balanced%d' % balancedNumSegs
+    fullIndexPath = '%s/%s' % (INDEX_DIR_BASE, indexName)
+    
+    if os.path.exists(fullIndexPath):
+      print 'Index %s already exists...' % fullIndexPath
+      return indexName
+
+    print 'Now create index %s...' % fullIndexPath
+
+    s = BASE_INDEX_ALG
+
+    if source == 'wiki':
+      other = '''doc.index.props = true
+content.source=org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource
+docs.file=%s
+''' % WIKI_FILE
+      #addDoc = 'AddDoc(1024)'
+      addDoc = 'AddDoc'
+    else:
+      other = '''doc.index.props = true
+content.source=org.apache.lucene.benchmark.byTask.feeds.SortableSingleDocSource
+'''
+      addDoc = 'AddDoc'
+    if INDEX_NUM_THREADS > 1:
+      #other += 'doc.reuse.fields=false\n'
+      s = s.replace('$INDEX_LINE$', '[ { "AddDocs" %s > : %s } : %s' % \
+                    (addDoc, numDocs/INDEX_NUM_THREADS, INDEX_NUM_THREADS))
+    else:
+      s = s.replace('$INDEX_LINE$', '{ "AddDocs" %s > : %s' % \
+                    (addDoc, numDocs))
+
+    s = s.replace('$WORKDIR$', fullIndexPath)
+
+    if deletePcts is not None:
+      dp = '# Do deletions\n'
+      dp += 'OpenReader(false)\n'
+      for pct in deletePcts:
+        if pct != 0:
+          dp += 'DeleteByPercent(%g)\n' % pct
+          dp += 'CommitIndex(dp%g)\n' % pct
+      dp += 'CloseReader()\n'
+    else:
+      dp = ''
+
+    s = s.replace('$DELETIONS$', dp)
+
+    if balancedNumSegs is not None:
+      other += '''  merge.factor=1000
+  max.buffered=%d
+  ram.flush.mb=2000
+  ''' % (numDocs/balancedNumSegs)
+    else:
+      if source == 'random':
+        other += 'ram.flush.mb=1.0\n'
+      else:
+        other += 'ram.flush.mb=32.0\n'
+
+    s = s.replace('$OTHER$', other)
+
+    try:
+      self.runOne(dir, s, 'index_%s' % indexName, isIndex=True)
+    except:
+      if os.path.exists(fullIndexPath):
+        shutil.rmtree(fullIndexPath)
+      raise
+    return indexName
+    
+  def getLogPrefix(self, **dArgs):
+    l = dArgs.items()
+    l.sort()
+    s = '_'.join(['%s=%s' % tup for tup in l])
+    s = s.replace(' ', '_')
+    s = s.replace('"', '_')
+    return s
+             
+  def runOne(self, dir, alg, logFileName, expectedMaxDocs=None, expectedNumDocs=None, queries=None, verify=False, isIndex=False):
+
+    fullDir = '%s/contrib/benchmark' % dir
+    if DEBUG:
+      print '  chdir %s' % fullDir
+    os.chdir(fullDir)
+               
+    if queries is not None:
+      if type(queries) in types.StringTypes:
+        queries = [queries]
+      open('queries.txt', 'wb').write('\n'.join(queries))
+
+    if DEBUG:
+      algFile = 'tmp.alg'
+    else:
+      algFile = 'tmp.%s.alg' % os.getpid()
+    open(algFile, 'wb').write(alg)
+
+    fullLogFileName = '%s/contrib/benchmark/%s/%s' % (dir, LOG_DIR, logFileName)
+    print '  log: %s' % fullLogFileName
+    if not os.path.exists(LOG_DIR):
+      print '  mkdir %s' % LOG_DIR
+      os.makedirs(LOG_DIR)
+
+    command = '%s -classpath ../../build/classes/java:../../build/classes/demo:../../build/contrib/highlighter/classes/java:lib/commons-digester-1.7.jar:lib/commons-collections-3.1.jar:lib/commons-compress-1.0.jar:lib/commons-logging-1.0.4.jar:lib/commons-beanutils-1.7.0.jar:lib/xerces-2.10.0.jar:lib/xml-apis-2.10.0.jar:../../build/contrib/benchmark/classes/java org.apache.lucene.benchmark.byTask.Benchmark %s > "%s" 2>&1' % (JAVA_COMMAND, algFile, fullLogFileName)
+
+    if DEBUG:
+      print 'command=%s' % command
+      
+    try:
+      t0 = time.time()
+      if os.system(command) != 0:
+        raise RuntimeError('FAILED')
+      t1 = time.time()
+    finally:
+      if not DEBUG:
+        os.remove(algFile)
+
+    if isIndex:
+      s = open(fullLogFileName, 'rb').read()
+      if s.find('Exception in thread "') != -1 or s.find('at org.apache.lucene') != -1:
+        raise RuntimeError('alg hit exceptions')
+      return
+
+    else:
+
+      # Parse results:
+      bestQPS = None
+      count = 0
+      nhits = None
+      numDocs = None
+      maxDocs = None
+      warmTime = None
+      r = re.compile('^  ([0-9]+): (.*)$')
+      topN = []
+
+      for line in open(fullLogFileName, 'rb').readlines():
+        m = r.match(line.rstrip())
+        if m is not None:
+          topN.append(m.group(2))
+        if line.startswith('totalHits = '):
+          nhits = int(line[12:].strip())
+        if line.startswith('maxDoc()  = '):
+          maxDocs = int(line[12:].strip())
+        if line.startswith('numDocs() = '):
+          numDocs = int(line[12:].strip())
+        if line.startswith('XSearchWarm'):
+          v = line.strip().split()
+          warmTime = float(v[5])
+        if line.startswith('XSearchReal'):
+          v = line.strip().split()
+          # print len(v), v
+          upto = 0
+          i = 0
+          qps = None
+          while i < len(v):
+            if v[i] == '-':
+              i += 1
+              continue
+            else:
+              upto += 1
+              i += 1
+              if upto == 5:
+                qps = float(v[i-1].replace(',', ''))
+                break
+
+          if qps is None:
+            raise RuntimeError('did not find qps')
+
+          count += 1
+          if bestQPS is None or qps > bestQPS:
+            bestQPS = qps
+
+      if not verify:
+        if count != NUM_ROUND:
+          raise RuntimeError('did not find %s rounds (got %s)' % (NUM_ROUND, count))
+        if warmTime is None:
+          raise RuntimeError('did not find warm time')
+      else:
+        bestQPS = 1.0
+        warmTime = None
+
+      if nhits is None:
+        raise RuntimeError('did not see "totalHits = XXX"')
+
+      if maxDocs is None:
+        raise RuntimeError('did not see "maxDoc() = XXX"')
+
+      if maxDocs != expectedMaxDocs:
+        raise RuntimeError('maxDocs() mismatch: expected %s but got %s' % (expectedMaxDocs, maxDocs))
+
+      if numDocs is None:
+        raise RuntimeError('did not see "numDocs() = XXX"')
+
+      if numDocs != expectedNumDocs:
+        raise RuntimeError('numDocs() mismatch: expected %s but got %s' % (expectedNumDocs, numDocs))
+      
+      return nhits, warmTime, bestQPS, topN
+
+  def getAlg(self, indexPath, searchTask, numHits, deletes=None, verify=False, printField=''):
+
+    s = BASE_SEARCH_ALG
+    s = s.replace('$PRINT_FIELD$', 'doctitle')
+
+    if not verify:
+      s = s.replace('$ROUNDS$',
+  '''                
+  { "Rounds"
+    { "Run"
+      { "TestSearchSpeed"
+        { "XSearchReal" $SEARCH$ > : 3.0s
+      }
+      NewRound
+    } : %d
+  } 
+  ''' % NUM_ROUND)
+    else:
+      s = s.replace('$ROUNDS$', '')
+
+    if deletes is None:
+      s = s.replace('$OPENREADER$', 'OpenReader')
+    else:
+      s = s.replace('$OPENREADER$', 'OpenReader(true,dp%g)' % deletes)
+    s = s.replace('$INDEX$', indexPath)
+    s = s.replace('$SEARCH$', searchTask)
+    s = s.replace('$NUM_HITS$', str(numHits))
+    
+    return s
+
+  def compare(self, baseline, new, *params):
+
+    if new[0] != baseline[0]:
+      raise RuntimeError('baseline found %d hits but new found %d hits' % (baseline[0], new[0]))
+
+    qpsOld = baseline[2]
+    qpsNew = new[2]
+    pct = 100.0*(qpsNew-qpsOld)/qpsOld
+    print '  diff: %.1f%%' % pct
+    self.results.append((qpsOld, qpsNew, params))
+
+    self.fOut.write('|%s|%.2f|%.2f|%.1f%%|\n' % \
+                    ('|'.join(str(x) for x in params),
+                     qpsOld, qpsNew, pct))
+    self.fOut.flush()
+
+  def save(self, name):
+    f = open('%s.pk' % name, 'wb')
+    cPickle.dump(self.results, f)
+    f.close()
+
+def verify(r1, r2):
+  if r1[0] != r2[0]:
+    raise RuntimeError('different total hits: %s vs %s' % (r1[0], r2[0]))
+                       
+  h1 = r1[3]
+  h2 = r2[3]
+  if len(h1) != len(h2):
+    raise RuntimeError('different number of results')
+  else:
+    for i in range(len(h1)):
+      s1 = h1[i].replace('score=NaN', 'score=na').replace('score=0.0', 'score=na')
+      s2 = h2[i].replace('score=NaN', 'score=na').replace('score=0.0', 'score=na')
+      if s1 != s2:
+        raise RuntimeError('hit %s differs: %s vs %s' % (i, s1 ,s2))
+
+def usage():
+  print
+  print 'Usage: python -u %s -run <name> | -report <name>' % sys.argv[0]
+  print
+  print '  -run <name> runs all tests, saving results to file <name>.pk'
+  print '  -report <name> opens <name>.pk and prints Jira table'
+  print '  -verify confirm old & new produce identical results'
+  print
+  sys.exit(1)
+
+def main():
+
+  if not os.path.exists(LOG_DIR):
+    os.makedirs(LOG_DIR)
+
+  if '-run' in sys.argv:
+    i = sys.argv.index('-run')
+    mode = 'run'
+    if i < len(sys.argv)-1:
+      name = sys.argv[1+i]
+    else:
+      usage()
+  elif '-report' in sys.argv:
+    i = sys.argv.index('-report')
+    mode = 'report'
+    if i < len(sys.argv)-1:
+      name = sys.argv[1+i]
+    else:
+      usage()
+  elif '-verify' in sys.argv:
+    mode = 'verify'
+    name = None
+  else:
+    usage()
+
+  if mode in ('run', 'verify'):
+    run(mode, name)
+  else:
+    report(name)
+
+def report(name):
+
+  print '||Query||Deletes %||Tot hits||QPS old||QPS new||Pct change||'
+
+  results = cPickle.load(open('%s.pk' % name))
+  for qpsOld, qpsNew, params in results:
+    pct = 100.0*(qpsNew-qpsOld)/qpsOld
+    if pct < 0.0:
+      c = 'red'
+    else:
+      c = 'green'
+
+    params = list(params)
+
+    query = params[0]
+    if query == '*:*':
+      query = '<all>'
+    params[0] = query
+    
+    pct = '{color:%s}%.1f%%{color}' % (c, pct)
+    print '|%s|%.2f|%.2f|%s|' % \
+          ('|'.join(str(x) for x in params),
+           qpsOld, qpsNew, pct)
+
+def run(mode, name):
+
+  for dir in (TRUNK_DIR, FLEX_DIR):
+    dir = '%s/contrib/benchmark' % dir
+    print '"ant compile" in %s...' % dir
+    os.chdir(dir)
+    if os.system('ant compile') != 0:
+      raise RuntimeError('ant compile failed')
+  
+  r = RunAlgs(name)
+
+  if not os.path.exists(WIKI_FILE):
+    print
+    print 'ERROR: wiki source file "%s" does not exist' % WIKI_FILE
+    print
+    sys.exit(1)
+
+  print
+  print 'JAVA:\n%s' % os.popen('java -version 2>&1').read()
+    
+  print
+  if osName != 'windows':
+    print 'OS:\n%s' % os.popen('uname -a 2>&1').read()
+  else:
+    print 'OS:\n%s' % sys.platform
+
+  deletePcts = (0.0, 0.1, 1.0, 10)
+
+  indexes = {}
+  for rev in ('baseline', 'flex'):
+    if rev == 'baseline':
+      dir = TRUNK_DIR
+    else:
+      dir = FLEX_DIR
+    source = 'wiki'
+    indexes[rev] = r.makeIndex(rev, dir, source, INDEX_NUM_DOCS, deletePcts=deletePcts)
+
+  doVerify = mode == 'verify'
+  source = 'wiki'
+  numHits = 10
+
+  queries = (
+    'body:[tec TO tet]',
+    'real*',
+    '1',
+    '2',
+    '+1 +2',
+    '+1 -2',
+    '1 2 3 -4',
+    '"world economy"')
+
+  for query in queries:
+
+    for deletePct in deletePcts:
+
+      print '\nRUN: query=%s deletes=%g%% nhits=%d' % \
+            (query, deletePct, numHits)
+
+      maxDocs = INDEX_NUM_DOCS
+      numDocs = int(INDEX_NUM_DOCS * (1.0-deletePct/100.))
+
+      prefix = r.getLogPrefix(query=query, deletePct=deletePct)
+      indexPath = '%s/%s' % (INDEX_DIR_BASE, indexes['baseline'])
+
+      # baseline (trunk)
+      s = r.getAlg(indexPath,
+                   'Search',
+                   numHits,
+                   deletes=deletePct,
+                   verify=doVerify,
+                   printField='doctitle')
+      baseline = r.runOne(TRUNK_DIR, s, 'baseline_%s' % prefix, maxDocs, numDocs, query, verify=doVerify)
+
+      # flex
+      indexPath = '%s/%s' % (INDEX_DIR_BASE, indexes['flex'])
+      s = r.getAlg(indexPath,
+                   'Search',
+                   numHits,
+                   deletes=deletePct,
+                   verify=doVerify,
+                   printField='doctitle')
+      flex = r.runOne(FLEX_DIR, s, 'flex_%s' % prefix, maxDocs, numDocs, query, verify=doVerify)
+
+      print '  %d hits' % flex[0]
+
+      verify(baseline, flex)
+
+      if mode == 'run' and not DEBUG:
+        r.compare(baseline, flex,
+                  query, deletePct, baseline[0])
+        r.save(name)
+
+def cleanScores(l):
+  for i in range(len(l)):
+    pos = l[i].find(' score=')
+    l[i] = l[i][:pos].strip()
+
+if __name__ == '__main__':
+  main()
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/Constants.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/Constants.java
new file mode 100644
index 0000000..4292632
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/Constants.java
@@ -0,0 +1,33 @@
+package org.apache.lucene.benchmark;
+/**
+ * Copyright 2005 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+
+/**
+ *
+ *
+ **/
+public class Constants
+{
+    public static final int DEFAULT_RUN_COUNT = 5;
+    public static final int DEFAULT_SCALE_UP = 5;
+    public static final int DEFAULT_LOG_STEP = 1000;
+
+    public static Boolean[] BOOLEANS = new Boolean[] { Boolean.FALSE, Boolean.TRUE };
+
+    public static final int DEFAULT_MAXIMUM_DOCUMENTS = Integer.MAX_VALUE;
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/Benchmark.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/Benchmark.java
new file mode 100644
index 0000000..9b85743
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/Benchmark.java
@@ -0,0 +1,135 @@
+package org.apache.lucene.benchmark.byTask;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.io.FileReader;
+import java.io.Reader;
+
+import org.apache.lucene.benchmark.byTask.utils.Algorithm;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+
+
+/**
+ * Run the benchmark algorithm.
+ * <p>Usage: java Benchmark  algorithm-file
+ * <ol>
+ * <li>Read algorithm.</li>
+ * <li> Run the algorithm.</li>
+ * </ol>
+ * Things to be added/fixed in "Benchmarking by tasks":
+ * <ol>
+ * <li>TODO - report into Excel and/or graphed view.</li>
+ * <li>TODO - perf comparison between Lucene releases over the years.</li>
+ * <li>TODO - perf report adequate to include in Lucene nightly build site? (so we can easily track performance changes.)</li>
+ * <li>TODO - add overall time control for repeated execution (vs. current by-count only).</li>
+ * <li>TODO - query maker that is based on index statistics.</li>
+ * </ol>
+ */
+public class Benchmark {
+
+  private PerfRunData runData;
+  private Algorithm algorithm;
+  private boolean executed;
+  
+  public Benchmark (Reader algReader) throws Exception {
+    // prepare run data
+    try {
+      runData = new PerfRunData(new Config(algReader));
+    } catch (Exception e) {
+      e.printStackTrace();
+      throw new Exception("Error: cannot init PerfRunData!",e);
+    }
+    
+    // parse algorithm
+    try {
+      algorithm = new Algorithm(runData);
+    } catch (Exception e) {
+      throw new Exception("Error: cannot understand algorithm!",e);
+    }
+  }
+  
+  public synchronized void  execute() throws Exception {
+    if (executed) {
+      throw new IllegalStateException("Benchmark was already executed");
+    }
+    executed = true;
+    runData.setStartTimeMillis();
+    algorithm.execute();
+  }
+  
+  /**
+   * Run the benchmark algorithm.
+   * @param args benchmark config and algorithm files
+   */
+  public static void main(String[] args) {
+    // verify command line args
+    if (args.length < 1) {
+      System.err.println("Usage: java Benchmark <algorithm file>");
+      System.exit(1);
+    }
+    
+    // verify input files 
+    File algFile = new File(args[0]);
+    if (!algFile.exists() || !algFile.isFile() || !algFile.canRead()) {
+      System.err.println("cannot find/read algorithm file: "+algFile.getAbsolutePath()); 
+      System.exit(1);
+    }
+    
+    System.out.println("Running algorithm from: "+algFile.getAbsolutePath());
+    
+    Benchmark benchmark = null;
+    try {
+      benchmark = new Benchmark(new FileReader(algFile));
+    } catch (Exception e) {
+      e.printStackTrace();
+      System.exit(1);
+    }
+
+    System.out.println("------------> algorithm:");
+    System.out.println(benchmark.getAlgorithm().toString());
+
+    // execute
+    try {
+      benchmark.execute();
+    } catch (Exception e) {
+      System.err.println("Error: cannot execute the algorithm! "+e.getMessage());
+      e.printStackTrace();
+    }
+
+    System.out.println("####################");
+    System.out.println("###  D O N E !!! ###");
+    System.out.println("####################");
+
+  }
+
+  /**
+   * @return Returns the algorithm.
+   */
+  public Algorithm getAlgorithm() {
+    return algorithm;
+  }
+
+  /**
+   * @return Returns the runData.
+   */
+  public PerfRunData getRunData() {
+    return runData;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/PerfRunData.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/PerfRunData.java
new file mode 100644
index 0000000..64f5731
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/PerfRunData.java
@@ -0,0 +1,297 @@
+package org.apache.lucene.benchmark.byTask;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Locale;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
+import org.apache.lucene.benchmark.byTask.feeds.QueryMaker;
+import org.apache.lucene.benchmark.byTask.stats.Points;
+import org.apache.lucene.benchmark.byTask.tasks.ReadTask;
+import org.apache.lucene.benchmark.byTask.tasks.SearchTask;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.benchmark.byTask.utils.FileUtils;
+import org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.store.RAMDirectory;
+
+/**
+ * Data maintained by a performance test run.
+ * <p>
+ * Data includes:
+ * <ul>
+ *  <li>Configuration.
+ *  <li>Directory, Writer, Reader.
+ *  <li>Docmaker and a few instances of QueryMaker.
+ *  <li>Analyzer.
+ *  <li>Statistics data which updated during the run.
+ * </ul>
+ * Config properties: work.dir=&lt;path to root of docs and index dirs| Default: work&gt;
+ * </ul>
+ */
+public class PerfRunData {
+
+  private Points points;
+  
+  // objects used during performance test run
+  // directory, analyzer, docMaker - created at startup.
+  // reader, writer, searcher - maintained by basic tasks. 
+  private Directory directory;
+  private Analyzer analyzer;
+  private DocMaker docMaker;
+  private Locale locale;
+  
+  // we use separate (identical) instances for each "read" task type, so each can iterate the quries separately.
+  private HashMap<Class<? extends ReadTask>,QueryMaker> readTaskQueryMaker;
+  private Class<? extends QueryMaker> qmkrClass;
+
+  private IndexReader indexReader;
+  private IndexSearcher indexSearcher;
+  private IndexWriter indexWriter;
+  private Config config;
+  private long startTimeMillis;
+  
+  // constructor
+  public PerfRunData (Config config) throws Exception {
+    this.config = config;
+    // analyzer (default is standard analyzer)
+    analyzer = NewAnalyzerTask.createAnalyzer(config.get("analyzer",
+        "org.apache.lucene.analysis.standard.StandardAnalyzer"));
+    // doc maker
+    docMaker = Class.forName(config.get("doc.maker",
+        "org.apache.lucene.benchmark.byTask.feeds.DocMaker")).asSubclass(DocMaker.class).newInstance();
+    docMaker.setConfig(config);
+    // query makers
+    readTaskQueryMaker = new HashMap<Class<? extends ReadTask>,QueryMaker>();
+    qmkrClass = Class.forName(config.get("query.maker","org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker")).asSubclass(QueryMaker.class);
+
+    // index stuff
+    reinit(false);
+    
+    // statistic points
+    points = new Points(config);
+    
+    if (Boolean.valueOf(config.get("log.queries","false")).booleanValue()) {
+      System.out.println("------------> queries:");
+      System.out.println(getQueryMaker(new SearchTask(this)).printQueries());
+    }
+  }
+
+  // clean old stuff, reopen 
+  public void reinit(boolean eraseIndex) throws Exception {
+
+    // cleanup index
+    if (indexWriter!=null) {
+      indexWriter.close();
+      indexWriter = null;
+    }
+    if (indexReader!=null) {
+      indexReader.close();
+      indexReader = null;
+    }
+    if (directory!=null) {
+      directory.close();
+    }
+    
+    // directory (default is ram-dir).
+    if ("FSDirectory".equals(config.get("directory","RAMDirectory"))) {
+      File workDir = new File(config.get("work.dir","work"));
+      File indexDir = new File(workDir,"index");
+      if (eraseIndex && indexDir.exists()) {
+        FileUtils.fullyDelete(indexDir);
+      }
+      indexDir.mkdirs();
+      directory = FSDirectory.open(indexDir);
+    } else {
+      directory = new RAMDirectory();
+    }
+
+    // inputs
+    resetInputs();
+    
+    // release unused stuff
+    System.runFinalization();
+    System.gc();
+
+    // Re-init clock
+    setStartTimeMillis();
+  }
+  
+  public long setStartTimeMillis() {
+    startTimeMillis = System.currentTimeMillis();
+    return startTimeMillis;
+  }
+
+  /**
+   * @return Start time in milliseconds
+   */
+  public long getStartTimeMillis() {
+    return startTimeMillis;
+  }
+
+  /**
+   * @return Returns the points.
+   */
+  public Points getPoints() {
+    return points;
+  }
+
+  /**
+   * @return Returns the directory.
+   */
+  public Directory getDirectory() {
+    return directory;
+  }
+
+  /**
+   * @param directory The directory to set.
+   */
+  public void setDirectory(Directory directory) {
+    this.directory = directory;
+  }
+
+  /**
+   * @return Returns the indexReader.  NOTE: this returns a
+   * reference.  You must call IndexReader.decRef() when
+   * you're done.
+   */
+  public synchronized IndexReader getIndexReader() {
+    if (indexReader != null) {
+      indexReader.incRef();
+    }
+    return indexReader;
+  }
+
+  /**
+   * @return Returns the indexSearcher.  NOTE: this returns
+   * a reference to the underlying IndexReader.  You must
+   * call IndexReader.decRef() when you're done.
+   */
+  public synchronized IndexSearcher getIndexSearcher() {
+    if (indexReader != null) {
+      indexReader.incRef();
+    }
+    return indexSearcher;
+  }
+
+  /**
+   * @param indexReader The indexReader to set.
+   */
+  public synchronized void setIndexReader(IndexReader indexReader) throws IOException {
+    if (this.indexReader != null) {
+      // Release current IR
+      this.indexReader.decRef();
+    }
+    this.indexReader = indexReader;
+    if (indexReader != null) {
+      // Hold reference to new IR
+      indexReader.incRef();
+      indexSearcher = new IndexSearcher(indexReader);
+    } else {
+      indexSearcher = null;
+    }
+  }
+
+  /**
+   * @return Returns the indexWriter.
+   */
+  public IndexWriter getIndexWriter() {
+    return indexWriter;
+  }
+
+  /**
+   * @param indexWriter The indexWriter to set.
+   */
+  public void setIndexWriter(IndexWriter indexWriter) {
+    this.indexWriter = indexWriter;
+  }
+
+  /**
+   * @return Returns the anlyzer.
+   */
+  public Analyzer getAnalyzer() {
+    return analyzer;
+  }
+
+
+  public void setAnalyzer(Analyzer analyzer) {
+    this.analyzer = analyzer;
+  }
+
+  /** Returns the docMaker. */
+  public DocMaker getDocMaker() {
+    return docMaker;
+  }
+
+  /**
+   * @return the locale
+   */
+  public Locale getLocale() {
+    return locale;
+  }
+
+  /**
+   * @param locale the locale to set
+   */
+  public void setLocale(Locale locale) {
+    this.locale = locale;
+  }
+
+  /**
+   * @return Returns the config.
+   */
+  public Config getConfig() {
+    return config;
+  }
+
+  public void resetInputs() throws IOException {
+    docMaker.resetInputs();
+    for (final QueryMaker queryMaker : readTaskQueryMaker.values()) {
+      queryMaker.resetInputs();
+    }
+  }
+
+  /**
+   * @return Returns the queryMaker by read task type (class)
+   */
+  synchronized public QueryMaker getQueryMaker(ReadTask readTask) {
+    // mapping the query maker by task class allows extending/adding new search/read tasks
+    // without needing to modify this class.
+    Class<? extends ReadTask> readTaskClass = readTask.getClass();
+    QueryMaker qm = readTaskQueryMaker.get(readTaskClass);
+    if (qm == null) {
+      try {
+        qm = qmkrClass.newInstance();
+        qm.setConfig(config);
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
+      readTaskQueryMaker.put(readTaskClass,qm);
+    }
+    return qm;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/AbstractQueryMaker.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/AbstractQueryMaker.java
new file mode 100644
index 0000000..a9c1c0d
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/AbstractQueryMaker.java
@@ -0,0 +1,72 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+/**
+ * Copyright 2005 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.search.Query;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+
+/**
+ * Abstract base query maker. 
+ * Each query maker should just implement the {@link #prepareQueries()} method.
+ **/
+public abstract class AbstractQueryMaker implements QueryMaker {
+
+  protected int qnum = 0;
+  protected Query[] queries;
+  protected Config config;
+
+  public void resetInputs() {
+    qnum = 0;
+  }
+
+  protected abstract Query[] prepareQueries() throws Exception;
+
+  public void setConfig(Config config) throws Exception {
+    this.config = config;
+    queries = prepareQueries();
+  }
+
+  public String printQueries() {
+    String newline = System.getProperty("line.separator");
+    StringBuilder sb = new StringBuilder();
+    if (queries != null) {
+      for (int i = 0; i < queries.length; i++) {
+        sb.append(i+". "+ queries[i].getClass().getSimpleName()+" - "+queries[i].toString());
+        sb.append(newline);
+      }
+    }
+    return sb.toString();
+  }
+
+  public Query makeQuery() throws Exception {
+    return queries[nextQnum()];
+  }
+  
+  // return next qnum
+  protected synchronized int nextQnum() {
+    int res = qnum;
+    qnum = (qnum+1) % queries.length;
+    return res;
+  }
+
+  /*
+  *  (non-Javadoc)
+  * @see org.apache.lucene.benchmark.byTask.feeds.QueryMaker#makeQuery(int)
+  */
+  public Query makeQuery(int size) throws Exception {
+    throw new Exception(this+".makeQuery(int size) is not supported!");
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ContentSource.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ContentSource.java
new file mode 100644
index 0000000..817e57d
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ContentSource.java
@@ -0,0 +1,206 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedInputStream;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.commons.compress.compressors.CompressorException;
+import org.apache.commons.compress.compressors.CompressorStreamFactory;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+
+/**
+ * Represents content from a specified source, such as TREC, Reuters etc. A
+ * {@link ContentSource} is responsible for creating {@link DocData} objects for
+ * its documents to be consumed by {@link DocMaker}. It also keeps track
+ * of various statistics, such as how many documents were generated, size in
+ * bytes etc.
+ * <p>
+ * Supports the following configuration parameters:
+ * <ul>
+ * <li><b>content.source.forever</b> - specifies whether to generate documents
+ * forever (<b>default=true</b>).
+ * <li><b>content.source.verbose</b> - specifies whether messages should be
+ * output by the content source (<b>default=false</b>).
+ * <li><b>content.source.encoding</b> - specifies which encoding to use when
+ * reading the files of that content source. Certain implementations may define
+ * a default value if this parameter is not specified. (<b>default=null</b>).
+ * <li><b>content.source.log.step</b> - specifies for how many documents a
+ * message should be logged. If set to 0 it means no logging should occur.
+ * <b>NOTE:</b> if verbose is set to false, logging should not occur even if
+ * logStep is not 0 (<b>default=0</b>).
+ * </ul>
+ */
+public abstract class ContentSource {
+  
+  private static final int BZIP = 0;
+  private static final int OTHER = 1;
+  private static final Map<String,Integer> extensionToType = new HashMap<String,Integer>();
+  static {
+    extensionToType.put(".bz2", Integer.valueOf(BZIP));
+    extensionToType.put(".bzip", Integer.valueOf(BZIP));
+  }
+  
+  protected static final int BUFFER_SIZE = 1 << 16; // 64K
+
+  private long bytesCount;
+  private long totalBytesCount;
+  private int docsCount;
+  private int totalDocsCount;
+  private Config config;
+
+  protected boolean forever;
+  protected int logStep;
+  protected boolean verbose;
+  protected String encoding;
+  
+  private CompressorStreamFactory csFactory = new CompressorStreamFactory();
+
+  protected final synchronized void addBytes(long numBytes) {
+    bytesCount += numBytes;
+    totalBytesCount += numBytes;
+  }
+  
+  protected final synchronized void addDoc() {
+    ++docsCount;
+    ++totalDocsCount;
+  }
+
+  /**
+   * A convenience method for collecting all the files of a content source from
+   * a given directory. The collected {@link File} instances are stored in the
+   * given <code>files</code>.
+   */
+  protected final void collectFiles(File dir, ArrayList<File> files) {
+    if (!dir.canRead()) {
+      return;
+    }
+    
+    File[] dirFiles = dir.listFiles();
+    Arrays.sort(dirFiles);
+    for (int i = 0; i < dirFiles.length; i++) {
+      File file = dirFiles[i];
+      if (file.isDirectory()) {
+        collectFiles(file, files);
+      } else if (file.canRead()) {
+        files.add(file);
+      }
+    }
+  }
+
+  /**
+   * Returns an {@link InputStream} over the requested file. This method
+   * attempts to identify the appropriate {@link InputStream} instance to return
+   * based on the file name (e.g., if it ends with .bz2 or .bzip, return a
+   * 'bzip' {@link InputStream}).
+   */
+  protected InputStream getInputStream(File file) throws IOException {
+    // First, create a FileInputStream, as this will be required by all types.
+    // Wrap with BufferedInputStream for better performance
+    InputStream is = new BufferedInputStream(new FileInputStream(file), BUFFER_SIZE);
+    
+    String fileName = file.getName();
+    int idx = fileName.lastIndexOf('.');
+    int type = OTHER;
+    if (idx != -1) {
+      Integer typeInt = extensionToType.get(fileName.substring(idx));
+      if (typeInt != null) {
+        type = typeInt.intValue();
+      }
+    }
+    switch (type) {
+      case BZIP:
+        try {
+          // According to BZip2CompressorInputStream's code, it reads the first 
+          // two file header chars ('B' and 'Z'). It is important to wrap the
+          // underlying input stream with a buffered one since
+          // Bzip2CompressorInputStream uses the read() method exclusively.
+          is = csFactory.createCompressorInputStream("bzip2", is);
+        } catch (CompressorException e) {
+          IOException ioe = new IOException(e.getMessage());
+          ioe.initCause(e);
+          throw ioe;
+        }
+        break;
+      default: // Do nothing, stay with FileInputStream
+    }
+    
+    return is;
+  }
+  
+  /**
+   * Returns true whether it's time to log a message (depending on verbose and
+   * the number of documents generated).
+   */
+  protected final boolean shouldLog() {
+    return verbose && logStep > 0 && docsCount % logStep == 0;
+  }
+
+  /** Called when reading from this content source is no longer required. */
+  public abstract void close() throws IOException;
+  
+  /** Returns the number of bytes generated since last reset. */
+  public final long getBytesCount() { return bytesCount; }
+
+  /** Returns the number of generated documents since last reset. */
+  public final int getDocsCount() { return docsCount; }
+  
+  public final Config getConfig() { return config; }
+
+  /** Returns the next {@link DocData} from the content source. */
+  public abstract DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException;
+
+  /** Returns the total number of bytes that were generated by this source. */ 
+  public final long getTotalBytesCount() { return totalBytesCount; }
+
+  /** Returns the total number of generated documents. */
+  public final int getTotalDocsCount() { return totalDocsCount; }
+
+  /**
+   * Resets the input for this content source, so that the test would behave as
+   * if it was just started, input-wise.
+   * <p>
+   * <b>NOTE:</b> the default implementation resets the number of bytes and
+   * documents generated since the last reset, so it's important to call
+   * super.resetInputs in case you override this method.
+   */
+  public void resetInputs() throws IOException {
+    bytesCount = 0;
+    docsCount = 0;
+  }
+
+  /**
+   * Sets the {@link Config} for this content source. If you override this
+   * method, you must call super.setConfig.
+   */
+  public void setConfig(Config config) {
+    this.config = config;
+    forever = config.get("content.source.forever", true);
+    logStep = config.get("content.source.log.step", 0);
+    verbose = config.get("content.source.verbose", false);
+    encoding = config.get("content.source.encoding", null);
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DemoHTMLParser.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DemoHTMLParser.java
new file mode 100755
index 0000000..d57777a
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DemoHTMLParser.java
@@ -0,0 +1,69 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.Reader;
+import java.text.DateFormat;
+import java.text.ParseException;
+import java.util.Date;
+import java.util.Properties;
+
+/**
+ * HTML Parser that is based on Lucene's demo HTML parser.
+ */
+public class DemoHTMLParser implements org.apache.lucene.benchmark.byTask.feeds.HTMLParser {
+
+  public DocData parse(DocData docData, String name, Date date, Reader reader, DateFormat dateFormat) throws IOException, InterruptedException {
+    org.apache.lucene.demo.html.HTMLParser p = new org.apache.lucene.demo.html.HTMLParser(reader);
+    
+    // title
+    String title = p.getTitle();
+    // properties 
+    Properties props = p.getMetaTags(); 
+    // body
+    Reader r = p.getReader();
+    char c[] = new char[1024];
+    StringBuilder bodyBuf = new StringBuilder();
+    int n;
+    while ((n = r.read(c)) >= 0) {
+      if (n>0) {
+        bodyBuf.append(c,0,n);
+      }
+    }
+    r.close();
+    if (date == null && props.getProperty("date")!=null) {
+      try {
+        date = dateFormat.parse(props.getProperty("date").trim());
+      } catch (ParseException e) {
+        // do not fail test just because a date could not be parsed
+        System.out.println("ignoring date parse exception (assigning 'now') for: "+props.getProperty("date"));
+        date = new Date(); // now 
+      }
+    }
+    
+    docData.clear();
+    docData.setName(name);
+    docData.setBody(bodyBuf.toString());
+    docData.setTitle(title);
+    docData.setProps(props);
+    docData.setDate(date);
+    return docData;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DirContentSource.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DirContentSource.java
new file mode 100644
index 0000000..66dcac8
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DirContentSource.java
@@ -0,0 +1,247 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.utils.Config;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileFilter;
+import java.io.FileReader;
+import java.io.IOException;
+import java.text.DateFormat;
+import java.text.ParsePosition;
+import java.text.SimpleDateFormat;
+import java.util.Arrays;
+import java.util.Date;
+import java.util.Locale;
+import java.util.Stack;
+
+/**
+ * A {@link ContentSource} using the Dir collection for its input. Supports
+ * the following configuration parameters (on top of {@link ContentSource}):
+ * <ul>
+ * <li><b>work.dir</b> - specifies the working directory. Required if "docs.dir"
+ * denotes a relative path (<b>default=work</b>).
+ * <li><b>docs.dir</b> - specifies the directory the Dir collection. Can be set
+ * to a relative path if "work.dir" is also specified (<b>default=dir-out</b>).
+ * </ul>
+ */
+public class DirContentSource extends ContentSource {
+
+  private static final class DateFormatInfo {
+    DateFormat df;
+    ParsePosition pos;
+  }
+  
+  public static class Iterator implements java.util.Iterator<File> {
+
+    static class Comparator implements java.util.Comparator<File> {
+      public int compare(File _a, File _b) {
+        String a = _a.toString();
+        String b = _b.toString();
+        int diff = a.length() - b.length();
+
+        if (diff > 0) {
+          while (diff-- > 0) {
+            b = "0" + b;
+          }
+        } else if (diff < 0) {
+          diff = -diff;
+          while (diff-- > 0) {
+            a = "0" + a;
+          }
+        }
+
+        /* note it's reversed because we're going to push,
+           which reverses again */
+        return b.compareTo(a);
+      }
+    }
+
+    int count = 0;
+
+    Stack<File> stack = new Stack<File>();
+
+    /* this seems silly ... there must be a better way ...
+       not that this is good, but can it matter? */
+
+    Comparator c = new Comparator();
+
+    public Iterator(File f) {
+      push(f);
+    }
+
+    void find() {
+      if (stack.empty()) {
+        return;
+      }
+      if (!(stack.peek()).isDirectory()) {
+        return;
+      }
+      File f = stack.pop();
+      push(f);
+    }
+
+    void push(File f) {
+      push(f.listFiles(new FileFilter() {
+
+        public boolean accept(File file) {
+          return file.isDirectory();
+        }
+      }));
+      push(f.listFiles(new FileFilter() {
+
+        public boolean accept(File file) {
+          return file.getName().endsWith(".txt");
+        }
+      }));
+      find();
+    }
+
+    void push(File[] files) {
+      Arrays.sort(files, c);
+      for(int i = 0; i < files.length; i++) {
+        // System.err.println("push " + files[i]);
+        stack.push(files[i]);
+      }
+    }
+
+    public int getCount(){
+      return count;
+    }
+
+    public boolean hasNext() {
+      return stack.size() > 0;
+    }
+    
+    public File next() {
+      assert hasNext();
+      count++;
+      File object = stack.pop();
+      // System.err.println("pop " + object);
+      find();
+      return object;
+    }
+
+    public void remove() {
+      throw new RuntimeException("cannot");
+    }
+
+  }
+  
+  private ThreadLocal<DateFormatInfo> dateFormat = new ThreadLocal<DateFormatInfo>();
+  private File dataDir = null;
+  private int iteration = 0;
+  private Iterator inputFiles = null;
+
+  // get/initiate a thread-local simple date format (must do so 
+  // because SimpleDateFormat is not thread-safe).
+  private DateFormatInfo getDateFormatInfo() {
+    DateFormatInfo dfi = dateFormat.get();
+    if (dfi == null) {
+      dfi = new DateFormatInfo();
+      dfi.pos = new ParsePosition(0);
+      // date format: 30-MAR-1987 14:22:36.87
+      dfi.df = new SimpleDateFormat("dd-MMM-yyyy kk:mm:ss.SSS", Locale.US);
+      dfi.df.setLenient(true);
+      dateFormat.set(dfi);
+    }
+    return dfi;
+  }
+  
+  private Date parseDate(String dateStr) {
+    DateFormatInfo dfi = getDateFormatInfo();
+    dfi.pos.setIndex(0);
+    dfi.pos.setErrorIndex(-1);
+    return dfi.df.parse(dateStr.trim(), dfi.pos);
+  }
+
+  @Override
+  public void close() throws IOException {
+    inputFiles = null;
+  }
+  
+  @Override
+  public DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException {
+    File f = null;
+    String name = null;
+    synchronized (this) {
+      if (!inputFiles.hasNext()) { 
+        // exhausted files, start a new round, unless forever set to false.
+        if (!forever) {
+          throw new NoMoreDataException();
+        }
+        inputFiles = new Iterator(dataDir);
+        iteration++;
+      }
+      f = inputFiles.next();
+      // System.err.println(f);
+      name = f.getCanonicalPath()+"_"+iteration;
+    }
+    
+    BufferedReader reader = new BufferedReader(new FileReader(f));
+    String line = null;
+    //First line is the date, 3rd is the title, rest is body
+    String dateStr = reader.readLine();
+    reader.readLine();//skip an empty line
+    String title = reader.readLine();
+    reader.readLine();//skip an empty line
+    StringBuilder bodyBuf = new StringBuilder(1024);
+    while ((line = reader.readLine()) != null) {
+      bodyBuf.append(line).append(' ');
+    }
+    reader.close();
+    addBytes(f.length());
+    
+    Date date = parseDate(dateStr);
+    
+    docData.clear();
+    docData.setName(name);
+    docData.setBody(bodyBuf.toString());
+    docData.setTitle(title);
+    docData.setDate(date);
+    return docData;
+  }
+  
+  @Override
+  public synchronized void resetInputs() throws IOException {
+    super.resetInputs();
+    inputFiles = new Iterator(dataDir);
+    iteration = 0;
+  }
+
+  @Override
+  public void setConfig(Config config) {
+    super.setConfig(config);
+    
+    File workDir = new File(config.get("work.dir", "work"));
+    String d = config.get("docs.dir", "dir-out");
+    dataDir = new File(d);
+    if (!dataDir.isAbsolute()) {
+      dataDir = new File(workDir, d);
+    }
+
+    inputFiles = new Iterator(dataDir);
+
+    if (inputFiles == null) {
+      throw new RuntimeException("No txt files in dataDir: " + dataDir.getAbsolutePath());
+    }
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocData.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocData.java
new file mode 100755
index 0000000..7415211
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocData.java
@@ -0,0 +1,106 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Date;
+import java.util.Properties;
+
+import org.apache.lucene.document.DateTools;
+
+/** Output of parsing (e.g. HTML parsing) of an input document. */
+public class DocData {
+  
+  private String name;
+  private String body;
+  private String title;
+  private String date;
+  private int id;
+  private Properties props;
+  
+  public void clear() {
+    name = null;
+    body = null;
+    title = null;
+    date = null;
+    props = null;
+    id = -1;
+  }
+  
+  public String getBody() {
+    return body;
+  }
+
+  /**
+   * @return the date. If the ctor with Date was called, then the String
+   *         returned is the output of
+   *         {@link DateTools#dateToString(Date, org.apache.lucene.document.DateTools.Resolution)}
+   *         . Otherwise it's the String passed to the other ctor.
+   */
+  public String getDate() {
+    return date;
+  }
+
+  public String getName() {
+    return name;
+  }
+
+  public int getID() {
+    return id;
+  }
+
+  public Properties getProps() {
+    return props;
+  }
+
+  public String getTitle() {
+    return title;
+  }
+
+  public void setBody(String body) {
+    this.body = body;
+  }
+
+  public void setDate(Date date) {
+    if (date != null) {
+      setDate(DateTools.dateToString(date, DateTools.Resolution.SECOND));
+    } else {
+      this.date = null;
+    }
+  }
+
+  public void setDate(String date) {
+    this.date = date;
+  }
+
+  public void setName(String name) {
+    this.name = name;
+  }
+
+  public void setID(int id) {
+    this.id = id;
+  }
+
+  public void setProps(Properties props) {
+    this.props = props;
+  }
+
+  public void setTitle(String title) {
+    this.title = title;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocMaker.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocMaker.java
new file mode 100644
index 0000000..142e408
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocMaker.java
@@ -0,0 +1,502 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.UnsupportedEncodingException;
+import java.util.HashMap;
+import java.util.Calendar;
+import java.util.Map;
+import java.util.Properties;
+import java.util.Locale;
+import java.util.Random;
+import java.util.Date;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.text.SimpleDateFormat;
+import java.text.ParsePosition;
+
+import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.benchmark.byTask.utils.Format;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.NumericField;
+import org.apache.lucene.document.Field.Index;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.Field.TermVector;
+
+/**
+ * Creates {@link Document} objects. Uses a {@link ContentSource} to generate
+ * {@link DocData} objects. Supports the following parameters:
+ * <ul>
+ * <li><b>content.source</b> - specifies the {@link ContentSource} class to use
+ * (default <b>SingleDocSource</b>).
+ * <li><b>doc.stored</b> - specifies whether fields should be stored (default
+ * <b>false</b>).
+ * <li><b>doc.body.stored</b> - specifies whether the body field should be stored (default
+ * = <b>doc.stored</b>).
+ * <li><b>doc.tokenized</b> - specifies whether fields should be tokenized
+ * (default <b>true</b>).
+ * <li><b>doc.body.tokenized</b> - specifies whether the
+ * body field should be tokenized (default = <b>doc.tokenized</b>).
+ * <li><b>doc.tokenized.norms</b> - specifies whether norms should be stored in
+ * the index or not. (default <b>false</b>).
+ * <li><b>doc.body.tokenized.norms</b> - specifies whether norms should be
+ * stored in the index for the body field. This can be set to true, while
+ * <code>doc.tokenized.norms</code> is set to false, to allow norms storing just
+ * for the body field. (default <b>true</b>).
+ * <li><b>doc.term.vector</b> - specifies whether term vectors should be stored
+ * for fields (default <b>false</b>).
+ * <li><b>doc.term.vector.positions</b> - specifies whether term vectors should
+ * be stored with positions (default <b>false</b>).
+ * <li><b>doc.term.vector.offsets</b> - specifies whether term vectors should be
+ * stored with offsets (default <b>false</b>).
+ * <li><b>doc.store.body.bytes</b> - specifies whether to store the raw bytes of
+ * the document's content in the document (default <b>false</b>).
+ * <li><b>doc.reuse.fields</b> - specifies whether Field and Document objects
+ * should be reused (default <b>true</b>).
+ * <li><b>doc.index.props</b> - specifies whether the properties returned by
+ * <li><b>doc.random.id.limit</b> - if specified, docs will be assigned random
+ * IDs from 0 to this limit.  This is useful with UpdateDoc
+ * for testing performance of IndexWriter.updateDocument.
+ * {@link DocData#getProps()} will be indexed. (default <b>false</b>).
+ * </ul>
+ */
+public class DocMaker {
+
+  private static class LeftOver {
+    private DocData docdata;
+    private int cnt;
+  }
+
+  private Random r;
+  private int updateDocIDLimit;
+
+  static class DocState {
+    
+    private final Map<String,Field> fields;
+    private final Map<String,NumericField> numericFields;
+    private final boolean reuseFields;
+    final Document doc;
+    DocData docData = new DocData();
+    
+    public DocState(boolean reuseFields, Store store, Store bodyStore, Index index, Index bodyIndex, TermVector termVector) {
+
+      this.reuseFields = reuseFields;
+      
+      if (reuseFields) {
+        fields =  new HashMap<String,Field>();
+        numericFields = new HashMap<String,NumericField>();
+        
+        // Initialize the map with the default fields.
+        fields.put(BODY_FIELD, new Field(BODY_FIELD, "", bodyStore, bodyIndex, termVector));
+        fields.put(TITLE_FIELD, new Field(TITLE_FIELD, "", store, index, termVector));
+        fields.put(DATE_FIELD, new Field(DATE_FIELD, "", store, index, termVector));
+        fields.put(ID_FIELD, new Field(ID_FIELD, "", Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
+        fields.put(NAME_FIELD, new Field(NAME_FIELD, "", store, index, termVector));
+
+        numericFields.put(DATE_MSEC_FIELD, new NumericField(DATE_MSEC_FIELD));
+        numericFields.put(TIME_SEC_FIELD, new NumericField(TIME_SEC_FIELD));
+        
+        doc = new Document();
+      } else {
+        numericFields = null;
+        fields = null;
+        doc = null;
+      }
+    }
+
+    /**
+     * Returns a field corresponding to the field name. If
+     * <code>reuseFields</code> was set to true, then it attempts to reuse a
+     * Field instance. If such a field does not exist, it creates a new one.
+     */
+    Field getField(String name, Store store, Index index, TermVector termVector) {
+      if (!reuseFields) {
+        return new Field(name, "", store, index, termVector);
+      }
+      
+      Field f = fields.get(name);
+      if (f == null) {
+        f = new Field(name, "", store, index, termVector);
+        fields.put(name, f);
+      }
+      return f;
+    }
+
+    NumericField getNumericField(String name) {
+      if (!reuseFields) {
+        return new NumericField(name);
+      }
+
+      NumericField f = numericFields.get(name);
+      if (f == null) {
+        f = new NumericField(name);
+        numericFields.put(name, f);
+      }
+      return f;
+    }
+  }
+  
+  private boolean storeBytes = false;
+
+  private static class DateUtil {
+    public SimpleDateFormat parser = new SimpleDateFormat("dd-MMM-yyyy HH:mm:ss", Locale.US);
+    public Calendar cal = Calendar.getInstance();
+    public ParsePosition pos = new ParsePosition(0);
+    public DateUtil() {
+      parser.setLenient(true);
+    }
+  }
+
+  // leftovers are thread local, because it is unsafe to share residues between threads
+  private ThreadLocal<LeftOver> leftovr = new ThreadLocal<LeftOver>();
+  private ThreadLocal<DocState> docState = new ThreadLocal<DocState>();
+  private ThreadLocal<DateUtil> dateParsers = new ThreadLocal<DateUtil>();
+
+  public static final String BODY_FIELD = "body";
+  public static final String TITLE_FIELD = "doctitle";
+  public static final String DATE_FIELD = "docdate";
+  public static final String DATE_MSEC_FIELD = "docdatenum";
+  public static final String TIME_SEC_FIELD = "doctimesecnum";
+  public static final String ID_FIELD = "docid";
+  public static final String BYTES_FIELD = "bytes";
+  public static final String NAME_FIELD = "docname";
+
+  protected Config config;
+
+  protected Store storeVal = Store.NO;
+  protected Store bodyStoreVal = Store.NO;
+  protected Index indexVal = Index.ANALYZED_NO_NORMS;
+  protected Index bodyIndexVal = Index.ANALYZED;
+  protected TermVector termVecVal = TermVector.NO;
+  
+  protected ContentSource source;
+  protected boolean reuseFields;
+  protected boolean indexProperties;
+  
+  private int lastPrintedNumUniqueTexts = 0;
+
+  private long lastPrintedNumUniqueBytes = 0;
+  private final AtomicInteger numDocsCreated = new AtomicInteger();
+
+  private int printNum = 0;
+
+  // create a doc
+  // use only part of the body, modify it to keep the rest (or use all if size==0).
+  // reset the docdata properties so they are not added more than once.
+  private Document createDocument(DocData docData, int size, int cnt) throws UnsupportedEncodingException {
+
+    final DocState ds = getDocState();
+    final Document doc = reuseFields ? ds.doc : new Document();
+    doc.getFields().clear();
+    
+    // Set ID_FIELD
+    Field idField = ds.getField(ID_FIELD, storeVal, Index.NOT_ANALYZED_NO_NORMS, termVecVal);
+    int id;
+    if (r != null) {
+      id = r.nextInt(updateDocIDLimit);
+    } else {
+      id = docData.getID();
+      if (id == -1) {
+        id = numDocsCreated.getAndIncrement();
+      }
+    }
+    idField.setValue(Integer.toString(id));
+    doc.add(idField);
+    
+    // Set NAME_FIELD
+    String name = docData.getName();
+    if (name == null) name = "";
+    name = cnt < 0 ? name : name + "_" + cnt;
+    Field nameField = ds.getField(NAME_FIELD, storeVal, indexVal, termVecVal);
+    nameField.setValue(name);
+    doc.add(nameField);
+    
+    // Set DATE_FIELD
+    DateUtil util = dateParsers.get();
+    if (util == null) {
+      util = new DateUtil();
+      dateParsers.set(util);
+    }
+    Date date = null;
+    String dateString = docData.getDate();
+    if (dateString != null) {
+      util.pos.setIndex(0);
+      date = util.parser.parse(dateString, util.pos);
+      //System.out.println(dateString + " parsed to " + date);
+    } else {
+      dateString = "";
+    }
+    Field dateStringField = ds.getField(DATE_FIELD, storeVal, indexVal, termVecVal);
+    dateStringField.setValue(dateString);
+    doc.add(dateStringField);
+
+    if (date == null) {
+      // just set to right now
+      date = new Date();
+    }
+
+    NumericField dateField = ds.getNumericField(DATE_MSEC_FIELD);
+    dateField.setLongValue(date.getTime());
+    doc.add(dateField);
+
+    util.cal.setTime(date);
+    final int sec = util.cal.get(Calendar.HOUR_OF_DAY)*3600 + util.cal.get(Calendar.MINUTE)*60 + util.cal.get(Calendar.SECOND);
+
+    NumericField timeSecField = ds.getNumericField(TIME_SEC_FIELD);
+    timeSecField.setIntValue(sec);
+    doc.add(timeSecField);
+    
+    // Set TITLE_FIELD
+    String title = docData.getTitle();
+    Field titleField = ds.getField(TITLE_FIELD, storeVal, indexVal, termVecVal);
+    titleField.setValue(title == null ? "" : title);
+    doc.add(titleField);
+    
+    String body = docData.getBody();
+    if (body != null && body.length() > 0) {
+      String bdy;
+      if (size <= 0 || size >= body.length()) {
+        bdy = body; // use all
+        docData.setBody(""); // nothing left
+      } else {
+        // attempt not to break words - if whitespace found within next 20 chars...
+        for (int n = size - 1; n < size + 20 && n < body.length(); n++) {
+          if (Character.isWhitespace(body.charAt(n))) {
+            size = n;
+            break;
+          }
+        }
+        bdy = body.substring(0, size); // use part
+        docData.setBody(body.substring(size)); // some left
+      }
+      Field bodyField = ds.getField(BODY_FIELD, bodyStoreVal, bodyIndexVal, termVecVal);
+      bodyField.setValue(bdy);
+      doc.add(bodyField);
+      
+      if (storeBytes) {
+        Field bytesField = ds.getField(BYTES_FIELD, Store.YES, Index.NOT_ANALYZED_NO_NORMS, TermVector.NO);
+        bytesField.setValue(bdy.getBytes("UTF-8"));
+        doc.add(bytesField);
+      }
+    }
+
+    if (indexProperties) {
+      Properties props = docData.getProps();
+      if (props != null) {
+        for (final Map.Entry<Object,Object> entry : props.entrySet()) {
+          Field f = ds.getField((String) entry.getKey(), storeVal, indexVal, termVecVal);
+          f.setValue((String) entry.getValue());
+          doc.add(f);
+        }
+        docData.setProps(null);
+      }
+    }
+    
+    //System.out.println("============== Created doc "+numDocsCreated+" :\n"+doc+"\n==========");
+    return doc;
+  }
+
+  private void resetLeftovers() {
+    leftovr.set(null);
+  }
+
+  protected DocState getDocState() {
+    DocState ds = docState.get();
+    if (ds == null) {
+      ds = new DocState(reuseFields, storeVal, bodyStoreVal, indexVal, bodyIndexVal, termVecVal);
+      docState.set(ds);
+    }
+    return ds;
+  }
+
+  /**
+   * Closes the {@link DocMaker}. The base implementation closes the
+   * {@link ContentSource}, and it can be overridden to do more work (but make
+   * sure to call super.close()).
+   */
+  public void close() throws IOException {
+    source.close();
+  }
+  
+  /**
+   * Returns the number of bytes generated by the content source since last
+   * reset.
+   */
+  public synchronized long getBytesCount() {
+    return source.getBytesCount();
+  }
+
+  /**
+   * Returns the total number of bytes that were generated by the content source
+   * defined to that doc maker.
+   */ 
+  public long getTotalBytesCount() {
+    return source.getTotalBytesCount();
+  }
+
+  /**
+   * Creates a {@link Document} object ready for indexing. This method uses the
+   * {@link ContentSource} to get the next document from the source, and creates
+   * a {@link Document} object from the returned fields. If
+   * <code>reuseFields</code> was set to true, it will reuse {@link Document}
+   * and {@link Field} instances.
+   */
+  public Document makeDocument() throws Exception {
+    resetLeftovers();
+    DocData docData = source.getNextDocData(getDocState().docData);
+    Document doc = createDocument(docData, 0, -1);
+    return doc;
+  }
+
+  /**
+   * Same as {@link #makeDocument()}, only this method creates a document of the
+   * given size input by <code>size</code>.
+   */
+  public Document makeDocument(int size) throws Exception {
+    LeftOver lvr = leftovr.get();
+    if (lvr == null || lvr.docdata == null || lvr.docdata.getBody() == null
+        || lvr.docdata.getBody().length() == 0) {
+      resetLeftovers();
+    }
+    DocData docData = getDocState().docData;
+    DocData dd = (lvr == null ? source.getNextDocData(docData) : lvr.docdata);
+    int cnt = (lvr == null ? 0 : lvr.cnt);
+    while (dd.getBody() == null || dd.getBody().length() < size) {
+      DocData dd2 = dd;
+      dd = source.getNextDocData(new DocData());
+      cnt = 0;
+      dd.setBody(dd2.getBody() + dd.getBody());
+    }
+    Document doc = createDocument(dd, size, cnt);
+    if (dd.getBody() == null || dd.getBody().length() == 0) {
+      resetLeftovers();
+    } else {
+      if (lvr == null) {
+        lvr = new LeftOver();
+        leftovr.set(lvr);
+      }
+      lvr.docdata = dd;
+      lvr.cnt = ++cnt;
+    }
+    return doc;
+  }
+  
+  public void printDocStatistics() {
+    boolean print = false;
+    String col = "                  ";
+    StringBuilder sb = new StringBuilder();
+    String newline = System.getProperty("line.separator");
+    sb.append("------------> ").append(getClass().getSimpleName()).append(" statistics (").append(printNum).append("): ").append(newline);
+    int nut = source.getTotalDocsCount();
+    if (nut > lastPrintedNumUniqueTexts) {
+      print = true;
+      sb.append("total count of unique texts: ").append(Format.format(0,nut,col)).append(newline);
+      lastPrintedNumUniqueTexts = nut;
+    }
+    long nub = getTotalBytesCount();
+    if (nub > lastPrintedNumUniqueBytes) {
+      print = true;
+      sb.append("total bytes of unique texts: ").append(Format.format(0,nub,col)).append(newline);
+      lastPrintedNumUniqueBytes = nub;
+    }
+    if (source.getDocsCount() > 0) {
+      print = true;
+      sb.append("num docs added since last inputs reset:   ").append(Format.format(0,source.getDocsCount(),col)).append(newline);
+      sb.append("total bytes added since last inputs reset: ").append(Format.format(0,getBytesCount(),col)).append(newline);
+    }
+    if (print) {
+      System.out.println(sb.append(newline).toString());
+      printNum++;
+    }
+  }
+  
+  /** Reset inputs so that the test run would behave, input wise, as if it just started. */
+  public synchronized void resetInputs() throws IOException {
+    printDocStatistics();
+    // re-initiate since properties by round may have changed.
+    setConfig(config);
+    source.resetInputs();
+    numDocsCreated.set(0);
+    resetLeftovers();
+  }
+  
+  /** Set the configuration parameters of this doc maker. */
+  public void setConfig(Config config) {
+    this.config = config;
+    try {
+      String sourceClass = config.get("content.source", "org.apache.lucene.benchmark.byTask.feeds.SingleDocSource");
+      source = Class.forName(sourceClass).asSubclass(ContentSource.class).newInstance();
+      source.setConfig(config);
+    } catch (Exception e) {
+      // Should not get here. Throw runtime exception.
+      throw new RuntimeException(e);
+    }
+
+    boolean stored = config.get("doc.stored", false);
+    boolean bodyStored = config.get("doc.body.stored", stored);
+    boolean tokenized = config.get("doc.tokenized", true);
+    boolean bodyTokenized = config.get("doc.body.tokenized", tokenized);
+    boolean norms = config.get("doc.tokenized.norms", false);
+    boolean bodyNorms = config.get("doc.body.tokenized.norms", true);
+    boolean termVec = config.get("doc.term.vector", false);
+    storeVal = (stored ? Field.Store.YES : Field.Store.NO);
+    bodyStoreVal = (bodyStored ? Field.Store.YES : Field.Store.NO);
+    if (tokenized) {
+      indexVal = norms ? Index.ANALYZED : Index.ANALYZED_NO_NORMS;
+    } else {
+      indexVal = norms ? Index.NOT_ANALYZED : Index.NOT_ANALYZED_NO_NORMS;
+    }
+
+    if (bodyTokenized) {
+      bodyIndexVal = bodyNorms ? Index.ANALYZED : Index.ANALYZED_NO_NORMS;
+    } else {
+      bodyIndexVal = bodyNorms ? Index.NOT_ANALYZED : Index.NOT_ANALYZED_NO_NORMS;
+    }
+
+    boolean termVecPositions = config.get("doc.term.vector.positions", false);
+    boolean termVecOffsets = config.get("doc.term.vector.offsets", false);
+    if (termVecPositions && termVecOffsets) {
+      termVecVal = TermVector.WITH_POSITIONS_OFFSETS;
+    } else if (termVecPositions) {
+      termVecVal = TermVector.WITH_POSITIONS;
+    } else if (termVecOffsets) {
+      termVecVal = TermVector.WITH_OFFSETS;
+    } else if (termVec) {
+      termVecVal = TermVector.YES;
+    } else {
+      termVecVal = TermVector.NO;
+    }
+    storeBytes = config.get("doc.store.body.bytes", false);
+    
+    reuseFields = config.get("doc.reuse.fields", true);
+
+    // In a multi-rounds run, it is important to reset DocState since settings
+    // of fields may change between rounds, and this is the only way to reset
+    // the cache of all threads.
+    docState = new ThreadLocal<DocState>();
+    
+    indexProperties = config.get("doc.index.props", false);
+
+    updateDocIDLimit = config.get("doc.random.id.limit", -1);
+    if (updateDocIDLimit != -1) {
+      r = new Random(179);
+    }
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiContentSource.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiContentSource.java
new file mode 100644
index 0000000..5c71c5a
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiContentSource.java
@@ -0,0 +1,307 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.util.ThreadInterruptedException;
+import org.xml.sax.Attributes;
+import org.xml.sax.InputSource;
+import org.xml.sax.SAXException;
+import org.xml.sax.XMLReader;
+import org.xml.sax.helpers.DefaultHandler;
+import org.xml.sax.helpers.XMLReaderFactory;
+
+/**
+ * A {@link ContentSource} which reads the English Wikipedia dump. You can read
+ * the .bz2 file directly (it will be decompressed on the fly). Config
+ * properties:
+ * <ul>
+ * <li>keep.image.only.docs=false|true (default <b>true</b>).
+ * <li>docs.file=&lt;path to the file&gt;
+ * </ul>
+ */
+public class EnwikiContentSource extends ContentSource {
+
+  private class Parser extends DefaultHandler implements Runnable {
+    private Thread t;
+    private boolean threadDone;
+    private String[] tuple;
+    private NoMoreDataException nmde;
+    private StringBuilder contents = new StringBuilder();
+    private String title;
+    private String body;
+    private String time;
+    private String id;
+    
+    String[] next() throws NoMoreDataException {
+      if (t == null) {
+        threadDone = false;
+        t = new Thread(this);
+        t.setDaemon(true);
+        t.start();
+      }
+      String[] result;
+      synchronized(this){
+        while(tuple == null && nmde == null && !threadDone) {
+          try {
+            wait();
+          } catch (InterruptedException ie) {
+            throw new ThreadInterruptedException(ie);
+          }
+        }
+        if (nmde != null) {
+          // Set to null so we will re-start thread in case
+          // we are re-used:
+          t = null;
+          throw nmde;
+        }
+        if (t != null && threadDone) {
+          // The thread has exited yet did not hit end of
+          // data, so this means it hit an exception.  We
+          // throw NoMorDataException here to force
+          // benchmark to stop the current alg:
+          throw new NoMoreDataException();
+        }
+        result = tuple;
+        tuple = null;
+        notify();
+      }
+      return result;
+    }
+    
+    String time(String original) {
+      StringBuilder buffer = new StringBuilder();
+
+      buffer.append(original.substring(8, 10));
+      buffer.append('-');
+      buffer.append(months[Integer.valueOf(original.substring(5, 7)).intValue() - 1]);
+      buffer.append('-');
+      buffer.append(original.substring(0, 4));
+      buffer.append(' ');
+      buffer.append(original.substring(11, 19));
+      buffer.append(".000");
+
+      return buffer.toString();
+    }
+    
+    @Override
+    public void characters(char[] ch, int start, int length) {
+      contents.append(ch, start, length);
+    }
+
+    @Override
+    public void endElement(String namespace, String simple, String qualified)
+      throws SAXException {
+      int elemType = getElementType(qualified);
+      switch (elemType) {
+        case PAGE:
+          // the body must be null and we either are keeping image docs or the
+          // title does not start with Image:
+          if (body != null && (keepImages || !title.startsWith("Image:"))) {
+            String[] tmpTuple = new String[LENGTH];
+            tmpTuple[TITLE] = title.replace('\t', ' ');
+            tmpTuple[DATE] = time.replace('\t', ' ');
+            tmpTuple[BODY] = body.replaceAll("[\t\n]", " ");
+            tmpTuple[ID] = id;
+            synchronized(this) {
+              while (tuple != null) {
+                try {
+                  wait();
+                } catch (InterruptedException ie) {
+                  throw new ThreadInterruptedException(ie);
+                }
+              }
+              tuple = tmpTuple;
+              notify();
+            }
+          }
+          break;
+        case BODY:
+          body = contents.toString();
+          //workaround that startswith doesn't have an ignore case option, get at least 20 chars.
+          String startsWith = body.substring(0, Math.min(10, contents.length())).toLowerCase();
+          if (startsWith.startsWith("#redirect")) {
+            body = null;
+          }
+          break;
+        case DATE:
+          time = time(contents.toString());
+          break;
+        case TITLE:
+          title = contents.toString();
+          break;
+        case ID:
+          //the doc id is the first one in the page.  All other ids after that one can be ignored according to the schema
+          if (id == null) {
+            id = contents.toString();
+          }
+          break;
+        default:
+          // this element should be discarded.
+      }
+    }
+
+    public void run() {
+
+      try {
+        XMLReader reader = XMLReaderFactory.createXMLReader();
+        reader.setContentHandler(this);
+        reader.setErrorHandler(this);
+        while(true){
+          final InputStream localFileIS = is;
+          try {
+            reader.parse(new InputSource(localFileIS));
+          } catch (IOException ioe) {
+            synchronized(EnwikiContentSource.this) {
+              if (localFileIS != is) {
+                // fileIS was closed on us, so, just fall
+                // through
+              } else
+                // Exception is real
+                throw ioe;
+            }
+          }
+          synchronized(this) {
+            if (!forever) {
+              nmde = new NoMoreDataException();
+              notify();
+              return;
+            } else if (localFileIS == is) {
+              // If file is not already re-opened then re-open it now
+              is = getInputStream(file);
+            }
+          }
+        }
+      } catch (SAXException sae) {
+        throw new RuntimeException(sae);
+      } catch (IOException ioe) {
+        throw new RuntimeException(ioe);
+      } finally {
+        synchronized(this) {
+          threadDone = true;
+          notify();
+        }
+      }
+    }
+
+    @Override
+    public void startElement(String namespace, String simple, String qualified,
+                             Attributes attributes) {
+      int elemType = getElementType(qualified);
+      switch (elemType) {
+        case PAGE:
+          title = null;
+          body = null;
+          time = null;
+          id = null;
+          break;
+        // intentional fall-through.
+        case BODY:
+        case DATE:
+        case TITLE:
+        case ID:
+          contents.setLength(0);
+          break;
+        default:
+          // this element should be discarded.
+      }
+    }
+  }
+
+  private static final Map<String,Integer> ELEMENTS = new HashMap<String,Integer>();
+  private static final int TITLE = 0;
+  private static final int DATE = TITLE + 1;
+  private static final int BODY = DATE + 1;
+  private static final int ID = BODY + 1;
+  private static final int LENGTH = ID + 1;
+  // LENGTH is used as the size of the tuple, so whatever constants we need that
+  // should not be part of the tuple, we should define them after LENGTH.
+  private static final int PAGE = LENGTH + 1;
+
+  private static final String[] months = {"JAN", "FEB", "MAR", "APR",
+                                  "MAY", "JUN", "JUL", "AUG",
+                                  "SEP", "OCT", "NOV", "DEC"};
+
+  static {
+    ELEMENTS.put("page", Integer.valueOf(PAGE));
+    ELEMENTS.put("text", Integer.valueOf(BODY));
+    ELEMENTS.put("timestamp", Integer.valueOf(DATE));
+    ELEMENTS.put("title", Integer.valueOf(TITLE));
+    ELEMENTS.put("id", Integer.valueOf(ID));
+  }
+  
+  /**
+   * Returns the type of the element if defined, otherwise returns -1. This
+   * method is useful in startElement and endElement, by not needing to compare
+   * the element qualified name over and over.
+   */
+  private final static int getElementType(String elem) {
+    Integer val = ELEMENTS.get(elem);
+    return val == null ? -1 : val.intValue();
+  }
+  
+  private File file;
+  private boolean keepImages = true;
+  private InputStream is;
+  private Parser parser = new Parser();
+  
+  @Override
+  public void close() throws IOException {
+    synchronized (EnwikiContentSource.this) {
+      if (is != null) {
+        is.close();
+        is = null;
+      }
+    }
+  }
+  
+  @Override
+  public synchronized DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException {
+    String[] tuple = parser.next();
+    docData.clear();
+    docData.setName(tuple[ID]);
+    docData.setBody(tuple[BODY]);
+    docData.setDate(tuple[DATE]);
+    docData.setTitle(tuple[TITLE]);
+    return docData;
+  }
+
+  @Override
+  public void resetInputs() throws IOException {
+    super.resetInputs();
+    is = getInputStream(file);
+  }
+  
+  @Override
+  public void setConfig(Config config) {
+    super.setConfig(config);
+    keepImages = config.get("keep.image.only.docs", true);
+    String fileName = config.get("docs.file", null);
+    if (fileName == null) {
+      throw new IllegalArgumentException("docs.file must be set");
+    }
+    file = new File(fileName).getAbsoluteFile();
+  }
+  
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiQueryMaker.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiQueryMaker.java
new file mode 100644
index 0000000..106c6c3
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiQueryMaker.java
@@ -0,0 +1,137 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.WildcardQuery;
+import org.apache.lucene.search.spans.SpanFirstQuery;
+import org.apache.lucene.search.spans.SpanNearQuery;
+import org.apache.lucene.search.spans.SpanQuery;
+import org.apache.lucene.search.spans.SpanTermQuery;
+import org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask;
+import org.apache.lucene.util.Version;
+
+/**
+ * A QueryMaker that uses common and uncommon actual Wikipedia queries for
+ * searching the English Wikipedia collection. 90 queries total.
+ */
+public class EnwikiQueryMaker extends AbstractQueryMaker implements
+    QueryMaker {
+
+  // common and a few uncommon queries from wikipedia search logs
+  private static String[] STANDARD_QUERIES = { "Images catbox gif",
+      "Imunisasi haram", "Favicon ico", "Michael jackson", "Unknown artist",
+      "Lily Thai", "Neda", "The Last Song", "Metallica", "Nicola Tesla",
+      "Max B", "Skil Corporation", "\"The 100 Greatest Artists of All Time\"",
+      "\"Top 100 Global Universities\"", "Pink floyd", "Bolton Sullivan",
+      "Frank Lucas Jr", "Drake Woods", "Radiohead", "George Freeman",
+      "Oksana Grigorieva", "The Elder Scrolls V", "Deadpool", "Green day",
+      "\"Red hot chili peppers\"", "Jennifer Bini Taylor",
+      "The Paradiso Girls", "Queen", "3Me4Ph", "Paloma Jimenez", "AUDI A4",
+      "Edith Bouvier Beale: A Life In Pictures", "\"Skylar James Deleon\"",
+      "Simple Explanation", "Juxtaposition", "The Woody Show", "London WITHER",
+      "In A Dark Place", "George Freeman", "LuAnn de Lesseps", "Muhammad.",
+      "U2", "List of countries by GDP", "Dean Martin Discography", "Web 3.0",
+      "List of American actors", "The Expendables",
+      "\"100 Greatest Guitarists of All Time\"", "Vince Offer.",
+      "\"List of ZIP Codes in the United States\"", "Blood type diet",
+      "Jennifer Gimenez", "List of hobbies", "The beatles", "Acdc",
+      "Nightwish", "Iron maiden", "Murder Was the Case", "Pelvic hernia",
+      "Naruto Shippuuden", "campaign", "Enthesopathy of hip region",
+      "operating system", "mouse",
+      "List of Xbox 360 games without region encoding", "Shakepearian sonnet",
+      "\"The Monday Night Miracle\"", "India", "Dad's Army",
+      "Solanum melanocerasum", "\"List of PlayStation Portable Wi-Fi games\"",
+      "Little Pixie Geldof", "Planes, Trains & Automobiles", "Freddy Ingalls",
+      "The Return of Chef", "Nehalem", "Turtle", "Calculus", "Superman-Prime",
+      "\"The Losers\"", "pen-pal", "Audio stream input output", "lifehouse",
+      "50 greatest gunners", "Polyfecalia", "freeloader", "The Filthy Youth" };
+
+  private static Query[] getPrebuiltQueries(String field) {
+    WildcardQuery wcq = new WildcardQuery(new Term(field, "fo*"));
+    wcq .setRewriteMethod(MultiTermQuery.CONSTANT_SCORE_FILTER_REWRITE);
+    // be wary of unanalyzed text
+    return new Query[] {
+        new SpanFirstQuery(new SpanTermQuery(new Term(field, "ford")), 5),
+        new SpanNearQuery(new SpanQuery[] {
+            new SpanTermQuery(new Term(field, "night")),
+            new SpanTermQuery(new Term(field, "trading")) }, 4, false),
+        new SpanNearQuery(new SpanQuery[] {
+            new SpanFirstQuery(new SpanTermQuery(new Term(field, "ford")), 10),
+            new SpanTermQuery(new Term(field, "credit")) }, 10, false), wcq, };
+  }
+
+  /**
+   * Parse the strings containing Lucene queries.
+   * 
+   * @param qs array of strings containing query expressions
+   * @param a analyzer to use when parsing queries
+   * @return array of Lucene queries
+   */
+  private static Query[] createQueries(List<Object> qs, Analyzer a) {
+    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, DocMaker.BODY_FIELD, a);
+    List<Object> queries = new ArrayList<Object>();
+    for (int i = 0; i < qs.size(); i++) {
+      try {
+
+        Object query = qs.get(i);
+        Query q = null;
+        if (query instanceof String) {
+          q = qp.parse((String) query);
+
+        } else if (query instanceof Query) {
+          q = (Query) query;
+
+        } else {
+          System.err.println("Unsupported Query Type: " + query);
+        }
+
+        if (q != null) {
+          queries.add(q);
+        }
+
+      } catch (Exception e) {
+        e.printStackTrace();
+      }
+    }
+
+    return queries.toArray(new Query[0]);
+  }
+
+  @Override
+  protected Query[] prepareQueries() throws Exception {
+    // analyzer (default is standard analyzer)
+    Analyzer anlzr = NewAnalyzerTask.createAnalyzer(config.get("analyzer", StandardAnalyzer.class.getName()));
+
+    List<Object> queryList = new ArrayList<Object>(20);
+    queryList.addAll(Arrays.asList(STANDARD_QUERIES));
+    if(!config.get("enwikiQueryMaker.disableSpanQueries", false))
+      queryList.addAll(Arrays.asList(getPrebuiltQueries(DocMaker.BODY_FIELD)));
+    return createQueries(queryList, anlzr);
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java
new file mode 100644
index 0000000..dbfc731
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java
@@ -0,0 +1,97 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask;
+import org.apache.lucene.util.Version;
+
+import java.io.*;
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ * <p/>
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Create queries from a FileReader.  One per line, pass them through the
+ * QueryParser.  Lines beginning with # are treated as comments
+ *
+ * File can be specified as a absolute, relative or resource.
+ * Two properties can be set:
+ * file.query.maker.file=&lt;Full path to file containing queries&gt;
+ * <br/>
+ * file.query.maker.default.field=&lt;Name of default field - Default value is "body"&gt;
+ *
+ * Example:
+ * file.query.maker.file=c:/myqueries.txt
+ * file.query.maker.default.field=body
+ */
+public class FileBasedQueryMaker extends AbstractQueryMaker implements QueryMaker{
+
+
+  @Override
+  protected Query[] prepareQueries() throws Exception {
+
+    Analyzer anlzr = NewAnalyzerTask.createAnalyzer(config.get("analyzer",
+            "org.apache.lucene.analysis.standard.StandardAnalyzer"));
+    String defaultField = config.get("file.query.maker.default.field", DocMaker.BODY_FIELD);
+    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, defaultField, anlzr);
+    qp.setAllowLeadingWildcard(true);
+
+    List<Query> qq = new ArrayList<Query>();
+    String fileName = config.get("file.query.maker.file", null);
+    if (fileName != null)
+    {
+      File file = new File(fileName);
+      Reader reader = null;
+      if (file.exists()) {
+        reader = new FileReader(file);
+      } else {
+        //see if we can find it as a resource
+        InputStream asStream = FileBasedQueryMaker.class.getClassLoader().getResourceAsStream(fileName);
+        if (asStream != null) {
+          reader = new InputStreamReader(asStream);
+        }
+      }
+      if (reader != null) {
+        try {
+          BufferedReader buffered = new BufferedReader(reader);
+          String line = null;
+          int lineNum = 0;
+          while ((line = buffered.readLine()) != null) {
+            line = line.trim();
+            if (line.length() != 0 && !line.startsWith("#")) {
+              try {
+                qq.add(qp.parse(line));
+              } catch (ParseException e) {
+                System.err.println("Exception: " + e.getMessage() + " occurred while parsing line: " + lineNum + " Text: " + line);
+              }
+            }
+            lineNum++;
+          }
+        } finally {
+          reader.close();
+        }
+      } else {
+        System.err.println("No Reader available for: " + fileName);
+      }
+      
+    }
+    return qq.toArray(new Query[qq.size()]) ;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/HTMLParser.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/HTMLParser.java
new file mode 100755
index 0000000..6c8b9fa
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/HTMLParser.java
@@ -0,0 +1,44 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.Reader;
+import java.text.DateFormat;
+import java.util.Date;
+
+/**
+ * HTML Parsing Interface for test purposes
+ */
+public interface HTMLParser {
+
+  /**
+   * Parse the input Reader and return DocData. 
+   * A provided name or date is used for the result, otherwise an attempt is 
+   * made to set them from the parsed data.
+   * @param dateFormat date formatter to use for extracting the date.   
+   * @param name name of the result doc data. If null, attempt to set by parsed data.
+   * @param date date of the result doc data. If null, attempt to set by parsed data.
+   * @param reader of html text to parse.
+   * @return Parsed doc data.
+   * @throws IOException
+   * @throws InterruptedException
+   */
+  public DocData parse(DocData docData, String name, Date date, Reader reader, DateFormat dateFormat) throws IOException, InterruptedException;
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LineDocSource.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LineDocSource.java
new file mode 100644
index 0000000..9ab6527
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LineDocSource.java
@@ -0,0 +1,129 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+
+import org.apache.lucene.benchmark.byTask.tasks.WriteLineDocTask;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+
+/**
+ * A {@link ContentSource} reading one line at a time as a
+ * {@link org.apache.lucene.document.Document} from a single file. This saves IO
+ * cost (over DirContentSource) of recursing through a directory and opening a
+ * new file for every document.<br>
+ * The expected format of each line is (arguments are separated by &lt;TAB&gt;):
+ * <i>title, date, body</i>. If a line is read in a different format, a
+ * {@link RuntimeException} will be thrown. In general, you should use this
+ * content source for files that were created with {@link WriteLineDocTask}.<br>
+ * <br>
+ * Config properties:
+ * <ul>
+ * <li>docs.file=&lt;path to the file&gt;
+ * <li>content.source.encoding - default to UTF-8.
+ * </ul>
+ */
+public class LineDocSource extends ContentSource {
+
+  private final static char SEP = WriteLineDocTask.SEP;
+
+  private File file;
+  private BufferedReader reader;
+  private int readCount;
+
+  private synchronized void openFile() {
+    try {
+      if (reader != null) {
+        reader.close();
+      }
+      InputStream is = getInputStream(file);
+      reader = new BufferedReader(new InputStreamReader(is, encoding), BUFFER_SIZE);
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (reader != null) {
+      reader.close();
+      reader = null;
+    }
+  }
+  
+  @Override
+  public DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException {
+    final String line;
+    final int myID;
+    
+    synchronized(this) {
+      line = reader.readLine();
+      myID = readCount++;
+      if (line == null) {
+        if (!forever) {
+          throw new NoMoreDataException();
+        }
+        // Reset the file
+        openFile();
+        return getNextDocData(docData);
+      }
+    }
+    
+    // A line must be in the following format. If it's not, fail !
+    // title <TAB> date <TAB> body <NEWLINE>
+    int spot = line.indexOf(SEP);
+    if (spot == -1) {
+      throw new RuntimeException("line: [" + line + "] is in an invalid format !");
+    }
+    int spot2 = line.indexOf(SEP, 1 + spot);
+    if (spot2 == -1) {
+      throw new RuntimeException("line: [" + line + "] is in an invalid format !");
+    }
+    // The date String was written in the format of DateTools.dateToString.
+    docData.clear();
+    docData.setID(myID);
+    docData.setBody(line.substring(1 + spot2, line.length()));
+    docData.setTitle(line.substring(0, spot));
+    docData.setDate(line.substring(1 + spot, spot2));
+    return docData;
+  }
+
+  @Override
+  public void resetInputs() throws IOException {
+    super.resetInputs();
+    openFile();
+  }
+  
+  @Override
+  public void setConfig(Config config) {
+    super.setConfig(config);
+    String fileName = config.get("docs.file", null);
+    if (fileName == null) {
+      throw new IllegalArgumentException("docs.file must be set");
+    }
+    file = new File(fileName).getAbsoluteFile();
+    if (encoding == null) {
+      encoding = "UTF-8";
+    }
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LongToEnglishContentSource.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LongToEnglishContentSource.java
new file mode 100644
index 0000000..4d20e91
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LongToEnglishContentSource.java
@@ -0,0 +1,61 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.English;
+
+import java.io.IOException;
+import java.util.Date;
+
+/**
+ * Creates documents whose content is a <code>long</code> number starting from
+ * <code>{@link Long#MIN_VALUE} + 10</code>.
+ */
+public class LongToEnglishContentSource extends ContentSource{
+  private long counter = 0;
+
+  @Override
+  public void close() throws IOException {
+  }
+  
+  @Override
+  public synchronized DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException {
+    docData.clear();
+    // store the current counter to avoid synchronization later on
+    long curCounter;
+    synchronized (this) {
+      curCounter = counter;
+      if (counter == Long.MAX_VALUE){
+        counter = Long.MIN_VALUE;//loop around
+      } else {
+        ++counter;
+      }
+    }    
+    docData.setBody(English.longToEnglish(curCounter));
+    docData.setName("doc_" + String.valueOf(curCounter));
+    docData.setTitle("title_" + String.valueOf(curCounter));
+    docData.setDate(new Date());
+    return docData;
+  }
+
+  @Override
+  public void resetInputs() throws IOException {
+    counter = Long.MIN_VALUE + 10;
+  }
+  
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LongToEnglishQueryMaker.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LongToEnglishQueryMaker.java
new file mode 100644
index 0000000..6abe9fc
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/LongToEnglishQueryMaker.java
@@ -0,0 +1,49 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.English;
+import org.apache.lucene.util.Version;
+
+
+/**
+ *
+ *
+ **/
+public class LongToEnglishQueryMaker implements QueryMaker {
+  long counter = Long.MIN_VALUE + 10;
+  protected QueryParser parser;
+
+  public Query makeQuery(int size) throws Exception {
+    throw new UnsupportedOperationException();
+  }
+
+  public synchronized Query makeQuery() throws Exception {
+
+    return parser.parse("" + English.longToEnglish(getNextCounter()) + "");
+  }
+
+  private synchronized long getNextCounter() {
+    if (counter == Long.MAX_VALUE){
+      counter = Long.MIN_VALUE + 10;
+    }
+    return counter++;
+  }
+
+  public void setConfig(Config config) throws Exception {
+    Analyzer anlzr = NewAnalyzerTask.createAnalyzer(config.get("analyzer", StandardAnalyzer.class.getName()));
+    parser = new QueryParser(Version.LUCENE_CURRENT, DocMaker.BODY_FIELD, anlzr);
+  }
+
+  public void resetInputs() {
+    counter = Long.MIN_VALUE + 10;
+  }
+
+  public String printQueries() {
+    return "LongToEnglish: [" + Long.MIN_VALUE + " TO " + counter + "]";
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/NoMoreDataException.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/NoMoreDataException.java
new file mode 100755
index 0000000..a4e9ed8
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/NoMoreDataException.java
@@ -0,0 +1,27 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Exception indicating there is no more data.
+ * Thrown by Docs Makers if doc.maker.forever is false and docs sources of that maker where exhausted.
+ * This is useful for iterating all document of a source, in case we don't know in advance how many docs there are.
+ */
+public class NoMoreDataException extends Exception {
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/QueryMaker.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/QueryMaker.java
new file mode 100644
index 0000000..4a409c6
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/QueryMaker.java
@@ -0,0 +1,49 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import org.apache.lucene.search.Query;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+
+
+/**
+ * Create queries for the test.
+ */
+public interface QueryMaker {
+
+  /** 
+   * Create the next query, of the given size.
+   * @param size the size of the query - number of terms, etc.
+   * @exception Exception if cannot make the query, or if size>0 was specified but this feature is not supported.
+   */ 
+  public Query makeQuery (int size) throws Exception;
+
+  /** Create the next query */ 
+  public Query makeQuery () throws Exception;
+
+  /** Set the properties 
+   * @throws Exception */
+  public void setConfig (Config config) throws Exception;
+  
+  /** Reset inputs so that the test run would behave, input wise, as if it just started. */
+  public void resetInputs();
+  
+  /** Print the queries */
+  public String printQueries();
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersContentSource.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersContentSource.java
new file mode 100644
index 0000000..11265bd
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersContentSource.java
@@ -0,0 +1,151 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileReader;
+import java.io.IOException;
+import java.text.DateFormat;
+import java.text.ParsePosition;
+import java.text.SimpleDateFormat;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.Locale;
+
+import org.apache.lucene.benchmark.byTask.utils.Config;
+
+/**
+ * A {@link ContentSource} reading from the Reuters collection.
+ * <p>
+ * Config properties:
+ * <ul>
+ * <li><b>work.dir</b> - path to the root of docs and indexes dirs (default
+ * <b>work</b>).
+ * <li><b>docs.dir</b> - path to the docs dir (default <b>reuters-out</b>).
+ * </ul>
+ */
+public class ReutersContentSource extends ContentSource {
+
+  private static final class DateFormatInfo {
+    DateFormat df;
+    ParsePosition pos;
+  }
+
+  private ThreadLocal<DateFormatInfo> dateFormat = new ThreadLocal<DateFormatInfo>();
+  private File dataDir = null;
+  private ArrayList<File> inputFiles = new ArrayList<File>();
+  private int nextFile = 0;
+  private int iteration = 0;
+  
+  @Override
+  public void setConfig(Config config) {
+    super.setConfig(config);
+    File workDir = new File(config.get("work.dir", "work"));
+    String d = config.get("docs.dir", "reuters-out");
+    dataDir = new File(d);
+    if (!dataDir.isAbsolute()) {
+      dataDir = new File(workDir, d);
+    }
+    inputFiles.clear();
+    collectFiles(dataDir, inputFiles);
+    if (inputFiles.size() == 0) {
+      throw new RuntimeException("No txt files in dataDir: "+dataDir.getAbsolutePath());
+    }
+  }
+
+  private synchronized DateFormatInfo getDateFormatInfo() {
+    DateFormatInfo dfi = dateFormat.get();
+    if (dfi == null) {
+      dfi = new DateFormatInfo();
+      // date format: 30-MAR-1987 14:22:36.87
+      dfi.df = new SimpleDateFormat("dd-MMM-yyyy kk:mm:ss.SSS",Locale.US);
+      dfi.df.setLenient(true);
+      dfi.pos = new ParsePosition(0);
+      dateFormat.set(dfi);
+    }
+    return dfi;
+  }
+  
+  private Date parseDate(String dateStr) {
+    DateFormatInfo dfi = getDateFormatInfo();
+    dfi.pos.setIndex(0);
+    dfi.pos.setErrorIndex(-1);
+    return dfi.df.parse(dateStr.trim(), dfi.pos);
+  }
+
+
+  @Override
+  public void close() throws IOException {
+    // TODO implement?
+  }
+  
+  @Override
+  public DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException {
+    File f = null;
+    String name = null;
+    synchronized (this) {
+      if (nextFile >= inputFiles.size()) {
+        // exhausted files, start a new round, unless forever set to false.
+        if (!forever) {
+          throw new NoMoreDataException();
+        }
+        nextFile = 0;
+        iteration++;
+      }
+      f = inputFiles.get(nextFile++);
+      name = f.getCanonicalPath() + "_" + iteration;
+    }
+
+    BufferedReader reader = new BufferedReader(new FileReader(f));
+    try {
+      // First line is the date, 3rd is the title, rest is body
+      String dateStr = reader.readLine();
+      reader.readLine();// skip an empty line
+      String title = reader.readLine();
+      reader.readLine();// skip an empty line
+      StringBuilder bodyBuf = new StringBuilder(1024);
+      String line = null;
+      while ((line = reader.readLine()) != null) {
+        bodyBuf.append(line).append(' ');
+      }
+      reader.close();
+      
+      addBytes(f.length());
+      
+      Date date = parseDate(dateStr.trim());
+      
+      docData.clear();
+      docData.setName(name);
+      docData.setBody(bodyBuf.toString());
+      docData.setTitle(title);
+      docData.setDate(date);
+      return docData;
+    } finally {
+      reader.close();
+    }
+  }
+
+  @Override
+  public synchronized void resetInputs() throws IOException {
+    super.resetInputs();
+    nextFile = 0;
+    iteration = 0;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersQueryMaker.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersQueryMaker.java
new file mode 100644
index 0000000..c12ed07
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersQueryMaker.java
@@ -0,0 +1,120 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.WildcardQuery;
+import org.apache.lucene.search.spans.SpanFirstQuery;
+import org.apache.lucene.search.spans.SpanNearQuery;
+import org.apache.lucene.search.spans.SpanQuery;
+import org.apache.lucene.search.spans.SpanTermQuery;
+import org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask;
+import org.apache.lucene.util.Version;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+
+/**
+ * A QueryMaker that makes queries devised manually (by Grant Ingersoll) for
+ * searching in the Reuters collection.
+ */
+public class ReutersQueryMaker extends AbstractQueryMaker implements QueryMaker {
+
+  private static String [] STANDARD_QUERIES = {
+    //Start with some short queries
+    "Salomon", "Comex", "night trading", "Japan Sony",
+    //Try some Phrase Queries
+    "\"Sony Japan\"", "\"food needs\"~3",
+    "\"World Bank\"^2 AND Nigeria", "\"World Bank\" -Nigeria",
+    "\"Ford Credit\"~5",
+    //Try some longer queries
+    "airline Europe Canada destination",
+    "Long term pressure by trade " +
+    "ministers is necessary if the current Uruguay round of talks on " +
+    "the General Agreement on Trade and Tariffs (GATT) is to " +
+    "succeed"
+  };
+  
+  private static Query[] getPrebuiltQueries(String field) {
+    //  be wary of unanalyzed text
+    return new Query[] {
+        new SpanFirstQuery(new SpanTermQuery(new Term(field, "ford")), 5),
+        new SpanNearQuery(new SpanQuery[]{new SpanTermQuery(new Term(field, "night")), new SpanTermQuery(new Term(field, "trading"))}, 4, false),
+        new SpanNearQuery(new SpanQuery[]{new SpanFirstQuery(new SpanTermQuery(new Term(field, "ford")), 10), new SpanTermQuery(new Term(field, "credit"))}, 10, false),
+        new WildcardQuery(new Term(field, "fo*")),
+    };
+  }
+  
+  /**
+   * Parse the strings containing Lucene queries.
+   *
+   * @param qs array of strings containing query expressions
+   * @param a  analyzer to use when parsing queries
+   * @return array of Lucene queries
+   */
+  private static Query[] createQueries(List<Object> qs, Analyzer a) {
+    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, DocMaker.BODY_FIELD, a);
+    List<Object> queries = new ArrayList<Object>();
+    for (int i = 0; i < qs.size(); i++)  {
+      try {
+        
+        Object query = qs.get(i);
+        Query q = null;
+        if (query instanceof String) {
+          q = qp.parse((String) query);
+          
+        } else if (query instanceof Query) {
+          q = (Query) query;
+          
+        } else {
+          System.err.println("Unsupported Query Type: " + query);
+        }
+        
+        if (q != null) {
+          queries.add(q);
+        }
+        
+      } catch (Exception e)  {
+        e.printStackTrace();
+      }
+    }
+    
+    return queries.toArray(new Query[0]);
+  }
+  
+  @Override
+  protected Query[] prepareQueries() throws Exception {
+    // analyzer (default is standard analyzer)
+    Analyzer anlzr= NewAnalyzerTask.createAnalyzer(config.get("analyzer",
+    "org.apache.lucene.analysis.standard.StandardAnalyzer")); 
+    
+    List<Object> queryList = new ArrayList<Object>(20);
+    queryList.addAll(Arrays.asList(STANDARD_QUERIES));
+    queryList.addAll(Arrays.asList(getPrebuiltQueries(DocMaker.BODY_FIELD)));
+    return createQueries(queryList, anlzr);
+  }
+
+
+  
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleQueryMaker.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleQueryMaker.java
new file mode 100644
index 0000000..c550f33
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleQueryMaker.java
@@ -0,0 +1,71 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask;
+import org.apache.lucene.util.Version;
+
+import java.util.ArrayList;
+
+/**
+ * A QueryMaker that makes queries for a collection created 
+ * using {@link org.apache.lucene.benchmark.byTask.feeds.SingleDocSource}.
+ */
+public class SimpleQueryMaker extends AbstractQueryMaker implements QueryMaker {
+
+
+  /**
+   * Prepare the queries for this test.
+   * Extending classes can override this method for preparing different queries. 
+   * @return prepared queries.
+   * @throws Exception if cannot prepare the queries.
+   */
+  @Override
+  protected Query[] prepareQueries() throws Exception {
+    // analyzer (default is standard analyzer)
+    Analyzer anlzr= NewAnalyzerTask.createAnalyzer(config.get("analyzer",
+        "org.apache.lucene.analysis.standard.StandardAnalyzer")); 
+    
+    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, DocMaker.BODY_FIELD,anlzr);
+    ArrayList<Query> qq = new ArrayList<Query>();
+    Query q1 = new TermQuery(new Term(DocMaker.ID_FIELD,"doc2"));
+    qq.add(q1);
+    Query q2 = new TermQuery(new Term(DocMaker.BODY_FIELD,"simple"));
+    qq.add(q2);
+    BooleanQuery bq = new BooleanQuery();
+    bq.add(q1,Occur.MUST);
+    bq.add(q2,Occur.MUST);
+    qq.add(bq);
+    qq.add(qp.parse("synthetic body"));
+    qq.add(qp.parse("\"synthetic body\""));
+    qq.add(qp.parse("synthetic text"));
+    qq.add(qp.parse("\"synthetic text\""));
+    qq.add(qp.parse("\"synthetic text\"~3"));
+    qq.add(qp.parse("zoom*"));
+    qq.add(qp.parse("synth*"));
+    return  qq.toArray(new Query[0]);
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker.java
new file mode 100644
index 0000000..84930b6
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker.java
@@ -0,0 +1,83 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.StringTokenizer;
+
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.Query;
+
+/**
+ * Create sloppy phrase queries for performance test, in an index created using simple doc maker.
+ */
+public class SimpleSloppyPhraseQueryMaker extends SimpleQueryMaker {
+
+  /* (non-Javadoc)
+   * @see org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker#prepareQueries()
+   */
+  @Override
+  protected Query[] prepareQueries() throws Exception {
+    // extract some 100 words from doc text to an array
+    String words[];
+    ArrayList<String> w = new ArrayList<String>();
+    StringTokenizer st = new StringTokenizer(SingleDocSource.DOC_TEXT);
+    while (st.hasMoreTokens() && w.size()<100) {
+      w.add(st.nextToken());
+    }
+    words = w.toArray(new String[0]);
+
+    // create queries (that would find stuff) with varying slops
+    ArrayList<Query> queries = new ArrayList<Query>(); 
+    for (int slop=0; slop<8; slop++) {
+      for (int qlen=2; qlen<6; qlen++) {
+        for (int wd=0; wd<words.length-qlen-slop; wd++) {
+          // ordered
+          int remainedSlop = slop;
+          PhraseQuery q = new PhraseQuery();
+          q.setSlop(slop);
+          int wind = wd;
+          for (int i=0; i<qlen; i++) {
+            q.add(new Term(DocMaker.BODY_FIELD,words[wind++]));
+            if (remainedSlop>0) {
+              remainedSlop--;
+              wind++;
+            }
+          }
+          queries.add(q);
+          // reversed
+          remainedSlop = slop;
+          q = new PhraseQuery();
+          q.setSlop(slop+2*qlen);
+          wind = wd+qlen+remainedSlop-1;
+          for (int i=0; i<qlen; i++) {
+            q.add(new Term(DocMaker.BODY_FIELD,words[wind--]));
+            if (remainedSlop>0) {
+              remainedSlop--;
+              wind--;
+            }
+          }
+          queries.add(q);
+        }
+      }
+    }
+    return queries.toArray(new Query[0]);
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SingleDocSource.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SingleDocSource.java
new file mode 100644
index 0000000..547b17f
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SingleDocSource.java
@@ -0,0 +1,72 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+import java.io.IOException;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Creates the same document each time {@link #getNextDocData(DocData)} is called.
+ */
+public class SingleDocSource extends ContentSource {
+  
+  private int docID = 0;
+
+  static final String DOC_TEXT =  
+    "Well, this is just some plain text we use for creating the " +
+    "test documents. It used to be a text from an online collection " +
+    "devoted to first aid, but if there was there an (online) lawyers " +
+    "first aid collection with legal advices, \"it\" might have quite " +
+    "probably advised one not to include \"it\"'s text or the text of " +
+    "any other online collection in one's code, unless one has money " +
+    "that one don't need and one is happy to donate for lawyers " +
+    "charity. Anyhow at some point, rechecking the usage of this text, " +
+    "it became uncertain that this text is free to use, because " +
+    "the web site in the disclaimer of he eBook containing that text " +
+    "was not responding anymore, and at the same time, in projGut, " +
+    "searching for first aid no longer found that eBook as well. " +
+    "So here we are, with a perhaps much less interesting " +
+    "text for the test, but oh much much safer. ";
+  
+  // return a new docid
+  private synchronized int newdocid() throws NoMoreDataException {
+    if (docID > 0 && !forever) {
+      throw new NoMoreDataException();
+    }
+    return docID++;
+  }
+
+  @Override
+  public void close() throws IOException {}
+  
+  @Override
+  public DocData getNextDocData(DocData docData) throws NoMoreDataException {
+    int id = newdocid();
+    addBytes(DOC_TEXT.length());
+    docData.clear();
+    docData.setName("doc" + id);
+    docData.setBody(DOC_TEXT);
+    return docData;
+  }
+
+  @Override
+  public synchronized void resetInputs() throws IOException {
+    super.resetInputs();
+    docID = 0;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SortableSingleDocSource.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SortableSingleDocSource.java
new file mode 100644
index 0000000..a7da954
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SortableSingleDocSource.java
@@ -0,0 +1,114 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Properties;
+import java.util.Random;
+
+import org.apache.lucene.benchmark.byTask.utils.Config;
+
+/**
+ * Adds fields appropriate for sorting: country, random_string and sort_field
+ * (int). Supports the following parameters:
+ * <ul>
+ * <li><b>sort.rng</b> - defines the range for sort-by-int field (default
+ * <b>20000</b>).
+ * <li><b>rand.seed</b> - defines the seed to initialize Random with (default
+ * <b>13</b>).
+ * </ul>
+ */
+public class SortableSingleDocSource extends SingleDocSource {
+  
+  private static String[] COUNTRIES = new String[] {
+    "European Union", "United States", "Japan", "Germany", "China (PRC)", 
+    "United Kingdom", "France", "Italy", "Spain", "Canada", "Brazil", "Russia",
+    "India", "South Korea", "Australia", "Mexico", "Netherlands", "Turkey", 
+    "Sweden", "Belgium", "Indonesia", "Switzerland", "Poland", "Norway", 
+    "Republic of China", "Saudi Arabia", "Austria", "Greece", "Denmark", "Iran", 
+    "South Africa", "Argentina", "Ireland", "Thailand", "Finland", "Venezuela", 
+    "Portugal", "Hong Kong", "United Arab Emirates", "Malaysia", 
+    "Czech Republic", "Colombia", "Nigeria", "Romania", "Chile", "Israel", 
+    "Singapore", "Philippines", "Pakistan", "Ukraine", "Hungary", "Algeria", 
+    "New Zealand", "Egypt", "Kuwait", "Peru", "Kazakhstan", "Slovakia", 
+    "Morocco", "Bangladesh", "Vietnam", "Qatar", "Angola", "Libya", "Iraq", 
+    "Croatia", "Luxembourg", "Sudan", "Slovenia", "Cuba", "Belarus", "Ecuador", 
+    "Serbia", "Oman", "Bulgaria", "Lithuania", "Syria", "Dominican Republic", 
+    "Tunisia", "Guatemala", "Azerbaijan", "Sri Lanka", "Kenya", "Latvia", 
+    "Turkmenistan", "Costa Rica", "Lebanon", "Uruguay", "Uzbekistan", "Yemen", 
+    "Cyprus", "Estonia", "Trinidad and Tobago", "Cameroon", "El Salvador", 
+    "Iceland", "Panama", "Bahrain", "Ivory Coast", "Ethiopia", "Tanzania", 
+    "Jordan", "Ghana", "Bosnia and Herzegovina", "Macau", "Burma", "Bolivia", 
+    "Brunei", "Botswana", "Honduras", "Gabon", "Uganda", "Jamaica", "Zambia", 
+    "Senegal", "Paraguay", "Albania", "Equatorial Guinea", "Georgia", 
+    "Democratic Republic of the Congo", "Nepal", "Afghanistan", "Cambodia", 
+    "Armenia", "Republic of the Congo", "Mozambique", "Republic of Macedonia", 
+    "Malta", "Namibia", "Madagascar", "Chad", "Burkina Faso", "Mauritius", 
+    "Mali", "The Bahamas", "Papua New Guinea", "Nicaragua", "Haiti", "Benin", 
+    "alestinian flag West Bank and Gaza", "Jersey", "Fiji", "Guinea", "Moldova", 
+    "Niger", "Laos", "Mongolia", "French Polynesia", "Kyrgyzstan", "Barbados", 
+    "Tajikistan", "Malawi", "Liechtenstein", "New Caledonia", "Kosovo", 
+    "Rwanda", "Montenegro", "Swaziland", "Guam", "Mauritania", "Guernsey", 
+    "Isle of Man", "Togo", "Somalia", "Suriname", "Aruba", "North Korea", 
+    "Zimbabwe", "Central African Republic", "Faroe Islands", "Greenland", 
+    "Sierra Leone", "Lesotho", "Cape Verde", "Eritrea", "Bhutan", "Belize", 
+    "Antigua and Barbuda", "Gibraltar", "Maldives", "San Marino", "Guyana", 
+    "Burundi", "Saint Lucia", "Djibouti", "British Virgin Islands", "Liberia", 
+    "Seychelles", "The Gambia", "Northern Mariana Islands", "Grenada", 
+    "Saint Vincent and the Grenadines", "Saint Kitts and Nevis", "East Timor", 
+    "Vanuatu", "Comoros", "Samoa", "Solomon Islands", "Guinea-Bissau", 
+    "American Samoa", "Dominica", "Micronesia", "Tonga", "Cook Islands", 
+    "Palau", "Marshall Islands", "S? Tom? and Pr?cipe", "Anguilla", 
+    "Kiribati", "Tuvalu", "Niue" };
+
+  private int sortRange;
+  private Random r;
+
+  @Override
+  public DocData getNextDocData(DocData docData) throws NoMoreDataException {
+    docData = super.getNextDocData(docData);
+    Properties props = new Properties();
+
+    // random int
+    props.put("sort_field", Integer.toString(r.nextInt(sortRange)));
+
+    // random string
+    int len = nextInt(2, 20);
+    char[] buffer = new char[len];
+    for (int i = 0; i < len; i++) {
+      buffer[i] = (char) r.nextInt(0x80); 
+    }
+    props.put("random_string", new String(buffer));
+
+    // random country
+    props.put("country", COUNTRIES[r.nextInt(COUNTRIES.length)]);
+    docData.setProps(props);
+    return docData;
+  }
+
+  private int nextInt(int start, int end) {
+    return start + r.nextInt(end - start);
+  }
+
+  @Override
+  public void setConfig(Config config) {
+    super.setConfig(config);
+    sortRange = config.get("sort.rng", 20000);
+    r = new Random(config.get("rand.seed", 13));
+  }
+  
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/TrecContentSource.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/TrecContentSource.java
new file mode 100644
index 0000000..1101e66
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/TrecContentSource.java
@@ -0,0 +1,349 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.io.Reader;
+import java.text.DateFormat;
+import java.text.ParsePosition;
+import java.text.SimpleDateFormat;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.Locale;
+import java.util.zip.GZIPInputStream;
+
+import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.benchmark.byTask.utils.StringBuilderReader;
+import org.apache.lucene.util.ThreadInterruptedException;
+
+/**
+ * Implements a {@link ContentSource} over the TREC collection.
+ * <p>
+ * Supports the following configuration parameters (on top of
+ * {@link ContentSource}):
+ * <ul>
+ * <li><b>work.dir</b> - specifies the working directory. Required if "docs.dir"
+ * denotes a relative path (<b>default=work</b>).
+ * <li><b>docs.dir</b> - specifies the directory where the TREC files reside.
+ * Can be set to a relative path if "work.dir" is also specified
+ * (<b>default=trec</b>).
+ * <li><b>html.parser</b> - specifies the {@link HTMLParser} class to use for
+ * parsing the TREC documents content (<b>default=DemoHTMLParser</b>).
+ * <li><b>content.source.encoding</b> - if not specified, ISO-8859-1 is used.
+ * <li><b>content.source.excludeIteration</b> - if true, do not append iteration number to docname
+ * </ul>
+ */
+public class TrecContentSource extends ContentSource {
+
+  private static final class DateFormatInfo {
+    DateFormat[] dfs;
+    ParsePosition pos;
+  }
+
+  private static final String DATE = "Date: ";
+  private static final String DOCHDR = "<DOCHDR>";
+  private static final String TERMINATING_DOCHDR = "</DOCHDR>";
+  private static final String DOCNO = "<DOCNO>";
+  private static final String TERMINATING_DOCNO = "</DOCNO>";
+  private static final String DOC = "<DOC>";
+  private static final String TERMINATING_DOC = "</DOC>";
+
+  private static final String NEW_LINE = System.getProperty("line.separator");
+
+  private static final String DATE_FORMATS [] = {
+       "EEE, dd MMM yyyy kk:mm:ss z",	  // Tue, 09 Dec 2003 22:39:08 GMT
+       "EEE MMM dd kk:mm:ss yyyy z",  	// Tue Dec 09 16:45:08 2003 EST
+       "EEE, dd-MMM-':'y kk:mm:ss z", 	// Tue, 09 Dec 2003 22:39:08 GMT
+       "EEE, dd-MMM-yyy kk:mm:ss z", 	  // Tue, 09 Dec 2003 22:39:08 GMT
+       "EEE MMM dd kk:mm:ss yyyy",  	  // Tue Dec 09 16:45:08 2003
+  };
+
+  private ThreadLocal<DateFormatInfo> dateFormats = new ThreadLocal<DateFormatInfo>();
+  private ThreadLocal<StringBuilderReader> trecDocReader = new ThreadLocal<StringBuilderReader>();
+  private ThreadLocal<StringBuilder> trecDocBuffer = new ThreadLocal<StringBuilder>();
+  private File dataDir = null;
+  private ArrayList<File> inputFiles = new ArrayList<File>();
+  private int nextFile = 0;
+  private int rawDocSize;
+
+  // Use to synchronize threads on reading from the TREC documents.
+  private Object lock = new Object();
+
+  // Required for test
+  BufferedReader reader;
+  int iteration = 0;
+  HTMLParser htmlParser;
+  private boolean excludeDocnameIteration;
+  
+  private DateFormatInfo getDateFormatInfo() {
+    DateFormatInfo dfi = dateFormats.get();
+    if (dfi == null) {
+      dfi = new DateFormatInfo();
+      dfi.dfs = new SimpleDateFormat[DATE_FORMATS.length];
+      for (int i = 0; i < dfi.dfs.length; i++) {
+        dfi.dfs[i] = new SimpleDateFormat(DATE_FORMATS[i], Locale.US);
+        dfi.dfs[i].setLenient(true);
+      }
+      dfi.pos = new ParsePosition(0);
+      dateFormats.set(dfi);
+    }
+    return dfi;
+  }
+
+  private StringBuilder getDocBuffer() {
+    StringBuilder sb = trecDocBuffer.get();
+    if (sb == null) {
+      sb = new StringBuilder();
+      trecDocBuffer.set(sb);
+    }
+    return sb;
+  }
+  
+  private Reader getTrecDocReader(StringBuilder docBuffer) {
+    StringBuilderReader r = trecDocReader.get();
+    if (r == null) {
+      r = new StringBuilderReader(docBuffer);
+      trecDocReader.set(r);
+    } else {
+      r.set(docBuffer);
+    }
+    return r;
+  }
+
+  // read until finding a line that starts with the specified prefix, or a terminating tag has been found.
+  private void read(StringBuilder buf, String prefix, boolean collectMatchLine,
+                    boolean collectAll, String terminatingTag)
+      throws IOException, NoMoreDataException {
+    String sep = "";
+    while (true) {
+      String line = reader.readLine();
+
+      if (line == null) {
+        openNextFile();
+        continue;
+      }
+
+      rawDocSize += line.length();
+
+      if (line.startsWith(prefix)) {
+        if (collectMatchLine) {
+          buf.append(sep).append(line);
+          sep = NEW_LINE;
+        }
+        break;
+      }
+
+      if (terminatingTag != null && line.startsWith(terminatingTag)) {
+        // didn't find the prefix that was asked, but the terminating
+        // tag was found. set the length to 0 to signal no match was
+        // found.
+        buf.setLength(0);
+        break;
+      }
+
+      if (collectAll) {
+        buf.append(sep).append(line);
+        sep = NEW_LINE;
+      }
+    }
+  }
+  
+  void openNextFile() throws NoMoreDataException, IOException {
+    close();
+    int retries = 0;
+    while (true) {
+      if (nextFile >= inputFiles.size()) { 
+        // exhausted files, start a new round, unless forever set to false.
+        if (!forever) {
+          throw new NoMoreDataException();
+        }
+        nextFile = 0;
+        iteration++;
+      }
+      File f = inputFiles.get(nextFile++);
+      if (verbose) {
+        System.out.println("opening: " + f + " length: " + f.length());
+      }
+      try {
+        GZIPInputStream zis = new GZIPInputStream(new FileInputStream(f), BUFFER_SIZE);
+        reader = new BufferedReader(new InputStreamReader(zis, encoding), BUFFER_SIZE);
+        return;
+      } catch (Exception e) {
+        retries++;
+        if (retries < 20 && verbose) {
+          System.out.println("Skipping 'bad' file " + f.getAbsolutePath() + "  #retries=" + retries);
+          continue;
+        }
+        throw new NoMoreDataException();
+      }
+    }
+  }
+
+  Date parseDate(String dateStr) {
+    dateStr = dateStr.trim();
+    DateFormatInfo dfi = getDateFormatInfo();
+    for (int i = 0; i < dfi.dfs.length; i++) {
+      DateFormat df = dfi.dfs[i];
+      dfi.pos.setIndex(0);
+      dfi.pos.setErrorIndex(-1);
+      Date d = df.parse(dateStr, dfi.pos);
+      if (d != null) {
+        // Parse succeeded.
+        return d;
+      }
+    }
+    // do not fail test just because a date could not be parsed
+    if (verbose) {
+      System.out.println("failed to parse date (assigning 'now') for: " + dateStr);
+    }
+    return null; 
+  }
+  
+  @Override
+  public void close() throws IOException {
+    if (reader == null) {
+      return;
+    }
+
+    try {
+      reader.close();
+    } catch (IOException e) {
+      if (verbose) {
+        System.out.println("failed to close reader !");
+        e.printStackTrace(System.out);
+      }
+    }
+    reader = null;
+  }
+
+  @Override
+  public DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException {
+    String dateStr = null, name = null;
+    Reader r = null;
+    // protect reading from the TREC files by multiple threads. The rest of the
+    // method, i.e., parsing the content and returning the DocData can run
+    // unprotected.
+    synchronized (lock) {
+      if (reader == null) {
+        openNextFile();
+      }
+
+      StringBuilder docBuf = getDocBuffer();
+      
+      // 1. skip until doc start
+      docBuf.setLength(0);
+      read(docBuf, DOC, false, false, null);
+
+      // 2. name
+      docBuf.setLength(0);
+      read(docBuf, DOCNO, true, false, null);
+      name = docBuf.substring(DOCNO.length(), docBuf.indexOf(TERMINATING_DOCNO,
+          DOCNO.length()));
+      if (!excludeDocnameIteration)
+        name = name + "_" + iteration;
+
+      // 3. skip until doc header
+      docBuf.setLength(0);
+      read(docBuf, DOCHDR, false, false, null);
+
+      boolean findTerminatingDocHdr = false;
+
+      // 4. date - look for the date only until /DOCHDR
+      docBuf.setLength(0);
+      read(docBuf, DATE, true, false, TERMINATING_DOCHDR);
+      if (docBuf.length() != 0) {
+        // Date found.
+        dateStr = docBuf.substring(DATE.length());
+        findTerminatingDocHdr = true;
+      }
+
+      // 5. skip until end of doc header
+      if (findTerminatingDocHdr) {
+        docBuf.setLength(0);
+        read(docBuf, TERMINATING_DOCHDR, false, false, null);
+      }
+
+      // 6. collect until end of doc
+      docBuf.setLength(0);
+      read(docBuf, TERMINATING_DOC, false, true, null);
+      
+      // 7. Set up a Reader over the read content
+      r = getTrecDocReader(docBuf);
+      // Resetting the thread's reader means it will reuse the instance
+      // allocated as well as re-read from docBuf.
+      r.reset();
+      
+      // count char length of parsed html text (larger than the plain doc body text).
+      addBytes(docBuf.length()); 
+    }
+
+    // This code segment relies on HtmlParser being thread safe. When we get 
+    // here, everything else is already private to that thread, so we're safe.
+    Date date = dateStr != null ? parseDate(dateStr) : null;
+    try {
+      docData = htmlParser.parse(docData, name, date, r, null);
+      addDoc();
+    } catch (InterruptedException ie) {
+      throw new ThreadInterruptedException(ie);
+    }
+
+    return docData;
+  }
+
+  @Override
+  public void resetInputs() throws IOException {
+    synchronized (lock) {
+      super.resetInputs();
+      close();
+      nextFile = 0;
+      iteration = 0;
+    }
+  }
+
+  @Override
+  public void setConfig(Config config) {
+    super.setConfig(config);
+    File workDir = new File(config.get("work.dir", "work"));
+    String d = config.get("docs.dir", "trec");
+    dataDir = new File(d);
+    if (!dataDir.isAbsolute()) {
+      dataDir = new File(workDir, d);
+    }
+    collectFiles(dataDir, inputFiles);
+    if (inputFiles.size() == 0) {
+      throw new IllegalArgumentException("No files in dataDir: " + dataDir);
+    }
+    try {
+      String parserClassName = config.get("html.parser",
+          "org.apache.lucene.benchmark.byTask.feeds.DemoHTMLParser");
+      htmlParser = Class.forName(parserClassName).asSubclass(HTMLParser.class).newInstance();
+    } catch (Exception e) {
+      // Should not get here. Throw runtime exception.
+      throw new RuntimeException(e);
+    }
+    if (encoding == null) {
+      encoding = "ISO-8859-1";
+    }
+    excludeDocnameIteration = config.get("content.source.excludeIteration", false);
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/package.html b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/package.html
new file mode 100644
index 0000000..3feb9e3
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/package.html
@@ -0,0 +1,23 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<body>
+Sources for benchmark inputs: documents and queries.
+</body>
+
+</html>
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/package.html b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/package.html
new file mode 100644
index 0000000..f5440bd
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/package.html
@@ -0,0 +1,717 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<HTML>
+<HEAD>
+    <TITLE>Benchmarking Lucene By Tasks</TITLE>
+</HEAD>
+<BODY>
+<DIV>
+Benchmarking Lucene By Tasks.
+<p>
+This package provides "task based" performance benchmarking of Lucene.
+One can use the predefined benchmarks, or create new ones.
+</p>
+<p>
+Contained packages:
+</p>
+
+<table border=1 cellpadding=4>
+ <tr>
+   <td><b>Package</b></td>
+   <td><b>Description</b></td>
+ </tr>
+ <tr>
+   <td><a href="stats/package-summary.html">stats</a></td>
+   <td>Statistics maintained when running benchmark tasks.</td>
+ </tr>
+ <tr>
+   <td><a href="tasks/package-summary.html">tasks</a></td>
+   <td>Benchmark tasks.</td>
+ </tr>
+ <tr>
+   <td><a href="feeds/package-summary.html">feeds</a></td>
+   <td>Sources for benchmark inputs: documents and queries.</td>
+ </tr>
+ <tr>
+   <td><a href="utils/package-summary.html">utils</a></td>
+   <td>Utilities used for the benchmark, and for the reports.</td>
+ </tr>
+ <tr>
+   <td><a href="programmatic/package-summary.html">programmatic</a></td>
+   <td>Sample performance test written programatically.</td>
+ </tr>
+</table>
+
+<h2>Table Of Contents</h2>
+<p>
+    <ol>
+        <li><a href="#concept">Benchmarking By Tasks</a></li>
+        <li><a href="#usage">How to use</a></li>
+        <li><a href="#algorithm">Benchmark "algorithm"</a></li>
+        <li><a href="#tasks">Supported tasks/commands</a></li>
+        <li><a href="#properties">Benchmark properties</a></li>
+        <li><a href="#example">Example input algorithm and the result benchmark
+                    report.</a></li>
+        <li><a href="#recsCounting">Results record counting clarified</a></li>
+    </ol>
+</p>
+<a name="concept"></a>
+<h2>Benchmarking By Tasks</h2>
+<p>
+Benchmark Lucene using task primitives.
+</p>
+
+<p>
+A benchmark is composed of some predefined tasks, allowing for creating an
+index, adding documents,
+optimizing, searching, generating reports, and more. A benchmark run takes an
+"algorithm" file
+that contains a description of the sequence of tasks making up the run, and some
+properties defining a few
+additional characteristics of the benchmark run.
+</p>
+
+<a name="usage"></a>
+<h2>How to use</h2>
+<p>
+Easiest way to run a benchmarks is using the predefined ant task:
+<ul>
+ <li>ant run-task
+     <br>- would run the <code>micro-standard.alg</code> "algorithm".
+ </li>
+ <li>ant run-task -Dtask.alg=conf/compound-penalty.alg
+     <br>- would run the <code>compound-penalty.alg</code> "algorithm".
+ </li>
+ <li>ant run-task -Dtask.alg=[full-path-to-your-alg-file]
+     <br>- would run <code>your perf test</code> "algorithm".
+ </li>
+ <li>java org.apache.lucene.benchmark.byTask.programmatic.Sample
+     <br>- would run a performance test programmatically - without using an alg
+     file. This is less readable, and less convinient, but possible.
+ </li>
+</ul>
+</p>
+
+<p>
+You may find existing tasks sufficient for defining the benchmark <i>you</i>
+need, otherwise, you can extend the framework to meet your needs, as explained
+herein.
+</p>
+
+<p>
+Each benchmark run has a DocMaker and a QueryMaker. These two should usually
+match, so that "meaningful" queries are used for a certain collection.
+Properties set at the header of the alg file define which "makers" should be
+used. You can also specify your own makers, extending DocMaker and implementing
+QureyMaker.
+	<blockquote>
+		<b>Note:</b> since 2.9, DocMaker is a concrete class which accepts a 
+		ContentSource. In most cases, you can use the DocMaker class to create 
+		Documents, while providing your own ContentSource implementation. For 
+		example, the current Benchmark package includes ContentSource 
+		implementations for TREC, Enwiki and Reuters collections, as well as 
+		others like LineDocSource which reads a 'line' file produced by 
+		WriteLineDocTask.
+	</blockquote>
+</p>
+
+<p>
+Benchmark .alg file contains the benchmark "algorithm". The syntax is described
+below. Within the algorithm, you can specify groups of commands, assign them
+names, specify commands that should be repeated,
+do commands in serial or in parallel,
+and also control the speed of "firing" the commands.
+</p>
+
+<p>
+This allows, for instance, to specify
+that an index should be opened for update,
+documents should be added to it one by one but not faster than 20 docs a minute,
+and, in parallel with this,
+some N queries should be searched against that index,
+again, no more than 2 queries a second.
+You can have the searches all share an index reader,
+or have them each open its own reader and close it afterwords.
+</p>
+
+<p>
+If the commands available for use in the algorithm do not meet your needs,
+you can add commands by adding a new task under
+org.apache.lucene.benchmark.byTask.tasks -
+you should extend the PerfTask abstract class.
+Make sure that your new task class name is suffixed by Task.
+Assume you added the class "WonderfulTask" - doing so also enables the
+command "Wonderful" to be used in the algorithm.
+</p>
+
+<p>
+<u>External classes</u>: It is sometimes useful to invoke the benchmark
+package with your external alg file that configures the use of your own
+doc/query maker and or html parser. You can work this out without
+modifying the benchmark package code, by passing your class path
+with the benchmark.ext.classpath property:
+<ul>
+  <li>ant run-task -Dtask.alg=[full-path-to-your-alg-file]
+      <font color="#FF0000">-Dbenchmark.ext.classpath=/mydir/classes
+      </font> -Dtask.mem=512M</li>
+</ul>
+</p>
+
+<a name="algorithm"></a>
+<h2>Benchmark "algorithm"</h2>
+
+<p>
+The following is an informal description of the supported syntax.
+</p>
+
+<ol>
+ <li>
+ <b>Measuring</b>: When a command is executed, statistics for the elapsed
+ execution time and memory consumption are collected.
+ At any time, those statistics can be printed, using one of the
+ available ReportTasks.
+ </li>
+ <li>
+ <b>Comments</b> start with '<font color="#FF0066">#</font>'.
+ </li>
+ <li>
+ <b>Serial</b> sequences are enclosed within '<font color="#FF0066">{ }</font>'.
+ </li>
+ <li>
+ <b>Parallel</b> sequences are enclosed within
+ '<font color="#FF0066">[ ]</font>'
+ </li>
+ <li>
+ <b>Sequence naming:</b> To name a sequence, put
+ '<font color="#FF0066">"name"</font>' just after
+ '<font color="#FF0066">{</font>' or '<font color="#FF0066">[</font>'.
+ <br>Example - <font color="#FF0066">{ "ManyAdds" AddDoc } : 1000000</font> -
+ would
+ name the sequence of 1M add docs "ManyAdds", and this name would later appear
+ in statistic reports.
+ If you don't specify a name for a sequence, it is given one: you can see it as
+ the  algorithm is printed just before benchmark execution starts.
+ </li>
+ <li>
+ <b>Repeating</b>:
+ To repeat sequence tasks N times, add '<font color="#FF0066">: N</font>' just
+ after the
+ sequence closing tag - '<font color="#FF0066">}</font>' or
+ '<font color="#FF0066">]</font>' or '<font color="#FF0066">></font>'.
+ <br>Example -  <font color="#FF0066">[ AddDoc ] : 4</font>  - would do 4 addDoc
+ in parallel, spawning 4 threads at once.
+ <br>Example -  <font color="#FF0066">[ AddDoc AddDoc ] : 4</font>  - would do
+ 8 addDoc in parallel, spawning 8 threads at once.
+ <br>Example -  <font color="#FF0066">{ AddDoc } : 30</font> - would do addDoc
+ 30 times in a row.
+ <br>Example -  <font color="#FF0066">{ AddDoc AddDoc } : 30</font> - would do
+ addDoc 60 times in a row.
+ <br><b>Exhaustive repeating</b>: use <font color="#FF0066">*</font> instead of
+ a number to repeat exhaustively.
+ This is sometimes useful, for adding as many files as a doc maker can create,
+ without iterating over the same file again, especially when the exact
+ number of documents is not known in advance. For insance, TREC files extracted
+ from a zip file. Note: when using this, you must also set
+ <font color="#FF0066">doc.maker.forever</font> to false.
+ <br>Example -  <font color="#FF0066">{ AddDoc } : *</font>  - would add docs
+ until the doc maker is "exhausted".
+ </li>
+ <li>
+ <b>Command parameter</b>: a command can optionally take a single parameter.
+ If the certain command does not support a parameter, or if the parameter is of
+ the wrong type,
+ reading the algorithm will fail with an exception and the test would not start.
+ Currently the following tasks take optional parameters:
+ <ul>
+   <li><b>AddDoc</b> takes a numeric parameter, indicating the required size of
+       added document. Note: if the DocMaker implementation used in the test
+       does not support makeDoc(size), an exception would be thrown and the test
+       would fail.
+   </li>
+   <li><b>DeleteDoc</b> takes numeric parameter, indicating the docid to be
+       deleted. The latter is not very useful for loops, since the docid is
+       fixed, so for deletion in loops it is better to use the
+       <code>doc.delete.step</code> property.
+   </li>
+   <li><b>SetProp</b> takes a <code>name,value<code> mandatory param,
+       ',' used as a separator.
+   </li>
+   <li><b>SearchTravRetTask</b> and <b>SearchTravTask</b> take a numeric
+              parameter, indicating the required traversal size.
+   </li>
+   <li><b>SearchTravRetLoadFieldSelectorTask</b> takes a string
+              parameter: a comma separated list of Fields to load.
+   </li>
+   <li><b>SearchTravRetHighlighterTask</b> takes a string
+              parameter: a comma separated list of parameters to define highlighting.  See that
+     tasks javadocs for more information
+   </li>
+ </ul>
+ <br>Example - <font color="#FF0066">AddDoc(2000)</font> - would add a document
+ of size 2000 (~bytes).
+ <br>See conf/task-sample.alg for how this can be used, for instance, to check
+ which is faster, adding
+ many smaller documents, or few larger documents.
+ Next candidates for supporting a parameter may be the Search tasks,
+ for controlling the qurey size.
+ </li>
+ <li>
+ <b>Statistic recording elimination</b>: - a sequence can also end with
+ '<font color="#FF0066">></font>',
+ in which case child tasks would not store their statistics.
+ This can be useful to avoid exploding stats data, for adding say 1M docs.
+ <br>Example - <font color="#FF0066">{ "ManyAdds" AddDoc > : 1000000</font> -
+ would add million docs, measure that total, but not save stats for each addDoc.
+ <br>Notice that the granularity of System.currentTimeMillis() (which is used
+ here) is system dependant,
+ and in some systems an operation that takes 5 ms to complete may show 0 ms
+ latency time in performance measurements.
+ Therefore it is sometimes more accurate to look at the elapsed time of a larger
+ sequence, as demonstrated here.
+ </li>
+ <li>
+ <b>Rate</b>:
+ To set a rate (ops/sec or ops/min) for a sequence, add
+ '<font color="#FF0066">: N : R</font>' just after sequence closing tag.
+ This would specify repetition of N with rate of R operations/sec.
+ Use '<font color="#FF0066">R/sec</font>' or
+ '<font color="#FF0066">R/min</font>'
+ to explicitely specify that the rate is per second or per minute.
+ The default is per second,
+ <br>Example -  <font color="#FF0066">[ AddDoc ] : 400 : 3</font> - would do 400
+ addDoc in parallel, starting up to 3 threads per second.
+ <br>Example -  <font color="#FF0066">{ AddDoc } : 100 : 200/min</font> - would
+ do 100 addDoc serially,
+ waiting before starting next add, if otherwise rate would exceed 200 adds/min.
+ </li>
+ <li>
+ <b>Disable Counting</b>: Each task executed contributes to the records count.
+ This count is reflected in reports under recs/s and under recsPerRun.
+ Most tasks count 1, some count 0, and some count more.
+ (See <a href="#recsCounting">Results record counting clarified</a> for more details.)
+ It is possible to disable counting for a task by preceding it with <font color="#FF0066">-</font>.
+ <br>Example -  <font color="#FF0066"> -CreateIndex </font> - would count 0 while
+ the default behavior for CreateIndex is to count 1.
+ </li>
+ <li>
+ <b>Command names</b>: Each class "AnyNameTask" in the
+ package org.apache.lucene.benchmark.byTask.tasks,
+ that extends PerfTask, is supported as command "AnyName" that can be
+ used in the benchmark "algorithm" description.
+ This allows to add new commands by just adding such classes.
+ </li>
+</ol>
+
+
+<a name="tasks"></a>
+<h2>Supported tasks/commands</h2>
+
+<p>
+Existing tasks can be divided into a few groups:
+regular index/search work tasks, report tasks, and control tasks.
+</p>
+
+<ol>
+
+ <li>
+ <b>Report tasks</b>: There are a few Report commands for generating reports.
+ Only task runs that were completed are reported.
+ (The 'Report tasks' themselves are not measured and not reported.)
+ <ul>
+             <li>
+            <font color="#FF0066">RepAll</font> - all (completed) task runs.
+            </li>
+            <li>
+            <font color="#FF0066">RepSumByName</font> - all statistics,
+            aggregated by name. So, if AddDoc was executed 2000 times,
+            only 1 report line would be created for it, aggregating all those
+            2000 statistic records.
+            </li>
+            <li>
+            <font color="#FF0066">RepSelectByPref &nbsp; prefixWord</font> - all
+            records for tasks whose name start with
+            <font color="#FF0066">prefixWord</font>.
+            </li>
+            <li>
+            <font color="#FF0066">RepSumByPref &nbsp; prefixWord</font> - all
+            records for tasks whose name start with
+            <font color="#FF0066">prefixWord</font>,
+            aggregated by their full task name.
+            </li>
+            <li>
+            <font color="#FF0066">RepSumByNameRound</font> - all statistics,
+            aggregated by name and by <font color="#FF0066">Round</font>.
+            So, if AddDoc was executed 2000 times in each of 3
+            <font color="#FF0066">rounds</font>, 3 report lines would be
+            created for it,
+            aggregating all those 2000 statistic records in each round.
+            See more about rounds in the <font color="#FF0066">NewRound</font>
+            command description below.
+            </li>
+            <li>
+            <font color="#FF0066">RepSumByPrefRound &nbsp; prefixWord</font> -
+            similar to <font color="#FF0066">RepSumByNameRound</font>,
+            just that only tasks whose name starts with
+            <font color="#FF0066">prefixWord</font> are included.
+            </li>
+ </ul>
+ If needed, additional reports can be added by extending the abstract class
+ ReportTask, and by
+ manipulating the statistics data in Points and TaskStats.
+ </li>
+
+ <li><b>Control tasks</b>: Few of the tasks control the benchmark algorithm
+ all over:
+ <ul>
+     <li>
+     <font color="#FF0066">ClearStats</font> - clears the entire statistics.
+     Further reports would only include task runs that would start after this
+     call.
+     </li>
+     <li>
+     <font color="#FF0066">NewRound</font> - virtually start a new round of
+     performance test.
+     Although this command can be placed anywhere, it mostly makes sense at
+     the end of an outermost sequence.
+     <br>This increments a global "round counter". All task runs that
+     would start now would
+     record the new, updated round counter as their round number.
+     This would appear in reports.
+     In particular, see <font color="#FF0066">RepSumByNameRound</font> above.
+     <br>An additional effect of NewRound, is that numeric and boolean
+     properties defined (at the head
+     of the .alg file) as a sequence of values, e.g. <font color="#FF0066">
+     merge.factor=mrg:10:100:10:100</font> would
+     increment (cyclic) to the next value.
+     Note: this would also be reflected in the reports, in this case under a
+     column that would be named "mrg".
+     </li>
+     <li>
+     <font color="#FF0066">ResetInputs</font> - DocMaker and the
+     various QueryMakers
+     would reset their counters to start.
+     The way these Maker interfaces work, each call for makeDocument()
+     or makeQuery() creates the next document or query
+     that it "knows" to create.
+     If that pool is "exhausted", the "maker" start over again.
+     The resetInpus command
+     therefore allows to make the rounds comparable.
+     It is therefore useful to invoke ResetInputs together with NewRound.
+     </li>
+     <li>
+     <font color="#FF0066">ResetSystemErase</font> - reset all index
+     and input data and call gc.
+     Does NOT reset statistics. This contains ResetInputs.
+     All writers/readers are nullified, deleted, closed.
+     Index is erased.
+     Directory is erased.
+     You would have to call CreateIndex once this was called...
+     </li>
+     <li>
+     <font color="#FF0066">ResetSystemSoft</font> -  reset all
+     index and input data and call gc.
+     Does NOT reset statistics. This contains ResetInputs.
+     All writers/readers are nullified, closed.
+     Index is NOT erased.
+     Directory is NOT erased.
+     This is useful for testing performance on an existing index,
+     for instance if the construction of a large index
+     took a very long time and now you would to test
+     its search or update performance.
+     </li>
+ </ul>
+ </li>
+
+ <li>
+ Other existing tasks are quite straightforward and would
+ just be briefly described here.
+ <ul>
+     <li>
+     <font color="#FF0066">CreateIndex</font> and
+     <font color="#FF0066">OpenIndex</font> both leave the
+     index open for later update operations.
+     <font color="#FF0066">CloseIndex</font> would close it.
+     <li>
+     <font color="#FF0066">OpenReader</font>, similarly, would
+     leave an index reader open for later search operations.
+     But this have further semantics.
+     If a Read operation is performed, and an open reader exists,
+     it would be used.
+     Otherwise, the read operation would open its own reader
+     and close it when the read operation is done.
+     This allows testing various scenarios - sharing a reader,
+     searching with "cold" reader, with "warmed" reader, etc.
+     The read operations affected by this are:
+     <font color="#FF0066">Warm</font>,
+     <font color="#FF0066">Search</font>,
+     <font color="#FF0066">SearchTrav</font> (search and traverse),
+     and <font color="#FF0066">SearchTravRet</font> (search
+     and traverse and retrieve).
+     Notice that each of the 3 search task types maintains
+     its own queryMaker instance.
+	 <li>
+	 <font color="#FF0066">CommitIndex</font> and 
+	 <font color="#FF0066">Optimize</font> can be used to commit
+	 changes to the index and/or optimize the index created thus
+	 far.
+	 <li>
+	 <font color="#FF0066">WriteLineDoc</font> prepares a 'line'
+	 file where each line holds a document with <i>title</i>, 
+	 <i>date</i> and <i>body</i> elements, seperated by [TAB].
+	 A line file is useful if one wants to measure pure indexing
+	 performance, without the overhead of parsing the data.<br>
+	 You can use LineDocSource as a ContentSource over a 'line'
+	 file.
+	 <li>
+	 <font color="#FF0066">ConsumeContentSource</font> consumes
+	 a ContentSource. Useful for e.g. testing a ContentSource
+	 performance, without the overhead of preparing a Document
+	 out of it.
+ </ul>
+ </li>
+ </ol>
+
+<a name="properties"></a>
+<h2>Benchmark properties</h2>
+
+<p>
+Properties are read from the header of the .alg file, and
+define several parameters of the performance test.
+As mentioned above for the <font color="#FF0066">NewRound</font> task,
+numeric and boolean properties that are defined as a sequence
+of values, e.g. <font color="#FF0066">merge.factor=mrg:10:100:10:100</font>
+would increment (cyclic) to the next value,
+when NewRound is called, and would also
+appear as a named column in the reports (column
+name would be "mrg" in this example).
+</p>
+
+<p>
+Some of the currently defined properties are:
+</p>
+
+<ol>
+    <li>
+    <font color="#FF0066">analyzer</font> - full
+    class name for the analyzer to use.
+    Same analyzer would be used in the entire test.
+    </li>
+
+    <li>
+    <font color="#FF0066">directory</font> - valid values are
+    This tells which directory to use for the performance test.
+    </li>
+
+    <li>
+    <b>Index work parameters</b>:
+    Multi int/boolean values would be iterated with calls to NewRound.
+    There would be also added as columns in the reports, first string in the
+    sequence is the column name.
+    (Make sure it is no shorter than any value in the sequence).
+    <ul>
+        <li><font color="#FF0066">max.buffered</font>
+        <br>Example: max.buffered=buf:10:10:100:100 -
+        this would define using maxBufferedDocs of 10 in iterations 0 and 1,
+        and 100 in iterations 2 and 3.
+        </li>
+        <li>
+        <font color="#FF0066">merge.factor</font> - which
+        merge factor to use.
+        </li>
+        <li>
+        <font color="#FF0066">compound</font> - whether the index is
+        using the compound format or not. Valid values are "true" and "false".
+        </li>
+    </ul>
+</ol>
+
+<p>
+Here is a list of currently defined properties:
+</p>
+<ol>
+
+  <li><b>Root directory for data and indexes:</b></li>
+    <ul><li>work.dir (default is System property "benchmark.work.dir" or "work".)
+    </li></ul>
+  </li>
+
+  <li><b>Docs and queries creation:</b></li>
+    <ul><li>analyzer
+    </li><li>doc.maker
+    </li><li>doc.maker.forever
+    </li><li>html.parser
+    </li><li>doc.stored
+    </li><li>doc.tokenized
+    </li><li>doc.term.vector
+    </li><li>doc.term.vector.positions
+    </li><li>doc.term.vector.offsets
+    </li><li>doc.store.body.bytes
+    </li><li>docs.dir
+    </li><li>query.maker
+    </li><li>file.query.maker.file
+    </li><li>file.query.maker.default.field
+    </li><li>search.num.hits
+    </li></ul>
+  </li>
+
+  <li><b>Logging</b>:
+    <ul><li>log.step
+	</li><li>log.step.[class name]Task ie log.step.DeleteDoc (e.g. log.step.Wonderful for the WonderfulTask example above).
+    </li><li>log.queries
+    </li><li>task.max.depth.log
+    </li></ul>
+  </li>
+
+  <li><b>Index writing</b>:
+    <ul><li>compound
+    </li><li>merge.factor
+    </li><li>max.buffered
+    </li><li>directory
+    </li><li>ram.flush.mb
+    </li></ul>
+  </li>
+
+  <li><b>Doc deletion</b>:
+    <ul><li>doc.delete.step
+    </li></ul>
+  </li>
+
+</ol>
+
+<p>
+For sample use of these properties see the *.alg files under conf.
+</p>
+
+<a name="example"></a>
+<h2>Example input algorithm and the result benchmark report</h2>
+<p>
+The following example is in conf/sample.alg:
+<pre>
+<font color="#003333"># --------------------------------------------------------
+#
+# Sample: what is the effect of doc size on indexing time?
+#
+# There are two parts in this test:
+# - PopulateShort adds 2N documents of length  L
+# - PopulateLong  adds  N documents of length 2L
+# Which one would be faster?
+# The comparison is done twice.
+#
+# --------------------------------------------------------
+</font>
+<font color="#990066"># -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+merge.factor=mrg:10:20
+max.buffered=buf:100:1000
+compound=true
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+
+doc.stored=true
+doc.tokenized=true
+doc.term.vector=false
+doc.add.log.step=500
+
+docs.dir=reuters-out
+
+doc.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleDocMaker
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=false
+# -------------------------------------------------------------------------------------</font>
+<font color="#3300FF">{
+
+    { "PopulateShort"
+        CreateIndex
+        { AddDoc(4000) > : 20000
+        Optimize
+        CloseIndex
+    >
+
+    ResetSystemErase
+
+    { "PopulateLong"
+        CreateIndex
+        { AddDoc(8000) > : 10000
+        Optimize
+        CloseIndex
+    >
+
+    ResetSystemErase
+
+    NewRound
+
+} : 2
+
+RepSumByName
+RepSelectByPref Populate
+</font>
+</pre>
+</p>
+
+<p>
+The command line for running this sample:
+<br><code>ant run-task -Dtask.alg=conf/sample.alg</code>
+</p>
+
+<p>
+The output report from running this test contains the following:
+<pre>
+Operation     round mrg  buf   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
+PopulateShort     0  10  100        1        20003        119.6      167.26    12,959,120     14,241,792
+PopulateLong -  - 0  10  100 -  -   1 -  -   10003 -  -  - 74.3 -  - 134.57 -  17,085,208 -   20,635,648
+PopulateShort     1  20 1000        1        20003        143.5      139.39    63,982,040     94,756,864
+PopulateLong -  - 1  20 1000 -  -   1 -  -   10003 -  -  - 77.0 -  - 129.92 -  87,309,608 -  100,831,232
+</pre>
+</p>
+
+<a name="recsCounting"></a>
+<h2>Results record counting clarified</h2>
+<p>
+Two columns in the results table indicate records counts: records-per-run and
+records-per-second. What does it mean?
+</p><p>
+Almost every task gets 1 in this count just for being executed.
+Task sequences aggregate the counts of their child tasks,
+plus their own count of 1.
+So, a task sequence containing 5 other task sequences, each running a single
+other task 10 times, would have a count of 1 + 5 * (1 + 10) = 56.
+</p><p>
+The traverse and retrieve tasks "count" more: a traverse task
+would add 1 for each traversed result (hit), and a retrieve task would
+additionally add 1 for each retrieved doc. So, regular Search would
+count 1, SearchTrav that traverses 10 hits would count 11, and a
+SearchTravRet task that retrieves (and traverses) 10, would count 21.
+</p><p>
+Confusing? this might help: always examine the <code>elapsedSec</code> column,
+and always compare "apples to apples", .i.e. it is interesting to check how the
+<code>rec/s</code> changed for the same task (or sequence) between two
+different runs, but it is not very useful to know how the <code>rec/s</code>
+differs between <code>Search</code> and <code>SearchTrav</code> tasks. For
+the latter, <code>elapsedSec</code> would bring more insight.
+</p>
+
+</DIV>
+<DIV>&nbsp;</DIV>
+</BODY>
+</HTML>
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/programmatic/Sample.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/programmatic/Sample.java
new file mode 100644
index 0000000..6a1a603
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/programmatic/Sample.java
@@ -0,0 +1,100 @@
+package org.apache.lucene.benchmark.byTask.programmatic;
+
+/**
+ * Copyright 2005 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Properties;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.tasks.AddDocTask;
+import org.apache.lucene.benchmark.byTask.tasks.CloseIndexTask;
+import org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask;
+import org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask;
+import org.apache.lucene.benchmark.byTask.tasks.TaskSequence;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+
+/**
+ * Sample performance test written programmatically - no algorithm file is needed here.
+ */
+public class Sample {
+
+  /**
+   * @param args
+   * @throws Exception 
+   * @throws IOException 
+   */
+  public static void main(String[] args) throws Exception {
+    Properties p = initProps();
+    Config conf = new Config(p);
+    PerfRunData runData = new PerfRunData(conf);
+    
+    // 1. top sequence
+    TaskSequence top = new TaskSequence(runData,null,null,false); // top level, not parallel
+    
+    // 2. task to create the index
+    CreateIndexTask create = new CreateIndexTask(runData);
+    top.addTask(create);
+    
+    // 3. task seq to add 500 docs (order matters - top to bottom - add seq to top, only then add to seq)
+    TaskSequence seq1 = new TaskSequence(runData,"AddDocs",top,false);
+    seq1.setRepetitions(500);
+    seq1.setNoChildReport();
+    top.addTask(seq1);
+
+    // 4. task to add the doc
+    AddDocTask addDoc = new AddDocTask(runData);
+    //addDoc.setParams("1200"); // doc size limit if supported
+    seq1.addTask(addDoc); // order matters 9see comment above)
+
+    // 5. task to close the index
+    CloseIndexTask close = new CloseIndexTask(runData);
+    top.addTask(close);
+
+    // task to report
+    RepSumByNameTask rep = new RepSumByNameTask(runData);
+    top.addTask(rep);
+
+    // print algorithm
+    System.out.println(top.toString());
+    
+    // execute
+    top.doLogic();
+  }
+
+  // Sample programmatic settings. Could also read from file.
+  private static Properties initProps() {
+    Properties p = new Properties();
+    p.setProperty ( "task.max.depth.log"  , "3" );
+    p.setProperty ( "max.buffered"        , "buf:10:10:100:100:10:10:100:100" );
+    p.setProperty ( "doc.maker"           , "org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource" );
+    p.setProperty ( "log.step"            , "2000" );
+    p.setProperty ( "doc.delete.step"     , "8" );
+    p.setProperty ( "analyzer"            , "org.apache.lucene.analysis.standard.StandardAnalyzer" );
+    p.setProperty ( "doc.term.vector"     , "false" );
+    p.setProperty ( "directory"           , "FSDirectory" );
+    p.setProperty ( "query.maker"         , "org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker" );
+    p.setProperty ( "doc.stored"          , "true" );
+    p.setProperty ( "docs.dir"            , "reuters-out" );
+    p.setProperty ( "compound"            , "cmpnd:true:true:true:true:false:false:false:false" );
+    p.setProperty ( "doc.tokenized"       , "true" );
+    p.setProperty ( "merge.factor"        , "mrg:10:100:10:100:10:100:10:100" );
+    return p;
+  }
+  
+  
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/programmatic/package.html b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/programmatic/package.html
new file mode 100644
index 0000000..7221c42
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/programmatic/package.html
@@ -0,0 +1,22 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<body>
+Sample performance test written programmatically - no algorithm file is needed here.
+</body>
+</html>
\ No newline at end of file
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/Points.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/Points.java
new file mode 100644
index 0000000..d2f8b2b
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/Points.java
@@ -0,0 +1,95 @@
+package org.apache.lucene.benchmark.byTask.stats;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.benchmark.byTask.tasks.PerfTask;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+
+
+/**
+ * Test run data points collected as the test proceeds.
+ */
+public class Points {
+
+  // stat points ordered by their start time. 
+  // for now we collect points as TaskStats objects.
+  // later might optimize to collect only native data.
+  private ArrayList<TaskStats> points = new ArrayList<TaskStats>();
+
+  private int nextTaskRunNum = 0;
+
+  private TaskStats currentStats;
+
+  /**
+   * Create a Points statistics object. 
+   */
+  public Points (Config config) {
+  }
+
+  /**
+   * Return the current task stats.
+   * the actual task stats are returned, so caller should not modify this task stats. 
+   * @return current {@link TaskStats}.
+   */
+  public List<TaskStats> taskStats () {
+    return points;
+  }
+
+  /**
+   * Mark that a task is starting. 
+   * Create a task stats for it and store it as a point.
+   * @param task the starting task.
+   * @return the new task stats created for the starting task.
+   */
+  public synchronized TaskStats markTaskStart (PerfTask task, int round) {
+    TaskStats stats = new TaskStats(task, nextTaskRunNum(), round);
+    this.currentStats = stats;
+    points.add(stats);
+    return stats;
+  }
+
+  public TaskStats getCurrentStats() {
+    return currentStats;
+  }
+  
+  // return next task num
+  private synchronized int nextTaskRunNum() {
+    return nextTaskRunNum++;
+  }
+  
+  /**
+   * mark the end of a task
+   */
+  public synchronized void markTaskEnd (TaskStats stats, int count) {
+    int numParallelTasks = nextTaskRunNum - 1 - stats.getTaskRunNum();
+    // note: if the stats were cleared, might be that this stats object is 
+    // no longer in points, but this is just ok.
+    stats.markEnd(numParallelTasks, count);
+  }
+
+  /**
+   * Clear all data, prepare for more tests.
+   */
+  public void clearData() {
+    points.clear();
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/Report.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/Report.java
new file mode 100644
index 0000000..1db8f6e
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/Report.java
@@ -0,0 +1,64 @@
+package org.apache.lucene.benchmark.byTask.stats;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Textual report of current statistics.
+ */
+public class Report {
+
+  private String text;
+  private int size;
+  private int outOf;
+  private int reported;
+
+  public Report (String text, int size, int reported, int outOf) {
+    this.text = text;
+    this.size = size;
+    this.reported = reported;
+    this.outOf = outOf;
+  }
+
+  /**
+   * Returns total number of stats points when this report was created.
+   */
+  public int getOutOf() {
+    return outOf;
+  }
+
+  /**
+   * Returns number of lines in the report.
+   */
+  public int getSize() {
+    return size;
+  }
+
+  /**
+   * Returns the report text.
+   */
+  public String getText() {
+    return text;
+  }
+
+  /**
+   * Returns number of stats points represented in this report.
+   */
+  public int getReported() {
+    return reported;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/TaskStats.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/TaskStats.java
new file mode 100644
index 0000000..6924670
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/TaskStats.java
@@ -0,0 +1,226 @@
+package org.apache.lucene.benchmark.byTask.stats;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.tasks.PerfTask;
+
+/**
+ * Statistics for a task run. 
+ * <br>The same task can run more than once, but, if that task records statistics, 
+ * each run would create its own TaskStats.
+ */
+public class TaskStats implements Cloneable {
+
+  /** task for which data was collected */
+  private PerfTask task; 
+
+  /** round in which task run started */
+  private int round;
+
+  /** task start time */
+  private long start;
+  
+  /** task elapsed time.  elapsed >= 0 indicates run completion! */
+  private long elapsed = -1;
+  
+  /** max tot mem during task */
+  private long maxTotMem;
+  
+  /** max used mem during task */
+  private long maxUsedMem;
+  
+  /** serial run number of this task run in the perf run */
+  private int taskRunNum;
+  
+  /** number of other tasks that started to run while this task was still running */ 
+  private int numParallelTasks;
+  
+  /** number of work items done by this task.
+   * For indexing that can be number of docs added.
+   * For warming that can be number of scanned items, etc. 
+   * For repeating tasks, this is a sum over repetitions.
+   */
+  private int count;
+
+  /** Number of similar tasks aggregated into this record.   
+   * Used when summing up on few runs/instances of similar tasks.
+   */
+  private int numRuns = 1;
+  
+  /**
+   * Create a run data for a task that is starting now.
+   * To be called from Points.
+   */
+  TaskStats (PerfTask task, int taskRunNum, int round) {
+    this.task = task;
+    this.taskRunNum = taskRunNum;
+    this.round = round;
+    maxTotMem = Runtime.getRuntime().totalMemory();
+    maxUsedMem = maxTotMem - Runtime.getRuntime().freeMemory();
+    start = System.currentTimeMillis();
+  }
+  
+  /**
+   * mark the end of a task
+   */
+  void markEnd (int numParallelTasks, int count) {
+    elapsed = System.currentTimeMillis() - start;
+    long totMem = Runtime.getRuntime().totalMemory();
+    if (totMem > maxTotMem) {
+      maxTotMem = totMem;
+    }
+    long usedMem = totMem - Runtime.getRuntime().freeMemory();
+    if (usedMem > maxUsedMem) {
+      maxUsedMem = usedMem;
+    }
+    this.numParallelTasks = numParallelTasks;
+    this.count = count;
+  }
+  
+  private int[] countsByTime;
+  private long countsByTimeStepMSec;
+
+  public void setCountsByTime(int[] counts, long msecStep) {
+    countsByTime = counts;
+    countsByTimeStepMSec = msecStep;
+  }
+
+  public int[] getCountsByTime() {
+    return countsByTime;
+  }
+
+  public long getCountsByTimeStepMSec() {
+    return countsByTimeStepMSec;
+  }
+
+  /**
+   * @return the taskRunNum.
+   */
+  public int getTaskRunNum() {
+    return taskRunNum;
+  }
+
+  /* (non-Javadoc)
+   * @see java.lang.Object#toString()
+   */
+  @Override
+  public String toString() {
+    StringBuilder res = new StringBuilder(task.getName());
+    res.append(" ");
+    res.append(count);
+    res.append(" ");
+    res.append(elapsed);
+    return res.toString();
+  }
+
+  /**
+   * @return Returns the count.
+   */
+  public int getCount() {
+    return count;
+  }
+
+  /**
+   * @return elapsed time.
+   */
+  public long getElapsed() {
+    return elapsed;
+  }
+
+  /**
+   * @return Returns the maxTotMem.
+   */
+  public long getMaxTotMem() {
+    return maxTotMem;
+  }
+
+  /**
+   * @return Returns the maxUsedMem.
+   */
+  public long getMaxUsedMem() {
+    return maxUsedMem;
+  }
+
+  /**
+   * @return Returns the numParallelTasks.
+   */
+  public int getNumParallelTasks() {
+    return numParallelTasks;
+  }
+
+  /**
+   * @return Returns the task.
+   */
+  public PerfTask getTask() {
+    return task;
+  }
+
+  /**
+   * @return Returns the numRuns.
+   */
+  public int getNumRuns() {
+    return numRuns;
+  }
+
+  /**
+   * Add data from another stat, for aggregation
+   * @param stat2 the added stat data.
+   */
+  public void add(TaskStats stat2) {
+    numRuns += stat2.getNumRuns();
+    elapsed += stat2.getElapsed();
+    maxTotMem += stat2.getMaxTotMem();
+    maxUsedMem += stat2.getMaxUsedMem();
+    count += stat2.getCount();
+    if (round != stat2.round) {
+      round = -1; // no meaning if aggregating tasks of different round. 
+    }
+
+    if (countsByTime != null && stat2.countsByTime != null) {
+      if (countsByTimeStepMSec != stat2.countsByTimeStepMSec) {
+        throw new IllegalStateException("different by-time msec step");
+      }
+      if (countsByTime.length != stat2.countsByTime.length) {
+        throw new IllegalStateException("different by-time msec count");
+      }
+      for(int i=0;i<stat2.countsByTime.length;i++) {
+        countsByTime[i] += stat2.countsByTime[i];
+      }
+    }
+  }
+
+  /* (non-Javadoc)
+   * @see java.lang.Object#clone()
+   */
+  @Override
+  public Object clone() throws CloneNotSupportedException {
+    TaskStats c = (TaskStats) super.clone();
+    if (c.countsByTime != null) {
+      c.countsByTime = c.countsByTime.clone();
+    }
+    return c;
+  }
+
+  /**
+   * @return the round number.
+   */
+  public int getRound() {
+    return round;
+  }
+  
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/package.html b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/package.html
new file mode 100644
index 0000000..fb44623
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/package.html
@@ -0,0 +1,22 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<body>
+  Statistics maintained when running benchmark tasks.
+</body>
+</html>
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddDocTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddDocTask.java
new file mode 100644
index 0000000..672d736
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddDocTask.java
@@ -0,0 +1,86 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
+import org.apache.lucene.document.Document;
+
+/**
+ * Add a document, optionally with of a certain size.
+ * <br>Other side effects: none.
+ * <br>Takes optional param: document size. 
+ */
+public class AddDocTask extends PerfTask {
+
+  public AddDocTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  private int docSize = 0;
+  
+  // volatile data passed between setup(), doLogic(), tearDown().
+  private Document doc = null;
+  
+  @Override
+  public void setup() throws Exception {
+    super.setup();
+    DocMaker docMaker = getRunData().getDocMaker();
+    if (docSize > 0) {
+      doc = docMaker.makeDocument(docSize);
+    } else {
+      doc = docMaker.makeDocument();
+    }
+  }
+
+  @Override
+  public void tearDown() throws Exception {
+    doc = null;
+    super.tearDown();
+  }
+
+  @Override
+  protected String getLogMessage(int recsCount) {
+    return "added " + recsCount + " docs";
+  }
+  
+  @Override
+  public int doLogic() throws Exception {
+    getRunData().getIndexWriter().addDocument(doc);
+    return 1;
+  }
+
+  /**
+   * Set the params (docSize only)
+   * @param params docSize, or 0 for no limit.
+   */
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    docSize = (int) Float.parseFloat(params); 
+  }
+
+  /* (non-Javadoc)
+   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#supportsParams()
+   */
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+  
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/BenchmarkHighlighter.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/BenchmarkHighlighter.java
new file mode 100644
index 0000000..c20720b
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/BenchmarkHighlighter.java
@@ -0,0 +1,27 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.IndexReader;
+
+public abstract class BenchmarkHighlighter {
+  public abstract int doHighlight( IndexReader reader, int doc, String field,
+      Document document, Analyzer analyzer, String text ) throws Exception ;
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ClearStatsTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ClearStatsTask.java
new file mode 100644
index 0000000..d1172d8
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ClearStatsTask.java
@@ -0,0 +1,46 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Clear statistics data.
+ * <br>Other side effects: None.
+ */
+public class ClearStatsTask extends PerfTask {
+
+  public ClearStatsTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public int doLogic() throws Exception {
+    getRunData().getPoints().clearData();
+    return 0;
+  }
+
+  /* (non-Javadoc)
+   * @see PerfTask#shouldNotRecordStats()
+   */
+  @Override
+  protected boolean shouldNotRecordStats() {
+    return true;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CloseIndexTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CloseIndexTask.java
new file mode 100644
index 0000000..992a3e59
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CloseIndexTask.java
@@ -0,0 +1,65 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.PrintStream;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.index.IndexWriter;
+
+/**
+ * Close index writer.
+ * <br>Other side effects: index writer object in perfRunData is nullified.
+ * <br>Takes optional param "doWait": if false, then close(false) is called.
+ */
+public class CloseIndexTask extends PerfTask {
+
+  public CloseIndexTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  boolean doWait = true;
+
+  @Override
+  public int doLogic() throws IOException {
+    IndexWriter iw = getRunData().getIndexWriter();
+    if (iw != null) {
+      // If infoStream was set to output to a file, close it.
+      PrintStream infoStream = iw.getInfoStream();
+      if (infoStream != null && infoStream != System.out
+          && infoStream != System.err) {
+        infoStream.close();
+      }
+      iw.close(doWait);
+      getRunData().setIndexWriter(null);
+    }
+    return 1;
+  }
+
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    doWait = Boolean.valueOf(params).booleanValue();
+  }
+
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CloseReaderTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CloseReaderTask.java
new file mode 100644
index 0000000..85c4d65
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CloseReaderTask.java
@@ -0,0 +1,47 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.index.IndexReader;
+
+/**
+ * Close index reader.
+ * <br>Other side effects: index reader in perfRunData is nullified.
+ * <br>This would cause read related tasks to reopen their own reader. 
+ */
+public class CloseReaderTask extends PerfTask {
+
+  public CloseReaderTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public int doLogic() throws IOException {
+    IndexReader reader = getRunData().getIndexReader();
+    getRunData().setIndexReader(null);
+    if (reader.getRefCount() != 1) {
+      System.out.println("WARNING: CloseReader: reference count is currently " + reader.getRefCount());
+    }
+    reader.decRef();
+    return 1;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CommitIndexTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CommitIndexTask.java
new file mode 100644
index 0000000..2b026d8
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CommitIndexTask.java
@@ -0,0 +1,66 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexReader;
+
+/**
+ * Commits the IndexWriter.
+ *
+ */
+public class CommitIndexTask extends PerfTask {
+  Map<String,String> commitUserData;
+
+  public CommitIndexTask(PerfRunData runData) {
+    super(runData);
+  }
+  
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+  
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    commitUserData = new HashMap<String,String>();
+    commitUserData.put(OpenReaderTask.USER_DATA, params);
+  }
+  
+  @Override
+  public int doLogic() throws Exception {
+    IndexWriter iw = getRunData().getIndexWriter();
+    if (iw != null) {
+      iw.commit(commitUserData);
+    } else {
+      IndexReader r = getRunData().getIndexReader();
+      if (r != null) {
+        r.commit(commitUserData);
+        r.decRef();
+      } else {
+        throw new IllegalStateException("neither IndexWriter nor IndexReader is currently open");
+      }
+    }
+    
+    return 1;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ConsumeContentSourceTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ConsumeContentSourceTask.java
new file mode 100644
index 0000000..5dbed92
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ConsumeContentSourceTask.java
@@ -0,0 +1,70 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.feeds.ContentSource;
+import org.apache.lucene.benchmark.byTask.feeds.DocData;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+
+/**
+ * Consumes a {@link org.apache.lucene.benchmark.byTask.feeds.ContentSource}.
+ * Supports the following parameters:
+ * <ul>
+ * <li>content.source - the content source to use. (mandatory)
+ * </ul>
+ */
+public class ConsumeContentSourceTask extends PerfTask {
+
+  private ContentSource source;
+  private DocData dd = new DocData();
+  
+  public ConsumeContentSourceTask(PerfRunData runData) {
+    super(runData);
+    Config config = runData.getConfig();
+    String sourceClass = config.get("content.source", null);
+    if (sourceClass == null) {
+      throw new IllegalArgumentException("content.source must be defined");
+    }
+    try {
+      source = Class.forName(sourceClass).asSubclass(ContentSource.class).newInstance();
+      source.setConfig(config);
+      source.resetInputs();
+    } catch (Exception e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  @Override
+  protected String getLogMessage(int recsCount) {
+    return "read " + recsCount + " documents from the content source";
+  }
+  
+  @Override
+  public void close() throws Exception {
+    source.close();
+    super.close();
+  }
+
+  @Override
+  public int doLogic() throws Exception {
+    dd = source.getNextDocData(dd);
+    return 1;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java
new file mode 100644
index 0000000..a347c9c
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java
@@ -0,0 +1,185 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.IndexCommit;
+import org.apache.lucene.index.IndexDeletionPolicy;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.LogMergePolicy;
+import org.apache.lucene.index.MergeScheduler;
+import org.apache.lucene.index.ConcurrentMergeScheduler;
+import org.apache.lucene.index.MergePolicy;
+import org.apache.lucene.index.NoDeletionPolicy;
+import org.apache.lucene.index.NoMergePolicy;
+import org.apache.lucene.index.NoMergeScheduler;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.index.codecs.CodecProvider;
+import org.apache.lucene.store.LockObtainFailedException;
+import org.apache.lucene.util.Version;
+
+import java.io.BufferedOutputStream;
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.PrintStream;
+
+/**
+ * Create an index. <br>
+ * Other side effects: index writer object in perfRunData is set. <br>
+ * Relevant properties: <code>merge.factor (default 10),
+ * max.buffered (default no flush), max.field.length (default
+ * 10,000 tokens), max.field.length, compound (default true), ram.flush.mb [default 0],
+ * merge.policy (default org.apache.lucene.index.LogByteSizeMergePolicy),
+ * merge.scheduler (default
+ * org.apache.lucene.index.ConcurrentMergeScheduler),
+ * concurrent.merge.scheduler.max.thread.count and
+ * concurrent.merge.scheduler.max.merge.count (defaults per
+ * ConcurrentMergeScheduler), default.codec </code>.
+ * <p>
+ * This task also supports a "writer.info.stream" property with the following
+ * values:
+ * <ul>
+ * <li>SystemOut - sets {@link IndexWriter#setInfoStream(java.io.PrintStream)}
+ * to {@link System#out}.
+ * <li>SystemErr - sets {@link IndexWriter#setInfoStream(java.io.PrintStream)}
+ * to {@link System#err}.
+ * <li>&lt;file_name&gt; - attempts to create a file given that name and sets
+ * {@link IndexWriter#setInfoStream(java.io.PrintStream)} to that file. If this
+ * denotes an invalid file name, or some error occurs, an exception will be
+ * thrown.
+ * </ul>
+ */
+public class CreateIndexTask extends PerfTask {
+
+  public CreateIndexTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  
+  
+  public static IndexDeletionPolicy getIndexDeletionPolicy(Config config) {
+    String deletionPolicyName = config.get("deletion.policy", "org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy");
+    if (deletionPolicyName.equals(NoDeletionPolicy.class.getName())) {
+      return NoDeletionPolicy.INSTANCE;
+    } else {
+      try {
+        return Class.forName(deletionPolicyName).asSubclass(IndexDeletionPolicy.class).newInstance();
+      } catch (Exception e) {
+        throw new RuntimeException("unable to instantiate class '" + deletionPolicyName + "' as IndexDeletionPolicy", e);
+      }
+    }
+  }
+  
+  @Override
+  public int doLogic() throws IOException {
+    PerfRunData runData = getRunData();
+    Config config = runData.getConfig();
+    runData.setIndexWriter(configureWriter(config, runData, OpenMode.CREATE, null));
+    return 1;
+  }
+  
+  public static IndexWriterConfig createWriterConfig(Config config, PerfRunData runData, OpenMode mode, IndexCommit commit) {
+    Version version = Version.valueOf(config.get("writer.version", Version.LUCENE_40.toString()));
+    IndexWriterConfig iwConf = new IndexWriterConfig(version, runData.getAnalyzer());
+    iwConf.setOpenMode(mode);
+    IndexDeletionPolicy indexDeletionPolicy = getIndexDeletionPolicy(config);
+    iwConf.setIndexDeletionPolicy(indexDeletionPolicy);
+    if(commit != null)
+      iwConf.setIndexCommit(commit);
+    
+
+    final String mergeScheduler = config.get("merge.scheduler",
+                                             "org.apache.lucene.index.ConcurrentMergeScheduler");
+    if (mergeScheduler.equals(NoMergeScheduler.class.getName())) {
+      iwConf.setMergeScheduler(NoMergeScheduler.INSTANCE);
+    } else {
+      try {
+        iwConf.setMergeScheduler(Class.forName(mergeScheduler).asSubclass(MergeScheduler.class).newInstance());
+      } catch (Exception e) {
+        throw new RuntimeException("unable to instantiate class '" + mergeScheduler + "' as merge scheduler", e);
+      }
+      
+      if (mergeScheduler.equals("org.apache.lucene.index.ConcurrentMergeScheduler")) {
+        ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) iwConf.getMergeScheduler();
+        int v = config.get("concurrent.merge.scheduler.max.thread.count", -1);
+        if (v != -1) {
+          cms.setMaxThreadCount(v);
+        }
+        v = config.get("concurrent.merge.scheduler.max.merge.count", -1);
+        if (v != -1) {
+          cms.setMaxMergeCount(v);
+        }
+      }
+    }
+
+    final String defaultCodec = config.get("default.codec", null);
+    if (defaultCodec != null) {
+      CodecProvider.getDefault().setDefaultFieldCodec(defaultCodec);
+    }
+
+    final String mergePolicy = config.get("merge.policy",
+                                          "org.apache.lucene.index.LogByteSizeMergePolicy");
+    boolean isCompound = config.get("compound", true);
+    if (mergePolicy.equals(NoMergePolicy.class.getName())) {
+      iwConf.setMergePolicy(isCompound ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES);
+    } else {
+      try {
+        iwConf.setMergePolicy(Class.forName(mergePolicy).asSubclass(MergePolicy.class).newInstance());
+      } catch (Exception e) {
+        throw new RuntimeException("unable to instantiate class '" + mergePolicy + "' as merge policy", e);
+      }
+      if(iwConf.getMergePolicy() instanceof LogMergePolicy) {
+        LogMergePolicy logMergePolicy = (LogMergePolicy) iwConf.getMergePolicy();
+        logMergePolicy.setUseCompoundFile(isCompound);
+        logMergePolicy.setMergeFactor(config.get("merge.factor",OpenIndexTask.DEFAULT_MERGE_PFACTOR));
+      }
+    }
+    iwConf.setMaxFieldLength(config.get("max.field.length",OpenIndexTask.DEFAULT_MAX_FIELD_LENGTH));
+    final double ramBuffer = config.get("ram.flush.mb",OpenIndexTask.DEFAULT_RAM_FLUSH_MB);
+    final int maxBuffered = config.get("max.buffered",OpenIndexTask.DEFAULT_MAX_BUFFERED);
+    if (maxBuffered == IndexWriterConfig.DISABLE_AUTO_FLUSH) {
+      iwConf.setRAMBufferSizeMB(ramBuffer);
+      iwConf.setMaxBufferedDocs(maxBuffered);
+    } else {
+      iwConf.setMaxBufferedDocs(maxBuffered);
+      iwConf.setRAMBufferSizeMB(ramBuffer);
+    }
+    
+    return iwConf;
+  }
+  
+  public static IndexWriter configureWriter(Config config, PerfRunData runData, OpenMode mode, IndexCommit commit) throws CorruptIndexException, LockObtainFailedException, IOException {
+    IndexWriter writer = new IndexWriter(runData.getDirectory(), createWriterConfig(config, runData, mode, commit));
+    String infoStreamVal = config.get("writer.info.stream", null);
+    if (infoStreamVal != null) {
+      if (infoStreamVal.equals("SystemOut")) {
+        writer.setInfoStream(System.out);
+      } else if (infoStreamVal.equals("SystemErr")) {
+        writer.setInfoStream(System.err);
+      } else {
+        File f = new File(infoStreamVal).getAbsoluteFile();
+        writer.setInfoStream(new PrintStream(new BufferedOutputStream(new FileOutputStream(f))));
+      }
+    }
+    return writer;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/DeleteByPercentTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/DeleteByPercentTask.java
new file mode 100644
index 0000000..46b603f
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/DeleteByPercentTask.java
@@ -0,0 +1,95 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Random;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.util.Bits;
+
+/**
+ * Deletes a percentage of documents from an index randomly
+ * over the number of documents.  The parameter, X, is in
+ * percent.  EG 50 means 1/2 of all documents will be
+ * deleted.
+ *
+ * <p><b>NOTE</b>: the param is an absolute percentage of
+ * maxDoc().  This means if you delete 50%, and then delete
+ * 50% again, the 2nd delete will do nothing.
+ *
+ * <p> Parameters:
+ * <ul>
+ * <li> delete.percent.rand.seed - defines the seed to
+ * initialize Random (default 1717)
+ * </ul>
+ */
+public class DeleteByPercentTask extends PerfTask {
+  double percent;
+  int numDeleted = 0;
+  final Random random;
+
+  public DeleteByPercentTask(PerfRunData runData) {
+    super(runData);
+    random = new Random(runData.getConfig().get("delete.percent.rand.seed", 1717));
+  }
+  
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    percent = Double.parseDouble(params)/100;
+  }
+
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+
+  @Override
+  public int doLogic() throws Exception {
+    IndexReader r = getRunData().getIndexReader();
+    int maxDoc = r.maxDoc();
+    int numDeleted = 0;
+    // percent is an absolute target:
+    int numToDelete = ((int) (maxDoc * percent)) - r.numDeletedDocs();
+    if (numToDelete < 0) {
+      r.undeleteAll();
+      numToDelete = (int) (maxDoc * percent);
+    }
+    while (numDeleted < numToDelete) {
+      double delRate = ((double) (numToDelete-numDeleted))/r.numDocs();
+      Bits delDocs = MultiFields.getDeletedDocs(r);
+      int doc = 0;
+      while (doc < maxDoc && numDeleted < numToDelete) {
+        if ((delDocs == null || !delDocs.get(doc)) && random.nextDouble() <= delRate) {
+          r.deleteDocument(doc);
+          numDeleted++;
+          if (delDocs == null) {
+            delDocs = MultiFields.getDeletedDocs(r);
+            assert delDocs != null;
+          }
+        }
+        doc++;
+      }
+    }
+    System.out.println("--> processed (delete) " + numDeleted + " docs");
+    r.decRef();
+    return numDeleted;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/DeleteDocTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/DeleteDocTask.java
new file mode 100644
index 0000000..52bf501
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/DeleteDocTask.java
@@ -0,0 +1,90 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.index.IndexReader;
+
+/**
+ * Delete a document by docid. If no docid param is supplied, deletes doc with
+ * <code>id = last-deleted-doc + doc.delete.step</code>.
+ */
+public class DeleteDocTask extends PerfTask {
+
+  /**
+   * Gap between ids of deleted docs, applies when no docid param is provided.
+   */
+  public static final int DEFAULT_DOC_DELETE_STEP = 8;
+  
+  public DeleteDocTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  private int deleteStep = -1;
+  private static int lastDeleted = -1;
+
+  private int docid = -1;
+  private boolean byStep = true;
+  
+  @Override
+  public int doLogic() throws Exception {
+    IndexReader r = getRunData().getIndexReader();
+    r.deleteDocument(docid);
+    lastDeleted = docid;
+    r.decRef();
+    return 1; // one work item done here
+  }
+
+  /* (non-Javadoc)
+   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#setup()
+   */
+  @Override
+  public void setup() throws Exception {
+    super.setup();
+    if (deleteStep<0) {
+      deleteStep = getRunData().getConfig().get("doc.delete.step",DEFAULT_DOC_DELETE_STEP);
+    }
+    // set the docid to be deleted
+    docid = (byStep ? lastDeleted + deleteStep : docid);
+  }
+
+  @Override
+  protected String getLogMessage(int recsCount) {
+    return "deleted " + recsCount + " docs, last deleted: " + lastDeleted;
+  }
+  
+  /**
+   * Set the params (docid only)
+   * @param params docid to delete, or -1 for deleting by delete gap settings.
+   */
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    docid = (int) Float.parseFloat(params);
+    byStep = (docid < 0);
+  }
+  
+  /* (non-Javadoc)
+   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#supportsParams()
+   */
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/FlushReaderTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/FlushReaderTask.java
new file mode 100644
index 0000000..1a9d8ec
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/FlushReaderTask.java
@@ -0,0 +1,58 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.index.IndexReader;
+
+public class FlushReaderTask extends PerfTask {
+  String userData = null;
+  
+  public FlushReaderTask(PerfRunData runData) {
+    super(runData);
+  }
+  
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+  
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    userData = params;
+  }
+  
+  @Override
+  public int doLogic() throws IOException {
+    IndexReader reader = getRunData().getIndexReader();
+    if (userData != null) {
+      Map<String,String> map = new HashMap<String,String>();
+      map.put(OpenReaderTask.USER_DATA, userData);
+      reader.flush(map);
+    } else {
+      reader.flush();
+    }
+    reader.decRef();
+    return 1;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NearRealtimeReaderTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NearRealtimeReaderTask.java
new file mode 100644
index 0000000..398c72f
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NearRealtimeReaderTask.java
@@ -0,0 +1,118 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.util.ArrayUtil;
+
+/**
+ * Spawns a BG thread that periodically (defaults to 3.0
+ * seconds, but accepts param in seconds) wakes up and asks
+ * IndexWriter for a near real-time reader.  Then runs a
+ * single query (body: 1) sorted by docdate, and prints
+ * time to reopen and time to run the search.
+ *
+ * @lucene.experimental It's also not generally usable, eg
+ * you cannot change which query is executed.
+ */
+public class NearRealtimeReaderTask extends PerfTask {
+
+  long pauseMSec = 3000L;
+
+  int reopenCount;
+  int[] reopenTimes = new int[1];
+
+  public NearRealtimeReaderTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public int doLogic() throws Exception {
+
+    final PerfRunData runData = getRunData();
+
+    // Get initial reader
+    IndexWriter w = runData.getIndexWriter();
+    if (w == null) {
+      throw new RuntimeException("please open the writer before invoking NearRealtimeReader");
+    }
+
+    if (runData.getIndexReader() != null) {
+      throw new RuntimeException("please close the existing reader before invoking NearRealtimeReader");
+    }
+    
+    long t = System.currentTimeMillis();
+    IndexReader r = IndexReader.open(w);
+    runData.setIndexReader(r);
+    // Transfer our reference to runData
+    r.decRef();
+
+    // TODO: gather basic metrics for reporting -- eg mean,
+    // stddev, min/max reopen latencies
+
+    // Parent sequence sets stopNow
+    reopenCount = 0;
+    while(!stopNow) {
+      long waitForMsec = (pauseMSec - (System.currentTimeMillis() - t));
+      if (waitForMsec > 0) {
+        Thread.sleep(waitForMsec);
+        //System.out.println("NRT wait: " + waitForMsec + " msec");
+      }
+
+      t = System.currentTimeMillis();
+      final IndexReader newReader = r.reopen();
+      if (r != newReader) {
+        final int delay = (int) (System.currentTimeMillis()-t);
+        if (reopenTimes.length == reopenCount) {
+          reopenTimes = ArrayUtil.grow(reopenTimes, 1+reopenCount);
+        }
+        reopenTimes[reopenCount++] = delay;
+        // TODO: somehow we need to enable warming, here
+        runData.setIndexReader(newReader);
+        // Transfer our reference to runData
+        newReader.decRef();
+        r = newReader;
+      }
+    }
+    stopNow = false;
+
+    return reopenCount;
+  }
+
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    pauseMSec = (long) (1000.0*Float.parseFloat(params));
+  }
+
+  @Override
+  public void close() {
+    System.out.println("NRT reopen times:");
+    for(int i=0;i<reopenCount;i++) {
+      System.out.print(" " + reopenTimes[i]);
+    }
+    System.out.println();
+  }
+
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewAnalyzerTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewAnalyzerTask.java
new file mode 100644
index 0000000..6d30114
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewAnalyzerTask.java
@@ -0,0 +1,101 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+/**
+ * Copyright 2005 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.util.Version;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.StringTokenizer;
+import java.lang.reflect.Constructor;
+
+/**
+ * Create a new {@link org.apache.lucene.analysis.Analyzer} and set it it in the getRunData() for use by all future tasks.
+ *
+ */
+public class NewAnalyzerTask extends PerfTask {
+  private List<String> analyzerClassNames;
+  private int current;
+
+  public NewAnalyzerTask(PerfRunData runData) {
+    super(runData);
+    analyzerClassNames = new ArrayList<String>();
+  }
+  
+  public static final Analyzer createAnalyzer(String className) throws Exception{
+    final Class<? extends Analyzer> clazz = Class.forName(className).asSubclass(Analyzer.class);
+    try {
+      // first try to use a ctor with version parameter (needed for many new Analyzers that have no default one anymore
+      Constructor<? extends Analyzer> cnstr = clazz.getConstructor(Version.class);
+      return cnstr.newInstance(Version.LUCENE_CURRENT);
+    } catch (NoSuchMethodException nsme) {
+      // otherwise use default ctor
+      return clazz.newInstance();
+    }
+  }
+
+  @Override
+  public int doLogic() throws IOException {
+    String className = null;
+    try {
+      if (current >= analyzerClassNames.size())
+      {
+        current = 0;
+      }
+      className = analyzerClassNames.get(current++);
+      if (className == null || className.equals(""))
+      {
+        className = "org.apache.lucene.analysis.standard.StandardAnalyzer"; 
+      }
+      if (className.indexOf(".") == -1  || className.startsWith("standard."))//there is no package name, assume o.a.l.analysis
+      {
+        className = "org.apache.lucene.analysis." + className;
+      }
+      getRunData().setAnalyzer(createAnalyzer(className));
+      System.out.println("Changed Analyzer to: " + className);
+    } catch (Exception e) {
+      throw new RuntimeException("Error creating Analyzer: " + className, e);
+    }
+    return 1;
+  }
+
+  /**
+   * Set the params (analyzerClassName only),  Comma-separate list of Analyzer class names.  If the Analyzer lives in
+   * org.apache.lucene.analysis, the name can be shortened by dropping the o.a.l.a part of the Fully Qualified Class Name.
+   * <p/>
+   * Example Declaration: {"NewAnalyzer" NewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer) >
+   * @param params analyzerClassName, or empty for the StandardAnalyzer
+   */
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    for (StringTokenizer tokenizer = new StringTokenizer(params, ","); tokenizer.hasMoreTokens();) {
+      String s = tokenizer.nextToken();
+      analyzerClassNames.add(s.trim());
+    }
+  }
+
+  /* (non-Javadoc)
+   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#supportsParams()
+   */
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewCollationAnalyzerTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewCollationAnalyzerTask.java
new file mode 100644
index 0000000..2dd29ec
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewCollationAnalyzerTask.java
@@ -0,0 +1,117 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.lang.reflect.Constructor;
+import java.lang.reflect.Method;
+import java.util.Locale;
+import java.util.StringTokenizer;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+
+/**
+ * Task to support benchmarking collation.
+ * <p>
+ * <ul>
+ *  <li> <code>NewCollationAnalyzer</code> with the default jdk impl
+ *  <li> <code>NewCollationAnalyzer(impl:icu)</code> specify an impl (jdk,icu)
+ * </ul>
+ * </p>
+ */
+public class NewCollationAnalyzerTask extends PerfTask {
+  public enum Implementation { 
+    JDK("org.apache.lucene.collation.CollationKeyAnalyzer", 
+        "java.text.Collator"),
+    ICU("org.apache.lucene.collation.ICUCollationKeyAnalyzer", 
+        "com.ibm.icu.text.Collator");
+    
+    String className;
+    String collatorClassName;
+    
+    Implementation(String className, String collatorClassName) {
+      this.className = className;
+      this.collatorClassName = collatorClassName;
+    }
+  }
+  
+  private Implementation impl = Implementation.JDK;
+
+  public NewCollationAnalyzerTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  static Analyzer createAnalyzer(Locale locale, Implementation impl)
+      throws Exception {
+    final Class<?> collatorClazz = Class.forName(impl.collatorClassName);
+    Method collatorMethod = collatorClazz.getMethod("getInstance",
+        new Class[] {Locale.class});
+    Object collator = collatorMethod.invoke(null, locale);
+    
+    final Class<? extends Analyzer> clazz = Class.forName(impl.className)
+        .asSubclass(Analyzer.class);
+    Constructor<? extends Analyzer> ctor = clazz.getConstructor(collatorClazz);
+    return ctor.newInstance(collator);
+  }
+  
+  @Override
+  public int doLogic() throws Exception {
+    try {
+      Locale locale = getRunData().getLocale();
+      if (locale == null) throw new RuntimeException(
+          "Locale must be set with the NewLocale task!");
+      Analyzer analyzer = createAnalyzer(locale, impl);
+      getRunData().setAnalyzer(analyzer);
+      System.out.println("Changed Analyzer to: "
+          + analyzer.getClass().getName() + "(" + locale + ")");
+    } catch (Exception e) {
+      throw new RuntimeException("Error creating Analyzer: impl=" + impl, e);
+    }
+    return 1;
+  }
+  
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    
+    StringTokenizer st = new StringTokenizer(params, ",");
+    while (st.hasMoreTokens()) {
+      String param = st.nextToken();
+      StringTokenizer expr = new StringTokenizer(param, ":");
+      String key = expr.nextToken();
+      String value = expr.nextToken();
+      // for now we only support the "impl" parameter.
+      // TODO: add strength, decomposition, etc
+      if (key.equals("impl")) {
+        if (value.equalsIgnoreCase("icu"))
+          impl = Implementation.ICU;
+        else if (value.equalsIgnoreCase("jdk"))
+          impl = Implementation.JDK;
+        else
+          throw new RuntimeException("Unknown parameter " + param);
+      } else {
+        throw new RuntimeException("Unknown parameter " + param);
+      }
+    }
+  }
+
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewLocaleTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewLocaleTask.java
new file mode 100644
index 0000000..196af26
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewLocaleTask.java
@@ -0,0 +1,89 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Locale;
+import java.util.StringTokenizer;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+
+/**
+ * Set a {@link java.util.Locale} for use in benchmarking.
+ * <p>
+ * Locales can be specified in the following ways:
+ * <ul>
+ *  <li><code>de</code>: Language "de"
+ *  <li><code>en,US</code>: Language "en", country "US"
+ *  <li><code>no,NO,NY</code>: Language "no", country "NO", variant "NY" 
+ *  <li><code>ROOT</code>: The root (language-agnostic) Locale
+ *  <li>&lt;empty string&gt;: Erase the Locale (null)
+ * </ul>
+ * </p>
+ */
+public class NewLocaleTask extends PerfTask {
+  private String language;
+  private String country;
+  private String variant;
+  
+  /**
+   * Create a new {@link java.util.Locale} and set it it in the getRunData() for
+   * use by all future tasks.
+   */
+  public NewLocaleTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  static Locale createLocale(String language, String country, String variant) {
+    if (language == null || language.length() == 0) 
+      return null;
+    
+    String lang = language;
+    if (lang.equalsIgnoreCase("ROOT"))
+      lang = ""; // empty language is the root locale in the JDK
+      
+    return new Locale(lang, country, variant);
+  }
+  
+  @Override
+  public int doLogic() throws Exception {
+    Locale locale = createLocale(language, country, variant);
+    getRunData().setLocale(locale);
+    System.out.println("Changed Locale to: " + 
+        (locale == null ? "null" : 
+        (locale.getDisplayName().length() == 0) ? "root locale" : locale));
+    return 1;
+  }
+  
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    language = country = variant = "";
+    StringTokenizer st = new StringTokenizer(params, ",");
+    if (st.hasMoreTokens())
+      language = st.nextToken();
+    if (st.hasMoreTokens())
+      country = st.nextToken();
+    if (st.hasMoreTokens())
+      variant = st.nextToken();
+  }
+
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewRoundTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewRoundTask.java
new file mode 100644
index 0000000..ec16979
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewRoundTask.java
@@ -0,0 +1,46 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/**
+ * Increment the counter for properties maintained by Round Number.
+ * <br>Other side effects: if there are props by round number, log value change.
+ */
+public class NewRoundTask extends PerfTask {
+
+  public NewRoundTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public int doLogic() throws Exception {
+    getRunData().getConfig().newRound();
+    return 0;
+  }
+
+  /* (non-Javadoc)
+   * @see PerfTask#shouldNotRecordStats()
+   */
+  @Override
+  protected boolean shouldNotRecordStats() {
+    return true;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewShingleAnalyzerTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewShingleAnalyzerTask.java
new file mode 100644
index 0000000..27b805c
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewShingleAnalyzerTask.java
@@ -0,0 +1,113 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.lang.reflect.Constructor;
+import java.util.StringTokenizer;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.shingle.ShingleAnalyzerWrapper;
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.util.Version;
+
+/**
+ * Task to support benchmarking ShingleFilter / ShingleAnalyzerWrapper
+ * <p>
+ * <ul>
+ *  <li> <code>NewShingleAnalyzer</code> (constructs with all defaults)
+ *  <li> <code>NewShingleAnalyzer(analyzer:o.a.l.analysis.StandardAnalyzer,maxShingleSize:2,outputUnigrams:true)</code>
+ * </ul>
+ * </p>
+ */
+public class NewShingleAnalyzerTask extends PerfTask {
+
+  private String analyzerClassName = "standard.StandardAnalyzer";
+  private int maxShingleSize = 2;
+  private boolean outputUnigrams = true;
+  
+  public NewShingleAnalyzerTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  private void setAnalyzer() throws Exception {
+    Class<? extends Analyzer> clazz = null;
+    Analyzer wrappedAnalyzer;
+    try {
+      if (analyzerClassName == null || analyzerClassName.equals("")) {
+        analyzerClassName 
+          = "org.apache.lucene.analysis.standard.StandardAnalyzer"; 
+      }
+      if (analyzerClassName.indexOf(".") == -1 
+          || analyzerClassName.startsWith("standard.")) {
+        //there is no package name, assume o.a.l.analysis
+        analyzerClassName = "org.apache.lucene.analysis." + analyzerClassName;
+      }
+      clazz = Class.forName(analyzerClassName).asSubclass(Analyzer.class);
+      // first try to use a ctor with version parameter (needed for many new 
+      // Analyzers that have no default one anymore)
+      Constructor<? extends Analyzer> ctor = clazz.getConstructor(Version.class);
+      wrappedAnalyzer = ctor.newInstance(Version.LUCENE_CURRENT);
+    } catch (NoSuchMethodException e) {
+      // otherwise use default ctor
+      wrappedAnalyzer = clazz.newInstance();
+    }
+    ShingleAnalyzerWrapper analyzer 
+      = new ShingleAnalyzerWrapper(wrappedAnalyzer, maxShingleSize);
+    analyzer.setOutputUnigrams(outputUnigrams);
+    getRunData().setAnalyzer(analyzer);
+  }
+  
+  @Override
+  public int doLogic() throws Exception {
+    try {
+      setAnalyzer();
+      System.out.println
+        ("Changed Analyzer to: ShingleAnalyzerWrapper, wrapping ShingleFilter over" 
+         + analyzerClassName);
+    } catch (Exception e) {
+      throw new RuntimeException("Error creating Analyzer", e);
+    }
+    return 1;
+  }
+  
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    StringTokenizer st = new StringTokenizer(params, ",");
+    while (st.hasMoreTokens()) {
+      String param = st.nextToken();
+      StringTokenizer expr = new StringTokenizer(param, ":");
+      String key = expr.nextToken();
+      String value = expr.nextToken();
+      if (key.equalsIgnoreCase("analyzer")) {
+        analyzerClassName = value;
+      } else if (key.equalsIgnoreCase("outputUnigrams")) {
+        outputUnigrams = Boolean.parseBoolean(value);
+      } else if (key.equalsIgnoreCase("maxShingleSize")) {
+        maxShingleSize = (int)Double.parseDouble(value);
+      } else {
+        throw new RuntimeException("Unknown parameter " + param);
+      }
+    }
+  }
+
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenIndexTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenIndexTask.java
new file mode 100644
index 0000000..fe61e44
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenIndexTask.java
@@ -0,0 +1,82 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexCommit;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.LogMergePolicy;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import java.io.IOException;
+
+
+/**
+ * Open an index writer.
+ * <br>Other side effects: index writer object in perfRunData is set.
+ * <br>Relevant properties: <code>merge.factor, max.buffered,
+ * max.field.length, ram.flush.mb [default 0]</code>.
+ *
+ * <p> Accepts a param specifying the commit point as
+ * previously saved with CommitIndexTask.  If you specify
+ * this, it rolls the index back to that commit on opening
+ * the IndexWriter.
+ */
+public class OpenIndexTask extends PerfTask {
+
+  public static final int DEFAULT_MAX_BUFFERED = IndexWriterConfig.DEFAULT_MAX_BUFFERED_DOCS;
+  public static final int DEFAULT_MAX_FIELD_LENGTH = IndexWriterConfig.UNLIMITED_FIELD_LENGTH;
+  public static final int DEFAULT_MERGE_PFACTOR = LogMergePolicy.DEFAULT_MERGE_FACTOR;
+  public static final double DEFAULT_RAM_FLUSH_MB = (int) IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB;
+  private String commitUserData;
+
+  public OpenIndexTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public int doLogic() throws IOException {
+    PerfRunData runData = getRunData();
+    Config config = runData.getConfig();
+    final IndexCommit ic;
+    if (commitUserData != null) {
+      ic = OpenReaderTask.findIndexCommit(runData.getDirectory(), commitUserData);
+    } else {
+      ic = null;
+    }
+    
+    final IndexWriter writer = CreateIndexTask.configureWriter(config, runData, OpenMode.APPEND, ic);
+    runData.setIndexWriter(writer);
+    return 1;
+  }
+
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    if (params != null) {
+      // specifies which commit point to open
+      commitUserData = params;
+    }
+  }
+
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenReaderTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenReaderTask.java
new file mode 100644
index 0000000..8bc1c94
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenReaderTask.java
@@ -0,0 +1,105 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Map;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.index.IndexCommit;
+import org.apache.lucene.index.IndexDeletionPolicy;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.store.Directory;
+
+/**
+ * Open an index reader.
+ * <br>Other side effects: index reader object in perfRunData is set.
+ * <br> Optional params readOnly,commitUserData eg. OpenReader(false,commit1)
+ */
+public class OpenReaderTask extends PerfTask {
+  public static final String USER_DATA = "userData";
+  private boolean readOnly = true;
+  private String commitUserData = null;
+
+  public OpenReaderTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public int doLogic() throws IOException {
+    Directory dir = getRunData().getDirectory();
+    Config config = getRunData().getConfig();
+    IndexReader r = null;
+    final IndexDeletionPolicy deletionPolicy;
+    if (readOnly) {
+      deletionPolicy = null;
+    } else {
+      deletionPolicy = CreateIndexTask.getIndexDeletionPolicy(config);
+    }
+    if (commitUserData != null) {
+      r = IndexReader.open(OpenReaderTask.findIndexCommit(dir, commitUserData),
+                           deletionPolicy,
+                           readOnly); 
+    } else {
+      r = IndexReader.open(dir,
+                           deletionPolicy,
+                           readOnly); 
+    }
+    getRunData().setIndexReader(r);
+    // We transfer reference to the run data
+    r.decRef();
+    return 1;
+  }
+ 
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    if (params != null) {
+      String[] split = params.split(",");
+      if (split.length > 0) {
+        readOnly = Boolean.valueOf(split[0]).booleanValue();
+      }
+      if (split.length > 1) {
+        commitUserData = split[1];
+      }
+    }
+  }
+
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+
+  public static IndexCommit findIndexCommit(Directory dir, String userData) throws IOException {
+    Collection<IndexCommit> commits = IndexReader.listCommits(dir);
+    for (final IndexCommit ic : commits) {
+      Map<String,String> map = ic.getUserData();
+      String ud = null;
+      if (map != null) {
+        ud = map.get(USER_DATA);
+      }
+      if (ud != null && ud.equals(userData)) {
+        return ic;
+      }
+    }
+
+    throw new IOException("index does not contain commit with userData: " + userData);
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OptimizeTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OptimizeTask.java
new file mode 100644
index 0000000..19947b6
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OptimizeTask.java
@@ -0,0 +1,53 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.index.IndexWriter;
+
+/**
+ * Optimize the index.
+ * <br>Other side effects: none.
+ */
+public class OptimizeTask extends PerfTask {
+
+  public OptimizeTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  int maxNumSegments = 1;
+
+  @Override
+  public int doLogic() throws Exception {
+    IndexWriter iw = getRunData().getIndexWriter();
+    iw.optimize(maxNumSegments);
+    //System.out.println("optimize called");
+    return 1;
+  }
+
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    maxNumSegments = Double.valueOf(params).intValue();
+  }
+
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PerfTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PerfTask.java
new file mode 100644
index 0000000..7ac051b
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PerfTask.java
@@ -0,0 +1,320 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.text.NumberFormat;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.stats.Points;
+import org.apache.lucene.benchmark.byTask.stats.TaskStats;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+
+/**
+ * An abstract task to be tested for performance. <br>
+ * Every performance task extends this class, and provides its own
+ * {@link #doLogic()} method, which performs the actual task. <br>
+ * Tasks performing some work that should be measured for the task, can override
+ * {@link #setup()} and/or {@link #tearDown()} and place that work there. <br>
+ * Relevant properties: <code>task.max.depth.log</code>.<br>
+ * Also supports the following logging attributes:
+ * <ul>
+ * <li>log.step - specifies how often to log messages about the current running
+ * task. Default is 1000 {@link #doLogic()} invocations. Set to -1 to disable
+ * logging.
+ * <li>log.step.[class Task Name] - specifies the same as 'log.step', only for a
+ * particular task name. For example, log.step.AddDoc will be applied only for
+ * {@link AddDocTask}, but not for {@link DeleteDocTask}. It's a way to control
+ * per task logging settings. If you want to omit logging for any other task,
+ * include log.step=-1. The syntax is "log.step." together with the Task's
+ * 'short' name (i.e., without the 'Task' part).
+ * </ul>
+ */
+public abstract class PerfTask implements Cloneable {
+
+  static final int DEFAULT_LOG_STEP = 1000;
+  
+  private PerfRunData runData;
+  
+  // propeties that all tasks have
+  private String name;
+  private int depth = 0;
+  protected int logStep;
+  private int logStepCount = 0;
+  private int maxDepthLogStart = 0;
+  private boolean disableCounting = false;
+  protected String params = null;
+
+  private boolean runInBackground;
+  private int deltaPri;
+
+  protected static final String NEW_LINE = System.getProperty("line.separator");
+
+  /** Should not be used externally */
+  private PerfTask() {
+    name = getClass().getSimpleName();
+    if (name.endsWith("Task")) {
+      name = name.substring(0, name.length() - 4);
+    }
+  }
+
+  public void setRunInBackground(int deltaPri) {
+    runInBackground = true;
+    this.deltaPri = deltaPri;
+  }
+
+  public boolean getRunInBackground() {
+    return runInBackground;
+  }
+
+  public int getBackgroundDeltaPriority() {
+    return deltaPri;
+  }
+
+  protected volatile boolean stopNow;
+
+  public void stopNow() {
+    stopNow = true;
+  }
+
+  public PerfTask(PerfRunData runData) {
+    this();
+    this.runData = runData;
+    Config config = runData.getConfig();
+    this.maxDepthLogStart = config.get("task.max.depth.log",0);
+
+    String logStepAtt = "log.step";
+    String taskLogStepAtt = "log.step." + name;
+    if (config.get(taskLogStepAtt, null) != null) {
+      logStepAtt = taskLogStepAtt;
+    }
+
+    // It's important to read this from Config, to support vals-by-round.
+    logStep = config.get(logStepAtt, DEFAULT_LOG_STEP);
+    // To avoid the check 'if (logStep > 0)' in tearDown(). This effectively
+    // turns logging off.
+    if (logStep <= 0) {
+      logStep = Integer.MAX_VALUE;
+    }
+  }
+  
+  @Override
+  protected Object clone() throws CloneNotSupportedException {
+    // tasks having non primitive data structures should override this.
+    // otherwise parallel running of a task sequence might not run correctly. 
+    return super.clone();
+  }
+
+  public void close() throws Exception {
+  }
+
+  /**
+   * Run the task, record statistics.
+   * @return number of work items done by this task.
+   */
+  public final int runAndMaybeStats(boolean reportStats) throws Exception {
+    if (!reportStats || shouldNotRecordStats()) {
+      setup();
+      int count = doLogic();
+      count = disableCounting ? 0 : count;
+      tearDown();
+      return count;
+    }
+    if (reportStats && depth <= maxDepthLogStart && !shouldNeverLogAtStart()) {
+      System.out.println("------------> starting task: " + getName());
+    }
+    setup();
+    Points pnts = runData.getPoints();
+    TaskStats ts = pnts.markTaskStart(this, runData.getConfig().getRoundNumber());
+    int count = doLogic();
+    count = disableCounting ? 0 : count;
+    pnts.markTaskEnd(ts, count);
+    tearDown();
+    return count;
+  }
+
+  /**
+   * Perform the task once (ignoring repetitions specification)
+   * Return number of work items done by this task.
+   * For indexing that can be number of docs added.
+   * For warming that can be number of scanned items, etc.
+   * @return number of work items done by this task.
+   */
+  public abstract int doLogic() throws Exception;
+  
+  /**
+   * @return Returns the name.
+   */
+  public String getName() {
+    if (params==null) {
+      return name;
+    } 
+    return new StringBuilder(name).append('(').append(params).append(')').toString();
+  }
+
+  /**
+   * @param name The name to set.
+   */
+  protected void setName(String name) {
+    this.name = name;
+  }
+
+  /**
+   * @return Returns the run data.
+   */
+  public PerfRunData getRunData() {
+    return runData;
+  }
+
+  /**
+   * @return Returns the depth.
+   */
+  public int getDepth() {
+    return depth;
+  }
+
+  /**
+   * @param depth The depth to set.
+   */
+  public void setDepth(int depth) {
+    this.depth = depth;
+  }
+  
+  // compute a blank string padding for printing this task indented by its depth  
+  String getPadding () {
+    char c[] = new char[4*getDepth()];
+    for (int i = 0; i < c.length; i++) c[i] = ' ';
+    return new String(c);
+  }
+  
+  /* (non-Javadoc)
+   * @see java.lang.Object#toString()
+   */
+  @Override
+  public String toString() {
+    String padd = getPadding();
+    StringBuilder sb = new StringBuilder(padd);
+    if (disableCounting) {
+      sb.append('-');
+    }
+    sb.append(getName());
+    if (getRunInBackground()) {
+      sb.append(" &");
+      int x = getBackgroundDeltaPriority();
+      if (x != 0) {
+        sb.append(x);
+      }
+    }
+    return sb.toString();
+  }
+
+  /**
+   * @return Returns the maxDepthLogStart.
+   */
+  int getMaxDepthLogStart() {
+    return maxDepthLogStart;
+  }
+
+  protected String getLogMessage(int recsCount) {
+    return "processed " + recsCount + " records";
+  }
+  
+  /**
+   * Tasks that should never log at start can override this.  
+   * @return true if this task should never log when it start.
+   */
+  protected boolean shouldNeverLogAtStart () {
+    return false;
+  }
+  
+  /**
+   * Tasks that should not record statistics can override this.  
+   * @return true if this task should never record its statistics.
+   */
+  protected boolean shouldNotRecordStats () {
+    return false;
+  }
+
+  /**
+   * Task setup work that should not be measured for that specific task.
+   * By default it does nothing, but tasks can implement this, moving work from 
+   * doLogic() to this method. Only the work done in doLogicis measured for this task.
+   * Notice that higher level (sequence) tasks containing this task would then 
+   * measure larger time than the sum of their contained tasks.
+   * @throws Exception 
+   */
+  public void setup () throws Exception {
+  }
+  
+  /**
+   * Task tearDown work that should not be measured for that specific task.
+   * By default it does nothing, but tasks can implement this, moving work from 
+   * doLogic() to this method. Only the work done in doLogicis measured for this task.
+   * Notice that higher level (sequence) tasks containing this task would then 
+   * measure larger time than the sum of their contained tasks.
+   */
+  public void tearDown() throws Exception {
+    if (++logStepCount % logStep == 0) {
+      double time = (System.currentTimeMillis() - runData.getStartTimeMillis()) / 1000.0;
+      NumberFormat nf = NumberFormat.getInstance();
+      nf.setMaximumFractionDigits(2);
+      System.out.println(nf.format(time) + " sec --> "
+          + Thread.currentThread().getName() + " " + getLogMessage(logStepCount));
+    }
+  }
+
+  /**
+   * Sub classes that supports parameters must override this method to return true.
+   * @return true iff this task supports command line params.
+   */
+  public boolean supportsParams () {
+    return false;
+  }
+  
+  /**
+   * Set the params of this task.
+   * @exception UnsupportedOperationException for tasks supporting command line parameters.
+   */
+  public void setParams(String params) {
+    if (!supportsParams()) {
+      throw new UnsupportedOperationException(getName()+" does not support command line parameters.");
+    }
+    this.params = params;
+  }
+  
+  /**
+   * @return Returns the Params.
+   */
+  public String getParams() {
+    return params;
+  }
+
+  /**
+   * Return true if counting is disabled for this task.
+   */
+  public boolean isDisableCounting() {
+    return disableCounting;
+  }
+
+  /**
+   * See {@link #isDisableCounting()}
+   */
+  public void setDisableCounting(boolean disableCounting) {
+    this.disableCounting = disableCounting;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PrintReaderTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PrintReaderTask.java
new file mode 100644
index 0000000..f8d9f96
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PrintReaderTask.java
@@ -0,0 +1,56 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.store.Directory;
+
+public class PrintReaderTask extends PerfTask {
+  private String userData = null;
+  
+  public PrintReaderTask(PerfRunData runData) {
+    super(runData);
+  }
+  
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    userData = params;
+  }
+  
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+  
+  @Override
+  public int doLogic() throws Exception {
+    Directory dir = getRunData().getDirectory();
+    IndexReader r = null;
+    if (userData == null) 
+      r = IndexReader.open(dir, true);
+    else
+      r = IndexReader.open(OpenReaderTask.findIndexCommit(dir, userData),
+                           null,
+                           true);
+    System.out.println("--> numDocs:"+r.numDocs()+" dels:"+r.numDeletedDocs());
+    r.close();
+    return 1;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java
new file mode 100644
index 0000000..b684814
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java
@@ -0,0 +1,307 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.HashSet;
+
+import java.util.List;
+import java.util.Set;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.feeds.QueryMaker;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Fieldable;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.TopFieldCollector;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.TopScoreDocCollector;
+import org.apache.lucene.search.Weight;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
+
+
+/**
+ * Read index (abstract) task.
+ * Sub classes implement withSearch(), withWarm(), withTraverse() and withRetrieve()
+ * methods to configure the actual action.
+ * <p/>
+ * <p>Note: All ReadTasks reuse the reader if it is already open.
+ * Otherwise a reader is opened at start and closed at the end.
+ * <p>
+ * The <code>search.num.hits</code> config parameter sets
+ * the top number of hits to collect during searching.  If
+ * <code>print.hits.field</code> is set, then each hit is
+ * printed along with the value of that field.</p>
+ *
+ * <p>Other side effects: none.
+ */
+public abstract class ReadTask extends PerfTask {
+
+  private final QueryMaker queryMaker;
+
+  public ReadTask(PerfRunData runData) {
+    super(runData);
+    if (withSearch()) {
+      queryMaker = getQueryMaker();
+    } else {
+      queryMaker = null;
+    }
+  }
+  @Override
+  public int doLogic() throws Exception {
+    int res = 0;
+
+    // open reader or use existing one
+    IndexSearcher searcher = getRunData().getIndexSearcher();
+
+    IndexReader reader;
+
+    final boolean closeSearcher;
+    if (searcher == null) {
+      // open our own reader
+      Directory dir = getRunData().getDirectory();
+      reader = IndexReader.open(dir, true);
+      searcher = new IndexSearcher(reader);
+      closeSearcher = true;
+    } else {
+      // use existing one; this passes +1 ref to us
+      reader = searcher.getIndexReader();
+      closeSearcher = false;
+    }
+
+    // optionally warm and add num docs traversed to count
+    if (withWarm()) {
+      Document doc = null;
+      Bits delDocs = reader.getDeletedDocs();
+      for (int m = 0; m < reader.maxDoc(); m++) {
+        if (!delDocs.get(m)) {
+          doc = reader.document(m);
+          res += (doc == null ? 0 : 1);
+        }
+      }
+    }
+
+    if (withSearch()) {
+      res++;
+      Query q = queryMaker.makeQuery();
+      Sort sort = getSort();
+      TopDocs hits = null;
+      final int numHits = numHits();
+      if (numHits > 0) {
+        if (withCollector() == false) {
+          if (sort != null) {
+            Weight w = q.weight(searcher);
+            TopFieldCollector collector = TopFieldCollector.create(sort, numHits,
+                                                                   true, withScore(),
+                                                                   withMaxScore(),
+                                                                   !w.scoresDocsOutOfOrder());
+            searcher.search(w, null, collector);
+            hits = collector.topDocs();
+          } else {
+            hits = searcher.search(q, numHits);
+          }
+        } else {
+          Collector collector = createCollector();
+          searcher.search(q, null, collector);
+          //hits = collector.topDocs();
+        }
+
+        final String printHitsField = getRunData().getConfig().get("print.hits.field", null);
+        if (hits != null && printHitsField != null && printHitsField.length() > 0) {
+          if (q instanceof MultiTermQuery) {
+            System.out.println("MultiTermQuery term count = " + ((MultiTermQuery) q).getTotalNumberOfTerms());
+          }
+          System.out.println("totalHits = " + hits.totalHits);
+          System.out.println("maxDoc()  = " + reader.maxDoc());
+          System.out.println("numDocs() = " + reader.numDocs());
+          for(int i=0;i<hits.scoreDocs.length;i++) {
+            final int docID = hits.scoreDocs[i].doc;
+            final Document doc = reader.document(docID);
+            System.out.println("  " + i + ": doc=" + docID + " score=" + hits.scoreDocs[i].score + " " + printHitsField + " =" + doc.get(printHitsField));
+          }
+        }
+
+        if (withTraverse()) {
+          final ScoreDoc[] scoreDocs = hits.scoreDocs;
+          int traversalSize = Math.min(scoreDocs.length, traversalSize());
+
+          if (traversalSize > 0) {
+            boolean retrieve = withRetrieve();
+            int numHighlight = Math.min(numToHighlight(), scoreDocs.length);
+            Analyzer analyzer = getRunData().getAnalyzer();
+            BenchmarkHighlighter highlighter = null;
+            if (numHighlight > 0) {
+              highlighter = getBenchmarkHighlighter(q);
+            }
+            for (int m = 0; m < traversalSize; m++) {
+              int id = scoreDocs[m].doc;
+              res++;
+              if (retrieve) {
+                Document document = retrieveDoc(reader, id);
+                res += document != null ? 1 : 0;
+                if (numHighlight > 0 && m < numHighlight) {
+                  Collection<String> fieldsToHighlight = getFieldsToHighlight(document);
+                  for (final String field : fieldsToHighlight) {
+                    String text = document.get(field);
+                    res += highlighter.doHighlight(reader, id, field, document, analyzer, text);
+                  }
+                }
+              }
+            }
+          }
+        }
+      }
+    }
+
+    if (closeSearcher) {
+      searcher.close();
+      reader.close();
+    } else {
+      // Release our +1 ref from above
+      reader.decRef();
+    }
+    return res;
+  }
+
+  protected Collector createCollector() throws Exception {
+    return TopScoreDocCollector.create(numHits(), true);
+  }
+
+
+  protected Document retrieveDoc(IndexReader ir, int id) throws IOException {
+    return ir.document(id);
+  }
+
+  /**
+   * Return query maker used for this task.
+   */
+  public abstract QueryMaker getQueryMaker();
+
+  /**
+   * Return true if search should be performed.
+   */
+  public abstract boolean withSearch();
+
+  public boolean withCollector(){
+    return false;
+  }
+  
+
+  /**
+   * Return true if warming should be performed.
+   */
+  public abstract boolean withWarm();
+
+  /**
+   * Return true if, with search, results should be traversed.
+   */
+  public abstract boolean withTraverse();
+
+  /** Whether scores should be computed (only useful with
+   *  field sort) */
+  public boolean withScore() {
+    return true;
+  }
+
+  /** Whether maxScores should be computed (only useful with
+   *  field sort) */
+  public boolean withMaxScore() {
+    return true;
+  }
+
+  /**
+   * Specify the number of hits to traverse.  Tasks should override this if they want to restrict the number
+   * of hits that are traversed when {@link #withTraverse()} is true. Must be greater than 0.
+   * <p/>
+   * Read task calculates the traversal as: Math.min(hits.length(), traversalSize())
+   *
+   * @return Integer.MAX_VALUE
+   */
+  public int traversalSize() {
+    return Integer.MAX_VALUE;
+  }
+
+  static final int DEFAULT_SEARCH_NUM_HITS = 10;
+  private int numHits;
+
+  @Override
+  public void setup() throws Exception {
+    super.setup();
+    numHits = getRunData().getConfig().get("search.num.hits", DEFAULT_SEARCH_NUM_HITS);
+  }
+
+  /**
+   * Specify the number of hits to retrieve.  Tasks should override this if they want to restrict the number
+   * of hits that are collected during searching. Must be greater than 0.
+   *
+   * @return 10 by default, or search.num.hits config if set.
+   */
+  public int numHits() {
+    return numHits;
+  }
+
+  /**
+   * Return true if, with search & results traversing, docs should be retrieved.
+   */
+  public abstract boolean withRetrieve();
+
+  /**
+   * Set to the number of documents to highlight.
+   *
+   * @return The number of the results to highlight.  O means no docs will be highlighted.
+   */
+  public int numToHighlight() {
+    return 0;
+  }
+
+  /**
+   * Return an appropriate highlighter to be used with
+   * highlighting tasks
+   */
+  protected BenchmarkHighlighter getBenchmarkHighlighter(Query q){
+    return null;
+  }
+  
+  protected Sort getSort() {
+    return null;
+  }
+
+  /**
+   * Define the fields to highlight.  Base implementation returns all fields
+   * @param document The Document
+   * @return A Collection of Field names (Strings)
+   */
+  protected Collection<String> getFieldsToHighlight(Document document) {
+    List<Fieldable> fieldables = document.getFields();
+    Set<String> result = new HashSet<String>(fieldables.size());
+    for (final Fieldable fieldable : fieldables) {
+      result.add(fieldable.name());
+    }
+    return result;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask.java
new file mode 100644
index 0000000..fa0ae99
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask.java
@@ -0,0 +1,146 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Reader;
+import java.util.List;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Fieldable;
+import org.apache.lucene.document.NumericField;
+
+/**
+ * Simple task to test performance of tokenizers.  It just
+ * creates a token stream for each field of the document and
+ * read all tokens out of that stream.
+ */
+public class ReadTokensTask extends PerfTask {
+
+  public ReadTokensTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  private int totalTokenCount = 0;
+  
+  // volatile data passed between setup(), doLogic(), tearDown().
+  private Document doc = null;
+  
+  @Override
+  public void setup() throws Exception {
+    super.setup();
+    DocMaker docMaker = getRunData().getDocMaker();
+    doc = docMaker.makeDocument();
+  }
+
+  @Override
+  protected String getLogMessage(int recsCount) {
+    return "read " + recsCount + " docs; " + totalTokenCount + " tokens";
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    doc = null;
+    super.tearDown();
+  }
+
+  @Override
+  public int doLogic() throws Exception {
+    List<Fieldable> fields = doc.getFields();
+    Analyzer analyzer = getRunData().getAnalyzer();
+    int tokenCount = 0;
+    for(final Fieldable field : fields) {
+      if (!field.isTokenized() || field instanceof NumericField) continue;
+      
+      final TokenStream stream;
+      final TokenStream streamValue = field.tokenStreamValue();
+
+      if (streamValue != null) 
+        stream = streamValue;
+      else {
+        // the field does not have a TokenStream,
+        // so we have to obtain one from the analyzer
+        final Reader reader;			  // find or make Reader
+        final Reader readerValue = field.readerValue();
+
+        if (readerValue != null)
+          reader = readerValue;
+        else {
+          String stringValue = field.stringValue();
+          if (stringValue == null)
+            throw new IllegalArgumentException("field must have either TokenStream, String or Reader value");
+          stringReader.init(stringValue);
+          reader = stringReader;
+        }
+        
+        // Tokenize field
+        stream = analyzer.reusableTokenStream(field.name(), reader);
+      }
+
+      // reset the TokenStream to the first token
+      stream.reset();
+
+      while(stream.incrementToken())
+        tokenCount++;
+    }
+    totalTokenCount += tokenCount;
+    return tokenCount;
+  }
+
+  /* Simple StringReader that can be reset to a new string;
+   * we use this when tokenizing the string value from a
+   * Field. */
+  ReusableStringReader stringReader = new ReusableStringReader();
+
+  private final static class ReusableStringReader extends Reader {
+    int upto;
+    int left;
+    String s;
+    void init(String s) {
+      this.s = s;
+      left = s.length();
+      this.upto = 0;
+    }
+    @Override
+    public int read(char[] c) {
+      return read(c, 0, c.length);
+    }
+    @Override
+    public int read(char[] c, int off, int len) {
+      if (left > len) {
+        s.getChars(upto, upto+len, c, off);
+        upto += len;
+        left -= len;
+        return len;
+      } else if (0 == left) {
+        return -1;
+      } else {
+        s.getChars(upto, upto+left, c, off);
+        int r = left;
+        left = 0;
+        upto = s.length();
+        return r;
+      }
+    }
+    @Override
+    public void close() {}
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReopenReaderTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReopenReaderTask.java
new file mode 100644
index 0000000..10198c5
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReopenReaderTask.java
@@ -0,0 +1,45 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import java.io.IOException;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.index.IndexReader;
+
+/**
+* Reopens IndexReader and closes old IndexReader.
+*
+*/
+public class ReopenReaderTask extends PerfTask {
+  public ReopenReaderTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public int doLogic() throws IOException {
+    IndexReader r = getRunData().getIndexReader();
+    IndexReader nr = r.reopen();
+    if (nr != r) {
+      getRunData().setIndexReader(nr);
+      nr.decRef();
+    }
+    r.decRef();
+    return 1;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepAllTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepAllTask.java
new file mode 100644
index 0000000..89ae30d
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepAllTask.java
@@ -0,0 +1,76 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.List;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.stats.Report;
+import org.apache.lucene.benchmark.byTask.stats.TaskStats;
+
+/**
+ * Report all statistics with no aggregations.
+ * <br>Other side effects: None.
+ */
+public class RepAllTask extends ReportTask {
+
+  public RepAllTask(PerfRunData runData) {
+    super(runData);
+   }
+
+  @Override
+  public int doLogic() throws Exception {
+    Report rp = reportAll(getRunData().getPoints().taskStats());
+    
+    System.out.println();
+    System.out.println("------------> Report All ("+rp.getSize()+" out of "+rp.getOutOf()+")");
+    System.out.println(rp.getText());
+    System.out.println();
+    return 0;
+  }
+  
+  /**
+   * Report detailed statistics as a string
+   * @return the report
+   */
+  protected Report reportAll(List<TaskStats> taskStats) {
+    String longestOp = longestOp(taskStats);
+    boolean first = true;
+    StringBuilder sb = new StringBuilder();
+    sb.append(tableTitle(longestOp));
+    sb.append(newline);
+    int reported = 0;
+    for (final TaskStats stat : taskStats) {
+      if (stat.getElapsed()>=0) { // consider only tasks that ended
+        if (!first) {
+          sb.append(newline);
+        }
+        first = false;
+        String line = taskReportLine(longestOp, stat);
+        reported++;
+        if (taskStats.size()>2 && reported%2==0) {
+          line = line.replaceAll("   "," - ");
+        }
+        sb.append(line);
+      }
+    }
+    String reptxt = (reported==0 ? "No Matching Entries Were Found!" : sb.toString());
+    return new Report(reptxt,reported,reported,taskStats.size());
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSelectByPrefTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSelectByPrefTask.java
new file mode 100644
index 0000000..30c1b60
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSelectByPrefTask.java
@@ -0,0 +1,74 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.List;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.stats.Report;
+import org.apache.lucene.benchmark.byTask.stats.TaskStats;
+
+/**
+ * Report by-name-prefix statistics with no aggregations.
+ * <br>Other side effects: None.
+ */
+public class RepSelectByPrefTask extends RepSumByPrefTask {
+
+  public RepSelectByPrefTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public int doLogic() throws Exception {
+    Report rp = reportSelectByPrefix(getRunData().getPoints().taskStats());
+    
+    System.out.println();
+    System.out.println("------------> Report Select By Prefix ("+prefix+") ("+
+        rp.getSize()+" about "+rp.getReported()+" out of "+rp.getOutOf()+")");
+    System.out.println(rp.getText());
+    System.out.println();
+
+    return 0;
+  }
+  
+  protected Report reportSelectByPrefix(List<TaskStats> taskStats) {
+    String longestOp = longestOp(taskStats);
+    boolean first = true;
+    StringBuilder sb = new StringBuilder();
+    sb.append(tableTitle(longestOp));
+    sb.append(newline);
+    int reported = 0;
+    for (final TaskStats stat : taskStats) {
+      if (stat.getElapsed()>=0 && stat.getTask().getName().startsWith(prefix)) { // only ended tasks with proper name
+        reported++;
+        if (!first) {
+          sb.append(newline);
+        }
+        first = false;
+        String line = taskReportLine(longestOp,stat);
+        if (taskStats.size()>2 && reported%2==0) {
+          line = line.replaceAll("   "," - ");
+        }
+        sb.append(line);
+      }
+    }
+    String reptxt = (reported==0 ? "No Matching Entries Were Found!" : sb.toString());
+    return new Report(reptxt,reported,reported, taskStats.size());
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameRoundTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameRoundTask.java
new file mode 100644
index 0000000..2fd31f3
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameRoundTask.java
@@ -0,0 +1,80 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.LinkedHashMap;
+import java.util.List;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.stats.Report;
+import org.apache.lucene.benchmark.byTask.stats.TaskStats;
+
+/**
+ * Report all statistics grouped/aggregated by name and round.
+ * <br>Other side effects: None.
+ */
+public class RepSumByNameRoundTask extends ReportTask {
+
+  public RepSumByNameRoundTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public int doLogic() throws Exception {
+    Report rp = reportSumByNameRound(getRunData().getPoints().taskStats());
+
+    System.out.println();
+    System.out.println("------------> Report Sum By (any) Name and Round ("+
+        rp.getSize()+" about "+rp.getReported()+" out of "+rp.getOutOf()+")");
+    System.out.println(rp.getText());
+    System.out.println();
+    
+    return 0;
+  }
+
+  /**
+   * Report statistics as a string, aggregate for tasks named the same, and from the same round.
+   * @return the report
+   */
+  protected Report reportSumByNameRound(List<TaskStats> taskStats) {
+    // aggregate by task name and round
+    LinkedHashMap<String,TaskStats> p2 = new LinkedHashMap<String,TaskStats>();
+    int reported = 0;
+    for (final TaskStats stat1 : taskStats) {
+      if (stat1.getElapsed()>=0) { // consider only tasks that ended
+        reported++;
+        String name = stat1.getTask().getName();
+        String rname = stat1.getRound()+"."+name; // group by round
+        TaskStats stat2 = p2.get(rname);
+        if (stat2 == null) {
+          try {
+            stat2 = (TaskStats) stat1.clone();
+          } catch (CloneNotSupportedException e) {
+            throw new RuntimeException(e);
+          }
+          p2.put(rname,stat2);
+        } else {
+          stat2.add(stat1);
+        }
+      }
+    }
+    // now generate report from secondary list p2    
+    return genPartialReport(reported, p2, taskStats.size());
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameTask.java
new file mode 100644
index 0000000..a55d26d
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameTask.java
@@ -0,0 +1,80 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.LinkedHashMap;
+import java.util.List;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.stats.Report;
+import org.apache.lucene.benchmark.byTask.stats.TaskStats;
+
+/**
+ * Report all statistics aggregated by name.
+ * <br>Other side effects: None.
+ */
+public class RepSumByNameTask extends ReportTask {
+
+  public RepSumByNameTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public int doLogic() throws Exception {
+    Report rp = reportSumByName(getRunData().getPoints().taskStats());
+
+    System.out.println();
+    System.out.println("------------> Report Sum By (any) Name ("+
+        rp.getSize()+" about "+rp.getReported()+" out of "+rp.getOutOf()+")");
+    System.out.println(rp.getText());
+    System.out.println();
+
+    return 0;
+  }
+
+  /**
+   * Report statistics as a string, aggregate for tasks named the same.
+   * @return the report
+   */
+  protected Report reportSumByName(List<TaskStats> taskStats) {
+    // aggregate by task name
+    int reported = 0;
+    LinkedHashMap<String,TaskStats> p2 = new LinkedHashMap<String,TaskStats>();
+    for (final TaskStats stat1: taskStats) {
+      if (stat1.getElapsed()>=0) { // consider only tasks that ended
+        reported++;
+        String name = stat1.getTask().getName();
+        TaskStats stat2 = p2.get(name);
+        if (stat2 == null) {
+          try {
+            stat2 = (TaskStats) stat1.clone();
+          } catch (CloneNotSupportedException e) {
+            throw new RuntimeException(e);
+          }
+          p2.put(name,stat2);
+        } else {
+          stat2.add(stat1);
+        }
+      }
+    }
+    // now generate report from secondary list p2    
+    return genPartialReport(reported, p2, taskStats.size());
+  }
+
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefRoundTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefRoundTask.java
new file mode 100644
index 0000000..070927b
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefRoundTask.java
@@ -0,0 +1,77 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.LinkedHashMap;
+import java.util.List;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.stats.Report;
+import org.apache.lucene.benchmark.byTask.stats.TaskStats;
+
+/**
+ * Report all prefix matching statistics grouped/aggregated by name and round.
+ * <br>Other side effects: None.
+ */
+public class RepSumByPrefRoundTask extends RepSumByPrefTask {
+
+  public RepSumByPrefRoundTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public int doLogic() throws Exception {
+    Report rp = reportSumByPrefixRound(getRunData().getPoints().taskStats());
+    
+    System.out.println();
+    System.out.println("------------> Report sum by Prefix ("+prefix+") and Round ("+
+        rp.getSize()+" about "+rp.getReported()+" out of "+rp.getOutOf()+")");
+    System.out.println(rp.getText());
+    System.out.println();
+
+    return 0;
+  }
+
+  protected Report reportSumByPrefixRound(List<TaskStats> taskStats) {
+    // aggregate by task name and by round
+    int reported = 0;
+    LinkedHashMap<String,TaskStats> p2 = new LinkedHashMap<String,TaskStats>();
+    for (final TaskStats stat1 : taskStats) {
+      if (stat1.getElapsed()>=0 && stat1.getTask().getName().startsWith(prefix)) { // only ended tasks with proper name
+        reported++;
+        String name = stat1.getTask().getName();
+        String rname = stat1.getRound()+"."+name; // group by round
+        TaskStats stat2 = p2.get(rname);
+        if (stat2 == null) {
+          try {
+            stat2 = (TaskStats) stat1.clone();
+          } catch (CloneNotSupportedException e) {
+            throw new RuntimeException(e);
+          }
+          p2.put(rname,stat2);
+        } else {
+          stat2.add(stat1);
+        }
+      }
+    }
+    // now generate report from secondary list p2    
+    return genPartialReport(reported, p2, taskStats.size());
+  }
+
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefTask.java
new file mode 100644
index 0000000..610f282
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefTask.java
@@ -0,0 +1,90 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.stats.Report;
+import org.apache.lucene.benchmark.byTask.stats.TaskStats;
+
+import java.util.LinkedHashMap;
+import java.util.List;
+
+/**
+ * Report by-name-prefix statistics aggregated by name.
+ * <br>Other side effects: None.
+ */
+public class RepSumByPrefTask extends ReportTask {
+
+  public RepSumByPrefTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  protected String prefix;
+
+  @Override
+  public int doLogic() throws Exception {
+    Report rp = reportSumByPrefix(getRunData().getPoints().taskStats());
+    
+    System.out.println();
+    System.out.println("------------> Report Sum By Prefix ("+prefix+") ("+
+        rp.getSize()+" about "+rp.getReported()+" out of "+rp.getOutOf()+")");
+    System.out.println(rp.getText());
+    System.out.println();
+
+    return 0;
+  }
+
+  protected Report reportSumByPrefix (List<TaskStats> taskStats) {
+    // aggregate by task name
+    int reported = 0;
+    LinkedHashMap<String,TaskStats> p2 = new LinkedHashMap<String,TaskStats>();
+    for (final TaskStats stat1 : taskStats) {
+      if (stat1.getElapsed()>=0 && stat1.getTask().getName().startsWith(prefix)) { // only ended tasks with proper name
+        reported++;
+        String name = stat1.getTask().getName();
+        TaskStats stat2 = p2.get(name);
+        if (stat2 == null) {
+          try {
+            stat2 = (TaskStats) stat1.clone();
+          } catch (CloneNotSupportedException e) {
+            throw new RuntimeException(e);
+          }
+          p2.put(name,stat2);
+        } else {
+          stat2.add(stat1);
+        }
+      }
+    }
+    // now generate report from secondary list p2    
+    return genPartialReport(reported, p2, taskStats.size());
+  }
+  
+
+  public void setPrefix(String prefix) {
+    this.prefix = prefix;
+  }
+
+  /* (non-Javadoc)
+   * @see PerfTask#toString()
+   */
+  @Override
+  public String toString() {
+    return super.toString()+" "+prefix;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReportTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReportTask.java
new file mode 100644
index 0000000..ed990a4
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReportTask.java
@@ -0,0 +1,176 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+import java.util.LinkedHashMap;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.stats.Report;
+import org.apache.lucene.benchmark.byTask.stats.TaskStats;
+import org.apache.lucene.benchmark.byTask.utils.Format;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Report (abstract) task - all report tasks extend this task.
+ */
+public abstract class ReportTask extends PerfTask {
+
+  public ReportTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  /* (non-Javadoc)
+   * @see PerfTask#shouldNeverLogAtStart()
+   */
+  @Override
+  protected boolean shouldNeverLogAtStart() {
+    return true;
+  }
+
+  /* (non-Javadoc)
+   * @see PerfTask#shouldNotRecordStats()
+   */
+  @Override
+  protected boolean shouldNotRecordStats() {
+    return true;
+  }
+
+  /*
+   * From here start the code used to generate the reports. 
+   * Subclasses would use this part to generate reports.
+   */
+  
+  protected static final String newline = System.getProperty("line.separator");
+  
+  /**
+   * Get a textual summary of the benchmark results, average from all test runs.
+   */
+  protected static final String OP =          "Operation  ";
+  protected static final String ROUND =       " round";
+  protected static final String RUNCNT =      "   runCnt";
+  protected static final String RECCNT =      "   recsPerRun";
+  protected static final String RECSEC =      "        rec/s";
+  protected static final String ELAPSED =     "  elapsedSec";
+  protected static final String USEDMEM =     "    avgUsedMem";
+  protected static final String TOTMEM =      "    avgTotalMem";
+  protected static final String COLS[] = {
+      RUNCNT,
+      RECCNT,
+      RECSEC,
+      ELAPSED,
+      USEDMEM,
+      TOTMEM
+  };
+
+  /**
+   * Compute a title line for a report table
+   * @param longestOp size of longest op name in the table
+   * @return the table title line.
+   */
+  protected String tableTitle (String longestOp) {
+    StringBuilder sb = new StringBuilder();
+    sb.append(Format.format(OP,longestOp));
+    sb.append(ROUND);
+    sb.append(getRunData().getConfig().getColsNamesForValsByRound());
+    for (int i = 0; i < COLS.length; i++) {
+      sb.append(COLS[i]);
+    }
+    return sb.toString(); 
+  }
+  
+  /**
+   * find the longest op name out of completed tasks.  
+   * @param taskStats completed tasks to be considered.
+   * @return the longest op name out of completed tasks.
+   */
+  protected String longestOp(Iterable<TaskStats> taskStats) {
+    String longest = OP;
+    for (final TaskStats stat : taskStats) {
+      if (stat.getElapsed()>=0) { // consider only tasks that ended
+        String name = stat.getTask().getName();
+        if (name.length() > longest.length()) {
+          longest = name;
+        }
+      }
+    }
+    return longest;
+  }
+  
+  /**
+   * Compute a report line for the given task stat.
+   * @param longestOp size of longest op name in the table.
+   * @param stat task stat to be printed.
+   * @return the report line.
+   */
+  protected String taskReportLine(String longestOp, TaskStats stat) {
+    PerfTask task = stat.getTask();
+    StringBuilder sb = new StringBuilder();
+    sb.append(Format.format(task.getName(), longestOp));
+    String round = (stat.getRound()>=0 ? ""+stat.getRound() : "-");
+    sb.append(Format.formatPaddLeft(round, ROUND));
+    sb.append(getRunData().getConfig().getColsValuesForValsByRound(stat.getRound()));
+    sb.append(Format.format(stat.getNumRuns(), RUNCNT)); 
+    sb.append(Format.format(stat.getCount() / stat.getNumRuns(), RECCNT));
+    long elapsed = (stat.getElapsed()>0 ? stat.getElapsed() : 1); // assume at least 1ms
+    sb.append(Format.format(2, (float) (stat.getCount() * 1000.0 / elapsed), RECSEC));
+    sb.append(Format.format(2, (float) stat.getElapsed() / 1000, ELAPSED));
+    sb.append(Format.format(0, (float) stat.getMaxUsedMem() / stat.getNumRuns(), USEDMEM)); 
+    sb.append(Format.format(0, (float) stat.getMaxTotMem() / stat.getNumRuns(), TOTMEM));
+    return sb.toString();
+  }
+
+  protected Report genPartialReport(int reported, LinkedHashMap<String,TaskStats> partOfTasks, int totalSize) {
+    String longetOp = longestOp(partOfTasks.values());
+    boolean first = true;
+    StringBuilder sb = new StringBuilder();
+    sb.append(tableTitle(longetOp));
+    sb.append(newline);
+    int lineNum = 0;
+    for (final TaskStats stat : partOfTasks.values()) {
+      if (!first) {
+        sb.append(newline);
+      }
+      first = false;
+      String line = taskReportLine(longetOp,stat);
+      lineNum++;
+      if (partOfTasks.size()>2 && lineNum%2==0) {
+        line = line.replaceAll("   "," - ");
+      }
+      sb.append(line);
+      int[] byTime = stat.getCountsByTime();
+      if (byTime != null) {
+        sb.append(newline);
+        int end = -1;
+        for(int i=byTime.length-1;i>=0;i--) {
+          if (byTime[i] != 0) {
+            end = i;
+            break;
+          }
+        }
+        if (end != -1) {
+          sb.append("  by time:");
+          for(int i=0;i<end;i++) {
+            sb.append(' ').append(byTime[i]);
+          }
+        }
+      }
+    }
+    
+    String reptxt = (reported==0 ? "No Matching Entries Were Found!" : sb.toString());
+    return new Report(reptxt,partOfTasks.size(),reported,totalSize);
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetInputsTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetInputsTask.java
new file mode 100644
index 0000000..57f7b69
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetInputsTask.java
@@ -0,0 +1,50 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+
+/**
+ * Reset inputs so that the test run would behave, input wise, 
+ * as if it just started. This affects e.g. the generation of docs and queries.
+ */
+public class ResetInputsTask extends PerfTask {
+
+  public ResetInputsTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public int doLogic() throws Exception {
+    getRunData().resetInputs();
+    return 0;
+  }
+  
+  /*
+   * (non-Javadoc)
+   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#shouldNotRecordStats()
+   */
+  @Override
+  protected boolean shouldNotRecordStats() {
+    return true;
+  }
+
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetSystemEraseTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetSystemEraseTask.java
new file mode 100644
index 0000000..ba6dc66
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetSystemEraseTask.java
@@ -0,0 +1,43 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+
+/**
+ * Reset all index and input data and call gc, erase index and dir, does NOT clear statistics.
+ * <br>This contains ResetInputs.
+ * <br>Other side effects: writers/readers nullified, deleted, closed.
+ * Index is erased.
+ * Directory is erased.
+ */
+public class ResetSystemEraseTask extends ResetSystemSoftTask {
+
+  public ResetSystemEraseTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public int doLogic() throws Exception {
+    getRunData().reinit(true);
+    return 0;
+  }
+  
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetSystemSoftTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetSystemSoftTask.java
new file mode 100644
index 0000000..80087ed
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ResetSystemSoftTask.java
@@ -0,0 +1,43 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+
+/**
+ * Reset all index and input data and call gc, does NOT erase index/dir, does NOT clear statistics.
+ * This contains ResetInputs.
+ * <br>Other side effects: writers/readers nullified, closed.
+ * Index is NOT erased.
+ * Directory is NOT erased.
+ */
+public class ResetSystemSoftTask extends ResetInputsTask {
+
+  public ResetSystemSoftTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public int doLogic() throws Exception {
+    getRunData().reinit(false);
+    return 0;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RollbackIndexTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RollbackIndexTask.java
new file mode 100644
index 0000000..64ee89d
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RollbackIndexTask.java
@@ -0,0 +1,52 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.PrintStream;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.index.IndexWriter;
+
+/**
+ * Rollback the index writer.
+ */
+public class RollbackIndexTask extends PerfTask {
+
+  public RollbackIndexTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  boolean doWait = true;
+
+  @Override
+  public int doLogic() throws IOException {
+    IndexWriter iw = getRunData().getIndexWriter();
+    if (iw != null) {
+      // If infoStream was set to output to a file, close it.
+      PrintStream infoStream = iw.getInfoStream();
+      if (infoStream != null && infoStream != System.out
+          && infoStream != System.err) {
+        infoStream.close();
+      }
+      iw.rollback();
+      getRunData().setIndexWriter(null);
+    }
+    return 1;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTask.java
new file mode 100644
index 0000000..da5aa92
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTask.java
@@ -0,0 +1,61 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.feeds.QueryMaker;
+
+/**
+ * Search task.
+ * 
+ * <p>Note: This task reuses the reader if it is already open. 
+ * Otherwise a reader is opened at start and closed at the end.
+ */
+public class SearchTask extends ReadTask {
+
+  public SearchTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public boolean withRetrieve() {
+    return false;
+  }
+
+  @Override
+  public boolean withSearch() {
+    return true;
+  }
+
+  @Override
+  public boolean withTraverse() {
+    return false;
+  }
+
+  @Override
+  public boolean withWarm() {
+    return false;
+  }
+
+  @Override
+  public QueryMaker getQueryMaker() {
+    return getRunData().getQueryMaker(this);
+  }
+
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetHighlightTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetHighlightTask.java
new file mode 100644
index 0000000..9ca1813
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetHighlightTask.java
@@ -0,0 +1,152 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.highlight.Highlighter;
+import org.apache.lucene.search.highlight.QueryScorer;
+import org.apache.lucene.search.highlight.SimpleHTMLFormatter;
+import org.apache.lucene.search.highlight.TextFragment;
+import org.apache.lucene.search.highlight.TokenSources;
+
+import java.util.Set;
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.Collections;
+
+/**
+ * Search and Traverse and Retrieve docs task.  Highlight the fields in the retrieved documents.
+ *
+ * Uses the {@link org.apache.lucene.search.highlight.SimpleHTMLFormatter} for formatting.
+ *
+ * <p>Note: This task reuses the reader if it is already open.
+ * Otherwise a reader is opened at start and closed at the end.
+ * </p>
+ *
+ * <p>Takes optional multivalued, comma separated param string as: size[&lt;traversal size&gt;],highlight[&lt;int&gt;],maxFrags[&lt;int&gt;],mergeContiguous[&lt;boolean&gt;],fields[name1;name2;...]</p>
+ * <ul>
+ * <li>traversal size - The number of hits to traverse, otherwise all will be traversed</li>
+ * <li>highlight - The number of the hits to highlight.  Will always be less than or equal to traversal size.  Default is Integer.MAX_VALUE (i.e. hits.length())</li>
+ * <li>maxFrags - The maximum number of fragments to score by the highlighter</li>
+ * <li>mergeContiguous - true if contiguous fragments should be merged.</li>
+ * <li>fields - The fields to highlight.  If not specified all fields will be highlighted (or at least attempted)</li>
+ * </ul>
+ * Example:
+ * <pre>"SearchHlgtSameRdr" SearchTravRetHighlight(size[10],highlight[10],mergeContiguous[true],maxFrags[3],fields[body]) > : 1000
+ * </pre>
+ *
+ * Documents must be stored in order for this task to work.  Additionally, term vector positions can be used as well.
+ *
+ * <p>Other side effects: counts additional 1 (record) for each traversed hit,
+ * and 1 more for each retrieved (non null) document and 1 for each fragment returned.</p>
+ */
+public class SearchTravRetHighlightTask extends SearchTravTask {
+
+  protected int numToHighlight = Integer.MAX_VALUE;
+  protected boolean mergeContiguous;
+  protected int maxFrags = 2;
+  protected Set<String> paramFields = Collections.emptySet();
+  protected Highlighter highlighter;
+  protected int maxDocCharsToAnalyze;
+
+  public SearchTravRetHighlightTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public void setup() throws Exception {
+    super.setup();
+    //check to make sure either the doc is being stored
+    PerfRunData data = getRunData();
+    if (data.getConfig().get("doc.stored", false) == false){
+      throw new Exception("doc.stored must be set to true");
+    }
+    maxDocCharsToAnalyze = data.getConfig().get("highlighter.maxDocCharsToAnalyze", Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);
+  }
+
+  @Override
+  public boolean withRetrieve() {
+    return true;
+  }
+
+  @Override
+  public int numToHighlight() {
+    return numToHighlight;
+  }
+  
+  @Override
+  protected BenchmarkHighlighter getBenchmarkHighlighter(Query q){
+    highlighter = new Highlighter(new SimpleHTMLFormatter(), new QueryScorer(q));
+    highlighter.setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);
+    return new BenchmarkHighlighter(){
+      @Override
+      public int doHighlight(IndexReader reader, int doc, String field,
+          Document document, Analyzer analyzer, String text) throws Exception {
+        TokenStream ts = TokenSources.getAnyTokenStream(reader, doc, field, document, analyzer);
+        TextFragment[] frag = highlighter.getBestTextFragments(ts, text, mergeContiguous, maxFrags);
+        return frag != null ? frag.length : 0;
+      }
+    };
+  }
+
+  @Override
+  protected Collection<String> getFieldsToHighlight(Document document) {
+    Collection<String> result = super.getFieldsToHighlight(document);
+    //if stored is false, then result will be empty, in which case just get all the param fields
+    if (paramFields.isEmpty() == false && result.isEmpty() == false) {
+      result.retainAll(paramFields);
+    } else {
+      result = paramFields;
+    }
+    return result;
+  }
+
+  @Override
+  public void setParams(String params) {
+    // can't call super because super doesn't understand our
+    // params syntax
+    this.params = params;
+    String [] splits = params.split(",");
+    for (int i = 0; i < splits.length; i++) {
+      if (splits[i].startsWith("size[") == true){
+        traversalSize = (int)Float.parseFloat(splits[i].substring("size[".length(),splits[i].length() - 1));
+      } else if (splits[i].startsWith("highlight[") == true){
+        numToHighlight = (int)Float.parseFloat(splits[i].substring("highlight[".length(),splits[i].length() - 1));
+      } else if (splits[i].startsWith("maxFrags[") == true){
+        maxFrags = (int)Float.parseFloat(splits[i].substring("maxFrags[".length(),splits[i].length() - 1));
+      } else if (splits[i].startsWith("mergeContiguous[") == true){
+        mergeContiguous = Boolean.valueOf(splits[i].substring("mergeContiguous[".length(),splits[i].length() - 1)).booleanValue();
+      } else if (splits[i].startsWith("fields[") == true){
+        paramFields = new HashSet<String>();
+        String fieldNames = splits[i].substring("fields[".length(), splits[i].length() - 1);
+        String [] fieldSplits = fieldNames.split(";");
+        for (int j = 0; j < fieldSplits.length; j++) {
+          paramFields.add(fieldSplits[j]);          
+        }
+
+      }
+    }
+  }
+
+
+}
\ No newline at end of file
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetLoadFieldSelectorTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetLoadFieldSelectorTask.java
new file mode 100644
index 0000000..26050b4
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetLoadFieldSelectorTask.java
@@ -0,0 +1,80 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+/**
+ * Copyright 2005 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.document.FieldSelector;
+import org.apache.lucene.document.SetBasedFieldSelector;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.IndexReader;
+
+import java.util.StringTokenizer;
+import java.util.Set;
+import java.util.HashSet;
+import java.util.Collections;
+import java.io.IOException;
+
+/**
+ * Search and Traverse and Retrieve docs task using a SetBasedFieldSelector.
+ *
+ * <p>Note: This task reuses the reader if it is already open.
+ * Otherwise a reader is opened at start and closed at the end.
+ *
+ * <p>Takes optional param: comma separated list of Fields to load.</p>
+ * 
+ * <p>Other side effects: counts additional 1 (record) for each traversed hit, 
+ * and 1 more for each retrieved (non null) document.</p>
+ */
+public class SearchTravRetLoadFieldSelectorTask extends SearchTravTask {
+
+  protected FieldSelector fieldSelector;
+  public SearchTravRetLoadFieldSelectorTask(PerfRunData runData) {
+    super(runData);
+    
+  }
+
+  @Override
+  public boolean withRetrieve() {
+    return true;
+  }
+
+
+  @Override
+  protected Document retrieveDoc(IndexReader ir, int id) throws IOException {
+    return ir.document(id, fieldSelector);
+  }
+
+  @Override
+  public void setParams(String params) {
+    this.params = params; // cannot just call super.setParams(), b/c it's params differ.
+    Set<String> fieldsToLoad = new HashSet<String>();
+    for (StringTokenizer tokenizer = new StringTokenizer(params, ","); tokenizer.hasMoreTokens();) {
+      String s = tokenizer.nextToken();
+      fieldsToLoad.add(s);
+    }
+    fieldSelector = new SetBasedFieldSelector(fieldsToLoad, Collections.<String> emptySet());
+  }
+
+
+  /* (non-Javadoc)
+  * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#supportsParams()
+  */
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetTask.java
new file mode 100644
index 0000000..fc450a3
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetTask.java
@@ -0,0 +1,45 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+
+/**
+ * Search and Traverse and Retrieve docs task.
+ * 
+ * <p>Note: This task reuses the reader if it is already open. 
+ * Otherwise a reader is opened at start and closed at the end.
+ * </p>
+ * 
+ * <p>Takes optional param: traversal size (otherwise all results are traversed).</p>
+ * 
+ * <p>Other side effects: counts additional 1 (record) for each traversed hit, 
+ * and 1 more for each retrieved (non null) document.</p>
+ */
+public class SearchTravRetTask extends SearchTravTask {
+
+  public SearchTravRetTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public boolean withRetrieve() {
+    return true;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetVectorHighlightTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetVectorHighlightTask.java
new file mode 100644
index 0000000..19570af
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetVectorHighlightTask.java
@@ -0,0 +1,146 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.vectorhighlight.FastVectorHighlighter;
+import org.apache.lucene.search.vectorhighlight.FieldQuery;
+
+import java.util.Set;
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.Collections;
+
+/**
+ * Search and Traverse and Retrieve docs task.  Highlight the fields in the retrieved documents by using FastVectorHighlighter.
+ *
+ * <p>Note: This task reuses the reader if it is already open.
+ * Otherwise a reader is opened at start and closed at the end.
+ * </p>
+ *
+ * <p>Takes optional multivalued, comma separated param string as: size[&lt;traversal size&gt;],highlight[&lt;int&gt;],maxFrags[&lt;int&gt;],mergeContiguous[&lt;boolean&gt;],fields[name1;name2;...]</p>
+ * <ul>
+ * <li>traversal size - The number of hits to traverse, otherwise all will be traversed</li>
+ * <li>highlight - The number of the hits to highlight.  Will always be less than or equal to traversal size.  Default is Integer.MAX_VALUE (i.e. hits.length())</li>
+ * <li>maxFrags - The maximum number of fragments to score by the highlighter</li>
+ * <li>fragSize - The length of fragments</li>
+ * <li>fields - The fields to highlight.  If not specified all fields will be highlighted (or at least attempted)</li>
+ * </ul>
+ * Example:
+ * <pre>"SearchVecHlgtSameRdr" SearchTravRetVectorHighlight(size[10],highlight[10],maxFrags[3],fields[body]) > : 1000
+ * </pre>
+ *
+ * Fields must be stored and term vector offsets and positions in order must be true for this task to work.
+ *
+ * <p>Other side effects: counts additional 1 (record) for each traversed hit,
+ * and 1 more for each retrieved (non null) document and 1 for each fragment returned.</p>
+ */
+public class SearchTravRetVectorHighlightTask extends SearchTravTask {
+
+  protected int numToHighlight = Integer.MAX_VALUE;
+  protected int maxFrags = 2;
+  protected int fragSize = 100;
+  protected Set<String> paramFields = Collections.emptySet();
+  protected FastVectorHighlighter highlighter;
+
+  public SearchTravRetVectorHighlightTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public void setup() throws Exception {
+    super.setup();
+    //check to make sure either the doc is being stored
+    PerfRunData data = getRunData();
+    if (data.getConfig().get("doc.stored", false) == false){
+      throw new Exception("doc.stored must be set to true");
+    }
+    if (data.getConfig().get("doc.term.vector.offsets", false) == false){
+      throw new Exception("doc.term.vector.offsets must be set to true");
+    }
+    if (data.getConfig().get("doc.term.vector.positions", false) == false){
+      throw new Exception("doc.term.vector.positions must be set to true");
+    }
+  }
+
+  @Override
+  public boolean withRetrieve() {
+    return true;
+  }
+
+  @Override
+  public int numToHighlight() {
+    return numToHighlight;
+  }
+  
+  @Override
+  protected BenchmarkHighlighter getBenchmarkHighlighter(Query q){
+    highlighter = new FastVectorHighlighter( false, false );
+    final FieldQuery fq = highlighter.getFieldQuery( q );
+    return new BenchmarkHighlighter(){
+      @Override
+      public int doHighlight(IndexReader reader, int doc, String field,
+          Document document, Analyzer analyzer, String text) throws Exception {
+        String[] fragments = highlighter.getBestFragments(fq, reader, doc, field, fragSize, maxFrags);
+        return fragments != null ? fragments.length : 0;
+      }
+    };
+  }
+
+  @Override
+  protected Collection<String> getFieldsToHighlight(Document document) {
+    Collection<String> result = super.getFieldsToHighlight(document);
+    //if stored is false, then result will be empty, in which case just get all the param fields
+    if (paramFields.isEmpty() == false && result.isEmpty() == false) {
+      result.retainAll(paramFields);
+    } else {
+      result = paramFields;
+    }
+    return result;
+  }
+
+  @Override
+  public void setParams(String params) {
+    // can't call super because super doesn't understand our
+    // params syntax
+    final String [] splits = params.split(",");
+    for (int i = 0; i < splits.length; i++) {
+      if (splits[i].startsWith("size[") == true){
+        traversalSize = (int)Float.parseFloat(splits[i].substring("size[".length(),splits[i].length() - 1));
+      } else if (splits[i].startsWith("highlight[") == true){
+        numToHighlight = (int)Float.parseFloat(splits[i].substring("highlight[".length(),splits[i].length() - 1));
+      } else if (splits[i].startsWith("maxFrags[") == true){
+        maxFrags = (int)Float.parseFloat(splits[i].substring("maxFrags[".length(),splits[i].length() - 1));
+      } else if (splits[i].startsWith("fragSize[") == true){
+        fragSize = (int)Float.parseFloat(splits[i].substring("fragSize[".length(),splits[i].length() - 1));
+      } else if (splits[i].startsWith("fields[") == true){
+        paramFields = new HashSet<String>();
+        String fieldNames = splits[i].substring("fields[".length(), splits[i].length() - 1);
+        String [] fieldSplits = fieldNames.split(";");
+        for (int j = 0; j < fieldSplits.length; j++) {
+          paramFields.add(fieldSplits[j]);          
+        }
+
+      }
+    }
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravTask.java
new file mode 100644
index 0000000..56893fd
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravTask.java
@@ -0,0 +1,86 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.feeds.QueryMaker;
+
+/**
+ * Search and Traverse task.
+ * 
+ * <p>Note: This task reuses the reader if it is already open. 
+ * Otherwise a reader is opened at start and closed at the end.
+ * <p/>
+ * 
+ * <p>Takes optional param: traversal size (otherwise all results are traversed).</p>
+ * 
+ * <p>Other side effects: counts additional 1 (record) for each traversed hit.</p>
+ */
+public class SearchTravTask extends ReadTask {
+  protected int traversalSize = Integer.MAX_VALUE;
+
+  public SearchTravTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public boolean withRetrieve() {
+    return false;
+  }
+
+  @Override
+  public boolean withSearch() {
+    return true;
+  }
+
+  @Override
+  public boolean withTraverse() {
+    return true;
+  }
+
+  @Override
+  public boolean withWarm() {
+    return false;
+  }
+
+  
+
+  @Override
+  public QueryMaker getQueryMaker() {
+    return getRunData().getQueryMaker(this);
+  }
+
+  @Override
+  public int traversalSize() {
+    return traversalSize;
+  }
+
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    traversalSize = (int)Float.parseFloat(params);
+  }
+
+  /* (non-Javadoc)
+   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#supportsParams()
+   */
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchWithCollectorTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchWithCollectorTask.java
new file mode 100644
index 0000000..b654540
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchWithCollectorTask.java
@@ -0,0 +1,93 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.feeds.QueryMaker;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.TopScoreDocCollector;
+
+/**
+ * Does search w/ a custom collector
+ */
+public class SearchWithCollectorTask extends SearchTask {
+
+  protected String clnName;
+
+  public SearchWithCollectorTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public void setup() throws Exception {
+    super.setup();
+    //check to make sure either the doc is being stored
+    PerfRunData runData = getRunData();
+    Config config = runData.getConfig();
+    clnName = config.get("collector.class", "");
+  }
+
+  
+
+  @Override
+  public boolean withCollector() {
+    return true;
+  }
+
+  @Override
+  protected Collector createCollector() throws Exception {
+    Collector collector = null;
+    if (clnName.equalsIgnoreCase("topScoreDocOrdered") == true) {
+      collector = TopScoreDocCollector.create(numHits(), true);
+    } else if (clnName.equalsIgnoreCase("topScoreDocUnOrdered") == true) {
+      collector = TopScoreDocCollector.create(numHits(), false);
+    } else if (clnName.length() > 0){
+      collector = Class.forName(clnName).asSubclass(Collector.class).newInstance();
+
+    } else {
+      collector = super.createCollector();
+    }
+    return collector;
+  }
+
+  @Override
+  public QueryMaker getQueryMaker() {
+    return getRunData().getQueryMaker(this);
+  }
+
+  @Override
+  public boolean withRetrieve() {
+    return false;
+  }
+
+  @Override
+  public boolean withSearch() {
+    return true;
+  }
+
+  @Override
+  public boolean withTraverse() {
+    return false;
+  }
+
+  @Override
+  public boolean withWarm() {
+    return false;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchWithSortTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchWithSortTask.java
new file mode 100644
index 0000000..e00583f
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchWithSortTask.java
@@ -0,0 +1,164 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.feeds.QueryMaker;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField;
+
+/**
+ * Does sort search on specified field.
+ * 
+ */
+public class SearchWithSortTask extends ReadTask {
+
+  private boolean doScore = true;
+  private boolean doMaxScore = true;
+  private Sort sort;
+
+  public SearchWithSortTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  /**
+   * SortFields: field:type,field:type[,noscore][,nomaxscore]
+   *
+   * If noscore is present, then we turn off score tracking
+   * in {@link org.apache.lucene.search.TopFieldCollector}.
+   * If nomaxscore is present, then we turn off maxScore tracking
+   * in {@link org.apache.lucene.search.TopFieldCollector}.
+   * 
+   * name:string,page:int,subject:string
+   * 
+   */
+  @Override
+  public void setParams(String sortField) {
+    super.setParams(sortField);
+    String[] fields = sortField.split(",");
+    SortField[] sortFields = new SortField[fields.length];
+    int upto = 0;
+    for (int i = 0; i < fields.length; i++) {
+      String field = fields[i];
+      SortField sortField0;
+      if (field.equals("doc")) {
+        sortField0 = SortField.FIELD_DOC;
+      } if (field.equals("score")) {
+        sortField0 = SortField.FIELD_SCORE;
+      } else if (field.equals("noscore")) {
+        doScore = false;
+        continue;
+      } else if (field.equals("nomaxscore")) {
+        doMaxScore = false;
+        continue;
+      } else {
+        int index = field.lastIndexOf(":");
+        String fieldName;
+        String typeString;
+        if (index != -1) {
+          fieldName = field.substring(0, index);
+          typeString = field.substring(1+index, field.length());
+        } else {
+          throw new RuntimeException("You must specify the sort type ie page:int,subject:string");
+        }
+        int type = getType(typeString);
+        sortField0 = new SortField(fieldName, type);
+      }
+      sortFields[upto++] = sortField0;
+    }
+
+    if (upto < sortFields.length) {
+      SortField[] newSortFields = new SortField[upto];
+      System.arraycopy(sortFields, 0, newSortFields, 0, upto);
+      sortFields = newSortFields;
+    }
+    this.sort = new Sort(sortFields);
+  }
+
+  private int getType(String typeString) {
+    int type;
+    if (typeString.equals("float")) {
+      type = SortField.FLOAT;
+    } else if (typeString.equals("double")) {
+      type = SortField.DOUBLE;
+    } else if (typeString.equals("byte")) {
+      type = SortField.BYTE;
+    } else if (typeString.equals("short")) {
+      type = SortField.SHORT;
+    } else if (typeString.equals("int")) {
+      type = SortField.INT;
+    } else if (typeString.equals("long")) {
+      type = SortField.LONG;
+    } else if (typeString.equals("string")) {
+      type = SortField.STRING;
+    } else if (typeString.equals("string_val")) {
+      type = SortField.STRING_VAL;
+    } else {
+      throw new RuntimeException("Unrecognized sort field type " + typeString);
+    }
+    return type;
+  }
+
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+
+  @Override
+  public QueryMaker getQueryMaker() {
+    return getRunData().getQueryMaker(this);
+  }
+
+  @Override
+  public boolean withRetrieve() {
+    return false;
+  }
+
+  @Override
+  public boolean withSearch() {
+    return true;
+  }
+
+  @Override
+  public boolean withTraverse() {
+    return false;
+  }
+
+  @Override
+  public boolean withWarm() {
+    return false;
+  }
+
+  @Override
+  public boolean withScore() {
+    return doScore;
+  }
+
+  @Override
+  public boolean withMaxScore() {
+    return doMaxScore;
+  }
+  
+  @Override
+  public Sort getSort() {
+    if (sort == null) {
+      throw new IllegalStateException("No sort field was set");
+    }
+    return sort;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SetPropTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SetPropTask.java
new file mode 100644
index 0000000..23c2799
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SetPropTask.java
@@ -0,0 +1,69 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Set a performance test configuration property.
+ * A property may have a single value, or a sequence of values, separated by ":". 
+ * If a sequence of values is specified, each time a new round starts, 
+ * the next (cyclic) value is taken.  
+ * <br>Other side effects: none.
+ * <br>Takes mandatory param: "name,value" pair. 
+ * @see org.apache.lucene.benchmark.byTask.tasks.NewRoundTask
+ */
+public class SetPropTask extends PerfTask {
+
+  public SetPropTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  private String name;
+  private String value;
+  
+  @Override
+  public int doLogic() throws Exception {
+    if (name==null || value==null) {
+      throw new Exception(getName()+" - undefined name or value: name="+name+" value="+value);
+    }
+    getRunData().getConfig().set(name,value);
+    return 0;
+  }
+
+  /**
+   * Set the params (property name and value).
+   * @param params property name and value separated by ','.
+   */
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    int k = params.indexOf(",");
+    name = params.substring(0,k).trim();
+    value = params.substring(k+1).trim();
+  }
+
+  /* (non-Javadoc)
+   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#supportsParams()
+   */
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/TaskSequence.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/TaskSequence.java
new file mode 100644
index 0000000..6e3b687
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/TaskSequence.java
@@ -0,0 +1,527 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.List;
+import java.text.NumberFormat;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.feeds.NoMoreDataException;
+import org.apache.lucene.benchmark.byTask.stats.TaskStats;
+import org.apache.lucene.util.ArrayUtil;
+
+/**
+ * Sequence of parallel or sequential tasks.
+ */
+public class TaskSequence extends PerfTask {
+  public static int REPEAT_EXHAUST = -2; 
+  private ArrayList<PerfTask> tasks;
+  private int repetitions = 1;
+  private boolean parallel;
+  private TaskSequence parent;
+  private boolean letChildReport = true;
+  private int rate = 0;
+  private boolean perMin = false; // rate, if set, is, by default, be sec.
+  private String seqName;
+  private boolean exhausted = false;
+  private boolean resetExhausted = false;
+  private PerfTask[] tasksArray;
+  private boolean anyExhaustibleTasks;
+  private boolean collapsable = false; // to not collapse external sequence named in alg.  
+  
+  private boolean fixedTime;                      // true if we run for fixed time
+  private double runTimeSec;                      // how long to run for
+  private final long logByTimeMsec;
+
+  public TaskSequence (PerfRunData runData, String name, TaskSequence parent, boolean parallel) {
+    super(runData);
+    collapsable = (name == null);
+    name = (name!=null ? name : (parallel ? "Par" : "Seq"));
+    setName(name);
+    setSequenceName();
+    this.parent = parent;
+    this.parallel = parallel;
+    tasks = new ArrayList<PerfTask>();
+    logByTimeMsec = runData.getConfig().get("report.time.step.msec", 0);
+  }
+
+  @Override
+  public void close() throws Exception {
+    initTasksArray();
+    for(int i=0;i<tasksArray.length;i++) {
+      tasksArray[i].close();
+    }
+    getRunData().getDocMaker().close();
+  }
+
+  private void initTasksArray() {
+    if (tasksArray == null) {
+      final int numTasks = tasks.size();
+      tasksArray = new PerfTask[numTasks];
+      for(int k=0;k<numTasks;k++) {
+        tasksArray[k] = tasks.get(k);
+        anyExhaustibleTasks |= tasksArray[k] instanceof ResetInputsTask;
+        anyExhaustibleTasks |= tasksArray[k] instanceof TaskSequence;
+      }
+    }
+    if (!parallel && logByTimeMsec != 0 && !letChildReport) {
+      countsByTime = new int[1];
+    }
+  }
+
+  /**
+   * @return Returns the parallel.
+   */
+  public boolean isParallel() {
+    return parallel;
+  }
+
+  /**
+   * @return Returns the repetitions.
+   */
+  public int getRepetitions() {
+    return repetitions;
+  }
+
+  private int[] countsByTime;
+
+  public void setRunTime(double sec) throws Exception {
+    runTimeSec = sec;
+    fixedTime = true;
+  }
+
+  /**
+   * @param repetitions The repetitions to set.
+   * @throws Exception 
+   */
+  public void setRepetitions(int repetitions) throws Exception {
+    fixedTime = false;
+    this.repetitions = repetitions;
+    if (repetitions==REPEAT_EXHAUST) {
+      if (isParallel()) {
+        throw new Exception("REPEAT_EXHAUST is not allowed for parallel tasks");
+      }
+    }
+    setSequenceName();
+  }
+
+  /**
+   * @return Returns the parent.
+   */
+  public TaskSequence getParent() {
+    return parent;
+  }
+
+  /*
+   * (non-Javadoc)
+   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#doLogic()
+   */
+  @Override
+  public int doLogic() throws Exception {
+    exhausted = resetExhausted = false;
+    return ( parallel ? doParallelTasks() : doSerialTasks());
+  }
+
+  private static class RunBackgroundTask extends Thread {
+    private final PerfTask task;
+    private final boolean letChildReport;
+    private volatile int count;
+
+    public RunBackgroundTask(PerfTask task, boolean letChildReport) {
+      this.task = task;
+      this.letChildReport = letChildReport;
+    }
+
+    public void stopNow() throws InterruptedException {
+      task.stopNow();
+    }
+
+    public int getCount() {
+      return count;
+    }
+
+    @Override
+    public void run() {
+      try {
+        count = task.runAndMaybeStats(letChildReport);
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
+    }
+  }
+
+  private int doSerialTasks() throws Exception {
+    if (rate > 0) {
+      return doSerialTasksWithRate();
+    }
+    
+    initTasksArray();
+    int count = 0;
+
+    final long runTime = (long) (runTimeSec*1000);
+    List<RunBackgroundTask> bgTasks = null;
+
+    final long t0 = System.currentTimeMillis();
+    for (int k=0; fixedTime || (repetitions==REPEAT_EXHAUST && !exhausted) || k<repetitions; k++) {
+      if (stopNow) {
+        break;
+      }
+      for(int l=0;l<tasksArray.length;l++) {
+        final PerfTask task = tasksArray[l];
+        if (task.getRunInBackground()) {
+          if (bgTasks == null) {
+            bgTasks = new ArrayList<RunBackgroundTask>();
+          }
+          RunBackgroundTask bgTask = new RunBackgroundTask(task, letChildReport);
+          bgTask.setPriority(task.getBackgroundDeltaPriority() + Thread.currentThread().getPriority());
+          bgTask.start();
+          bgTasks.add(bgTask);
+        } else {
+          try {
+            final int inc = task.runAndMaybeStats(letChildReport);
+            count += inc;
+            if (countsByTime != null) {
+              final int slot = (int) ((System.currentTimeMillis()-t0)/logByTimeMsec);
+              if (slot >= countsByTime.length) {
+                countsByTime = ArrayUtil.grow(countsByTime, 1+slot);
+              }
+              countsByTime[slot] += inc;
+            }
+            if (anyExhaustibleTasks)
+              updateExhausted(task);
+          } catch (NoMoreDataException e) {
+            exhausted = true;
+          }
+        }
+      }
+      if (fixedTime && System.currentTimeMillis()-t0 > runTime) {
+        repetitions = k+1;
+        break;
+      }
+    }
+
+    if (bgTasks != null) {
+      for(RunBackgroundTask bgTask : bgTasks) {
+        bgTask.stopNow();
+      }
+      for(RunBackgroundTask bgTask : bgTasks) {
+        bgTask.join();
+        count += bgTask.getCount();
+      }
+    }
+
+    if (countsByTime != null) {
+      getRunData().getPoints().getCurrentStats().setCountsByTime(countsByTime, logByTimeMsec);
+    }
+
+    stopNow = false;
+
+    return count;
+  }
+
+  private int doSerialTasksWithRate() throws Exception {
+    initTasksArray();
+    long delayStep = (perMin ? 60000 : 1000) /rate;
+    long nextStartTime = System.currentTimeMillis();
+    int count = 0;
+    final long t0 = System.currentTimeMillis();
+    for (int k=0; (repetitions==REPEAT_EXHAUST && !exhausted) || k<repetitions; k++) {
+      if (stopNow) {
+        break;
+      }
+      for (int l=0;l<tasksArray.length;l++) {
+        final PerfTask task = tasksArray[l];
+        while(!stopNow) {
+          long waitMore = nextStartTime - System.currentTimeMillis();
+          if (waitMore > 0) {
+            // TODO: better to use condition to notify
+            Thread.sleep(1);
+          } else {
+            break;
+          }
+        }
+        if (stopNow) {
+          break;
+        }
+        nextStartTime += delayStep; // this aims at avarage rate. 
+        try {
+          final int inc = task.runAndMaybeStats(letChildReport);
+          count += inc;
+          if (countsByTime != null) {
+            final int slot = (int) ((System.currentTimeMillis()-t0)/logByTimeMsec);
+            if (slot >= countsByTime.length) {
+              countsByTime = ArrayUtil.grow(countsByTime, 1+slot);
+            }
+            countsByTime[slot] += inc;
+          }
+
+          if (anyExhaustibleTasks)
+            updateExhausted(task);
+        } catch (NoMoreDataException e) {
+          exhausted = true;
+        }
+      }
+    }
+    stopNow = false;
+    return count;
+  }
+
+  // update state regarding exhaustion.
+  private void updateExhausted(PerfTask task) {
+    if (task instanceof ResetInputsTask) {
+      exhausted = false;
+      resetExhausted = true;
+    } else if (task instanceof TaskSequence) {
+      TaskSequence t = (TaskSequence) task;
+      if (t.resetExhausted) {
+        exhausted = false;
+        resetExhausted = true;
+        t.resetExhausted = false;
+      } else {
+        exhausted |= t.exhausted;
+      }
+    }
+  }
+
+  private class ParallelTask extends Thread {
+
+    public int count;
+    public final PerfTask task;
+
+    public ParallelTask(PerfTask task) {
+      this.task = task;
+    }
+
+    @Override
+    public void run() {
+      try {
+        int n = task.runAndMaybeStats(letChildReport);
+        if (anyExhaustibleTasks) {
+          updateExhausted(task);
+        }
+        count += n;
+      } catch (NoMoreDataException e) {
+        exhausted = true;
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
+    }
+  }
+
+  @Override
+  public void stopNow() {
+    super.stopNow();
+    // Forwards top request to children
+    if (runningParallelTasks != null) {
+      for(ParallelTask t : runningParallelTasks) {
+        t.task.stopNow();
+      }
+    }
+  }
+
+  ParallelTask[] runningParallelTasks;
+
+  private int doParallelTasks() throws Exception {
+
+    final TaskStats stats = getRunData().getPoints().getCurrentStats();
+
+    initTasksArray();
+    ParallelTask t[] = runningParallelTasks = new ParallelTask[repetitions * tasks.size()];
+    // prepare threads
+    int index = 0;
+    for (int k=0; k<repetitions; k++) {
+      for (int i = 0; i < tasksArray.length; i++) {
+        final PerfTask task = (PerfTask) tasksArray[i].clone();
+        t[index++] = new ParallelTask(task);
+      }
+    }
+    // run threads
+    startThreads(t);
+
+    // wait for all threads to complete
+    int count = 0;
+    for (int i = 0; i < t.length; i++) {
+      t[i].join();
+      count += t[i].count;
+      if (t[i].task instanceof TaskSequence) {
+        TaskSequence sub = (TaskSequence) t[i].task;
+        if (sub.countsByTime != null) {
+          if (countsByTime == null) {
+            countsByTime = new int[sub.countsByTime.length];
+          } else if (countsByTime.length < sub.countsByTime.length) {
+            countsByTime = ArrayUtil.grow(countsByTime, sub.countsByTime.length);
+          }
+          for(int j=0;j<sub.countsByTime.length;j++) {
+            countsByTime[j] += sub.countsByTime[j];
+          }
+        }
+      }
+    }
+
+    if (countsByTime != null) {
+      stats.setCountsByTime(countsByTime, logByTimeMsec);
+    }
+
+    // return total count
+    return count;
+  }
+
+  // run threads
+  private void startThreads(ParallelTask[] t) throws InterruptedException {
+    if (rate > 0) {
+      startlThreadsWithRate(t);
+      return;
+    }
+    for (int i = 0; i < t.length; i++) {
+      t[i].start();
+    }
+  }
+
+  // run threads with rate
+  private void startlThreadsWithRate(ParallelTask[] t) throws InterruptedException {
+    long delayStep = (perMin ? 60000 : 1000) /rate;
+    long nextStartTime = System.currentTimeMillis();
+    for (int i = 0; i < t.length; i++) {
+      long waitMore = nextStartTime - System.currentTimeMillis();
+      if (waitMore > 0) {
+        Thread.sleep(waitMore);
+      }
+      nextStartTime += delayStep; // this aims at average rate of starting threads. 
+      t[i].start();
+    }
+  }
+
+  public void addTask(PerfTask task) {
+    tasks.add(task);
+    task.setDepth(getDepth()+1);
+  }
+  
+  /* (non-Javadoc)
+   * @see java.lang.Object#toString()
+   */
+  @Override
+  public String toString() {
+    String padd = getPadding();
+    StringBuilder sb = new StringBuilder(super.toString());
+    sb.append(parallel ? " [" : " {");
+    sb.append(NEW_LINE);
+    for (final PerfTask task : tasks) {
+      sb.append(task.toString());
+      sb.append(NEW_LINE);
+    }
+    sb.append(padd);
+    sb.append(!letChildReport ? ">" : (parallel ? "]" : "}"));
+    if (fixedTime) {
+      sb.append(" " + NumberFormat.getNumberInstance().format(runTimeSec) + "s");
+    } else if (repetitions>1) {
+      sb.append(" * " + repetitions);
+    } else if (repetitions==REPEAT_EXHAUST) {
+      sb.append(" * EXHAUST");
+    }
+    if (rate>0) {
+      sb.append(",  rate: " + rate+"/"+(perMin?"min":"sec"));
+    }
+    if (getRunInBackground()) {
+      sb.append(" &");
+      int x = getBackgroundDeltaPriority();
+      if (x != 0) {
+        sb.append(x);
+      }
+    }
+    return sb.toString();
+  }
+
+  /**
+   * Execute child tasks in a way that they do not report their time separately.
+   */
+  public void setNoChildReport() {
+    letChildReport  = false;
+    for (final PerfTask task : tasks) {
+      if (task instanceof TaskSequence) {
+        ((TaskSequence)task).setNoChildReport();
+  }
+    }
+  }
+
+  /**
+   * Returns the rate per minute: how many operations should be performed in a minute.
+   * If 0 this has no effect.
+   * @return the rate per min: how many operations should be performed in a minute.
+   */
+  public int getRate() {
+    return (perMin ? rate : 60*rate);
+  }
+
+  /**
+   * @param rate The rate to set.
+   */
+  public void setRate(int rate, boolean perMin) {
+    this.rate = rate;
+    this.perMin = perMin;
+    setSequenceName();
+  }
+
+  private void setSequenceName() {
+    seqName = super.getName();
+    if (repetitions==REPEAT_EXHAUST) {
+      seqName += "_Exhaust";
+    } else if (repetitions>1) {
+      seqName += "_"+repetitions;
+    }
+    if (rate>0) {
+      seqName += "_" + rate + (perMin?"/min":"/sec"); 
+    }
+    if (parallel && seqName.toLowerCase().indexOf("par")<0) {
+      seqName += "_Par";
+    }
+  }
+
+  @Override
+  public String getName() {
+    return seqName; // override to include more info 
+  }
+
+  /**
+   * @return Returns the tasks.
+   */
+  public ArrayList<PerfTask> getTasks() {
+    return tasks;
+  }
+
+  /* (non-Javadoc)
+   * @see java.lang.Object#clone()
+   */
+  @Override
+  protected Object clone() throws CloneNotSupportedException {
+    TaskSequence res = (TaskSequence) super.clone();
+    res.tasks = new ArrayList<PerfTask>();
+    for (int i = 0; i < tasks.size(); i++) {
+      res.tasks.add((PerfTask)tasks.get(i).clone());
+    }
+    return res;
+  }
+
+  /**
+   * Return true if can be collapsed in case it is outermost sequence
+   */
+  public boolean isCollapsable() {
+    return collapsable;
+  }
+  
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/UpdateDocTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/UpdateDocTask.java
new file mode 100644
index 0000000..759029b
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/UpdateDocTask.java
@@ -0,0 +1,94 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexWriter;
+
+/**
+ * Update a document, using IndexWriter.updateDocument,
+ * optionally with of a certain size.
+ * <br>Other side effects: none.
+ * <br>Takes optional param: document size. 
+ */
+public class UpdateDocTask extends PerfTask {
+
+  public UpdateDocTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  private int docSize = 0;
+  
+  // volatile data passed between setup(), doLogic(), tearDown().
+  private Document doc = null;
+  
+  @Override
+  public void setup() throws Exception {
+    super.setup();
+    DocMaker docMaker = getRunData().getDocMaker();
+    if (docSize > 0) {
+      doc = docMaker.makeDocument(docSize);
+    } else {
+      doc = docMaker.makeDocument();
+    }
+  }
+
+  @Override
+  public void tearDown() throws Exception {
+    doc = null;
+    super.tearDown();
+  }
+
+  @Override
+  public int doLogic() throws Exception {
+    final String docID = doc.get(DocMaker.ID_FIELD);
+    if (docID == null) {
+      throw new IllegalStateException("document must define the docid field");
+    }
+    final IndexWriter iw = getRunData().getIndexWriter();
+    iw.updateDocument(new Term(DocMaker.ID_FIELD, docID), doc);
+    return 1;
+  }
+
+  @Override
+  protected String getLogMessage(int recsCount) {
+    return "updated " + recsCount + " docs";
+  }
+  
+  /**
+   * Set the params (docSize only)
+   * @param params docSize, or 0 for no limit.
+   */
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    docSize = (int) Float.parseFloat(params); 
+  }
+
+  /* (non-Javadoc)
+   * @see org.apache.lucene.benchmark.byTask.tasks.PerfTask#supportsParams()
+   */
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+  
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WaitTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WaitTask.java
new file mode 100644
index 0000000..39d526c
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WaitTask.java
@@ -0,0 +1,75 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+
+/**
+ * Simply waits for the specified (via the parameter) amount
+ * of time.  For example Wait(30s) waits for 30 seconds.
+ * This is useful with background tasks to control how long
+ * the tasks run.
+ *
+ *<p>You can specify h, m, or s (hours, minutes, seconds) as
+ *the trailing time unit.  No unit is interpreted as
+ *seconds.</p>
+ */
+public class WaitTask extends PerfTask {
+
+  private double waitTimeSec;
+
+  public WaitTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    if (params != null) {
+      int multiplier;
+      if (params.endsWith("s")) {
+        multiplier = 1;
+        params = params.substring(0, params.length()-1);
+      } else if (params.endsWith("m")) {
+        multiplier = 60;
+        params = params.substring(0, params.length()-1);
+      } else if (params.endsWith("h")) {
+        multiplier = 3600;
+        params = params.substring(0, params.length()-1);
+      } else {
+        // Assume seconds
+        multiplier = 1;
+      }
+
+      waitTimeSec = Double.parseDouble(params) * multiplier;
+    } else {
+      throw new IllegalArgumentException("you must specify the wait time, eg: 10.0s, 4.5m, 2h");
+    }
+  }
+
+  @Override
+  public int doLogic() throws Exception {
+    Thread.sleep((long) (1000*waitTimeSec));
+    return 0;
+  }
+
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WarmTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WarmTask.java
new file mode 100644
index 0000000..ba1a666
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WarmTask.java
@@ -0,0 +1,65 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.feeds.QueryMaker;
+
+/**
+ * Warm reader task: retrieve all reader documents.
+ * 
+ * <p>Note: This task reuses the reader if it is already open. 
+ * Otherwise a reader is opened at start and closed at the end.
+ * </p>
+ * 
+ * <p>Other side effects: counts additional 1 (record) for each 
+ * retrieved (non null) document.</p>
+ */
+public class WarmTask extends ReadTask {
+
+  public WarmTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public boolean withRetrieve() {
+    return false;
+  }
+
+  @Override
+  public boolean withSearch() {
+    return false;
+  }
+
+  @Override
+  public boolean withTraverse() {
+    return false;
+  }
+
+  @Override
+  public boolean withWarm() {
+    return true;
+  }
+
+  @Override
+  public QueryMaker getQueryMaker() {
+    return null; // not required for this task.
+  }
+
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTask.java
new file mode 100644
index 0000000..c5a3adb
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTask.java
@@ -0,0 +1,154 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedOutputStream;
+import java.io.BufferedWriter;
+import java.io.FileOutputStream;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.io.PrintWriter;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.commons.compress.compressors.CompressorStreamFactory;
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+
+/**
+ * A task which writes documents, one line per document. Each line is in the
+ * following format: title &lt;TAB&gt; date &lt;TAB&gt; body. The output of this
+ * task can be consumed by
+ * {@link org.apache.lucene.benchmark.byTask.feeds.LineDocSource} and is intended
+ * to save the IO overhead of opening a file per document to be indexed.<br>
+ * Supports the following parameters:
+ * <ul>
+ * <li>line.file.out - the name of the file to write the output to. That
+ * parameter is mandatory. <b>NOTE:</b> the file is re-created.
+ * <li>bzip.compression - whether the output should be bzip-compressed. This is
+ * recommended when the output file is expected to be large. (optional, default:
+ * false).
+ * </ul>
+ * <b>NOTE:</b> this class is not thread-safe and if used by multiple threads the
+ * output is unspecified (as all will write to the same output file in a
+ * non-synchronized way).
+ */
+public class WriteLineDocTask extends PerfTask {
+
+  public final static char SEP = '\t';
+  
+  private int docSize = 0;
+  private PrintWriter lineFileOut = null;
+  private DocMaker docMaker;
+  private ThreadLocal<StringBuilder> threadBuffer = new ThreadLocal<StringBuilder>();
+  private ThreadLocal<Matcher> threadNormalizer = new ThreadLocal<Matcher>();
+  
+  public WriteLineDocTask(PerfRunData runData) throws Exception {
+    super(runData);
+    Config config = runData.getConfig();
+    String fileName = config.get("line.file.out", null);
+    if (fileName == null) {
+      throw new IllegalArgumentException("line.file.out must be set");
+    }
+
+    OutputStream out = new FileOutputStream(fileName);
+    boolean doBzipCompression = false;
+    String doBZCompress = config.get("bzip.compression", null);
+    if (doBZCompress != null) {
+      // Property was set, use the value.
+      doBzipCompression = Boolean.valueOf(doBZCompress).booleanValue();
+    } else {
+      // Property was not set, attempt to detect based on file's extension
+      doBzipCompression = fileName.endsWith("bz2");
+    }
+
+    if (doBzipCompression) {
+      // Wrap with BOS since BZip2CompressorOutputStream calls out.write(int) 
+      // and does not use the write(byte[]) version. This proved to speed the 
+      // compression process by 70% !
+      out = new BufferedOutputStream(out, 1 << 16);
+      out = new CompressorStreamFactory().createCompressorOutputStream("bzip2", out);
+    }
+    lineFileOut = new PrintWriter(new BufferedWriter(new OutputStreamWriter(out, "UTF-8"), 1 << 16));
+    docMaker = runData.getDocMaker();
+  }
+
+  @Override
+  protected String getLogMessage(int recsCount) {
+    return "Wrote " + recsCount + " line docs";
+  }
+  
+  @Override
+  public int doLogic() throws Exception {
+    Document doc = docSize > 0 ? docMaker.makeDocument(docSize) : docMaker.makeDocument();
+
+    Matcher matcher = threadNormalizer.get();
+    if (matcher == null) {
+      matcher = Pattern.compile("[\t\r\n]+").matcher("");
+      threadNormalizer.set(matcher);
+    }
+    
+    Field f = doc.getField(DocMaker.BODY_FIELD);
+    String body = f != null ? matcher.reset(f.stringValue()).replaceAll(" ") : "";
+    
+    f = doc.getField(DocMaker.TITLE_FIELD);
+    String title = f != null ? matcher.reset(f.stringValue()).replaceAll(" ") : "";
+    
+    if (body.length() > 0 || title.length() > 0) {
+      
+      f = doc.getField(DocMaker.DATE_FIELD);
+      String date = f != null ? matcher.reset(f.stringValue()).replaceAll(" ") : "";
+      
+      StringBuilder sb = threadBuffer.get();
+      if (sb == null) {
+        sb = new StringBuilder();
+        threadBuffer.set(sb);
+      }
+      sb.setLength(0);
+      sb.append(title).append(SEP).append(date).append(SEP).append(body);
+      // lineFileOut is a PrintWriter, which synchronizes internally in println.
+      lineFileOut.println(sb.toString());
+    }
+    return 1;
+  }
+
+  @Override
+  public void close() throws Exception {
+    lineFileOut.close();
+    super.close();
+  }
+  
+  /**
+   * Set the params (docSize only)
+   * @param params docSize, or 0 for no limit.
+   */
+  @Override
+  public void setParams(String params) {
+    super.setParams(params);
+    docSize = (int) Float.parseFloat(params); 
+  }
+
+  @Override
+  public boolean supportsParams() {
+    return true;
+  }
+  
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/package.html b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/package.html
new file mode 100644
index 0000000..9c17edc
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/package.html
@@ -0,0 +1,26 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+   <meta name="Author" content="Doron Cohen">
+</head>
+<body>
+Extendable benchmark tasks.
+</body>
+</html>
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Algorithm.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Algorithm.java
new file mode 100644
index 0000000..b4d6198
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Algorithm.java
@@ -0,0 +1,301 @@
+package org.apache.lucene.benchmark.byTask.utils;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.StreamTokenizer;
+import java.io.StringReader;
+import java.lang.reflect.Constructor;
+import java.util.ArrayList;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.tasks.PerfTask;
+import org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask;
+import org.apache.lucene.benchmark.byTask.tasks.TaskSequence;
+
+/**
+ * Test algorithm, as read from file
+ */
+public class Algorithm {
+  
+  private TaskSequence sequence;
+  
+  /**
+   * Read algorithm from file
+   * @param runData perf-run-data used at running the tasks.
+   * @throws Exception if errors while parsing the algorithm 
+   */
+  @SuppressWarnings("fallthrough")
+  public Algorithm (PerfRunData runData) throws Exception {
+    String algTxt = runData.getConfig().getAlgorithmText();
+    sequence = new TaskSequence(runData,null,null,false);
+    TaskSequence currSequence = sequence;
+    PerfTask prevTask = null;
+    StreamTokenizer stok = new StreamTokenizer(new StringReader(algTxt));
+    stok.commentChar('#');
+    stok.eolIsSignificant(false);
+    stok.ordinaryChar('"');
+    stok.ordinaryChar('/');
+    stok.ordinaryChar('(');
+    stok.ordinaryChar(')');
+    boolean colonOk = false; 
+    boolean isDisableCountNextTask = false; // only for primitive tasks
+    currSequence.setDepth(0);
+    String taskPackage = PerfTask.class.getPackage().getName() + ".";
+    
+    while (stok.nextToken() != StreamTokenizer.TT_EOF) { 
+      switch(stok.ttype) {
+  
+        case StreamTokenizer.TT_WORD:
+          String s = stok.sval;
+          Constructor<? extends PerfTask> cnstr = Class.forName(taskPackage+s+"Task")
+            .asSubclass(PerfTask.class).getConstructor(PerfRunData.class);
+          PerfTask task = cnstr.newInstance(runData);
+          task.setDisableCounting(isDisableCountNextTask);
+          isDisableCountNextTask = false;
+          currSequence.addTask(task);
+          if (task instanceof RepSumByPrefTask) {
+            stok.nextToken();
+            String prefix = stok.sval;
+            if (prefix==null || prefix.length()==0) { 
+              throw new Exception("named report prefix problem - "+stok.toString()); 
+            }
+            ((RepSumByPrefTask) task).setPrefix(prefix);
+          }
+          // check for task param: '(' someParam ')'
+          stok.nextToken();
+          if (stok.ttype!='(') {
+            stok.pushBack();
+          } else {
+            // get params, for tasks that supports them, - anything until next ')'
+            StringBuilder params = new StringBuilder();
+            stok.nextToken();
+            while (stok.ttype!=')') { 
+              switch (stok.ttype) {
+                case StreamTokenizer.TT_NUMBER:  
+                  params.append(stok.nval);
+                  break;
+                case StreamTokenizer.TT_WORD:    
+                  params.append(stok.sval);             
+                  break;
+                case StreamTokenizer.TT_EOF:     
+                  throw new Exception("unexpexted EOF: - "+stok.toString());
+                default:
+                  params.append((char)stok.ttype);
+              }
+              stok.nextToken();
+            }
+            String prm = params.toString().trim();
+            if (prm.length()>0) {
+              task.setParams(prm);
+            }
+          }
+
+          // ---------------------------------------
+          colonOk = false; prevTask = task;
+          break;
+  
+        default:
+          char c = (char)stok.ttype;
+          
+          switch(c) {
+          
+            case ':' :
+              if (!colonOk) throw new Exception("colon unexpexted: - "+stok.toString());
+              colonOk = false;
+              // get repetitions number
+              stok.nextToken();
+              if ((char)stok.ttype == '*') {
+                ((TaskSequence)prevTask).setRepetitions(TaskSequence.REPEAT_EXHAUST);
+              } else {
+                if (stok.ttype!=StreamTokenizer.TT_NUMBER)  {
+                  throw new Exception("expected repetitions number or XXXs: - "+stok.toString());
+                } else {
+                  double num = stok.nval;
+                  stok.nextToken();
+                  if (stok.ttype == StreamTokenizer.TT_WORD && stok.sval.equals("s")) {
+                    ((TaskSequence) prevTask).setRunTime(num);
+                  } else {
+                    stok.pushBack();
+                    ((TaskSequence) prevTask).setRepetitions((int) num);
+                  }
+                }
+              }
+              // check for rate specification (ops/min)
+              stok.nextToken();
+              if (stok.ttype!=':') {
+                stok.pushBack();
+              } else {
+                // get rate number
+                stok.nextToken();
+                if (stok.ttype!=StreamTokenizer.TT_NUMBER) throw new Exception("expected rate number: - "+stok.toString());
+                // check for unit - min or sec, sec is default
+                stok.nextToken();
+                if (stok.ttype!='/') {
+                  stok.pushBack();
+                  ((TaskSequence)prevTask).setRate((int)stok.nval,false); // set rate per sec
+                } else {
+                  stok.nextToken();
+                  if (stok.ttype!=StreamTokenizer.TT_WORD) throw new Exception("expected rate unit: 'min' or 'sec' - "+stok.toString());
+                  String unit = stok.sval.toLowerCase();
+                  if ("min".equals(unit)) {
+                    ((TaskSequence)prevTask).setRate((int)stok.nval,true); // set rate per min
+                  } else if ("sec".equals(unit)) {
+                    ((TaskSequence)prevTask).setRate((int)stok.nval,false); // set rate per sec
+                  } else {
+                    throw new Exception("expected rate unit: 'min' or 'sec' - "+stok.toString());
+                  }
+                }
+              }
+              colonOk = false;
+              break;
+    
+            case '{' : 
+            case '[' :  
+              // a sequence
+              // check for sequence name
+              String name = null;
+              stok.nextToken();
+              if (stok.ttype!='"') {
+                stok.pushBack();
+              } else {
+                stok.nextToken();
+                name = stok.sval;
+                stok.nextToken();
+                if (stok.ttype!='"' || name==null || name.length()==0) { 
+                  throw new Exception("sequence name problem - "+stok.toString()); 
+                }
+              }
+              // start the sequence
+              TaskSequence seq2 = new TaskSequence(runData, name, currSequence, c=='[');
+              currSequence.addTask(seq2);
+              currSequence = seq2;
+              colonOk = false;
+              break;
+
+            case '&' :
+              if (currSequence.isParallel()) {
+                throw new Exception("Can only create background tasks within a serial task");
+              }
+              stok.nextToken();
+              final int deltaPri;
+              if (stok.ttype != StreamTokenizer.TT_NUMBER) {
+                stok.pushBack();
+                deltaPri = 0;
+              } else {
+                // priority
+                deltaPri = (int) stok.nval;
+              }
+
+              if (prevTask == null) {
+                throw new Exception("& was unexpected");
+              } else if (prevTask.getRunInBackground()) {
+                throw new Exception("double & was unexpected");
+              } else {
+                prevTask.setRunInBackground(deltaPri);
+              }
+              break;
+    
+            case '>' :
+              currSequence.setNoChildReport(); /* intentional fallthrough */
+            case '}' : 
+            case ']' : 
+              // end sequence
+              colonOk = true; prevTask = currSequence;
+              currSequence = currSequence.getParent();
+              break;
+          
+            case '-' :
+              isDisableCountNextTask = true;
+              break;
+              
+          } //switch(c)
+          break;
+          
+      } //switch(stok.ttype)
+      
+    }
+    
+    if (sequence != currSequence) {
+      throw new Exception("Unmatched sequences");
+    }
+    
+    // remove redundant top level enclosing sequences
+    while (sequence.isCollapsable() && sequence.getRepetitions()==1 && sequence.getRate()==0) {
+      ArrayList<PerfTask> t = sequence.getTasks();
+      if (t!=null && t.size()==1) {
+        PerfTask p = t.get(0);
+        if (p instanceof TaskSequence) {
+          sequence = (TaskSequence) p;
+          continue;
+        }
+      }
+      break;
+    }
+  }
+
+  /* (non-Javadoc)
+   * @see java.lang.Object#toString()
+   */
+  @Override
+  public String toString() {
+    String newline = System.getProperty("line.separator");
+    StringBuilder sb = new StringBuilder();
+    sb.append(sequence.toString());
+    sb.append(newline);
+    return sb.toString();
+  }
+
+  /**
+   * Execute this algorithm
+   * @throws Exception 
+   */
+  public void execute() throws Exception {
+    try {
+      sequence.runAndMaybeStats(true);
+    } finally {
+      sequence.close();
+    }
+  }
+
+  /**
+   * Expert: for test purposes, return all tasks participating in this algorithm.
+   * @return all tasks participating in this algorithm.
+   */
+  public ArrayList<PerfTask> extractTasks() {
+    ArrayList<PerfTask> res = new ArrayList<PerfTask>();
+    extractTasks(res, sequence);
+    return res;
+  }
+  private void extractTasks (ArrayList<PerfTask> extrct, TaskSequence seq) {
+    if (seq==null) 
+      return;
+    extrct.add(seq);
+    ArrayList<PerfTask> t = sequence.getTasks();
+    if (t==null) 
+      return;
+    for (final PerfTask p : t) {
+      if (p instanceof TaskSequence) {
+        extractTasks(extrct, (TaskSequence)p);
+      } else {
+        extrct.add(p);
+      }
+    }
+  }
+  
+}
+
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java
new file mode 100644
index 0000000..bb166bf
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java
@@ -0,0 +1,452 @@
+package org.apache.lucene.benchmark.byTask.utils;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedReader;
+import java.io.ByteArrayInputStream;
+import java.io.IOException;
+import java.io.Reader;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Properties;
+import java.util.StringTokenizer;
+
+/**
+ * Perf run configuration properties.
+ * <p/>
+ * Numeric property containing ":", e.g. "10:100:5" is interpreted
+ * as array of numeric values. It is extracted once, on first use, and
+ * maintain a round number to return the appropriate value.
+ * <p/>
+ * The config property "work.dir" tells where is the root of
+ * docs data dirs and indexes dirs. It is set to either of: <ul>
+ * <li>value supplied for it in the alg file;</li>
+ * <li>otherwise, value of System property "benchmark.work.dir";</li>
+ * <li>otherwise, "work".</li>
+ * </ul>
+ */
+public class Config {
+
+  // For tests, if verbose is not turned on, don't print the props.
+  private static final String DEFAULT_PRINT_PROPS = System.getProperty("tests.verbose", "true");
+  private static final String NEW_LINE = System.getProperty("line.separator");
+
+  private int roundNumber = 0;
+  private Properties props;
+  private HashMap<String, Object> valByRound = new HashMap<String, Object>();
+  private HashMap<String, String> colForValByRound = new HashMap<String, String>();
+  private String algorithmText;
+
+  /**
+   * Read both algorithm and config properties.
+   *
+   * @param algReader from where to read algorithm and config properties.
+   * @throws IOException
+   */
+  public Config(Reader algReader) throws IOException {
+    // read alg file to array of lines
+    ArrayList<String> lines = new ArrayList<String>();
+    BufferedReader r = new BufferedReader(algReader);
+    int lastConfigLine = 0;
+    for (String line = r.readLine(); line != null; line = r.readLine()) {
+      lines.add(line);
+      if (line.indexOf('=') > 0) {
+        lastConfigLine = lines.size();
+      }
+    }
+    r.close();
+    // copy props lines to string
+    StringBuilder sb = new StringBuilder();
+    for (int i = 0; i < lastConfigLine; i++) {
+      sb.append(lines.get(i));
+      sb.append(NEW_LINE);
+    }
+    // read props from string
+    this.props = new Properties();
+    props.load(new ByteArrayInputStream(sb.toString().getBytes()));
+
+    // make sure work dir is set properly 
+    if (props.get("work.dir") == null) {
+      props.setProperty("work.dir", System.getProperty("benchmark.work.dir", "work"));
+    }
+
+    if (Boolean.valueOf(props.getProperty("print.props", DEFAULT_PRINT_PROPS)).booleanValue()) {
+      printProps();
+    }
+
+    // copy algorithm lines
+    sb = new StringBuilder();
+    for (int i = lastConfigLine; i < lines.size(); i++) {
+      sb.append(lines.get(i));
+      sb.append(NEW_LINE);
+    }
+    algorithmText = sb.toString();
+  }
+
+  /**
+   * Create config without algorithm - useful for a programmatic perf test.
+   * @param props - configuration properties.
+   */
+  public Config (Properties props) {
+    this.props = props;
+    if (Boolean.valueOf(props.getProperty("print.props",DEFAULT_PRINT_PROPS)).booleanValue()) {
+      printProps();
+    }
+  }
+
+  @SuppressWarnings({"unchecked", "rawtypes"})
+  private void printProps() {
+    System.out.println("------------> config properties:");
+    List<String> propKeys = new ArrayList(props.keySet());
+    Collections.sort(propKeys);
+    for (final String propName : propKeys) {
+      System.out.println(propName + " = " + props.getProperty(propName));
+    }
+    System.out.println("-------------------------------");
+  }
+
+  /**
+   * Return a string property.
+   *
+   * @param name name of property.
+   * @param dflt default value.
+   * @return a string property.
+   */
+  public String get(String name, String dflt) {
+    String vals[] = (String[]) valByRound.get(name);
+    if (vals != null) {
+      return vals[roundNumber % vals.length];
+    }
+    // done if not by round
+    String sval = props.getProperty(name, dflt);
+    if (sval == null) {
+      return null;
+    }
+    if (sval.indexOf(":") < 0) {
+      return sval;
+    } else if (sval.indexOf(":\\") >= 0 || sval.indexOf(":/") >= 0) {
+      // this previously messed up absolute path names on Windows. Assuming
+      // there is no real value that starts with \ or /
+      return sval;
+    }
+    // first time this prop is extracted by round
+    int k = sval.indexOf(":");
+    String colName = sval.substring(0, k);
+    sval = sval.substring(k + 1);
+    colForValByRound.put(name, colName);
+    vals = propToStringArray(sval);
+    valByRound.put(name, vals);
+    return vals[roundNumber % vals.length];
+  }
+
+  /**
+   * Set a property.
+   * Note: once a multiple values property is set, it can no longer be modified.
+   *
+   * @param name  name of property.
+   * @param value either single or multiple property value (multiple values are separated by ":")
+   * @throws Exception
+   */
+  public void set(String name, String value) throws Exception {
+    if (valByRound.get(name) != null) {
+      throw new Exception("Cannot modify a multi value property!");
+    }
+    props.setProperty(name, value);
+  }
+
+  /**
+   * Return an int property.
+   * If the property contain ":", e.g. "10:100:5", it is interpreted
+   * as array of ints. It is extracted once, on first call
+   * to get() it, and a by-round-value is returned.
+   *
+   * @param name name of property
+   * @param dflt default value
+   * @return a int property.
+   */
+  public int get(String name, int dflt) {
+    // use value by round if already parsed
+    int vals[] = (int[]) valByRound.get(name);
+    if (vals != null) {
+      return vals[roundNumber % vals.length];
+    }
+    // done if not by round 
+    String sval = props.getProperty(name, "" + dflt);
+    if (sval.indexOf(":") < 0) {
+      return Integer.parseInt(sval);
+    }
+    // first time this prop is extracted by round
+    int k = sval.indexOf(":");
+    String colName = sval.substring(0, k);
+    sval = sval.substring(k + 1);
+    colForValByRound.put(name, colName);
+    vals = propToIntArray(sval);
+    valByRound.put(name, vals);
+    return vals[roundNumber % vals.length];
+  }
+
+  /**
+   * Return a double property.
+   * If the property contain ":", e.g. "10:100:5", it is interpreted
+   * as array of doubles. It is extracted once, on first call
+   * to get() it, and a by-round-value is returned.
+   *
+   * @param name name of property
+   * @param dflt default value
+   * @return a double property.
+   */
+  public double get(String name, double dflt) {
+    // use value by round if already parsed
+    double vals[] = (double[]) valByRound.get(name);
+    if (vals != null) {
+      return vals[roundNumber % vals.length];
+    }
+    // done if not by round 
+    String sval = props.getProperty(name, "" + dflt);
+    if (sval.indexOf(":") < 0) {
+      return Double.parseDouble(sval);
+    }
+    // first time this prop is extracted by round
+    int k = sval.indexOf(":");
+    String colName = sval.substring(0, k);
+    sval = sval.substring(k + 1);
+    colForValByRound.put(name, colName);
+    vals = propToDoubleArray(sval);
+    valByRound.put(name, vals);
+    return vals[roundNumber % vals.length];
+  }
+
+  /**
+   * Return a boolean property.
+   * If the property contain ":", e.g. "true.true.false", it is interpreted
+   * as array of booleans. It is extracted once, on first call
+   * to get() it, and a by-round-value is returned.
+   *
+   * @param name name of property
+   * @param dflt default value
+   * @return a int property.
+   */
+  public boolean get(String name, boolean dflt) {
+    // use value by round if already parsed
+    boolean vals[] = (boolean[]) valByRound.get(name);
+    if (vals != null) {
+      return vals[roundNumber % vals.length];
+    }
+    // done if not by round 
+    String sval = props.getProperty(name, "" + dflt);
+    if (sval.indexOf(":") < 0) {
+      return Boolean.valueOf(sval).booleanValue();
+    }
+    // first time this prop is extracted by round 
+    int k = sval.indexOf(":");
+    String colName = sval.substring(0, k);
+    sval = sval.substring(k + 1);
+    colForValByRound.put(name, colName);
+    vals = propToBooleanArray(sval);
+    valByRound.put(name, vals);
+    return vals[roundNumber % vals.length];
+  }
+
+  /**
+   * Increment the round number, for config values that are extracted by round number.
+   *
+   * @return the new round number.
+   */
+  public int newRound() {
+    roundNumber++;
+
+    StringBuilder sb = new StringBuilder("--> Round ").append(roundNumber - 1).append("-->").append(roundNumber);
+
+    // log changes in values
+    if (valByRound.size() > 0) {
+      sb.append(": ");
+      for (final String name : valByRound.keySet()) {
+        Object a = valByRound.get(name);
+        if (a instanceof int[]) {
+          int ai[] = (int[]) a;
+          int n1 = (roundNumber - 1) % ai.length;
+          int n2 = roundNumber % ai.length;
+          sb.append("  ").append(name).append(":").append(ai[n1]).append("-->").append(ai[n2]);
+        } else if (a instanceof double[]) {
+          double ad[] = (double[]) a;
+          int n1 = (roundNumber - 1) % ad.length;
+          int n2 = roundNumber % ad.length;
+          sb.append("  ").append(name).append(":").append(ad[n1]).append("-->").append(ad[n2]);
+        } else if (a instanceof String[]) {
+          String ad[] = (String[]) a;
+          int n1 = (roundNumber - 1) % ad.length;
+          int n2 = roundNumber % ad.length;
+          sb.append("  ").append(name).append(":").append(ad[n1]).append("-->").append(ad[n2]);
+        } else {
+          boolean ab[] = (boolean[]) a;
+          int n1 = (roundNumber - 1) % ab.length;
+          int n2 = roundNumber % ab.length;
+          sb.append("  ").append(name).append(":").append(ab[n1]).append("-->").append(ab[n2]);
+        }
+      }
+    }
+
+    System.out.println();
+    System.out.println(sb.toString());
+    System.out.println();
+
+    return roundNumber;
+  }
+
+  private String[] propToStringArray(String s) {
+    if (s.indexOf(":") < 0) {
+      return new String[]{s};
+    }
+
+    ArrayList<String> a = new ArrayList<String>();
+    StringTokenizer st = new StringTokenizer(s, ":");
+    while (st.hasMoreTokens()) {
+      String t = st.nextToken();
+      a.add(t);
+    }
+    return a.toArray(new String[a.size()]);
+  }
+
+  // extract properties to array, e.g. for "10:100:5" return int[]{10,100,5}. 
+  private int[] propToIntArray(String s) {
+    if (s.indexOf(":") < 0) {
+      return new int[]{Integer.parseInt(s)};
+    }
+
+    ArrayList<Integer> a = new ArrayList<Integer>();
+    StringTokenizer st = new StringTokenizer(s, ":");
+    while (st.hasMoreTokens()) {
+      String t = st.nextToken();
+      a.add(Integer.valueOf(t));
+    }
+    int res[] = new int[a.size()];
+    for (int i = 0; i < a.size(); i++) {
+      res[i] = a.get(i).intValue();
+    }
+    return res;
+  }
+
+  // extract properties to array, e.g. for "10.7:100.4:-2.3" return int[]{10.7,100.4,-2.3}. 
+  private double[] propToDoubleArray(String s) {
+    if (s.indexOf(":") < 0) {
+      return new double[]{Double.parseDouble(s)};
+    }
+
+    ArrayList<Double> a = new ArrayList<Double>();
+    StringTokenizer st = new StringTokenizer(s, ":");
+    while (st.hasMoreTokens()) {
+      String t = st.nextToken();
+      a.add(Double.valueOf(t));
+    }
+    double res[] = new double[a.size()];
+    for (int i = 0; i < a.size(); i++) {
+      res[i] = a.get(i).doubleValue();
+    }
+    return res;
+  }
+
+  // extract properties to array, e.g. for "true:true:false" return boolean[]{true,false,false}. 
+  private boolean[] propToBooleanArray(String s) {
+    if (s.indexOf(":") < 0) {
+      return new boolean[]{Boolean.valueOf(s).booleanValue()};
+    }
+
+    ArrayList<Boolean> a = new ArrayList<Boolean>();
+    StringTokenizer st = new StringTokenizer(s, ":");
+    while (st.hasMoreTokens()) {
+      String t = st.nextToken();
+      a.add(new Boolean(t));
+    }
+    boolean res[] = new boolean[a.size()];
+    for (int i = 0; i < a.size(); i++) {
+      res[i] = a.get(i).booleanValue();
+    }
+    return res;
+  }
+
+  /**
+   * @return names of params set by round, for reports title
+   */
+  public String getColsNamesForValsByRound() {
+    if (colForValByRound.size() == 0) {
+      return "";
+    }
+    StringBuilder sb = new StringBuilder();
+    for (final String name : colForValByRound.keySet()) {
+      String colName = colForValByRound.get(name);
+      sb.append(" ").append(colName);
+    }
+    return sb.toString();
+  }
+
+  /**
+   * @return values of params set by round, for reports lines.
+   */
+  public String getColsValuesForValsByRound(int roundNum) {
+    if (colForValByRound.size() == 0) {
+      return "";
+    }
+    StringBuilder sb = new StringBuilder();
+    for (final String name : colForValByRound.keySet()) {
+      String colName = colForValByRound.get(name);
+      String template = " " + colName;
+      if (roundNum < 0) {
+        // just append blanks
+        sb.append(Format.formatPaddLeft("-", template));
+      } else {
+        // append actual values, for that round
+        Object a = valByRound.get(name);
+        if (a instanceof int[]) {
+          int ai[] = (int[]) a;
+          int n = roundNum % ai.length;
+          sb.append(Format.format(ai[n], template));
+        } else if (a instanceof double[]) {
+          double ad[] = (double[]) a;
+          int n = roundNum % ad.length;
+          sb.append(Format.format(2, ad[n], template));
+        } else if (a instanceof String[]) {
+          String ad[] = (String[]) a;
+          int n = roundNum % ad.length;
+          sb.append(ad[n]);
+        } else {
+          boolean ab[] = (boolean[]) a;
+          int n = roundNum % ab.length;
+          sb.append(Format.formatPaddLeft("" + ab[n], template));
+        }
+      }
+    }
+    return sb.toString();
+  }
+
+  /**
+   * @return the round number.
+   */
+  public int getRoundNumber() {
+    return roundNumber;
+  }
+
+  /**
+   * @return Returns the algorithmText.
+   */
+  public String getAlgorithmText() {
+    return algorithmText;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/FileUtils.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/FileUtils.java
new file mode 100644
index 0000000..1712038
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/FileUtils.java
@@ -0,0 +1,54 @@
+package org.apache.lucene.benchmark.byTask.utils;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.io.IOException;
+
+/**
+ * File utilities.
+ */
+public class FileUtils {
+
+  /**
+   * Delete files and directories, even if non-empty.
+   *
+   * @param dir file or directory
+   * @return true on success, false if no or part of files have been deleted
+   * @throws java.io.IOException
+   */
+  public static boolean fullyDelete(File dir) throws IOException {
+    if (dir == null || !dir.exists()) return false;
+    File contents[] = dir.listFiles();
+    if (contents != null) {
+      for (int i = 0; i < contents.length; i++) {
+        if (contents[i].isFile()) {
+          if (!contents[i].delete()) {
+            return false;
+          }
+        } else {
+          if (!fullyDelete(contents[i])) {
+            return false;
+          }
+        }
+      }
+    }
+    return dir.delete();
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Format.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Format.java
new file mode 100644
index 0000000..a571fb5
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Format.java
@@ -0,0 +1,110 @@
+package org.apache.lucene.benchmark.byTask.utils;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.text.NumberFormat;
+
+/**
+ * Formatting utilities (for reports).
+ */
+public class Format {
+
+  private static NumberFormat numFormat [] = { 
+    NumberFormat.getInstance(), 
+    NumberFormat.getInstance(),
+    NumberFormat.getInstance(),
+  };
+  private static final String padd = "                                                 ";
+  
+  static {
+    numFormat[0].setMaximumFractionDigits(0);
+    numFormat[0].setMinimumFractionDigits(0);
+    numFormat[1].setMaximumFractionDigits(1);
+    numFormat[1].setMinimumFractionDigits(1);
+    numFormat[2].setMaximumFractionDigits(2);
+    numFormat[2].setMinimumFractionDigits(2);
+  }
+
+  /**
+   * Padd a number from left.
+   * @param numFracDigits number of digits in fraction part - must be 0 or 1 or 2.
+   * @param f number to be formatted.
+   * @param col column name (used for deciding on length).
+   * @return formatted string.
+   */
+  public static String format(int numFracDigits, float f, String col) {
+    String res = padd + numFormat[numFracDigits].format(f);
+    return res.substring(res.length() - col.length());
+  }
+
+  public static String format(int numFracDigits, double f, String col) {
+    String res = padd + numFormat[numFracDigits].format(f);
+    return res.substring(res.length() - col.length());
+  }
+
+  /**
+   * Pad a number from right.
+   * @param numFracDigits number of digits in fraction part - must be 0 or 1 or 2.
+   * @param f number to be formatted.
+   * @param col column name (used for deciding on length).
+   * @return formatted string.
+   */
+  public static String formatPaddRight(int numFracDigits, float f, String col) {
+    String res = numFormat[numFracDigits].format(f) + padd;
+    return res.substring(0, col.length());
+  }
+
+  public static String formatPaddRight(int numFracDigits, double f, String col) {
+    String res = numFormat[numFracDigits].format(f) + padd;
+    return res.substring(0, col.length());
+  }
+
+  /**
+   * Pad a number from left.
+   * @param n number to be formatted.
+   * @param col column name (used for deciding on length).
+   * @return formatted string.
+   */
+  public static String format(int n, String col) {
+    String res = padd + n;
+    return res.substring(res.length() - col.length());
+  }
+
+  /**
+   * Pad a string from right.
+   * @param s string to be formatted.
+   * @param col column name (used for deciding on length).
+   * @return formatted string.
+   */
+  public static String format(String s, String col) {
+    String s1 = (s + padd);
+    return s1.substring(0, Math.min(col.length(), s1.length()));
+  }
+
+  /**
+   * Pad a string from left.
+   * @param s string to be formatted.
+   * @param col column name (used for deciding on length).
+   * @return formatted string.
+   */
+  public static String formatPaddLeft(String s, String col) {
+    String res = padd + s;
+    return res.substring(res.length() - col.length());
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/StringBuilderReader.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/StringBuilderReader.java
new file mode 100644
index 0000000..c6e9510
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/StringBuilderReader.java
@@ -0,0 +1,179 @@
+package org.apache.lucene.benchmark.byTask.utils;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.Reader;
+
+/**
+ * Implements a {@link Reader} over a {@link StringBuilder} instance. Although
+ * one can use {@link java.io.StringReader} by passing it
+ * {@link StringBuilder#toString()}, it is better to use this class, as it
+ * doesn't mark the passed-in {@link StringBuilder} as shared (which will cause
+ * inner char[] allocations at the next append() attempt).<br>
+ * Notes:
+ * <ul>
+ * <li>This implementation assumes the underlying {@link StringBuilder} is not
+ * changed during the use of this {@link Reader} implementation.
+ * <li>This implementation is thread-safe.
+ * <li>The implementation looks very much like {@link java.io.StringReader} (for
+ * the right reasons).
+ * <li>If one wants to reuse that instance, then the following needs to be done:
+ * <pre>
+ * StringBuilder sb = new StringBuilder("some text");
+ * Reader reader = new StringBuilderReader(sb);
+ * ... read from reader - don't close it ! ...
+ * sb.setLength(0);
+ * sb.append("some new text");
+ * reader.reset();
+ * ... read the new string from the reader ...
+ * </pre>
+ * </ul>
+ */
+public class StringBuilderReader extends Reader {
+  
+  // The StringBuilder to read from.
+  private StringBuilder sb;
+
+  // The length of 'sb'.
+  private int length;
+
+  // The next position to read from the StringBuilder.
+  private int next = 0;
+
+  // The mark position. The default value 0 means the start of the text.
+  private int mark = 0;
+
+  public StringBuilderReader(StringBuilder sb) {
+    set(sb);
+  }
+
+  /** Check to make sure that the stream has not been closed. */
+  private void ensureOpen() throws IOException {
+    if (sb == null) {
+      throw new IOException("Stream has already been closed");
+    }
+  }
+
+  @Override
+  public void close() {
+    synchronized (lock) {
+      sb = null;
+    }
+  }
+
+  /**
+   * Mark the present position in the stream. Subsequent calls to reset() will
+   * reposition the stream to this point.
+   * 
+   * @param readAheadLimit Limit on the number of characters that may be read
+   *        while still preserving the mark. Because the stream's input comes
+   *        from a StringBuilder, there is no actual limit, so this argument 
+   *        must not be negative, but is otherwise ignored.
+   * @exception IllegalArgumentException If readAheadLimit is < 0
+   * @exception IOException If an I/O error occurs
+   */
+  @Override
+  public void mark(int readAheadLimit) throws IOException {
+    if (readAheadLimit < 0){
+      throw new IllegalArgumentException("Read-ahead limit cannpt be negative: " + readAheadLimit);
+    }
+    synchronized (lock) {
+      ensureOpen();
+      mark = next;
+    }
+  }
+
+  @Override
+  public boolean markSupported() {
+    return true;
+  }
+
+  @Override
+  public int read() throws IOException {
+    synchronized (lock) {
+      ensureOpen();
+      return next >= length ? -1 : sb.charAt(next++);
+    }
+  }
+
+  @Override
+  public int read(char cbuf[], int off, int len) throws IOException {
+    synchronized (lock) {
+      ensureOpen();
+
+      // Validate parameters
+      if (off < 0 || off > cbuf.length || len < 0 || off + len > cbuf.length) {
+        throw new IndexOutOfBoundsException("off=" + off + " len=" + len + " cbuf.length=" + cbuf.length);
+      }
+
+      if (len == 0) {
+        return 0;
+      }
+
+      if (next >= length) {
+        return -1;
+      }
+
+      int n = Math.min(length - next, len);
+      sb.getChars(next, next + n, cbuf, off);
+      next += n;
+      return n;
+    }
+  }
+
+  @Override
+  public boolean ready() throws IOException {
+    synchronized (lock) {
+      ensureOpen();
+      return true;
+    }
+  }
+
+  @Override
+  public void reset() throws IOException {
+    synchronized (lock) {
+      ensureOpen();
+      next = mark;
+      length = sb.length();
+    }
+  }
+
+  public void set(StringBuilder sb) {
+    synchronized (lock) {
+      this.sb = sb;
+      length = sb.length();
+    }
+  }
+  @Override
+  public long skip(long ns) throws IOException {
+    synchronized (lock) {
+      ensureOpen();
+      if (next >= length) {
+        return 0;
+      }
+
+      // Bound skip by beginning and end of the source
+      long n = Math.min(length - next, ns);
+      n = Math.max(-next, n);
+      next += n;
+      return n;
+    }
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/package.html b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/package.html
new file mode 100644
index 0000000..6a71c2f
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/package.html
@@ -0,0 +1,22 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<body>
+Utilities used for the benchmark, and for the reports.
+</body>
+</html>
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/package.html b/modules/benchmark/src/java/org/apache/lucene/benchmark/package.html
new file mode 100644
index 0000000..dc28bc8
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/package.html
@@ -0,0 +1,45 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<HTML>
+<HEAD>
+    <TITLE>Lucene Benchmarking Package</TITLE>
+</HEAD>
+<BODY>
+<DIV>
+    <p/>
+    The benchmark contribution contains tools for benchmarking Lucene using standard, freely available corpora. ANT will
+    download the corpus automatically, place it in a temp directory and then unpack it to the working.dir directory specified in the build.
+    The temp directory
+    and working directory can be safely removed after a run. However, the next time the task is run, it will need to download the files again.
+<p/>
+    Classes implementing the Benchmarker interface should have a no-argument constructor if they are to be used with the Driver class. The Driver
+    class is provided for convenience only. Feel free to implement your own main class for your benchmarker.
+<p/>
+    The StandardBenchmarker is meant to be just that, a standard that runs out of the box with no configuration or changes needed.
+    Other benchmarking classes may derive from it to provide alternate views or to take in command line options. When reporting benchmarking runs
+    you should state any alterations you have made.
+    <p/>
+    To run the short version of the StandardBenchmarker, call "ant run-micro-standard". This should take a minute or so to complete and give you a preliminary idea of how your change affects the code
+    <p/>
+    To run the long version of the StandardBenchmarker, call "ant run-standard". This takes considerably longer.
+    <p/>
+    The original code for these classes was donated by Andrzej Bialecki at http://issues.apache.org/jira/browse/LUCENE-675 and has been updated by Grant Ingersoll to make some parts of the code reusable in other benchmarkers
+</DIV>
+<DIV>&nbsp;</DIV>
+</BODY>
+</HTML>
\ No newline at end of file
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/Judge.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/Judge.java
new file mode 100755
index 0000000..78e5718
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/Judge.java
@@ -0,0 +1,53 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.quality;
+
+import java.io.PrintWriter;
+
+/**
+ * Judge if a document is relevant for a quality query.
+ */
+public interface Judge {
+
+  /**
+   * Judge if document <code>docName</code> is relevant for the given quality query.
+   * @param docName name of doc tested for relevancy.
+   * @param query tested quality query. 
+   * @return true if relevant, false if not.
+   */
+  public boolean isRelevant(String docName, QualityQuery query);
+
+  /**
+   * Validate that queries and this Judge match each other.
+   * To be perfectly valid, this Judge must have some data for each and every 
+   * input quality query, and must not have any data on any other quality query.  
+   * <b>Note</b>: the quality benchmark run would not fail in case of imperfect
+   * validity, just a warning message would be logged.  
+   * @param qq quality queries to be validated.
+   * @param logger if not null, validation issues are logged.
+   * @return true if perfectly valid, false if not.
+   */
+  public boolean validateData (QualityQuery qq[], PrintWriter logger);
+  
+  /**
+   * Return the maximal recall for the input quality query. 
+   * It is the number of relevant docs this Judge "knows" for the query. 
+   * @param query the query whose maximal recall is needed.
+   */
+  public int maxRecall (QualityQuery query);
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityBenchmark.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityBenchmark.java
new file mode 100644
index 0000000..73a76b5
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityBenchmark.java
@@ -0,0 +1,161 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.quality;
+
+import java.io.IOException;
+import java.io.PrintWriter;
+
+import org.apache.lucene.benchmark.quality.utils.DocNameExtractor;
+import org.apache.lucene.benchmark.quality.utils.SubmissionReport;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Searcher;
+import org.apache.lucene.search.TopDocs;
+
+/**
+ * Main entry point for running a quality benchmark.
+ * <p>
+ * There are two main configurations for running a quality benchmark: <ul>
+ * <li>Against existing judgements.</li>
+ * <li>For submission (e.g. for a contest).</li>
+ * </ul>
+ * The first configuration requires a non null
+ * {@link org.apache.lucene.benchmark.quality.Judge Judge}. 
+ * The second configuration requires a non null 
+ * {@link org.apache.lucene.benchmark.quality.utils.SubmissionReport SubmissionLogger}.
+ */
+public class QualityBenchmark {
+
+  /** Quality Queries that this quality benchmark would execute. */
+  protected QualityQuery qualityQueries[];
+  
+  /** Parser for turning QualityQueries into Lucene Queries. */
+  protected QualityQueryParser qqParser;
+  
+  /** Index to be searched. */
+  protected Searcher searcher;
+
+  /** index field to extract doc name for each search result; used for judging the results. */  
+  protected String docNameField;
+  
+  /** maximal number of queries that this quality benchmark runs. Default: maxint. Useful for debugging. */
+  private int maxQueries = Integer.MAX_VALUE;
+  
+  /** maximal number of results to collect for each query. Default: 1000. */
+  private int maxResults = 1000;
+
+  /**
+   * Create a QualityBenchmark.
+   * @param qqs quality queries to run.
+   * @param qqParser parser for turning QualityQueries into Lucene Queries. 
+   * @param searcher index to be searched.
+   * @param docNameField name of field containing the document name.
+   *        This allows to extract the doc name for search results,
+   *        and is important for judging the results.  
+   */
+  public QualityBenchmark(QualityQuery qqs[], QualityQueryParser qqParser, 
+      Searcher searcher, String docNameField) {
+    this.qualityQueries = qqs;
+    this.qqParser = qqParser;
+    this.searcher = searcher;
+    this.docNameField = docNameField;
+  }
+
+  /**
+   * Run the quality benchmark.
+   * @param judge the judge that can tell if a certain result doc is relevant for a certain quality query. 
+   *        If null, no judgements would be made. Usually null for a submission run. 
+   * @param submitRep submission report is created if non null.
+   * @param qualityLog If not null, quality run data would be printed for each query.
+   * @return QualityStats of each quality query that was executed.
+   * @throws Exception if quality benchmark failed to run.
+   */
+  public  QualityStats [] execute(Judge judge, SubmissionReport submitRep, 
+                                  PrintWriter qualityLog) throws Exception {
+    int nQueries = Math.min(maxQueries, qualityQueries.length);
+    QualityStats stats[] = new QualityStats[nQueries]; 
+    for (int i=0; i<nQueries; i++) {
+      QualityQuery qq = qualityQueries[i];
+      // generate query
+      Query q = qqParser.parse(qq);
+      // search with this query 
+      long t1 = System.currentTimeMillis();
+      TopDocs td = searcher.search(q,null,maxResults);
+      long searchTime = System.currentTimeMillis()-t1;
+      //most likely we either submit or judge, but check both 
+      if (judge!=null) {
+        stats[i] = analyzeQueryResults(qq, q, td, judge, qualityLog, searchTime);
+      }
+      if (submitRep!=null) {
+        submitRep.report(qq,td,docNameField,searcher);
+      }
+    } 
+    if (submitRep!=null) {
+      submitRep.flush();
+    }
+    return stats;
+  }
+  
+  /* Analyze/judge results for a single quality query; optionally log them. */  
+  private QualityStats analyzeQueryResults(QualityQuery qq, Query q, TopDocs td, Judge judge, PrintWriter logger, long searchTime) throws IOException {
+    QualityStats stts = new QualityStats(judge.maxRecall(qq),searchTime);
+    ScoreDoc sd[] = td.scoreDocs;
+    long t1 = System.currentTimeMillis(); // extraction of first doc name we measure also construction of doc name extractor, just in case.
+    DocNameExtractor xt = new DocNameExtractor(docNameField);
+    for (int i=0; i<sd.length; i++) {
+      String docName = xt.docName(searcher,sd[i].doc);
+      long docNameExtractTime = System.currentTimeMillis() - t1;
+      t1 = System.currentTimeMillis();
+      boolean isRelevant = judge.isRelevant(docName,qq);
+      stts.addResult(i+1,isRelevant, docNameExtractTime);
+    }
+    if (logger!=null) {
+      logger.println(qq.getQueryID()+"  -  "+q);
+      stts.log(qq.getQueryID()+" Stats:",1,logger,"  ");
+    }
+    return stts;
+  }
+
+  /**
+   * @return the maximum number of quality queries to run. Useful at debugging.
+   */
+  public int getMaxQueries() {
+    return maxQueries;
+  }
+
+  /**
+   * Set the maximum number of quality queries to run. Useful at debugging.
+   */
+  public void setMaxQueries(int maxQueries) {
+    this.maxQueries = maxQueries;
+  }
+
+  /**
+   * @return the maximum number of results to collect for each quality query.
+   */
+  public int getMaxResults() {
+    return maxResults;
+  }
+
+  /**
+   * set the maximum number of results to collect for each quality query.
+   */
+  public void setMaxResults(int maxResults) {
+    this.maxResults = maxResults;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQuery.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQuery.java
new file mode 100755
index 0000000..deb5005
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQuery.java
@@ -0,0 +1,86 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.quality;
+
+import java.util.Map;
+
+/**
+ * A QualityQuery has an ID and some name-value pairs.
+ * <p> 
+ * The ID allows to map the quality query with its judgements.
+ * <p>
+ * The name-value pairs are used by a 
+ * {@link org.apache.lucene.benchmark.quality.QualityQueryParser}
+ * to create a Lucene {@link org.apache.lucene.search.Query}.
+ * <p>
+ * It is very likely that name-value-pairs would be mapped into fields in a Lucene query,
+ * but it is up to the QualityQueryParser how to map - e.g. all values in a single field, 
+ * or each pair as its own field, etc., - and this of course must match the way the 
+ * searched index was constructed.
+ */
+public class QualityQuery implements Comparable<QualityQuery> {
+  private String queryID;
+  private Map<String,String> nameValPairs;
+
+  /**
+   * Create a QualityQuery with given ID and name-value pairs.
+   * @param queryID ID of this quality query.
+   * @param nameValPairs the contents of this quality query.
+   */
+  public QualityQuery(String queryID, Map<String,String> nameValPairs) {
+    this.queryID = queryID;
+    this.nameValPairs = nameValPairs;
+  }
+  
+  /**
+   * Return all the names of name-value-pairs in this QualityQuery.
+   */
+  public String[] getNames() {
+    return nameValPairs.keySet().toArray(new String[0]);
+  }
+
+  /**
+   * Return the value of a certain name-value pair.
+   * @param name the name whose value should be returned. 
+   */
+  public String getValue(String name) {
+    return nameValPairs.get(name);
+  }
+
+  /**
+   * Return the ID of this query.
+   * The ID allows to map the quality query with its judgements.
+   */
+  public String getQueryID() {
+    return queryID;
+  }
+
+  /* for a nicer sort of input queries before running them.
+   * Try first as ints, fall back to string if not int. */ 
+  public int compareTo(QualityQuery other) {
+    try {
+      // compare as ints when ids ints
+      int n = Integer.parseInt(queryID);
+      int nOther = Integer.parseInt(other.queryID);
+      return n - nOther;
+    } catch (NumberFormatException e) {
+      // fall back to string comparison
+      return queryID.compareTo(other.queryID);
+    }
+  }
+  
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQueryParser.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQueryParser.java
new file mode 100755
index 0000000..66bd275
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQueryParser.java
@@ -0,0 +1,34 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.quality;
+
+import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.search.Query;
+
+/**
+ * Parse a QualityQuery into a Lucene query.
+ */
+public interface QualityQueryParser {
+
+  /**
+   * Parse a given QualityQuery into a Lucene query.
+   * @param qq the quality query to be parsed.
+   * @throws ParseException if parsing failed.
+   */
+  public Query parse(QualityQuery qq) throws ParseException;
+  
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityStats.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityStats.java
new file mode 100644
index 0000000..ccf606f
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityStats.java
@@ -0,0 +1,294 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.quality;
+
+import java.io.PrintWriter;
+import java.text.NumberFormat;
+import java.util.ArrayList;
+
+/**
+ * Results of quality benchmark run for a single query or for a set of queries.
+ */
+public class QualityStats {
+
+  /** Number of points for which precision is computed. */
+  public static final int MAX_POINTS = 20;
+  
+  private double maxGoodPoints;
+  private double recall;
+  private double pAt[];
+  private double pReleventSum = 0;
+  private double numPoints = 0;
+  private double numGoodPoints = 0;
+  private double mrr = 0;
+  private long searchTime;
+  private long docNamesExtractTime;
+
+  /**
+   * A certain rank in which a relevant doc was found.
+   */
+  public static class RecallPoint {
+    private int rank;
+    private double recall;
+    private RecallPoint(int rank, double recall) {
+      this.rank = rank;
+      this.recall = recall;
+    }
+    /** Returns the rank: where on the list of returned docs this relevant doc appeared. */
+    public int getRank() {
+      return rank;
+    }
+    /** Returns the recall: how many relevant docs were returned up to this point, inclusive. */
+    public double getRecall() {
+      return recall;
+    }
+  }
+  
+  private ArrayList<RecallPoint> recallPoints;
+  
+  /**
+   * Construct a QualityStats object with anticipated maximal number of relevant hits. 
+   * @param maxGoodPoints maximal possible relevant hits.
+   */
+  public QualityStats(double maxGoodPoints, long searchTime) {
+    this.maxGoodPoints = maxGoodPoints;
+    this.searchTime = searchTime;
+    this.recallPoints = new ArrayList<RecallPoint>();
+    pAt = new double[MAX_POINTS+1]; // pAt[0] unused. 
+  }
+
+  /**
+   * Add a (possibly relevant) doc.
+   * @param n rank of the added doc (its ordinal position within the query results).
+   * @param isRelevant true if the added doc is relevant, false otherwise.
+   */
+  public void addResult(int n, boolean isRelevant, long docNameExtractTime) {
+    if (Math.abs(numPoints+1 - n) > 1E-6) {
+      throw new IllegalArgumentException("point "+n+" illegal after "+numPoints+" points!");
+    }
+    if (isRelevant) {
+      numGoodPoints+=1;
+      recallPoints.add(new RecallPoint(n,numGoodPoints));
+      if (recallPoints.size()==1 && n<=5) { // first point, but only within 5 top scores. 
+        mrr =  1.0 / n;
+      }
+    }
+    numPoints = n;
+    double p = numGoodPoints / numPoints;
+    if (isRelevant) {
+      pReleventSum += p;
+    }
+    if (n<pAt.length) {
+      pAt[n] = p;
+    }
+    recall = maxGoodPoints<=0 ? p : numGoodPoints/maxGoodPoints;
+    docNamesExtractTime += docNameExtractTime;
+  }
+
+  /**
+   * Return the precision at rank n:
+   * |{relevant hits within first <code>n</code> hits}| / <code>n</code>.
+   * @param n requested precision point, must be at least 1 and at most {@link #MAX_POINTS}. 
+   */
+  public double getPrecisionAt(int n) {
+    if (n<1 || n>MAX_POINTS) {
+      throw new IllegalArgumentException("n="+n+" - but it must be in [1,"+MAX_POINTS+"] range!"); 
+    }
+    if (n>numPoints) {
+      return (numPoints * pAt[(int)numPoints])/n;
+    }
+    return pAt[n];
+  }
+
+  /**
+   * Return the average precision at recall points.
+   */
+  public double getAvp() {
+    return maxGoodPoints==0 ? 0 : pReleventSum/maxGoodPoints;
+  }
+  
+  /**
+   * Return the recall: |{relevant hits found}| / |{relevant hits existing}|.
+   */
+  public double getRecall() {
+    return recall;
+  }
+
+  /**
+   * Log information on this QualityStats object.
+   * @param logger Logger.
+   * @param prefix prefix before each log line.
+   */
+  public void log(String title, int paddLines, PrintWriter logger, String prefix) {
+    for (int i=0; i<paddLines; i++) {  
+      logger.println();
+    }
+    if (title!=null && title.trim().length()>0) {
+      logger.println(title);
+    }
+    prefix = prefix==null ? "" : prefix;
+    NumberFormat nf = NumberFormat.getInstance();
+    nf.setMaximumFractionDigits(3);
+    nf.setMinimumFractionDigits(3);
+    nf.setGroupingUsed(true);
+    int M = 19;
+    logger.println(prefix+format("Search Seconds: ",M)+
+        fracFormat(nf.format((double)searchTime/1000)));
+    logger.println(prefix+format("DocName Seconds: ",M)+
+        fracFormat(nf.format((double)docNamesExtractTime/1000)));
+    logger.println(prefix+format("Num Points: ",M)+
+        fracFormat(nf.format(numPoints)));
+    logger.println(prefix+format("Num Good Points: ",M)+
+        fracFormat(nf.format(numGoodPoints)));
+    logger.println(prefix+format("Max Good Points: ",M)+
+        fracFormat(nf.format(maxGoodPoints)));
+    logger.println(prefix+format("Average Precision: ",M)+
+        fracFormat(nf.format(getAvp())));
+    logger.println(prefix+format("MRR: ",M)+
+        fracFormat(nf.format(getMRR())));
+    logger.println(prefix+format("Recall: ",M)+
+        fracFormat(nf.format(getRecall())));
+    for (int i=1; i<(int)numPoints && i<pAt.length; i++) {
+      logger.println(prefix+format("Precision At "+i+": ",M)+
+          fracFormat(nf.format(getPrecisionAt(i))));
+    }
+    for (int i=0; i<paddLines; i++) {  
+      logger.println();
+    }
+  }
+
+  private static String padd = "                                    ";
+  private String format(String s, int minLen) {
+    s = (s==null ? "" : s);
+    int n = Math.max(minLen,s.length());
+    return (s+padd).substring(0,n);
+  }
+  private String fracFormat(String frac) {
+    int k = frac.indexOf('.');
+    String s1 = padd+frac.substring(0,k);
+    int n = Math.max(k,6);
+    s1 = s1.substring(s1.length()-n);
+    return s1 + frac.substring(k);
+  }
+  
+  /**
+   * Create a QualityStats object that is the average of the input QualityStats objects. 
+   * @param stats array of input stats to be averaged.
+   * @return an average over the input stats.
+   */
+  public static QualityStats average(QualityStats[] stats) {
+    QualityStats avg = new QualityStats(0,0);
+    if (stats.length==0) {
+      // weired, no stats to average!
+      return avg;
+    }
+    int m = 0; // queries with positive judgements
+    // aggregate
+    for (int i=0; i<stats.length; i++) {
+      avg.searchTime += stats[i].searchTime;
+      avg.docNamesExtractTime += stats[i].docNamesExtractTime;
+      if (stats[i].maxGoodPoints>0) {
+        m++;
+        avg.numGoodPoints += stats[i].numGoodPoints;
+        avg.numPoints += stats[i].numPoints;
+        avg.pReleventSum += stats[i].getAvp();
+        avg.recall += stats[i].recall;
+        avg.mrr += stats[i].getMRR();
+        avg.maxGoodPoints += stats[i].maxGoodPoints;
+        for (int j=1; j<avg.pAt.length; j++) {
+          avg.pAt[j] += stats[i].getPrecisionAt(j);
+        }
+      }
+    }
+    assert m>0 : "Fishy: no \"good\" queries!";
+    // take average: times go by all queries, other measures go by "good" queries only.
+    avg.searchTime /= stats.length;
+    avg.docNamesExtractTime /= stats.length;
+    avg.numGoodPoints /= m;
+    avg.numPoints /= m;
+    avg.recall /= m;
+    avg.mrr /= m;
+    avg.maxGoodPoints /= m;
+    for (int j=1; j<avg.pAt.length; j++) {
+      avg.pAt[j] /= m;
+    }
+    avg.pReleventSum /= m;                 // this is actually avgp now 
+    avg.pReleventSum *= avg.maxGoodPoints; // so that getAvgP() would be correct
+    
+    return avg;
+  }
+
+  /**
+   * Returns the time it took to extract doc names for judging the measured query, in milliseconds.
+   */
+  public long getDocNamesExtractTime() {
+    return docNamesExtractTime;
+  }
+
+  /**
+   * Returns the maximal number of good points.
+   * This is the number of relevant docs known by the judge for the measured query.
+   */
+  public double getMaxGoodPoints() {
+    return maxGoodPoints;
+  }
+
+  /**
+   * Returns the number of good points (only relevant points).
+   */
+  public double getNumGoodPoints() {
+    return numGoodPoints;
+  }
+
+  /**
+   * Returns the number of points (both relevant and irrelevant points).
+   */
+  public double getNumPoints() {
+    return numPoints;
+  }
+
+  /**
+   * Returns the recallPoints.
+   */
+  public RecallPoint [] getRecallPoints() {
+    return recallPoints.toArray(new RecallPoint[0]);
+  }
+
+  /**
+   * Returns the Mean reciprocal rank over the queries or RR for a single query.
+   * <p>
+   * Reciprocal rank is defined as <code>1/r</code> where <code>r</code> is the 
+   * rank of the first correct result, or <code>0</code> if there are no correct 
+   * results within the top 5 results. 
+   * <p>
+   * This follows the definition in 
+   * <a href="http://www.cnlp.org/publications/02cnlptrec10.pdf"> 
+   * Question Answering - CNLP at the TREC-10 Question Answering Track</a>.
+   */
+  public double getMRR() {
+    return mrr;
+  }
+
+  
+  /**
+   * Returns the search time in milliseconds for the measured query.
+   */
+  public long getSearchTime() {
+    return searchTime;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/package.html b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/package.html
new file mode 100755
index 0000000..be2622c
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/package.html
@@ -0,0 +1,82 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<body>
+<h2>Search Quality Benchmarking.</h2>
+<p>
+This package allows to benchmark search quality of a Lucene application.
+<p>
+In order to use this package you should provide:
+<ul>
+  <li>A <a href="../../search/Searcher.html">searcher</a>.</li>
+  <li><a href="QualityQuery.html">Quality queries</a>.</li>
+  <li><a href="Judge.html">Judging object</a>.</li>
+  <li><a href="utils/SubmissionReport.html">Reporting object</a>.</li>
+</ul>
+<p>
+For benchmarking TREC collections with TREC QRels, take a look at the 
+<a href="trec/package-summary.html">trec package</a>.
+<p>
+Here is a sample code used to run the TREC 2006 queries 701-850 on the .Gov2 collection:
+
+<pre>
+    File topicsFile = new File("topics-701-850.txt");
+    File qrelsFile = new File("qrels-701-850.txt");
+    Searcher searcher = new IndexSearcher("index");
+
+    int maxResults = 1000;
+    String docNameField = "docname"; 
+    
+    PrintWriter logger = new PrintWriter(System.out,true); 
+
+    // use trec utilities to read trec topics into quality queries
+    TrecTopicsReader qReader = new TrecTopicsReader();
+    QualityQuery qqs[] = qReader.readQueries(new BufferedReader(new FileReader(topicsFile)));
+    
+    // prepare judge, with trec utilities that read from a QRels file
+    Judge judge = new TrecJudge(new BufferedReader(new FileReader(qrelsFile)));
+    
+    // validate topics & judgments match each other
+    judge.validateData(qqs, logger);
+    
+    // set the parsing of quality queries into Lucene queries.
+    QualityQueryParser qqParser = new SimpleQQParser("title", "body");
+    
+    // run the benchmark
+    QualityBenchmark qrun = new QualityBenchmark(qqs, qqParser, searcher, docNameField);
+    SubmissionReport submitLog = null;
+    QualityStats stats[] = qrun.execute(maxResults, judge, submitLog, logger);
+    
+    // print an avarage sum of the results
+    QualityStats avg = QualityStats.average(stats);
+    avg.log("SUMMARY",2,logger, "  ");
+</pre>
+
+<p>
+Some immediate ways to modify this program to your needs are:
+<ul>
+  <li>To run on different formats of queries and judgements provide your own 
+      <a href="Judge.html">Judge</a> and 
+      <a href="QualityQuery.html">Quality queries</a>.</li>
+  <li>Create sophisticated Lucene queries by supplying a different 
+  <a href="QualityQueryParser.html">Quality query parser</a>.</li>
+</ul>
+
+</body>
+
+</html>
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/QueryDriver.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/QueryDriver.java
new file mode 100644
index 0000000..5f92f08
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/QueryDriver.java
@@ -0,0 +1,93 @@
+package org.apache.lucene.benchmark.quality.trec;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.quality.trec.TrecJudge;
+import org.apache.lucene.benchmark.quality.trec.TrecTopicsReader;
+import org.apache.lucene.benchmark.quality.utils.SimpleQQParser;
+import org.apache.lucene.benchmark.quality.utils.SubmissionReport;
+import org.apache.lucene.benchmark.quality.*;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Searcher;
+import org.apache.lucene.store.FSDirectory;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileReader;
+import java.io.PrintWriter;
+import java.util.HashSet;
+import java.util.Set;
+
+
+/**
+ *
+ *
+ **/
+public class QueryDriver {
+  public static void main(String[] args) throws Exception {
+    if (args.length < 4 || args.length > 5) {
+      System.err.println("Usage: QueryDriver <topicsFile> <qrelsFile> <submissionFile> <indexDir> [querySpec]");
+      System.err.println("topicsFile: input file containing queries");
+      System.err.println("qrelsFile: input file containing relevance judgements");
+      System.err.println("submissionFile: output submission file for trec_eval");
+      System.err.println("indexDir: index directory");
+      System.err.println("querySpec: string composed of fields to use in query consisting of T=title,D=description,N=narrative:");
+      System.err.println("\texample: TD (query on Title + Description). The default is T (title only)");
+      System.exit(1);
+    }
+    
+    File topicsFile = new File(args[0]);
+    File qrelsFile = new File(args[1]);
+    SubmissionReport submitLog = new SubmissionReport(new PrintWriter(args[2]), "lucene");
+    FSDirectory dir = FSDirectory.open(new File(args[3]));
+    String fieldSpec = args.length == 5 ? args[4] : "T"; // default to Title-only if not specified.
+    Searcher searcher = new IndexSearcher(dir, true);
+
+    int maxResults = 1000;
+    String docNameField = "docname";
+
+    PrintWriter logger = new PrintWriter(System.out, true);
+
+    // use trec utilities to read trec topics into quality queries
+    TrecTopicsReader qReader = new TrecTopicsReader();
+    QualityQuery qqs[] = qReader.readQueries(new BufferedReader(new FileReader(topicsFile)));
+
+    // prepare judge, with trec utilities that read from a QRels file
+    Judge judge = new TrecJudge(new BufferedReader(new FileReader(qrelsFile)));
+
+    // validate topics & judgments match each other
+    judge.validateData(qqs, logger);
+
+    Set<String> fieldSet = new HashSet<String>();
+    if (fieldSpec.indexOf('T') >= 0) fieldSet.add("title");
+    if (fieldSpec.indexOf('D') >= 0) fieldSet.add("description");
+    if (fieldSpec.indexOf('N') >= 0) fieldSet.add("narrative");
+    
+    // set the parsing of quality queries into Lucene queries.
+    QualityQueryParser qqParser = new SimpleQQParser(fieldSet.toArray(new String[0]), "body");
+
+    // run the benchmark
+    QualityBenchmark qrun = new QualityBenchmark(qqs, qqParser, searcher, docNameField);
+    qrun.setMaxResults(maxResults);
+    QualityStats stats[] = qrun.execute(judge, submitLog, logger);
+
+    // print an avarage sum of the results
+    QualityStats avg = QualityStats.average(stats);
+    avg.log("SUMMARY", 2, logger, "  ");
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/Trec1MQReader.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/Trec1MQReader.java
new file mode 100755
index 0000000..02dddd3
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/Trec1MQReader.java
@@ -0,0 +1,87 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.quality.trec;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+
+import org.apache.lucene.benchmark.quality.QualityQuery;
+
+/**
+ * Read topics of TREC 1MQ track.
+ * <p>
+ * Expects this topic format -
+ * <pre>
+ *   qnum:qtext
+ * </pre>
+ * Comment lines starting with '#' are ignored.
+ * <p>
+ * All topics will have a single name value pair.
+ */
+public class Trec1MQReader {
+
+  private String name;
+  
+  /**
+   *  Constructor for Trec's 1MQ TopicsReader
+   *  @param name name of name-value pair to set for all queries.
+   */
+  public Trec1MQReader(String name) {
+    super();
+    this.name = name;
+  }
+
+  /**
+   * Read quality queries from trec 1MQ format topics file.
+   * @param reader where queries are read from.
+   * @return the result quality queries.
+   * @throws IOException if cannot read the queries.
+   */
+  public QualityQuery[] readQueries(BufferedReader reader) throws IOException {
+    ArrayList<QualityQuery> res = new ArrayList<QualityQuery>();
+    String line;
+    try {
+      while (null!=(line=reader.readLine())) {
+        line = line.trim();
+        if (line.startsWith("#")) {
+          continue;
+        }
+        // id
+        int k = line.indexOf(":");
+        String id = line.substring(0,k).trim();
+        // qtext
+        String qtext = line.substring(k+1).trim();
+        // we got a topic!
+        HashMap<String,String> fields = new HashMap<String,String>();
+        fields.put(name,qtext);
+        //System.out.println("id: "+id+" qtext: "+qtext+"  line: "+line);
+        QualityQuery topic = new QualityQuery(id,fields);
+        res.add(topic);
+      }
+    } finally {
+      reader.close();
+    }
+    // sort result array (by ID) 
+    QualityQuery qq[] = res.toArray(new QualityQuery[0]);
+    Arrays.sort(qq);
+    return qq;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecJudge.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecJudge.java
new file mode 100644
index 0000000..3ca3877
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecJudge.java
@@ -0,0 +1,156 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.quality.trec;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.PrintWriter;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.StringTokenizer;
+
+import org.apache.lucene.benchmark.quality.Judge;
+import org.apache.lucene.benchmark.quality.QualityQuery;
+
+/**
+ * Judge if given document is relevant to given quality query, based on Trec format for judgements.
+ */
+public class TrecJudge implements Judge {
+
+  HashMap<String,QRelJudgement> judgements;
+  
+  /**
+   * Constructor from a reader.
+   * <p>
+   * Expected input format:
+   * <pre>
+   *     qnum  0   doc-name     is-relevant
+   * </pre> 
+   * Two sample lines:
+   * <pre> 
+   *     19    0   doc303       1
+   *     19    0   doc7295      0
+   * </pre> 
+   * @param reader where judgments are read from.
+   * @throws IOException 
+   */
+  public TrecJudge (BufferedReader reader) throws IOException {
+    judgements = new HashMap<String,QRelJudgement>();
+    QRelJudgement curr = null;
+    String zero = "0";
+    String line;
+    
+    try {
+      while (null!=(line=reader.readLine())) {
+        line = line.trim();
+        if (line.length()==0 || '#'==line.charAt(0)) {
+          continue;
+        }
+        StringTokenizer st = new StringTokenizer(line);
+        String queryID = st.nextToken();
+        st.nextToken();
+        String docName = st.nextToken();
+        boolean relevant = !zero.equals(st.nextToken());
+        assert !st.hasMoreTokens() : "wrong format: "+line+"  next: "+st.nextToken();
+        if (relevant) { // only keep relevant docs
+          if (curr==null || !curr.queryID.equals(queryID)) {
+            curr = judgements.get(queryID);
+            if (curr==null) {
+              curr = new QRelJudgement(queryID);
+              judgements.put(queryID,curr);
+            }
+          }
+          curr.addRelevandDoc(docName);
+        }
+      }
+    } finally {
+      reader.close();
+    }
+  }
+  
+  // inherit javadocs
+  public boolean isRelevant(String docName, QualityQuery query) {
+    QRelJudgement qrj = judgements.get(query.getQueryID());
+    return qrj!=null && qrj.isRelevant(docName);
+  }
+
+  /** single Judgement of a trec quality query */
+  private static class QRelJudgement {
+    private String queryID;
+    private HashMap<String,String> relevantDocs;
+    
+    QRelJudgement(String queryID) {
+      this.queryID = queryID;
+      relevantDocs = new HashMap<String,String>();
+    }
+    
+    public void addRelevandDoc(String docName) {
+      relevantDocs.put(docName,docName);
+    }
+
+    boolean isRelevant(String docName) {
+      return relevantDocs.containsKey(docName);
+    }
+
+    public int maxRecall() {
+      return relevantDocs.size();
+    }
+  }
+
+  // inherit javadocs
+  public boolean validateData(QualityQuery[] qq, PrintWriter logger) {
+    HashMap<String,QRelJudgement> missingQueries = new HashMap<String, QRelJudgement>(judgements);
+    ArrayList<String> missingJudgements = new ArrayList<String>();
+    for (int i=0; i<qq.length; i++) {
+      String id = qq[i].getQueryID();
+      if (missingQueries.containsKey(id)) {
+        missingQueries.remove(id);
+      } else {
+        missingJudgements.add(id);
+      }
+    }
+    boolean isValid = true;
+    if (missingJudgements.size()>0) {
+      isValid = false;
+      if (logger!=null) {
+        logger.println("WARNING: "+missingJudgements.size()+" queries have no judgments! - ");
+        for (int i=0; i<missingJudgements.size(); i++) {
+          logger.println("   "+ missingJudgements.get(i));
+        }
+      }
+    }
+    if (missingQueries.size()>0) {
+      isValid = false;
+      if (logger!=null) {
+        logger.println("WARNING: "+missingQueries.size()+" judgments match no query! - ");
+        for (final String id : missingQueries.keySet()) {
+          logger.println("   "+id);
+        }
+      }
+    }
+    return isValid;
+  }
+
+  // inherit javadocs
+  public int maxRecall(QualityQuery query) {
+    QRelJudgement qrj = judgements.get(query.getQueryID());
+    if (qrj!=null) {
+      return qrj.maxRecall();
+    }
+    return 0;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecTopicsReader.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecTopicsReader.java
new file mode 100644
index 0000000..e84aa51
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecTopicsReader.java
@@ -0,0 +1,138 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.quality.trec;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+
+import org.apache.lucene.benchmark.quality.QualityQuery;
+
+/**
+ * Read TREC topics.
+ * <p>
+ * Expects this topic format -
+ * <pre>
+ *   &lt;top&gt;
+ *   &lt;num&gt; Number: nnn
+ *     
+ *   &lt;title&gt; title of the topic
+ *     
+ *   &lt;desc&gt; Description:
+ *   description of the topic
+ *     
+ *   &lt;narr&gt; Narrative:
+ *   "story" composed by assessors.
+ *    
+ *   &lt;/top&gt;
+ * </pre>
+ * Comment lines starting with '#' are ignored.
+ */
+public class TrecTopicsReader {
+
+  private static final String newline = System.getProperty("line.separator");
+  
+  /**
+   *  Constructor for Trec's TopicsReader
+   */
+  public TrecTopicsReader() {
+    super();
+  }
+
+  /**
+   * Read quality queries from trec format topics file.
+   * @param reader where queries are read from.
+   * @return the result quality queries.
+   * @throws IOException if cannot read the queries.
+   */
+  public QualityQuery[] readQueries(BufferedReader reader) throws IOException {
+    ArrayList<QualityQuery> res = new ArrayList<QualityQuery>();
+    StringBuilder sb;
+    try {
+      while (null!=(sb=read(reader,"<top>",null,false,false))) {
+        HashMap<String,String> fields = new HashMap<String,String>();
+        // id
+        sb = read(reader,"<num>",null,true,false);
+        int k = sb.indexOf(":");
+        String id = sb.substring(k+1).trim();
+        // title
+        sb = read(reader,"<title>",null,true,false);
+        k = sb.indexOf(">");
+        String title = sb.substring(k+1).trim();
+        // description
+        read(reader,"<desc>",null,false,false);
+        sb.setLength(0);
+        String line = null;
+        while ((line = reader.readLine()) != null) {
+          if (line.startsWith("<narr>"))
+            break;
+          if (sb.length() > 0) sb.append(' ');
+          sb.append(line);
+        }
+        String description = sb.toString().trim();
+        // narrative
+        sb.setLength(0);
+        while ((line = reader.readLine()) != null) {
+          if (line.startsWith("</top>"))
+            break;
+          if (sb.length() > 0) sb.append(' ');
+          sb.append(line);
+        }
+        String narrative = sb.toString().trim();
+        // we got a topic!
+        fields.put("title",title);
+        fields.put("description",description);
+        fields.put("narrative", narrative);
+        QualityQuery topic = new QualityQuery(id,fields);
+        res.add(topic);
+      }
+    } finally {
+      reader.close();
+    }
+    // sort result array (by ID) 
+    QualityQuery qq[] = res.toArray(new QualityQuery[0]);
+    Arrays.sort(qq);
+    return qq;
+  }
+
+  // read until finding a line that starts with the specified prefix
+  private StringBuilder read (BufferedReader reader, String prefix, StringBuilder sb, boolean collectMatchLine, boolean collectAll) throws IOException {
+    sb = (sb==null ? new StringBuilder() : sb);
+    String sep = "";
+    while (true) {
+      String line = reader.readLine();
+      if (line==null) {
+        return null;
+      }
+      if (line.startsWith(prefix)) {
+        if (collectMatchLine) {
+          sb.append(sep+line);
+          sep = newline;
+        }
+        break;
+      }
+      if (collectAll) {
+        sb.append(sep+line);
+        sep = newline;
+      }
+    }
+    //System.out.println("read: "+sb);
+    return sb;
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/package.html b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/package.html
new file mode 100755
index 0000000..dafccb7
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/package.html
@@ -0,0 +1,23 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<body>
+Utilities for Trec related quality benchmarking, feeding from Trec Topics and QRels inputs.
+</body>
+
+</html>
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/DocNameExtractor.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/DocNameExtractor.java
new file mode 100755
index 0000000..3639956
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/DocNameExtractor.java
@@ -0,0 +1,59 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.quality.utils;
+
+import java.io.IOException;
+
+import org.apache.lucene.document.FieldSelector;
+import org.apache.lucene.document.FieldSelectorResult;
+import org.apache.lucene.search.Searcher;
+
+/**
+ * Utility: extract doc names from an index
+ */
+public class DocNameExtractor {
+
+  private FieldSelector fldSel;
+  private String docNameField;
+  
+  /**
+   * Constructor for DocNameExtractor.
+   * @param docNameField name of the stored field containing the doc name. 
+   */
+  public DocNameExtractor (final String docNameField) {
+    this.docNameField = docNameField;
+    fldSel = new FieldSelector() {
+      public FieldSelectorResult accept(String fieldName) {
+        return fieldName.equals(docNameField) ? 
+            FieldSelectorResult.LOAD_AND_BREAK :
+              FieldSelectorResult.NO_LOAD;
+      }
+    };
+  }
+  
+  /**
+   * Extract the name of the input doc from the index.
+   * @param searcher access to the index.
+   * @param docid ID of doc whose name is needed.
+   * @return the name of the input doc as extracted from the index.
+   * @throws IOException if cannot extract the doc name from the index.
+   */
+  public String docName(Searcher searcher, int docid) throws IOException {
+    return searcher.doc(docid,fldSel).get(docNameField);
+  }
+  
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/QualityQueriesFinder.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/QualityQueriesFinder.java
new file mode 100755
index 0000000..228b7fb
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/QualityQueriesFinder.java
@@ -0,0 +1,135 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.quality.utils;
+
+import java.io.File;
+import java.io.IOException;
+
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.util.PriorityQueue;
+
+/**
+ * Suggest Quality queries based on an index contents.
+ * Utility class, used for making quality test benchmarks.
+ */
+public class QualityQueriesFinder {
+
+  private static final String newline = System.getProperty("line.separator");
+  private Directory dir;
+  
+  /**
+   * Constructor over a directory containing the index.
+   * @param dir directory containing the index we search for the quality test. 
+   */
+  private QualityQueriesFinder(Directory dir) {
+    this.dir = dir;
+  }
+
+  /**
+   * @param args {index-dir}
+   * @throws IOException  if cannot access the index.
+   */
+  public static void main(String[] args) throws IOException {
+    if (args.length<1) {
+      System.err.println("Usage: java QualityQueriesFinder <index-dir>");
+      System.exit(1);
+    }
+    QualityQueriesFinder qqf = new QualityQueriesFinder(FSDirectory.open(new File(args[0])));
+    String q[] = qqf.bestQueries("body",20);
+    for (int i=0; i<q.length; i++) {
+      System.out.println(newline+formatQueryAsTrecTopic(i,q[i],null,null));
+    }
+  }
+
+  private String [] bestQueries(String field,int numQueries) throws IOException {
+    String words[] = bestTerms("body",4*numQueries);
+    int n = words.length;
+    int m = n/4;
+    String res[] = new String[m];
+    for (int i=0; i<res.length; i++) {
+      res[i] = words[i] + " " + words[m+i]+ "  " + words[n-1-m-i]  + " " + words[n-1-i];
+      //System.out.println("query["+i+"]:  "+res[i]);
+    }
+    return res;
+  }
+  
+  private static String formatQueryAsTrecTopic (int qnum, String title, String description, String narrative) {
+    return 
+      "<top>" + newline +
+      "<num> Number: " + qnum             + newline + newline + 
+      "<title> " + (title==null?"":title) + newline + newline + 
+      "<desc> Description:"               + newline +
+      (description==null?"":description)  + newline + newline +
+      "<narr> Narrative:"                 + newline +
+      (narrative==null?"":narrative)      + newline + newline +
+      "</top>";
+  }
+  
+  private String [] bestTerms(String field,int numTerms) throws IOException {
+    PriorityQueue<TermDf> pq = new TermsDfQueue(numTerms);
+    IndexReader ir = IndexReader.open(dir, true);
+    try {
+      int threshold = ir.maxDoc() / 10; // ignore words too common.
+      Terms terms = MultiFields.getTerms(ir, field);
+      if (terms != null) {
+        TermsEnum termsEnum = terms.iterator();
+        while (termsEnum.next() != null) {
+          int df = termsEnum.docFreq();
+          if (df<threshold) {
+            String ttxt = termsEnum.term().utf8ToString();
+            pq.insertWithOverflow(new TermDf(ttxt,df));
+          }
+        }
+      }
+    } finally {
+      ir.close();
+    }
+    String res[] = new String[pq.size()];
+    int i = 0;
+    while (pq.size()>0) {
+      TermDf tdf = pq.pop(); 
+      res[i++] = tdf.word;
+      System.out.println(i+".   word:  "+tdf.df+"   "+tdf.word);
+    }
+    return res;
+  }
+
+  private static class TermDf {
+    String word;
+    int df;
+    TermDf (String word, int freq) {
+      this.word = word;
+      this.df = freq;
+    }
+  }
+  
+  private static class TermsDfQueue extends PriorityQueue<TermDf> {
+    TermsDfQueue (int maxSize) {
+      initialize(maxSize);
+    }
+    @Override
+    protected boolean lessThan(TermDf tf1, TermDf tf2) {
+      return tf1.df < tf2.df;
+    }
+  }
+  
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java
new file mode 100755
index 0000000..765bdc6
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java
@@ -0,0 +1,74 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.quality.utils;
+
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.benchmark.quality.QualityQuery;
+import org.apache.lucene.benchmark.quality.QualityQueryParser;
+import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.Version;
+
+/**
+ * Simplistic quality query parser. A Lucene query is created by passing 
+ * the value of the specified QualityQuery name-value pair(s) into 
+ * a Lucene's QueryParser using StandardAnalyzer. */
+public class SimpleQQParser implements QualityQueryParser {
+
+  private String qqNames[];
+  private String indexField;
+  ThreadLocal<QueryParser> queryParser = new ThreadLocal<QueryParser>();
+
+  /**
+   * Constructor of a simple qq parser.
+   * @param qqNames name-value pairs of quality query to use for creating the query
+   * @param indexField corresponding index field  
+   */
+  public SimpleQQParser(String qqNames[], String indexField) {
+    this.qqNames = qqNames;
+    this.indexField = indexField;
+  }
+
+  /**
+   * Constructor of a simple qq parser.
+   * @param qqName name-value pair of quality query to use for creating the query
+   * @param indexField corresponding index field  
+   */
+  public SimpleQQParser(String qqName, String indexField) {
+    this(new String[] { qqName }, indexField);
+  }
+
+  /* (non-Javadoc)
+   * @see org.apache.lucene.benchmark.quality.QualityQueryParser#parse(org.apache.lucene.benchmark.quality.QualityQuery)
+   */
+  public Query parse(QualityQuery qq) throws ParseException {
+    QueryParser qp = queryParser.get();
+    if (qp==null) {
+      qp = new QueryParser(Version.LUCENE_CURRENT, indexField, new StandardAnalyzer(Version.LUCENE_CURRENT));
+      queryParser.set(qp);
+    }
+    BooleanQuery bq = new BooleanQuery();
+    for (int i = 0; i < qqNames.length; i++)
+      bq.add(qp.parse(QueryParser.escape(qq.getValue(qqNames[i]))), BooleanClause.Occur.SHOULD);
+    
+    return bq;
+  }
+
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SubmissionReport.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SubmissionReport.java
new file mode 100644
index 0000000..f9ea2d0
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SubmissionReport.java
@@ -0,0 +1,93 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.quality.utils;
+
+import java.io.IOException;
+import java.io.PrintWriter;
+import java.text.NumberFormat;
+
+import org.apache.lucene.benchmark.quality.QualityQuery;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Searcher;
+import org.apache.lucene.search.TopDocs;
+
+/**
+ * Create a log ready for submission.
+ * Extend this class and override
+ * {@link #report(QualityQuery, TopDocs, String, Searcher)}
+ * to create different reports. 
+ */
+public class SubmissionReport {
+
+  private NumberFormat nf;
+  private PrintWriter logger;
+  private String name;
+  
+  /**
+   * Constructor for SubmissionReport.
+   * @param logger if null, no submission data is created. 
+   * @param name name of this run.
+   */
+  public SubmissionReport (PrintWriter logger, String name) {
+    this.logger = logger;
+    this.name = name;
+    nf = NumberFormat.getInstance();
+    nf.setMaximumFractionDigits(4);
+    nf.setMinimumFractionDigits(4);
+  }
+  
+  /**
+   * Report a search result for a certain quality query.
+   * @param qq quality query for which the results are reported.
+   * @param td search results for the query.
+   * @param docNameField stored field used for fetching the result doc name.  
+   * @param searcher index access for fetching doc name.
+   * @throws IOException in case of a problem.
+   */
+  public void report(QualityQuery qq, TopDocs td, String docNameField, Searcher searcher) throws IOException {
+    if (logger==null) {
+      return;
+    }
+    ScoreDoc sd[] = td.scoreDocs;
+    String sep = " \t ";
+    DocNameExtractor xt = new DocNameExtractor(docNameField);
+    for (int i=0; i<sd.length; i++) {
+      String docName = xt.docName(searcher,sd[i].doc);
+      logger.println(
+          qq.getQueryID()       + sep +
+          "Q0"                   + sep +
+          format(docName,20)    + sep +
+          format(""+i,7)        + sep +
+          nf.format(sd[i].score) + sep +
+          name
+          );
+    }
+  }
+
+  public void flush() {
+    if (logger!=null) {
+      logger.flush();
+    }
+  }
+  
+  private static String padd = "                                    ";
+  private String format(String s, int minLen) {
+    s = (s==null ? "" : s);
+    int n = Math.max(minLen,s.length());
+    return (s+padd).substring(0,n);
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/package.html b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/package.html
new file mode 100755
index 0000000..7fde1a8
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/package.html
@@ -0,0 +1,23 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<body>
+Miscellaneous utilities for search quality benchmarking: query parsing, submission reports.
+</body>
+
+</html>
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/stats/MemUsage.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/stats/MemUsage.java
new file mode 100644
index 0000000..2ebb015
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/stats/MemUsage.java
@@ -0,0 +1,43 @@
+package org.apache.lucene.benchmark.stats;
+/**
+ * Copyright 2005 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * This class holds a set of memory usage values.
+ *
+ */
+public class MemUsage {
+  public long maxFree, minFree, avgFree;
+
+  public long maxTotal, minTotal, avgTotal;
+
+  @Override
+  public String toString() {
+    return toScaledString(1, "B");
+  }
+
+  /** Scale down the values by divisor, append the unit string. */
+  public String toScaledString(int div, String unit) {
+    StringBuilder sb = new StringBuilder();
+      sb.append("free=").append(minFree / div);
+      sb.append("/").append(avgFree / div);
+      sb.append("/").append(maxFree / div).append(" ").append(unit);
+      sb.append(", total=").append(minTotal / div);
+      sb.append("/").append(avgTotal / div);
+      sb.append("/").append(maxTotal / div).append(" ").append(unit);
+    return sb.toString();
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/stats/QueryData.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/stats/QueryData.java
new file mode 100644
index 0000000..cdf7f52
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/stats/QueryData.java
@@ -0,0 +1,79 @@
+package org.apache.lucene.benchmark.stats;
+/**
+ * Copyright 2005 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Vector;
+
+import org.apache.lucene.search.Query;
+import org.apache.lucene.benchmark.Constants;
+
+/**
+ * This class holds parameters for a query benchmark.
+ *
+ */
+public class QueryData {
+  /** Benchmark id */
+  public String id;
+  /** Lucene query */
+  public Query q;
+  /** If true, re-open index reader before benchmark. */
+  public boolean reopen;
+  /** If true, warm-up the index reader before searching by sequentially
+   * retrieving all documents from index.
+   */
+  public boolean warmup;
+  /**
+   * If true, actually retrieve documents returned in Hits.
+   */
+  public boolean retrieve;
+
+  /**
+   * Prepare a list of benchmark data, using all possible combinations of
+   * benchmark parameters.
+   * @param queries source Lucene queries
+   * @return The QueryData
+   */
+  public static QueryData[] getAll(Query[] queries) {
+    Vector<QueryData> vqd = new Vector<QueryData>();
+    for (int i = 0; i < queries.length; i++) {
+      for (int r = 1; r >= 0; r--) {
+        for (int w = 1; w >= 0; w--) {
+          for (int t = 0; t < 2; t++) {
+            QueryData qd = new QueryData();
+            qd.id="qd-" + i + r + w + t;
+            qd.reopen = Constants.BOOLEANS[r].booleanValue();
+            qd.warmup = Constants.BOOLEANS[w].booleanValue();
+            qd.retrieve = Constants.BOOLEANS[t].booleanValue();
+            qd.q = queries[i];
+            vqd.add(qd);
+          }
+        }
+      }
+    }
+    return vqd.toArray(new QueryData[0]);
+  }
+
+  /** Short legend for interpreting toString() output. */
+  public static String getLabels() {
+    return "# Query data: R-reopen, W-warmup, T-retrieve, N-no";
+  }
+
+  @Override
+  public String toString() {
+    return id + " " + (reopen ? "R" : "NR") + " " + (warmup ? "W" : "NW") +
+      " " + (retrieve ? "T" : "NT") + " [" + q.toString() + "]";
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/stats/TestData.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/stats/TestData.java
new file mode 100644
index 0000000..58b840d
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/stats/TestData.java
@@ -0,0 +1,574 @@
+package org.apache.lucene.benchmark.stats;
+/**
+ * Copyright 2005 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import java.io.File;
+import java.text.NumberFormat;
+import java.util.ArrayList;
+
+import java.util.Date;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Vector;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.benchmark.Constants;
+import org.apache.lucene.store.Directory;
+
+
+/**
+ * This class holds together all parameters related to a test. Single test is
+ * performed several times, and all results are averaged.
+ *
+ */
+public class TestData
+{
+    public static int[] MAX_BUFFERED_DOCS_COUNTS = new int[]{10, 20, 50, 100, 200, 500};
+    public static int[] MERGEFACTOR_COUNTS = new int[]{10, 20, 50, 100, 200, 500};
+
+    /**
+     * ID of this test data.
+     */
+    private String id;
+    /**
+     * Heap size.
+     */
+    private long heap;
+    /**
+     * List of results for each test run with these parameters.
+     */
+    private Vector<TestRunData> runData = new Vector<TestRunData>();
+    private int maxBufferedDocs, mergeFactor;
+    /**
+     * Directory containing source files.
+     */
+    private File source;
+    /**
+     * Lucene Directory implementation for creating an index.
+     */
+    private Directory directory;
+    /**
+     * Analyzer to use when adding documents.
+     */
+    private Analyzer analyzer;
+    /**
+     * If true, use compound file format.
+     */
+    private boolean compound;
+    /**
+     * If true, optimize index when finished adding documents.
+     */
+    private boolean optimize;
+    /**
+     * Data for search benchmarks.
+     */
+    private QueryData[] queries;
+
+    public TestData()
+    {
+        heap = Runtime.getRuntime().maxMemory();
+    }
+
+    private static class DCounter
+    {
+        double total;
+        int count, recordCount;
+    }
+
+    private static class LCounter
+    {
+        long total;
+        int count;
+    }
+
+    private static class LDCounter
+    {
+      double Dtotal;
+      int Dcount, DrecordCount;
+      long Ltotal0;
+      int Lcount0;
+      long Ltotal1;
+      int Lcount1;
+    }
+
+    /**
+     * Get a textual summary of the benchmark results, average from all test runs.
+     */
+    static final String ID =      "# testData id     ";
+    static final String OP =      "operation      ";
+    static final String RUNCNT =  "     runCnt";
+    static final String RECCNT =  "     recCnt";
+    static final String RECSEC =  "          rec/s";
+    static final String FREEMEM = "       avgFreeMem";
+    static final String TOTMEM =  "      avgTotalMem";
+    static final String COLS[] = {
+        ID,
+        OP,
+        RUNCNT,
+        RECCNT,
+        RECSEC,
+        FREEMEM,
+        TOTMEM
+    };
+    public String showRunData(String prefix)
+    {
+        if (runData.size() == 0)
+        {
+            return "# [NO RUN DATA]";
+        }
+        HashMap<String,LDCounter> resByTask = new HashMap<String,LDCounter>(); 
+        StringBuilder sb = new StringBuilder();
+        String lineSep = System.getProperty("line.separator");
+        sb.append("warm = Warm Index Reader").append(lineSep).append("srch = Search Index").append(lineSep).append("trav = Traverse Hits list, optionally retrieving document").append(lineSep).append(lineSep);
+        for (int i = 0; i < COLS.length; i++) {
+          sb.append(COLS[i]);
+        }
+        sb.append("\n");
+        LinkedHashMap<String,TestData.LCounter[]> mapMem = new LinkedHashMap<String,TestData.LCounter[]>();
+        LinkedHashMap<String,DCounter> mapSpeed = new LinkedHashMap<String,DCounter>();
+        for (int i = 0; i < runData.size(); i++)
+        {
+            TestRunData trd = runData.get(i);
+            for (final String label : trd.getLabels()) 
+            {
+                MemUsage mem = trd.getMemUsage(label);
+                if (mem != null)
+                {
+                    TestData.LCounter[] tm = mapMem.get(label);
+                    if (tm == null)
+                    {
+                        tm = new TestData.LCounter[2];
+                        tm[0] = new TestData.LCounter();
+                        tm[1] = new TestData.LCounter();
+                        mapMem.put(label, tm);
+                    }
+                    tm[0].total += mem.avgFree;
+                    tm[0].count++;
+                    tm[1].total += mem.avgTotal;
+                    tm[1].count++;
+                }
+                TimeData td = trd.getTotals(label);
+                if (td != null)
+                {
+                    TestData.DCounter dc = mapSpeed.get(label);
+                    if (dc == null)
+                    {
+                        dc = new TestData.DCounter();
+                        mapSpeed.put(label, dc);
+                    }
+                    dc.count++;
+                    //dc.total += td.getRate();
+                    dc.total += (td.count>0 && td.elapsed<=0 ? 1 : td.elapsed); // assume at least 1ms for any countable op
+                    dc.recordCount += td.count;
+                }
+            }
+        }
+        LinkedHashMap<String,String> res = new LinkedHashMap<String,String>();
+        Iterator<String> it = mapSpeed.keySet().iterator();
+        while (it.hasNext())
+        {
+            String label = it.next();
+            TestData.DCounter dc = mapSpeed.get(label);
+            res.put(label, 
+                format(dc.count, RUNCNT) + 
+                format(dc.recordCount / dc.count, RECCNT) +
+                format(1,(float) (dc.recordCount * 1000.0 / (dc.total>0 ? dc.total : 1.0)), RECSEC)
+                //format((float) (dc.total / (double) dc.count), RECSEC)
+                );
+            
+            // also sum by task
+            String task = label.substring(label.lastIndexOf("-")+1);
+            LDCounter ldc = resByTask.get(task);
+            if (ldc==null) {
+              ldc = new LDCounter();
+              resByTask.put(task,ldc);
+            }
+            ldc.Dcount += dc.count;
+            ldc.DrecordCount += dc.recordCount;
+            ldc.Dtotal += (dc.count>0 && dc.total<=0 ? 1 : dc.total); // assume at least 1ms for any countable op 
+        }
+        it = mapMem.keySet().iterator();
+        while (it.hasNext())
+        {
+            String label = it.next();
+            TestData.LCounter[] lc =  mapMem.get(label);
+            String speed = res.get(label);
+            boolean makeSpeed = false;
+            if (speed == null)
+            {
+                makeSpeed = true;
+                speed =  
+                  format(lc[0].count, RUNCNT) + 
+                  format(0, RECCNT) + 
+                  format(0,(float)0.0, RECSEC);
+            }
+            res.put(label, speed + 
+                format(0, lc[0].total / lc[0].count, FREEMEM) + 
+                format(0, lc[1].total / lc[1].count, TOTMEM));
+            
+            // also sum by task
+            String task = label.substring(label.lastIndexOf("-")+1);
+            LDCounter ldc = resByTask.get(task);
+            if (ldc==null) {
+              ldc = new LDCounter();
+              resByTask.put(task,ldc);
+              makeSpeed = true;
+            }
+            if (makeSpeed) {
+              ldc.Dcount += lc[0].count;
+            }
+            ldc.Lcount0 += lc[0].count;
+            ldc.Lcount1 += lc[1].count;
+            ldc.Ltotal0 += lc[0].total;
+            ldc.Ltotal1 += lc[1].total;
+        }
+        it = res.keySet().iterator();
+        while (it.hasNext())
+        {
+            String label = it.next();
+            sb.append(format(prefix, ID));
+            sb.append(format(label, OP));
+            sb.append(res.get(label)).append("\n");
+        }
+        // show results by task (srch, optimize, etc.) 
+        sb.append("\n");
+        for (int i = 0; i < COLS.length; i++) {
+          sb.append(COLS[i]);
+        }
+        sb.append("\n");
+        it = resByTask.keySet().iterator();
+        while (it.hasNext())
+        {
+            String task = it.next();
+            LDCounter ldc = resByTask.get(task);
+            sb.append(format("    ", ID));
+            sb.append(format(task, OP));
+            sb.append(format(ldc.Dcount, RUNCNT)); 
+            sb.append(format(ldc.DrecordCount / ldc.Dcount, RECCNT));
+            sb.append(format(1,(float) (ldc.DrecordCount * 1000.0 / (ldc.Dtotal>0 ? ldc.Dtotal : 1.0)), RECSEC));
+            sb.append(format(0, ldc.Ltotal0 / ldc.Lcount0, FREEMEM)); 
+            sb.append(format(0, ldc.Ltotal1 / ldc.Lcount1, TOTMEM));
+            sb.append("\n");
+        }
+        return sb.toString();
+    }
+
+    private static NumberFormat numFormat [] = { NumberFormat.getInstance(), NumberFormat.getInstance()};
+    private static final String padd = "                                  ";
+    static {
+      numFormat[0].setMaximumFractionDigits(0);
+      numFormat[0].setMinimumFractionDigits(0);
+      numFormat[1].setMaximumFractionDigits(1);
+      numFormat[1].setMinimumFractionDigits(1);
+    }
+
+    // pad number from left
+    // numFracDigits must be 0 or 1.
+    static String format(int numFracDigits, float f, String col) {
+      String res = padd + numFormat[numFracDigits].format(f);
+      return res.substring(res.length() - col.length());
+    }
+
+    // pad number from left
+    static String format(int n, String col) {
+      String res = padd + n;
+      return res.substring(res.length() - col.length());
+    }
+
+    // pad string from right
+    static String format(String s, String col) {
+      return (s + padd).substring(0,col.length());
+    }
+
+    /**
+     * Prepare a list of benchmark data, using all possible combinations of
+     * benchmark parameters.
+     *
+     * @param sources   list of directories containing different source document
+     *                  collections
+     * @param analyzers of analyzers to use.
+     */
+    public static TestData[] getAll(File[] sources, Analyzer[] analyzers)
+    {
+        List<TestData> res = new ArrayList<TestData>(50);
+        TestData ref = new TestData();
+        for (int q = 0; q < analyzers.length; q++)
+        {
+            for (int m = 0; m < sources.length; m++)
+            {
+                for (int i = 0; i < MAX_BUFFERED_DOCS_COUNTS.length; i++)
+                {
+                    for (int k = 0; k < MERGEFACTOR_COUNTS.length; k++)
+                    {
+                        for (int n = 0; n < Constants.BOOLEANS.length; n++)
+                        {
+                            for (int p = 0; p < Constants.BOOLEANS.length; p++)
+                            {
+                                ref.id = "td-" + q + m + i + k + n + p;
+                                ref.source = sources[m];
+                                ref.analyzer = analyzers[q];
+                                ref.maxBufferedDocs = MAX_BUFFERED_DOCS_COUNTS[i];
+                                ref.mergeFactor = MERGEFACTOR_COUNTS[k];
+                                ref.compound = Constants.BOOLEANS[n].booleanValue();
+                                ref.optimize = Constants.BOOLEANS[p].booleanValue();
+                                try
+                                {
+                                    res.add((TestData)ref.clone());
+                                }
+                                catch (Exception e)
+                                {
+                                    e.printStackTrace();
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+        }
+        return res.toArray(new TestData[0]);
+    }
+
+    /**
+     * Similar to {@link #getAll(java.io.File[], org.apache.lucene.analysis.Analyzer[])} but only uses
+     * maxBufferedDocs of 10 and 100 and same for mergeFactor, thus reducing the number of permutations significantly.
+     * It also only uses compound file and optimize is always true.
+     *
+     * @param sources
+     * @param analyzers
+     * @return An Array of {@link TestData}
+     */
+    public static TestData[] getTestDataMinMaxMergeAndMaxBuffered(File[] sources, Analyzer[] analyzers)
+    {
+        List<TestData> res = new ArrayList<TestData>(50);
+        TestData ref = new TestData();
+        for (int q = 0; q < analyzers.length; q++)
+        {
+            for (int m = 0; m < sources.length; m++)
+            {
+                ref.id = "td-" + q + m + "_" + 10 + "_" + 10;
+                ref.source = sources[m];
+                ref.analyzer = analyzers[q];
+                ref.maxBufferedDocs = 10;
+                ref.mergeFactor = 10;//MERGEFACTOR_COUNTS[k];
+                ref.compound = true;
+                ref.optimize = true;
+                try
+                {
+                    res.add((TestData)ref.clone());
+                }
+                catch (Exception e)
+                {
+                    e.printStackTrace();
+                }
+                ref.id = "td-" + q + m  + "_" + 10 + "_" + 100;
+                ref.source = sources[m];
+                ref.analyzer = analyzers[q];
+                ref.maxBufferedDocs = 10;
+                ref.mergeFactor = 100;//MERGEFACTOR_COUNTS[k];
+                ref.compound = true;
+                ref.optimize = true;
+                try
+                {
+                    res.add((TestData)ref.clone());
+                }
+                catch (Exception e)
+                {
+                    e.printStackTrace();
+                }
+                ref.id = "td-" + q + m + "_" + 100 + "_" + 10;
+                ref.source = sources[m];
+                ref.analyzer = analyzers[q];
+                ref.maxBufferedDocs = 100;
+                ref.mergeFactor = 10;//MERGEFACTOR_COUNTS[k];
+                ref.compound = true;
+                ref.optimize = true;
+                try
+                {
+                    res.add((TestData)ref.clone());
+                }
+                catch (Exception e)
+                {
+                    e.printStackTrace();
+                }
+                ref.id = "td-" + q + m + "_" + 100 + "_" + 100;
+                ref.source = sources[m];
+                ref.analyzer = analyzers[q];
+                ref.maxBufferedDocs = 100;
+                ref.mergeFactor = 100;//MERGEFACTOR_COUNTS[k];
+                ref.compound = true;
+                ref.optimize = true;
+                try
+                {
+                    res.add((TestData)ref.clone());
+                }
+                catch (Exception e)
+                {
+                    e.printStackTrace();
+                }
+            }
+        }
+        return res.toArray(new TestData[0]);
+    }
+
+    @Override
+    protected Object clone()
+    {
+        TestData cl = new TestData();
+        cl.id = id;
+        cl.compound = compound;
+        cl.heap = heap;
+        cl.mergeFactor = mergeFactor;
+        cl.maxBufferedDocs = maxBufferedDocs;
+        cl.optimize = optimize;
+        cl.source = source;
+        cl.directory = directory;
+        cl.analyzer = analyzer;
+        // don't clone runData
+        return cl;
+    }
+
+    @Override
+    public String toString()
+    {
+        StringBuilder res = new StringBuilder();
+        res.append("#-- ID: ").append(id).append(", ").append(new Date()).append(", heap=").append(heap).append(" --\n");
+        res.append("# source=").append(source).append(", directory=").append(directory).append("\n");
+        res.append("# maxBufferedDocs=").append(maxBufferedDocs).append(", mergeFactor=").append(mergeFactor);
+        res.append(", compound=").append(compound).append(", optimize=").append(optimize).append("\n");
+        if (queries != null)
+        {
+            res.append(QueryData.getLabels()).append("\n");
+            for (int i = 0; i < queries.length; i++)
+            {
+                res.append("# ").append(queries[i].toString()).append("\n");
+            }
+        }
+        return res.toString();
+    }
+
+    public Analyzer getAnalyzer()
+    {
+        return analyzer;
+    }
+
+    public void setAnalyzer(Analyzer analyzer)
+    {
+        this.analyzer = analyzer;
+    }
+
+    public boolean isCompound()
+    {
+        return compound;
+    }
+
+    public void setCompound(boolean compound)
+    {
+        this.compound = compound;
+    }
+
+    public Directory getDirectory()
+    {
+        return directory;
+    }
+
+    public void setDirectory(Directory directory)
+    {
+        this.directory = directory;
+    }
+
+    public long getHeap()
+    {
+        return heap;
+    }
+
+    public void setHeap(long heap)
+    {
+        this.heap = heap;
+    }
+
+    public String getId()
+    {
+        return id;
+    }
+
+    public void setId(String id)
+    {
+        this.id = id;
+    }
+
+    public int getMaxBufferedDocs()
+    {
+        return maxBufferedDocs;
+    }
+
+    public void setMaxBufferedDocs(int maxBufferedDocs)
+    {
+        this.maxBufferedDocs = maxBufferedDocs;
+    }
+
+    public int getMergeFactor()
+    {
+        return mergeFactor;
+    }
+
+    public void setMergeFactor(int mergeFactor)
+    {
+        this.mergeFactor = mergeFactor;
+    }
+
+    public boolean isOptimize()
+    {
+        return optimize;
+    }
+
+    public void setOptimize(boolean optimize)
+    {
+        this.optimize = optimize;
+    }
+
+    public QueryData[] getQueries()
+    {
+        return queries;
+    }
+
+    public void setQueries(QueryData[] queries)
+    {
+        this.queries = queries;
+    }
+
+    public Vector<TestRunData> getRunData()
+    {
+        return runData;
+    }
+
+    public void setRunData(Vector<TestRunData> runData)
+    {
+        this.runData = runData;
+    }
+
+    public File getSource()
+    {
+        return source;
+    }
+
+    public void setSource(File source)
+    {
+        this.source = source;
+    }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/stats/TestRunData.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/stats/TestRunData.java
new file mode 100644
index 0000000..d047155
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/stats/TestRunData.java
@@ -0,0 +1,172 @@
+package org.apache.lucene.benchmark.stats;
+/**
+ * Copyright 2005 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import java.util.LinkedHashMap;
+import java.util.Vector;
+import java.util.Collection;
+import java.util.Iterator;
+
+/**
+ * This class holds series of TimeData related to a single test run. TimeData
+ * values may contribute to different measurements, so this class provides also
+ * some useful methods to separate them.
+ *
+ */
+public class TestRunData {
+  private String id;
+
+  /** Start and end time of this test run. */
+  private long start = 0L, end = 0L;
+
+  private LinkedHashMap<String,Vector<TimeData>> data = new LinkedHashMap<String,Vector<TimeData>>();
+
+  public TestRunData() {}
+
+  public TestRunData(String id) {
+    this.id = id;
+  }
+
+    public LinkedHashMap<String,Vector<TimeData>> getData()
+    {
+        return data;
+    }
+
+    public String getId()
+    {
+        return id;
+    }
+
+    public void setId(String id)
+    {
+        this.id = id;
+    }
+
+    public long getEnd()
+    {
+        return end;
+    }
+
+    public long getStart()
+    {
+        return start;
+    }
+
+    /** Mark the starting time of this test run. */
+  public void startRun() {
+    start = System.currentTimeMillis();
+  }
+
+  /** Mark the ending time of this test run. */
+  public void endRun() {
+    end = System.currentTimeMillis();
+  }
+
+  /** Add a data point. */
+  public void addData(TimeData td) {
+    td.recordMemUsage();
+    Vector<TimeData> v = data.get(td.name);
+    if (v == null) {
+      v = new Vector<TimeData>();
+      data.put(td.name, v);
+    }
+    v.add((TimeData)td.clone());
+  }
+
+  /** Get a list of all available types of data points. */
+  public Collection<String> getLabels() {
+    return data.keySet();
+  }
+
+  /** Get total values from all data points of a given type. */
+  public TimeData getTotals(String label) {
+    Vector<TimeData> v = data.get(label);
+      if (v == null)
+      {
+          return null;
+      }
+    TimeData res = new TimeData("TOTAL " + label);
+    for (int i = 0; i < v.size(); i++) {
+      TimeData td = v.get(i);
+      res.count += td.count;
+      res.elapsed += td.elapsed;
+    }
+    return res;
+  }
+
+  /** Get total values from all data points of all types.
+   * @return a list of TimeData values for all types.
+   */
+  public Vector<TimeData> getTotals() {
+    Collection<String> labels = getLabels();
+    Vector<TimeData> v = new Vector<TimeData>();
+    Iterator<String> it = labels.iterator();
+    while (it.hasNext()) {
+      TimeData td = getTotals(it.next());
+      v.add(td);
+    }
+    return v;
+  }
+
+  /** Get memory usage stats for a given data type. */
+  public MemUsage getMemUsage(String label) {
+    Vector<TimeData> v = data.get(label);
+      if (v == null)
+      {
+          return null;
+      }
+    MemUsage res = new MemUsage();
+    res.minFree = Long.MAX_VALUE;
+    res.minTotal = Long.MAX_VALUE;
+    long avgFree = 0L, avgTotal = 0L;
+    for (int i = 0; i < v.size(); i++) {
+      TimeData td = v.get(i);
+        if (res.maxFree < td.freeMem)
+        {
+            res.maxFree = td.freeMem;
+        }
+        if (res.maxTotal < td.totalMem)
+        {
+            res.maxTotal = td.totalMem;
+        }
+        if (res.minFree > td.freeMem)
+        {
+            res.minFree = td.freeMem;
+        }
+        if (res.minTotal > td.totalMem)
+        {
+            res.minTotal = td.totalMem;
+        }
+      avgFree += td.freeMem;
+      avgTotal += td.totalMem;
+    }
+    res.avgFree = avgFree / v.size();
+    res.avgTotal = avgTotal / v.size();
+    return res;
+  }
+
+  /** Return a string representation. */
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder();
+    for (final String label : getLabels()) {
+        sb.append(id).append("-").append(label).append(" ").append(getTotals(label).toString(false)).append(" ");
+        sb.append(getMemUsage(label).toScaledString(1024 * 1024, "MB")).append("\n");
+    }
+    return sb.toString();
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/stats/TimeData.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/stats/TimeData.java
new file mode 100644
index 0000000..c210939
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/stats/TimeData.java
@@ -0,0 +1,103 @@
+package org.apache.lucene.benchmark.stats;
+/**
+ * Copyright 2005 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/**
+ * This class holds a data point measuring speed of processing.
+ *
+ */
+public class TimeData {
+  /** Name of the data point - usually one of a data series with the same name */
+  public String name;
+  /** Number of records processed. */
+  public long count = 0;
+  /** Elapsed time in milliseconds. */
+  public long elapsed = 0L;
+
+  private long delta = 0L;
+  /** Free memory at the end of measurement interval. */
+  public long freeMem = 0L;
+  /** Total memory at the end of measurement interval. */
+  public long totalMem = 0L;
+
+  public TimeData() {}
+
+  public TimeData(String name) {
+    this.name = name;
+  }
+
+  /** Start counting elapsed time. */
+  public void start() {
+    delta = System.currentTimeMillis();
+  }
+
+  /** Stop counting elapsed time. */
+  public void stop() {
+    count++;
+    elapsed += (System.currentTimeMillis() - delta);
+  }
+
+  /** Record memory usage. */
+  public void recordMemUsage() {
+    freeMem = Runtime.getRuntime().freeMemory();
+    totalMem = Runtime.getRuntime().totalMemory();
+  }
+
+  /** Reset counters. */
+  public void reset() {
+    count = 0;
+    elapsed = 0L;
+    delta = elapsed;
+  }
+
+  @Override
+  protected Object clone() {
+    TimeData td = new TimeData(name);
+    td.name = name;
+    td.elapsed = elapsed;
+    td.count = count;
+    td.delta = delta;
+    td.freeMem = freeMem;
+    td.totalMem = totalMem;
+    return td;
+  }
+
+  /** Get rate of processing, defined as number of processed records per second. */
+  public double getRate() {
+    double rps = count * 1000.0 / (elapsed > 0 ? elapsed : 1); // assume at least 1ms for any countable op
+    return rps;
+  }
+
+  /** Get a short legend for toString() output. */
+  public static String getLabels() {
+    return "# count\telapsed\trec/s\tfreeMem\ttotalMem";
+  }
+
+  @Override
+  public String toString() { return toString(true); }
+  /**
+   * Return a tab-separated string containing this data.
+   * @param withMem if true, append also memory information
+   * @return The String
+   */
+  public String toString(boolean withMem) {
+    StringBuilder sb = new StringBuilder();
+    sb.append(count + "\t" + elapsed + "\t" + getRate());
+    if (withMem) sb.append("\t" + freeMem + "\t" + totalMem);
+    return sb.toString();
+  }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/utils/ExtractReuters.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/utils/ExtractReuters.java
new file mode 100644
index 0000000..3e4104b
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/utils/ExtractReuters.java
@@ -0,0 +1,174 @@
+package org.apache.lucene.benchmark.utils;
+/**
+ * Copyright 2005 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileFilter;
+import java.io.FileReader;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+
+/**
+ * Split the Reuters SGML documents into Simple Text files containing: Title, Date, Dateline, Body
+ */
+public class ExtractReuters
+{
+    private File reutersDir;
+    private File outputDir;
+    private static final String LINE_SEPARATOR = System.getProperty("line.separator");
+
+    public ExtractReuters(File reutersDir, File outputDir)
+    {
+        this.reutersDir = reutersDir;
+        this.outputDir = outputDir;
+        System.out.println("Deleting all files in " + outputDir);
+        File [] files = outputDir.listFiles();
+        for (int i = 0; i < files.length; i++)
+        {
+            files[i].delete();
+        }
+
+    }
+
+    public void extract()
+    {
+        File [] sgmFiles = reutersDir.listFiles(new FileFilter()
+        {
+            public boolean accept(File file)
+            {
+                return file.getName().endsWith(".sgm");
+            }
+        });
+        if (sgmFiles != null && sgmFiles.length > 0)
+        {
+            for (int i = 0; i < sgmFiles.length; i++)
+            {
+                File sgmFile = sgmFiles[i];
+                extractFile(sgmFile);
+            }
+        }
+        else
+        {
+            System.err.println("No .sgm files in " + reutersDir);
+        }
+    }
+
+    Pattern EXTRACTION_PATTERN = Pattern.compile("<TITLE>(.*?)</TITLE>|<DATE>(.*?)</DATE>|<BODY>(.*?)</BODY>");
+
+    private static String[] META_CHARS
+            = {"&", "<", ">", "\"", "'"};
+
+    private static String[] META_CHARS_SERIALIZATIONS
+            = {"&amp;", "&lt;", "&gt;", "&quot;", "&apos;"};
+
+    /**
+     * Override if you wish to change what is extracted
+     *
+     * @param sgmFile
+     */
+    protected void extractFile(File sgmFile)
+    {
+        try
+        {
+            BufferedReader reader = new BufferedReader(new FileReader(sgmFile));
+
+            StringBuilder buffer = new StringBuilder(1024);
+            StringBuilder outBuffer = new StringBuilder(1024);
+
+            String line = null;
+            int docNumber = 0;
+            while ((line = reader.readLine()) != null)
+            {
+                //when we see a closing reuters tag, flush the file
+
+                if (line.indexOf("</REUTERS") == -1) {
+                    //Replace the SGM escape sequences
+
+                    buffer.append(line).append(' ');//accumulate the strings for now, then apply regular expression to get the pieces,
+                }
+                else
+                {
+                    //Extract the relevant pieces and write to a file in the output dir
+                    Matcher matcher = EXTRACTION_PATTERN.matcher(buffer);
+                    while (matcher.find())
+                    {
+                        for (int i = 1; i <= matcher.groupCount(); i++)
+                        {
+                            if (matcher.group(i) != null)
+                            {
+                                outBuffer.append(matcher.group(i));
+                            }
+                        }
+                        outBuffer.append(LINE_SEPARATOR).append(LINE_SEPARATOR);
+                    }
+                    String out = outBuffer.toString();
+                    for (int i = 0; i < META_CHARS_SERIALIZATIONS.length; i++)
+                    {
+                        out = out.replaceAll(META_CHARS_SERIALIZATIONS[i], META_CHARS[i]);
+                    }
+                    File outFile = new File(outputDir, sgmFile.getName() + "-" + (docNumber++) + ".txt");
+                    //System.out.println("Writing " + outFile);
+                    FileWriter writer = new FileWriter(outFile);
+                    writer.write(out);
+                    writer.close();
+                    outBuffer.setLength(0);
+                    buffer.setLength(0);
+                }
+            }
+            reader.close();
+        }
+
+        catch (
+                IOException e
+                )
+
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+
+    public static void main(String[] args)
+    {
+        if (args.length != 2)
+        {
+            printUsage();
+        }
+        File reutersDir = new File(args[0]);
+
+        if (reutersDir.exists())
+        {
+            File outputDir = new File(args[1]);
+            outputDir.mkdirs();
+            ExtractReuters extractor = new ExtractReuters(reutersDir, outputDir);
+            extractor.extract();
+        }
+        else
+        {
+            printUsage();
+        }
+    }
+
+    private static void printUsage()
+    {
+        System.err.println("Usage: java -cp <...> org.apache.lucene.benchmark.utils.ExtractReuters <Path to Reuters SGM files> <Output Path>");
+    }
+}
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/utils/ExtractWikipedia.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/utils/ExtractWikipedia.java
new file mode 100644
index 0000000..feeb6da
--- /dev/null
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/utils/ExtractWikipedia.java
@@ -0,0 +1,151 @@
+package org.apache.lucene.benchmark.utils;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.util.Properties;
+
+import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
+import org.apache.lucene.benchmark.byTask.feeds.NoMoreDataException;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.document.Document;
+
+/**
+ * Extract the downloaded Wikipedia dump into separate files for indexing.
+ */
+public class ExtractWikipedia {
+
+  private File outputDir;
+
+  static public int count = 0;
+
+  static final int BASE = 10;
+  protected DocMaker docMaker;
+
+  public ExtractWikipedia(DocMaker docMaker, File outputDir) {
+    this.outputDir = outputDir;
+    this.docMaker = docMaker;
+    System.out.println("Deleting all files in " + outputDir);
+    File[] files = outputDir.listFiles();
+    for (int i = 0; i < files.length; i++) {
+      files[i].delete();
+    }
+  }
+
+  public File directory(int count, File directory) {
+    if (directory == null) {
+      directory = outputDir;
+    }
+    int base = BASE;
+    while (base <= count) {
+      base *= BASE;
+    }
+    if (count < BASE) {
+      return directory;
+    }
+    directory = new File(directory, (Integer.toString(base / BASE)));
+    directory = new File(directory, (Integer.toString(count / (base / BASE))));
+    return directory(count % (base / BASE), directory);
+  }
+
+  public void create(String id, String title, String time, String body) {
+
+    File d = directory(count++, null);
+    d.mkdirs();
+    File f = new File(d, id + ".txt");
+
+    StringBuilder contents = new StringBuilder();
+
+    contents.append(time);
+    contents.append("\n\n");
+    contents.append(title);
+    contents.append("\n\n");
+    contents.append(body);
+    contents.append("\n");
+
+    try {
+      FileWriter writer = new FileWriter(f);
+      writer.write(contents.toString());
+      writer.close();
+    } catch (IOException ioe) {
+      throw new RuntimeException(ioe);
+    }
+
+  }
+
+  public void extract() throws Exception {
+    Document doc = null;
+    System.out.println("Starting Extraction");
+    long start = System.currentTimeMillis();
+    try {
+      while ((doc = docMaker.makeDocument()) != null) {
+        create(doc.get(DocMaker.ID_FIELD), doc.get(DocMaker.TITLE_FIELD), doc
+            .get(DocMaker.DATE_FIELD), doc.get(DocMaker.BODY_FIELD));
+      }
+    } catch (NoMoreDataException e) {
+      //continue
+    }
+    long finish = System.currentTimeMillis();
+    System.out.println("Extraction took " + (finish - start) + " ms");
+  }
+
+  public static void main(String[] args) throws Exception {
+
+    File wikipedia = null;
+    File outputDir = new File("./enwiki");
+    boolean keepImageOnlyDocs = true;
+    for (int i = 0; i < args.length; i++) {
+      String arg = args[i];
+      if (arg.equals("--input") || arg.equals("-i")) {
+        wikipedia = new File(args[i + 1]);
+        i++;
+      } else if (arg.equals("--output") || arg.equals("-o")) {
+        outputDir = new File(args[i + 1]);
+        i++;
+      } else if (arg.equals("--discardImageOnlyDocs") || arg.equals("-d")) {
+        keepImageOnlyDocs = false;
+      }
+
+    }
+    DocMaker docMaker = new DocMaker();
+    Properties properties = new Properties();
+    properties.setProperty("content.source", "org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource");
+    properties.setProperty("docs.file", wikipedia.getAbsolutePath());
+    properties.setProperty("content.source.forever", "false");
+    properties.setProperty("keep.image.only.docs", String.valueOf(keepImageOnlyDocs));
+    docMaker.setConfig(new Config(properties));
+    docMaker.resetInputs();
+    if (wikipedia.exists()) {
+      System.out.println("Extracting Wikipedia to: " + outputDir + " using EnwikiContentSource");
+      outputDir.mkdirs();
+      ExtractWikipedia extractor = new ExtractWikipedia(docMaker, outputDir);
+      extractor.extract();
+    } else {
+      printUsage();
+    }
+  }
+
+  private static void printUsage() {
+    System.err.println("Usage: java -cp <...> org.apache.lucene.benchmark.utils.ExtractWikipedia --input|-i <Path to Wikipedia XML file> " +
+            "[--output|-o <Output Path>] [--discardImageOnlyDocs|-d]");
+    System.err.println("--discardImageOnlyDocs tells the extractor to skip Wiki docs that contain only images");
+  }
+
+}
\ No newline at end of file
diff --git a/modules/benchmark/src/java/overview.html b/modules/benchmark/src/java/overview.html
new file mode 100644
index 0000000..c0f44cd
--- /dev/null
+++ b/modules/benchmark/src/java/overview.html
@@ -0,0 +1,26 @@
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+  <head>
+    <title>
+      benchmark
+    </title>
+  </head>
+  <body>
+  benchmark
+  </body>
+</html>
\ No newline at end of file
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/BenchmarkTestCase.java b/modules/benchmark/src/test/org/apache/lucene/benchmark/BenchmarkTestCase.java
new file mode 100644
index 0000000..4fbac12
--- /dev/null
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/BenchmarkTestCase.java
@@ -0,0 +1,100 @@
+package org.apache.lucene.benchmark;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.io.StringReader;
+
+import org.apache.lucene.benchmark.byTask.Benchmark;
+import org.apache.lucene.util.LuceneTestCase;
+
+/** Base class for all Benchmark unit tests. */
+public abstract class BenchmarkTestCase extends LuceneTestCase {
+  
+  public File getWorkDir() {
+    return TEMP_DIR;
+  }
+  
+  /** Copy a resource into the workdir */
+  public void copyToWorkDir(String resourceName) throws IOException {
+    InputStream resource = getClass().getResourceAsStream(resourceName);
+    OutputStream dest = new FileOutputStream(new File(getWorkDir(), resourceName));
+    byte[] buffer = new byte[8192];
+    int len;
+    
+    while ((len = resource.read(buffer)) > 0) {
+        dest.write(buffer, 0, len);
+    }
+
+    resource.close();
+    dest.close();
+  }
+  
+  /** Return a path, suitable for a .alg config file, for a resource in the workdir */
+  public String getWorkDirResourcePath(String resourceName) {
+    return new File(getWorkDir(), resourceName).getAbsolutePath().replace("\\", "/");
+  }
+  
+  /** Return a path, suitable for a .alg config file, for the workdir */
+  public String getWorkDirPath() {
+    return getWorkDir().getAbsolutePath().replace("\\", "/");
+  }
+  
+  // create the benchmark and execute it. 
+  public Benchmark execBenchmark(String[] algLines) throws Exception {
+    String algText = algLinesToText(algLines);
+    logTstLogic(algText);
+    Benchmark benchmark = new Benchmark(new StringReader(algText));
+    benchmark.execute();
+    return benchmark;
+  }
+  
+  // properties in effect in all tests here
+  final String propLines [] = {
+    "work.dir=" + getWorkDirPath(),
+    "directory=RAMDirectory",
+    "print.props=false",
+  };
+  
+  static final String NEW_LINE = System.getProperty("line.separator");
+  
+  // catenate alg lines to make the alg text
+  private String algLinesToText(String[] algLines) {
+    String indent = "  ";
+    StringBuilder sb = new StringBuilder();
+    for (int i = 0; i < propLines.length; i++) {
+      sb.append(indent).append(propLines[i]).append(NEW_LINE);
+    }
+    for (int i = 0; i < algLines.length; i++) {
+      sb.append(indent).append(algLines[i]).append(NEW_LINE);
+    }
+    return sb.toString();
+  }
+
+  private static void logTstLogic (String txt) {
+    if (!VERBOSE) 
+      return;
+    System.out.println("Test logic of:");
+    System.out.println(txt);
+  }
+
+}
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
new file mode 100755
index 0000000..90c970f
--- /dev/null
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
@@ -0,0 +1,1052 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.benchmark.byTask;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileReader;
+import java.io.StringReader;
+import java.text.Collator;
+import java.util.List;
+import java.util.Locale;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.benchmark.BenchmarkTestCase;
+import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
+import org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker;
+import org.apache.lucene.benchmark.byTask.stats.TaskStats;
+import org.apache.lucene.benchmark.byTask.tasks.CountingHighlighterTestTask;
+import org.apache.lucene.benchmark.byTask.tasks.CountingSearchTestTask;
+import org.apache.lucene.collation.CollationKeyAnalyzer;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.LogDocMergePolicy;
+import org.apache.lucene.index.LogMergePolicy;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.index.SerialMergeScheduler;
+import org.apache.lucene.index.TermFreqVector;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.FieldCache.DocTermsIndex;
+import org.apache.lucene.search.FieldCache;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * Test very simply that perf tasks - simple algorithms - are doing what they should.
+ */
+public class TestPerfTasksLogic extends BenchmarkTestCase {
+
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    copyToWorkDir("reuters.first20.lines.txt");
+  }
+
+  /**
+   * Test index creation logic
+   */
+  public void testIndexAndSearchTasks() throws Exception {
+    // 1. alg definition (required in every "logic" test)
+    String algLines[] = {
+        "ResetSystemErase",
+        "CreateIndex",
+        "{ AddDoc } : 1000",
+        "Optimize",
+        "CloseIndex",
+        "OpenReader",
+        "{ CountingSearchTest } : 200",
+        "CloseReader",
+        "[ CountingSearchTest > : 70",
+        "[ CountingSearchTest > : 9",
+    };
+    
+    // 2. we test this value later
+    CountingSearchTestTask.numSearches = 0;
+    
+    // 3. execute the algorithm  (required in every "logic" test)
+    Benchmark benchmark = execBenchmark(algLines);
+
+    // 4. test specific checks after the benchmark run completed.
+    assertEquals("TestSearchTask was supposed to be called!",279,CountingSearchTestTask.numSearches);
+    assertTrue("Index does not exist?...!", IndexReader.indexExists(benchmark.getRunData().getDirectory()));
+    // now we should be able to open the index for write. 
+    IndexWriter iw = new IndexWriter(benchmark.getRunData().getDirectory(),
+        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())
+            .setOpenMode(OpenMode.APPEND));
+    iw.close();
+    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
+    assertEquals("1000 docs were added to the index, this is what we expect to find!",1000,ir.numDocs());
+    ir.close();
+  }
+
+  /**
+   * Test timed sequence task.
+   */
+  public void testTimedSearchTask() throws Exception {
+    String algLines[] = {
+        "log.step=100000",
+        "ResetSystemErase",
+        "CreateIndex",
+        "{ AddDoc } : 100",
+        "Optimize",
+        "CloseIndex",
+        "OpenReader",
+        "{ CountingSearchTest } : .5s",
+        "CloseReader",
+    };
+
+    CountingSearchTestTask.numSearches = 0;
+    execBenchmark(algLines);
+    assertTrue(CountingSearchTestTask.numSearches > 0);
+    long elapsed = CountingSearchTestTask.prevLastMillis - CountingSearchTestTask.startMillis;
+    assertTrue("elapsed time was " + elapsed + " msec", elapsed <= 1500);
+  }
+
+  // disabled until we fix BG thread prio -- this test
+  // causes build to hang
+  public void testBGSearchTaskThreads() throws Exception {
+    String algLines[] = {
+        "log.time.step.msec = 100",
+        "log.step=100000",
+        "ResetSystemErase",
+        "CreateIndex",
+        "{ AddDoc } : 1000",
+        "Optimize",
+        "CloseIndex",
+        "OpenReader",
+        "{",
+        "  [ \"XSearch\" { CountingSearchTest > : * ] : 2 &-1",
+        "  Wait(0.5)",
+        "}",
+        "CloseReader",
+        "RepSumByPref X"
+    };
+
+    CountingSearchTestTask.numSearches = 0;
+    execBenchmark(algLines);
+    assertTrue(CountingSearchTestTask.numSearches > 0);
+  }
+
+  public void testHighlighting() throws Exception {
+    // 1. alg definition (required in every "logic" test)
+    String algLines[] = {
+        "doc.stored=true",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters20LinesFile(),
+        "query.maker=" + ReutersQueryMaker.class.getName(),
+        "ResetSystemErase",
+        "CreateIndex",
+        "{ AddDoc } : 100",
+        "Optimize",
+        "CloseIndex",
+        "OpenReader(true)",
+        "{ CountingHighlighterTest(size[1],highlight[1],mergeContiguous[true],maxFrags[1],fields[body]) } : 200",
+        "CloseReader",
+    };
+
+    // 2. we test this value later
+    CountingHighlighterTestTask.numHighlightedResults = 0;
+    CountingHighlighterTestTask.numDocsRetrieved = 0;
+    // 3. execute the algorithm  (required in every "logic" test)
+    Benchmark benchmark = execBenchmark(algLines);
+
+    // 4. test specific checks after the benchmark run completed.
+    assertEquals("TestSearchTask was supposed to be called!",92,CountingHighlighterTestTask.numDocsRetrieved);
+    //pretty hard to figure out a priori how many docs are going to have highlighted fragments returned, but we can never have more than the number of docs
+    //we probably should use a different doc/query maker, but...
+    assertTrue("TestSearchTask was supposed to be called!", CountingHighlighterTestTask.numDocsRetrieved >= CountingHighlighterTestTask.numHighlightedResults && CountingHighlighterTestTask.numHighlightedResults > 0);
+
+    assertTrue("Index does not exist?...!", IndexReader.indexExists(benchmark.getRunData().getDirectory()));
+    // now we should be able to open the index for write.
+    IndexWriter iw = new IndexWriter(benchmark.getRunData().getDirectory(), new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));
+    iw.close();
+    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
+    assertEquals("100 docs were added to the index, this is what we expect to find!",100,ir.numDocs());
+    ir.close();
+  }
+
+  public void testHighlightingTV() throws Exception {
+    // 1. alg definition (required in every "logic" test)
+    String algLines[] = {
+        "doc.stored=true",//doc storage is required in order to have text to highlight
+        "doc.term.vector.offsets=true",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters20LinesFile(),
+        "query.maker=" + ReutersQueryMaker.class.getName(),
+        "ResetSystemErase",
+        "CreateIndex",
+        "{ AddDoc } : 1000",
+        "Optimize",
+        "CloseIndex",
+        "OpenReader(false)",
+        "{ CountingHighlighterTest(size[1],highlight[1],mergeContiguous[true],maxFrags[1],fields[body]) } : 200",
+        "CloseReader",
+    };
+
+    // 2. we test this value later
+    CountingHighlighterTestTask.numHighlightedResults = 0;
+    CountingHighlighterTestTask.numDocsRetrieved = 0;
+    // 3. execute the algorithm  (required in every "logic" test)
+    Benchmark benchmark = execBenchmark(algLines);
+
+    // 4. test specific checks after the benchmark run completed.
+    assertEquals("TestSearchTask was supposed to be called!",92,CountingHighlighterTestTask.numDocsRetrieved);
+    //pretty hard to figure out a priori how many docs are going to have highlighted fragments returned, but we can never have more than the number of docs
+    //we probably should use a different doc/query maker, but...
+    assertTrue("TestSearchTask was supposed to be called!", CountingHighlighterTestTask.numDocsRetrieved >= CountingHighlighterTestTask.numHighlightedResults && CountingHighlighterTestTask.numHighlightedResults > 0);
+
+    assertTrue("Index does not exist?...!", IndexReader.indexExists(benchmark.getRunData().getDirectory()));
+    // now we should be able to open the index for write.
+    IndexWriter iw = new IndexWriter(benchmark.getRunData().getDirectory(), new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));
+    iw.close();
+    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
+    assertEquals("1000 docs were added to the index, this is what we expect to find!",1000,ir.numDocs());
+    ir.close();
+  }
+
+  public void testHighlightingNoTvNoStore() throws Exception {
+    // 1. alg definition (required in every "logic" test)
+    String algLines[] = {
+        "doc.stored=false",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters20LinesFile(),
+        "query.maker=" + ReutersQueryMaker.class.getName(),
+        "ResetSystemErase",
+        "CreateIndex",
+        "{ AddDoc } : 1000",
+        "Optimize",
+        "CloseIndex",
+        "OpenReader",
+        "{ CountingHighlighterTest(size[1],highlight[1],mergeContiguous[true],maxFrags[1],fields[body]) } : 200",
+        "CloseReader",
+    };
+
+    // 2. we test this value later
+    CountingHighlighterTestTask.numHighlightedResults = 0;
+    CountingHighlighterTestTask.numDocsRetrieved = 0;
+    // 3. execute the algorithm  (required in every "logic" test)
+    try {
+      Benchmark benchmark = execBenchmark(algLines);
+      assertTrue("CountingHighlighterTest should have thrown an exception", false);
+      assertNotNull(benchmark); // (avoid compile warning on unused variable)
+    } catch (Exception e) {
+      assertTrue(true);
+    }
+  }
+
+  /**
+   * Test Exhasting Doc Maker logic
+   */
+  public void testExhaustContentSource() throws Exception {
+    // 1. alg definition (required in every "logic" test)
+    String algLines[] = {
+        "# ----- properties ",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource",
+        "content.source.log.step=1",
+        "doc.term.vector=false",
+        "content.source.forever=false",
+        "directory=RAMDirectory",
+        "doc.stored=false",
+        "doc.tokenized=false",
+        "# ----- alg ",
+        "CreateIndex",
+        "{ AddDoc } : * ",
+        "Optimize",
+        "CloseIndex",
+        "OpenReader",
+        "{ CountingSearchTest } : 100",
+        "CloseReader",
+        "[ CountingSearchTest > : 30",
+        "[ CountingSearchTest > : 9",
+    };
+    
+    // 2. we test this value later
+    CountingSearchTestTask.numSearches = 0;
+    
+    // 3. execute the algorithm  (required in every "logic" test)
+    Benchmark benchmark = execBenchmark(algLines);
+
+    // 4. test specific checks after the benchmark run completed.
+    assertEquals("TestSearchTask was supposed to be called!",139,CountingSearchTestTask.numSearches);
+    assertTrue("Index does not exist?...!", IndexReader.indexExists(benchmark.getRunData().getDirectory()));
+    // now we should be able to open the index for write. 
+    IndexWriter iw = new IndexWriter(benchmark.getRunData().getDirectory(), new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));
+    iw.close();
+    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
+    assertEquals("1 docs were added to the index, this is what we expect to find!",1,ir.numDocs());
+    ir.close();
+  }
+
+  // LUCENE-1994: test thread safety of SortableSingleDocMaker
+  public void testDocMakerThreadSafety() throws Exception {
+    // 1. alg definition (required in every "logic" test)
+    String algLines[] = {
+        "# ----- properties ",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.SortableSingleDocSource",
+        "doc.term.vector=false",
+        "log.step.AddDoc=10000",
+        "content.source.forever=true",
+        "directory=RAMDirectory",
+        "doc.reuse.fields=false",
+        "doc.stored=false",
+        "doc.tokenized=false",
+        "doc.index.props=true",
+        "# ----- alg ",
+        "CreateIndex",
+        "[ { AddDoc > : 250 ] : 4",
+        "CloseIndex",
+    };
+    
+    // 2. we test this value later
+    CountingSearchTestTask.numSearches = 0;
+    
+    // 3. execute the algorithm  (required in every "logic" test)
+    Benchmark benchmark = execBenchmark(algLines);
+
+    IndexReader r = IndexReader.open(benchmark.getRunData().getDirectory(), true);
+    DocTermsIndex idx = FieldCache.DEFAULT.getTermsIndex(r, "country");
+    final int maxDoc = r.maxDoc();
+    assertEquals(1000, maxDoc);
+    BytesRef br = new BytesRef();
+    for(int i=0;i<1000;i++) {
+      assertNotNull("doc " + i + " has null country", idx.getTerm(i, br));
+    }
+    r.close();
+  }
+
+  /**
+   * Test Parallel Doc Maker logic (for LUCENE-940)
+   */
+  public void testParallelDocMaker() throws Exception {
+    // 1. alg definition (required in every "logic" test)
+    String algLines[] = {
+        "# ----- properties ",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters20LinesFile(),
+        "content.source.log.step=3",
+        "doc.term.vector=false",
+        "content.source.forever=false",
+        "directory=FSDirectory",
+        "doc.stored=false",
+        "doc.tokenized=false",
+        "# ----- alg ",
+        "CreateIndex",
+        "[ { AddDoc } : * ] : 4 ",
+        "CloseIndex",
+    };
+    
+    // 2. execute the algorithm  (required in every "logic" test)
+    Benchmark benchmark = execBenchmark(algLines);
+
+    // 3. test number of docs in the index
+    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
+    int ndocsExpected = 20; // first 20 reuters docs.
+    assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
+    ir.close();
+  }
+
+  /**
+   * Test WriteLineDoc and LineDocSource.
+   */
+  public void testLineDocFile() throws Exception {
+    File lineFile = new File(TEMP_DIR, "test.reuters.lines.txt");
+
+    // We will call WriteLineDocs this many times
+    final int NUM_TRY_DOCS = 50;
+
+    // Creates a line file with first 50 docs from SingleDocSource
+    String algLines1[] = {
+      "# ----- properties ",
+      "content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource",
+      "content.source.forever=true",
+      "line.file.out=" + lineFile.getAbsolutePath().replace('\\', '/'),
+      "# ----- alg ",
+      "{WriteLineDoc()}:" + NUM_TRY_DOCS,
+    };
+
+    // Run algo
+    Benchmark benchmark = execBenchmark(algLines1);
+
+    BufferedReader r = new BufferedReader(new FileReader(lineFile));
+    int numLines = 0;
+    while(r.readLine() != null)
+      numLines++;
+    r.close();
+    assertEquals("did not see the right number of docs; should be " + NUM_TRY_DOCS + " but was " + numLines, NUM_TRY_DOCS, numLines);
+    
+    // Index the line docs
+    String algLines2[] = {
+      "# ----- properties ",
+      "analyzer=org.apache.lucene.analysis.MockAnalyzer",
+      "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+      "docs.file=" + lineFile.getAbsolutePath().replace('\\', '/'),
+      "content.source.forever=false",
+      "doc.reuse.fields=false",
+      "ram.flush.mb=4",
+      "# ----- alg ",
+      "ResetSystemErase",
+      "CreateIndex",
+      "{AddDoc}: *",
+      "CloseIndex",
+    };
+    
+    // Run algo
+    benchmark = execBenchmark(algLines2);
+
+    // now we should be able to open the index for write. 
+    IndexWriter iw = new IndexWriter(benchmark.getRunData().getDirectory(),
+        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())
+            .setOpenMode(OpenMode.APPEND));
+    iw.close();
+
+    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
+    assertEquals(numLines + " lines were created but " + ir.numDocs() + " docs are in the index", numLines, ir.numDocs());
+    ir.close();
+
+    lineFile.delete();
+  }
+  
+  /**
+   * Test ReadTokensTask
+   */
+  public void testReadTokens() throws Exception {
+
+    // We will call ReadTokens on this many docs
+    final int NUM_DOCS = 20;
+
+    // Read tokens from first NUM_DOCS docs from Reuters and
+    // then build index from the same docs
+    String algLines1[] = {
+      "# ----- properties ",
+      "analyzer=org.apache.lucene.analysis.MockAnalyzer",
+      "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+      "docs.file=" + getReuters20LinesFile(),
+      "# ----- alg ",
+      "{ReadTokens}: " + NUM_DOCS,
+      "ResetSystemErase",
+      "CreateIndex",
+      "{AddDoc}: " + NUM_DOCS,
+      "CloseIndex",
+    };
+
+    // Run algo
+    Benchmark benchmark = execBenchmark(algLines1);
+
+    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();
+
+    // Count how many tokens all ReadTokens saw
+    int totalTokenCount1 = 0;
+    for (final TaskStats stat : stats) {
+      if (stat.getTask().getName().equals("ReadTokens")) {
+        totalTokenCount1 += stat.getCount();
+      }
+    }
+
+    // Separately count how many tokens are actually in the index:
+    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);
+    assertEquals(NUM_DOCS, reader.numDocs());
+
+    int totalTokenCount2 = 0;
+
+    FieldsEnum fields = MultiFields.getFields(reader).iterator();
+    String fieldName = null;
+    while((fieldName = fields.next()) != null) {
+      if (fieldName == DocMaker.ID_FIELD || fieldName == DocMaker.DATE_MSEC_FIELD || fieldName == DocMaker.TIME_SEC_FIELD) {
+        continue;
+      }
+      TermsEnum terms = fields.terms();
+      DocsEnum docs = null;
+      while(terms.next() != null) {
+        docs = terms.docs(MultiFields.getDeletedDocs(reader), docs);
+        while(docs.nextDoc() != docs.NO_MORE_DOCS) {
+          totalTokenCount2 += docs.freq();
+        }
+      }
+    }
+    reader.close();
+
+    // Make sure they are the same
+    assertEquals(totalTokenCount1, totalTokenCount2);
+  }
+  
+  /**
+   * Test that " {[AddDoc(4000)]: 4} : * " works corrcetly (for LUCENE-941)
+   */
+  public void testParallelExhausted() throws Exception {
+    // 1. alg definition (required in every "logic" test)
+    String algLines[] = {
+        "# ----- properties ",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters20LinesFile(),
+        "content.source.log.step=3",
+        "doc.term.vector=false",
+        "content.source.forever=false",
+        "directory=RAMDirectory",
+        "doc.stored=false",
+        "doc.tokenized=false",
+        "task.max.depth.log=1",
+        "# ----- alg ",
+        "CreateIndex",
+        "{ [ AddDoc]: 4} : * ",
+        "ResetInputs ",
+        "{ [ AddDoc]: 4} : * ",
+        "CloseIndex",
+    };
+    
+    // 2. execute the algorithm  (required in every "logic" test)
+    Benchmark benchmark = execBenchmark(algLines);
+
+    // 3. test number of docs in the index
+    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
+    int ndocsExpected = 2 * 20; // first 20 reuters docs.
+    assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
+    ir.close();
+  }
+
+
+  /**
+   * Test that exhaust in loop works as expected (LUCENE-1115).
+   */
+  public void testExhaustedLooped() throws Exception {
+    // 1. alg definition (required in every "logic" test)
+    String algLines[] = {
+        "# ----- properties ",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters20LinesFile(),
+        "content.source.log.step=3",
+        "doc.term.vector=false",
+        "content.source.forever=false",
+        "directory=RAMDirectory",
+        "doc.stored=false",
+        "doc.tokenized=false",
+        "task.max.depth.log=1",
+        "# ----- alg ",
+        "{ \"Rounds\"",
+        "  ResetSystemErase",
+        "  CreateIndex",
+        "  { \"AddDocs\"  AddDoc > : * ",
+        "  CloseIndex",
+        "} : 2",
+    };
+    
+    // 2. execute the algorithm  (required in every "logic" test)
+    Benchmark benchmark = execBenchmark(algLines);
+
+    // 3. test number of docs in the index
+    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
+    int ndocsExpected = 20;  // first 20 reuters docs.
+    assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
+    ir.close();
+  }
+  
+  /**
+   * Test that we can close IndexWriter with argument "false".
+   */
+  public void testCloseIndexFalse() throws Exception {
+    // 1. alg definition (required in every "logic" test)
+    String algLines[] = {
+        "# ----- properties ",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters20LinesFile(),
+        "ram.flush.mb=-1",
+        "max.buffered=2",
+        "content.source.log.step=3",
+        "doc.term.vector=false",
+        "content.source.forever=false",
+        "directory=RAMDirectory",
+        "doc.stored=false",
+        "doc.tokenized=false",
+        "debug.level=1",
+        "# ----- alg ",
+        "{ \"Rounds\"",
+        "  ResetSystemErase",
+        "  CreateIndex",
+        "  { \"AddDocs\"  AddDoc > : * ",
+        "  CloseIndex(false)",
+        "} : 2",
+    };
+    
+    // 2. execute the algorithm  (required in every "logic" test)
+    Benchmark benchmark = execBenchmark(algLines);
+
+    // 3. test number of docs in the index
+    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
+    int ndocsExpected = 20; // first 20 reuters docs.
+    assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
+    ir.close();
+  }
+
+  public static class MyMergeScheduler extends SerialMergeScheduler {
+    boolean called;
+    public MyMergeScheduler() {
+      super();
+      called = true;
+    }
+  }
+
+  public void testDeleteByPercent() throws Exception {
+    // 1. alg definition (required in every "logic" test)
+    String algLines[] = {
+        "# ----- properties ",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters20LinesFile(),
+        "ram.flush.mb=-1",
+        "max.buffered=2",
+        "content.source.log.step=3",
+        "doc.term.vector=false",
+        "content.source.forever=false",
+        "directory=RAMDirectory",
+        "doc.stored=false",
+        "doc.tokenized=false",
+        "debug.level=1",
+        "# ----- alg ",
+        "CreateIndex",
+        "{ \"AddDocs\"  AddDoc > : * ",
+        "CloseIndex()",
+        "OpenReader(false)",
+        "DeleteByPercent(20)",
+        "CloseReader"
+    };
+    
+    // 2. execute the algorithm  (required in every "logic" test)
+    Benchmark benchmark = execBenchmark(algLines);
+
+    // 3. test number of docs in the index
+    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
+    int ndocsExpected = 16; // first 20 reuters docs, minus 20%
+    assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
+    ir.close();
+  }
+
+  /**
+   * Test that we can set merge scheduler".
+   */
+  public void testMergeScheduler() throws Exception {
+    // 1. alg definition (required in every "logic" test)
+    String algLines[] = {
+        "# ----- properties ",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters20LinesFile(),
+        "content.source.log.step=3",
+        "doc.term.vector=false",
+        "content.source.forever=false",
+        "directory=RAMDirectory",
+        "merge.scheduler=" + MyMergeScheduler.class.getName(),
+        "doc.stored=false",
+        "doc.tokenized=false",
+        "debug.level=1",
+        "# ----- alg ",
+        "{ \"Rounds\"",
+        "  ResetSystemErase",
+        "  CreateIndex",
+        "  { \"AddDocs\"  AddDoc > : * ",
+        "} : 2",
+    };
+    // 2. execute the algorithm  (required in every "logic" test)
+    Benchmark benchmark = execBenchmark(algLines);
+
+    assertTrue("did not use the specified MergeScheduler",
+        ((MyMergeScheduler) benchmark.getRunData().getIndexWriter().getConfig()
+            .getMergeScheduler()).called);
+    benchmark.getRunData().getIndexWriter().close();
+
+    // 3. test number of docs in the index
+    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
+    int ndocsExpected = 20; // first 20 reuters docs.
+    assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
+    ir.close();
+  }
+
+  public static class MyMergePolicy extends LogDocMergePolicy {
+    boolean called;
+    public MyMergePolicy() {
+      called = true;
+    }
+  }
+  
+  /**
+   * Test that we can set merge policy".
+   */
+  public void testMergePolicy() throws Exception {
+    // 1. alg definition (required in every "logic" test)
+    String algLines[] = {
+        "# ----- properties ",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters20LinesFile(),
+        "content.source.log.step=3",
+        "ram.flush.mb=-1",
+        "max.buffered=2",
+        "doc.term.vector=false",
+        "content.source.forever=false",
+        "directory=RAMDirectory",
+        "merge.policy=" + MyMergePolicy.class.getName(),
+        "doc.stored=false",
+        "doc.tokenized=false",
+        "debug.level=1",
+        "# ----- alg ",
+        "{ \"Rounds\"",
+        "  ResetSystemErase",
+        "  CreateIndex",
+        "  { \"AddDocs\"  AddDoc > : * ",
+        "} : 2",
+    };
+
+    // 2. execute the algorithm  (required in every "logic" test)
+    Benchmark benchmark = execBenchmark(algLines);
+    assertTrue("did not use the specified MergePolicy", ((MyMergePolicy) benchmark.getRunData().getIndexWriter().getConfig().getMergePolicy()).called);
+    benchmark.getRunData().getIndexWriter().close();
+    
+    // 3. test number of docs in the index
+    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
+    int ndocsExpected = 20; // first 20 reuters docs.
+    assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
+    ir.close();
+  }
+
+  /**
+   * Test that IndexWriter settings stick.
+   */
+  public void testIndexWriterSettings() throws Exception {
+    // 1. alg definition (required in every "logic" test)
+    String algLines[] = {
+        "# ----- properties ",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters20LinesFile(),
+        "content.source.log.step=3",
+        "ram.flush.mb=-1",
+        "max.buffered=2",
+        "compound=cmpnd:true:false",
+        "doc.term.vector=vector:false:true",
+        "content.source.forever=false",
+        "directory=RAMDirectory",
+        "doc.stored=false",
+        "merge.factor=3",
+        "doc.tokenized=false",
+        "debug.level=1",
+        "# ----- alg ",
+        "{ \"Rounds\"",
+        "  ResetSystemErase",
+        "  CreateIndex",
+        "  { \"AddDocs\"  AddDoc > : * ",
+        "  NewRound",
+        "} : 2",
+    };
+
+    // 2. execute the algorithm  (required in every "logic" test)
+    Benchmark benchmark = execBenchmark(algLines);
+    final IndexWriter writer = benchmark.getRunData().getIndexWriter();
+    assertEquals(2, writer.getConfig().getMaxBufferedDocs());
+    assertEquals(IndexWriterConfig.DISABLE_AUTO_FLUSH, (int) writer.getConfig().getRAMBufferSizeMB());
+    assertEquals(3, ((LogMergePolicy) writer.getConfig().getMergePolicy()).getMergeFactor());
+    assertFalse(((LogMergePolicy) writer.getConfig().getMergePolicy()).getUseCompoundFile());
+    writer.close();
+    Directory dir = benchmark.getRunData().getDirectory();
+    IndexReader reader = IndexReader.open(dir, true);
+    TermFreqVector [] tfv = reader.getTermFreqVectors(0);
+    assertNotNull(tfv);
+    assertTrue(tfv.length > 0);
+    reader.close();
+  }
+
+  /**
+   * Test that we can call optimize(maxNumSegments).
+   */
+  public void testOptimizeMaxNumSegments() throws Exception {
+    // 1. alg definition (required in every "logic" test)
+    String algLines[] = {
+        "# ----- properties ",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters20LinesFile(),
+        "content.source.log.step=3",
+        "ram.flush.mb=-1",
+        "max.buffered=3",
+        "doc.term.vector=false",
+        "content.source.forever=false",
+        "directory=RAMDirectory",
+        "merge.policy=org.apache.lucene.index.LogDocMergePolicy",
+        "doc.stored=false",
+        "doc.tokenized=false",
+        "debug.level=1",
+        "# ----- alg ",
+        "{ \"Rounds\"",
+        "  ResetSystemErase",
+        "  CreateIndex",
+        "  { \"AddDocs\"  AddDoc > : * ",
+        "  Optimize(3)",
+        "  CloseIndex()",
+        "} : 2",
+    };
+    
+    // 2. execute the algorithm  (required in every "logic" test)
+    Benchmark benchmark = execBenchmark(algLines);
+
+    // 3. test number of docs in the index
+    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);
+    int ndocsExpected = 20; // first 20 reuters docs.
+    assertEquals("wrong number of docs in the index!", ndocsExpected, ir.numDocs());
+    ir.close();
+
+    // Make sure we have 3 segments:
+    SegmentInfos infos = new SegmentInfos();
+    infos.read(benchmark.getRunData().getDirectory());
+    assertEquals(3, infos.size());
+  }
+  
+  /**
+   * Test disabling task count (LUCENE-1136).
+   */
+  public void testDisableCounting() throws Exception {
+    doTestDisableCounting(true);
+    doTestDisableCounting(false);
+  }
+
+  private void doTestDisableCounting(boolean disable) throws Exception {
+    // 1. alg definition (required in every "logic" test)
+    String algLines[] = disableCountingLines(disable);
+    
+    // 2. execute the algorithm  (required in every "logic" test)
+    Benchmark benchmark = execBenchmark(algLines);
+
+    // 3. test counters
+    int n = disable ? 0 : 1;
+    int nChecked = 0;
+    for (final TaskStats stats : benchmark.getRunData().getPoints().taskStats()) {
+      String taskName = stats.getTask().getName();
+      if (taskName.equals("Rounds")) {
+        assertEquals("Wrong total count!",20+2*n,stats.getCount());
+        nChecked++;
+      } else if (taskName.equals("CreateIndex")) {
+        assertEquals("Wrong count for CreateIndex!",n,stats.getCount());
+        nChecked++;
+      } else if (taskName.equals("CloseIndex")) {
+        assertEquals("Wrong count for CloseIndex!",n,stats.getCount());
+        nChecked++;
+      }
+    }
+    assertEquals("Missing some tasks to check!",3,nChecked);
+  }
+
+  private String[] disableCountingLines (boolean disable) {
+    String dis = disable ? "-" : "";
+    return new String[] {
+        "# ----- properties ",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters20LinesFile(),
+        "content.source.log.step=30",
+        "doc.term.vector=false",
+        "content.source.forever=false",
+        "directory=RAMDirectory",
+        "doc.stored=false",
+        "doc.tokenized=false",
+        "task.max.depth.log=1",
+        "# ----- alg ",
+        "{ \"Rounds\"",
+        "  ResetSystemErase",
+        "  "+dis+"CreateIndex",            // optionally disable counting here
+        "  { \"AddDocs\"  AddDoc > : * ",
+        "  "+dis+"  CloseIndex",             // optionally disable counting here (with extra blanks)
+        "}",
+        "RepSumByName",
+    };
+  }
+
+  /**
+   * Test that we can change the Locale in the runData,
+   * that it is parsed as we expect.
+   */
+  public void testLocale() throws Exception {
+    // empty Locale: clear it (null)
+    Benchmark benchmark = execBenchmark(getLocaleConfig(""));
+    assertNull(benchmark.getRunData().getLocale());
+
+    // ROOT locale
+    benchmark = execBenchmark(getLocaleConfig("ROOT"));
+    assertEquals(new Locale(""), benchmark.getRunData().getLocale());
+    
+    // specify just a language 
+    benchmark = execBenchmark(getLocaleConfig("de"));
+    assertEquals(new Locale("de"), benchmark.getRunData().getLocale());
+    
+    // specify language + country
+    benchmark = execBenchmark(getLocaleConfig("en,US"));
+    assertEquals(new Locale("en", "US"), benchmark.getRunData().getLocale());
+    
+    // specify language + country + variant
+    benchmark = execBenchmark(getLocaleConfig("no,NO,NY"));
+    assertEquals(new Locale("no", "NO", "NY"), benchmark.getRunData().getLocale());
+  }
+   
+  private String[] getLocaleConfig(String localeParam) {
+    String algLines[] = {
+        "# ----- properties ",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters20LinesFile(),
+        "content.source.log.step=3",
+        "content.source.forever=false",
+        "directory=RAMDirectory",
+        "# ----- alg ",
+        "{ \"Rounds\"",
+        "  ResetSystemErase",
+        "  NewLocale(" + localeParam + ")",
+        "  CreateIndex",
+        "  { \"AddDocs\"  AddDoc > : * ",
+        "  NewRound",
+        "} : 1",
+    };
+    return algLines;
+  }
+  
+  /**
+   * Test that we can create CollationAnalyzers.
+   */
+  public void testCollator() throws Exception {
+    // ROOT locale
+    Benchmark benchmark = execBenchmark(getCollatorConfig("ROOT", "impl:jdk"));
+    CollationKeyAnalyzer expected = new CollationKeyAnalyzer(Collator
+        .getInstance(new Locale("")));
+    assertEqualCollation(expected, benchmark.getRunData().getAnalyzer(), "foobar");
+    
+    // specify just a language
+    benchmark = execBenchmark(getCollatorConfig("de", "impl:jdk"));
+    expected = new CollationKeyAnalyzer(Collator.getInstance(new Locale("de")));
+    assertEqualCollation(expected, benchmark.getRunData().getAnalyzer(), "foobar");
+    
+    // specify language + country
+    benchmark = execBenchmark(getCollatorConfig("en,US", "impl:jdk"));
+    expected = new CollationKeyAnalyzer(Collator.getInstance(new Locale("en",
+        "US")));
+    assertEqualCollation(expected, benchmark.getRunData().getAnalyzer(), "foobar");
+    
+    // specify language + country + variant
+    benchmark = execBenchmark(getCollatorConfig("no,NO,NY", "impl:jdk"));
+    expected = new CollationKeyAnalyzer(Collator.getInstance(new Locale("no",
+        "NO", "NY")));
+    assertEqualCollation(expected, benchmark.getRunData().getAnalyzer(), "foobar");
+  }
+  
+  private void assertEqualCollation(Analyzer a1, Analyzer a2, String text)
+      throws Exception {
+    TokenStream ts1 = a1.tokenStream("bogus", new StringReader(text));
+    TokenStream ts2 = a2.tokenStream("bogus", new StringReader(text));
+    ts1.reset();
+    ts2.reset();
+    CharTermAttribute termAtt1 = ts1.addAttribute(CharTermAttribute.class);
+    CharTermAttribute termAtt2 = ts2.addAttribute(CharTermAttribute.class);
+    assertTrue(ts1.incrementToken());
+    assertTrue(ts2.incrementToken());
+    assertEquals(termAtt1.toString(), termAtt2.toString());
+    assertFalse(ts1.incrementToken());
+    assertFalse(ts2.incrementToken());
+    ts1.close();
+    ts2.close();
+  }
+  
+  private String[] getCollatorConfig(String localeParam, 
+      String collationParam) {
+    String algLines[] = {
+        "# ----- properties ",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters20LinesFile(),
+        "content.source.log.step=3",
+        "content.source.forever=false",
+        "directory=RAMDirectory",
+        "# ----- alg ",
+        "{ \"Rounds\"",
+        "  ResetSystemErase",
+        "  NewLocale(" + localeParam + ")",
+        "  NewCollationAnalyzer(" + collationParam + ")",
+        "  CreateIndex",
+        "  { \"AddDocs\"  AddDoc > : * ",
+        "  NewRound",
+        "} : 1",
+    };
+    return algLines;
+  }
+  
+  /**
+   * Test that we can create ShingleAnalyzerWrappers.
+   */
+  public void testShingleAnalyzer() throws Exception {
+    String text = "one,two,three, four five six";
+    
+    // Default analyzer, maxShingleSize, and outputUnigrams
+    Benchmark benchmark = execBenchmark(getShingleConfig(""));
+    benchmark.getRunData().getAnalyzer().tokenStream
+      ("bogus", new StringReader(text)).close();
+    assertEqualShingle(benchmark.getRunData().getAnalyzer(), text,
+                       new String[] {"one", "one two", "two", "two three",
+                                     "three", "three four", "four", "four five",
+                                     "five", "five six", "six"});
+    // Default analyzer, maxShingleSize = 3, and outputUnigrams = false
+    benchmark = execBenchmark
+      (getShingleConfig("maxShingleSize:3,outputUnigrams:false"));
+    assertEqualShingle(benchmark.getRunData().getAnalyzer(), text,
+                       new String[] { "one two", "one two three", "two three",
+                                      "two three four", "three four", 
+                                      "three four five", "four five",
+                                      "four five six", "five six" });
+    // MockAnalyzer, default maxShingleSize and outputUnigrams
+    benchmark = execBenchmark
+      (getShingleConfig("analyzer:MockAnalyzer"));
+    assertEqualShingle(benchmark.getRunData().getAnalyzer(), text,
+                       new String[] { "one,two,three,", "one,two,three, four",
+                                      "four", "four five", "five", "five six", 
+                                      "six" });
+    
+    // MockAnalyzer, maxShingleSize=3 and outputUnigrams=false
+    benchmark = execBenchmark
+      (getShingleConfig
+        ("outputUnigrams:false,maxShingleSize:3,analyzer:MockAnalyzer"));
+    assertEqualShingle(benchmark.getRunData().getAnalyzer(), text,
+                       new String[] { "one,two,three, four", 
+                                      "one,two,three, four five",
+                                      "four five", "four five six",
+                                      "five six" });
+  }
+  
+  private void assertEqualShingle
+    (Analyzer analyzer, String text, String[] expected) throws Exception {
+    BaseTokenStreamTestCase.assertAnalyzesTo(analyzer, text, expected);
+  }
+  
+  private String[] getShingleConfig(String params) { 
+    String algLines[] = {
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters20LinesFile(),
+        "content.source.forever=false",
+        "directory=RAMDirectory",
+        "NewShingleAnalyzer(" + params + ")",
+        "CreateIndex",
+        "{ \"AddDocs\"  AddDoc > : * "
+    };
+    return algLines;
+  }
+  
+  private String getReuters20LinesFile() {
+    return getWorkDirResourcePath("reuters.first20.lines.txt");
+  }  
+}
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksParse.java b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksParse.java
new file mode 100755
index 0000000..bc4bc90
--- /dev/null
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksParse.java
@@ -0,0 +1,80 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.benchmark.byTask;
+
+import java.io.StringReader;
+import java.util.ArrayList;
+
+import org.apache.lucene.benchmark.byTask.tasks.PerfTask;
+import org.apache.lucene.benchmark.byTask.tasks.TaskSequence;
+import org.apache.lucene.benchmark.byTask.utils.Algorithm;
+import org.apache.lucene.util.LuceneTestCase;
+
+/** Test very simply that perf tasks are parses as expected. */
+public class TestPerfTasksParse extends LuceneTestCase {
+
+  static final String NEW_LINE = System.getProperty("line.separator");
+  static final String INDENT = "  ";
+
+  // properties in effect in all tests here
+  static final String propPart = 
+    INDENT + "directory=RAMDirectory" + NEW_LINE +
+    INDENT + "print.props=false" + NEW_LINE
+  ;
+
+  /** Test the repetiotion parsing for parallel tasks */
+  public void testParseParallelTaskSequenceRepetition() throws Exception {
+    String taskStr = "AddDoc";
+    String parsedTasks = "[ "+taskStr+" ] : 1000";
+    Benchmark benchmark = new Benchmark(new StringReader(propPart+parsedTasks));
+    Algorithm alg = benchmark.getAlgorithm();
+    ArrayList<PerfTask> algTasks = alg.extractTasks();
+    boolean foundAdd = false;
+    for (final PerfTask task : algTasks) {
+       if (task.toString().indexOf(taskStr)>=0) {
+          foundAdd = true;
+       }
+       if (task instanceof TaskSequence) {
+         assertEquals("repetions should be 1000 for "+parsedTasks, 1000, ((TaskSequence) task).getRepetitions());
+         assertTrue("sequence for "+parsedTasks+" should be parallel!", ((TaskSequence) task).isParallel());
+       }
+       assertTrue("Task "+taskStr+" was not found in "+alg.toString(),foundAdd);
+    }
+  }
+
+  /** Test the repetiotion parsing for sequential  tasks */
+  public void testParseTaskSequenceRepetition() throws Exception {
+    String taskStr = "AddDoc";
+    String parsedTasks = "{ "+taskStr+" } : 1000";
+    Benchmark benchmark = new Benchmark(new StringReader(propPart+parsedTasks));
+    Algorithm alg = benchmark.getAlgorithm();
+    ArrayList<PerfTask> algTasks = alg.extractTasks();
+    boolean foundAdd = false;
+    for (final PerfTask task : algTasks) {
+       if (task.toString().indexOf(taskStr)>=0) {
+          foundAdd = true;
+       }
+       if (task instanceof TaskSequence) {
+         assertEquals("repetions should be 1000 for "+parsedTasks, 1000, ((TaskSequence) task).getRepetitions());
+         assertFalse("sequence for "+parsedTasks+" should be sequential!", ((TaskSequence) task).isParallel());
+       }
+       assertTrue("Task "+taskStr+" was not found in "+alg.toString(),foundAdd);
+    }
+  }
+
+}
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/DocMakerTest.java b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/DocMakerTest.java
new file mode 100644
index 0000000..5ee7b13
--- /dev/null
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/DocMakerTest.java
@@ -0,0 +1,163 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Properties;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.benchmark.BenchmarkTestCase;
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.tasks.AddDocTask;
+import org.apache.lucene.benchmark.byTask.tasks.CloseIndexTask;
+import org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask;
+import org.apache.lucene.benchmark.byTask.tasks.TaskSequence;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+
+/** Tests the functionality of {@link DocMaker}. */
+public class DocMakerTest extends BenchmarkTestCase {
+
+  static final class OneDocSource extends ContentSource {
+
+    private boolean finish = false;
+    
+    @Override
+    public void close() throws IOException {
+    }
+
+    @Override
+    public DocData getNextDocData(DocData docData) throws NoMoreDataException,
+        IOException {
+      if (finish) {
+        throw new NoMoreDataException();
+      }
+      
+      docData.setBody("body");
+      docData.setDate("date");
+      docData.setTitle("title");
+      Properties props = new Properties();
+      props.setProperty("key", "value");
+      docData.setProps(props);
+      finish = true;
+      
+      return docData;
+    }
+    
+  }
+
+  private void doTestIndexProperties(boolean setIndexProps,
+      boolean indexPropsVal, int numExpectedResults) throws Exception {
+    Properties props = new Properties();
+    
+    // Indexing configuration.
+    props.setProperty("analyzer", MockAnalyzer.class.getName());
+    props.setProperty("content.source", OneDocSource.class.getName());
+    props.setProperty("directory", "RAMDirectory");
+    if (setIndexProps) {
+      props.setProperty("doc.index.props", Boolean.toString(indexPropsVal));
+    }
+    
+    // Create PerfRunData
+    Config config = new Config(props);
+    PerfRunData runData = new PerfRunData(config);
+
+    TaskSequence tasks = new TaskSequence(runData, getName(), null, false);
+    tasks.addTask(new CreateIndexTask(runData));
+    tasks.addTask(new AddDocTask(runData));
+    tasks.addTask(new CloseIndexTask(runData));
+    tasks.doLogic();
+    
+    IndexSearcher searcher = new IndexSearcher(runData.getDirectory(), true);
+    TopDocs td = searcher.search(new TermQuery(new Term("key", "value")), 10);
+    assertEquals(numExpectedResults, td.totalHits);
+    searcher.close();
+  }
+  
+  private Document createTestNormsDocument(boolean setNormsProp,
+      boolean normsPropVal, boolean setBodyNormsProp, boolean bodyNormsVal)
+      throws Exception {
+    Properties props = new Properties();
+    
+    // Indexing configuration.
+    props.setProperty("analyzer", MockAnalyzer.class.getName());
+    props.setProperty("content.source", OneDocSource.class.getName());
+    props.setProperty("directory", "RAMDirectory");
+    if (setNormsProp) {
+      props.setProperty("doc.tokenized.norms", Boolean.toString(normsPropVal));
+    }
+    if (setBodyNormsProp) {
+      props.setProperty("doc.body.tokenized.norms", Boolean.toString(bodyNormsVal));
+    }
+    
+    // Create PerfRunData
+    Config config = new Config(props);
+    
+    DocMaker dm = new DocMaker();
+    dm.setConfig(config);
+    return dm.makeDocument();
+  }
+  
+  /* Tests doc.index.props property. */
+  public void testIndexProperties() throws Exception {
+    // default is to not index properties.
+    doTestIndexProperties(false, false, 0);
+    
+    // set doc.index.props to false.
+    doTestIndexProperties(true, false, 0);
+    
+    // set doc.index.props to true.
+    doTestIndexProperties(true, true, 1);
+  }
+  
+  /* Tests doc.tokenized.norms and doc.body.tokenized.norms properties. */
+  public void testNorms() throws Exception {
+    
+    Document doc;
+    
+    // Don't set anything, use the defaults
+    doc = createTestNormsDocument(false, false, false, false);
+    assertTrue(doc.getField(DocMaker.TITLE_FIELD).getOmitNorms());
+    assertFalse(doc.getField(DocMaker.BODY_FIELD).getOmitNorms());
+    
+    // Set norms to false
+    doc = createTestNormsDocument(true, false, false, false);
+    assertTrue(doc.getField(DocMaker.TITLE_FIELD).getOmitNorms());
+    assertFalse(doc.getField(DocMaker.BODY_FIELD).getOmitNorms());
+    
+    // Set norms to true
+    doc = createTestNormsDocument(true, true, false, false);
+    assertFalse(doc.getField(DocMaker.TITLE_FIELD).getOmitNorms());
+    assertFalse(doc.getField(DocMaker.BODY_FIELD).getOmitNorms());
+    
+    // Set body norms to false
+    doc = createTestNormsDocument(false, false, true, false);
+    assertTrue(doc.getField(DocMaker.TITLE_FIELD).getOmitNorms());
+    assertTrue(doc.getField(DocMaker.BODY_FIELD).getOmitNorms());
+    
+    // Set body norms to true
+    doc = createTestNormsDocument(false, false, true, true);
+    assertTrue(doc.getField(DocMaker.TITLE_FIELD).getOmitNorms());
+    assertFalse(doc.getField(DocMaker.BODY_FIELD).getOmitNorms());
+  }
+  
+}
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/LineDocSourceTest.java b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/LineDocSourceTest.java
new file mode 100644
index 0000000..8629dd9
--- /dev/null
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/LineDocSourceTest.java
@@ -0,0 +1,145 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.util.Properties;
+
+import org.apache.commons.compress.compressors.CompressorStreamFactory;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.benchmark.BenchmarkTestCase;
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.tasks.AddDocTask;
+import org.apache.lucene.benchmark.byTask.tasks.CloseIndexTask;
+import org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask;
+import org.apache.lucene.benchmark.byTask.tasks.TaskSequence;
+import org.apache.lucene.benchmark.byTask.tasks.WriteLineDocTask;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+
+/** Tests the functionality of {@link LineDocSource}. */
+public class LineDocSourceTest extends BenchmarkTestCase {
+
+  private static final CompressorStreamFactory csFactory = new CompressorStreamFactory();
+
+  private void createBZ2LineFile(File file) throws Exception {
+    OutputStream out = new FileOutputStream(file);
+    out = csFactory.createCompressorOutputStream("bzip2", out);
+    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(out, "utf-8"));
+    StringBuilder doc = new StringBuilder();
+    doc.append("title").append(WriteLineDocTask.SEP).append("date").append(WriteLineDocTask.SEP).append("body");
+    writer.write(doc.toString());
+    writer.newLine();
+    writer.close();
+  }
+
+  private void createRegularLineFile(File file) throws Exception {
+    OutputStream out = new FileOutputStream(file);
+    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(out, "utf-8"));
+    StringBuilder doc = new StringBuilder();
+    doc.append("title").append(WriteLineDocTask.SEP).append("date").append(WriteLineDocTask.SEP).append("body");
+    writer.write(doc.toString());
+    writer.newLine();
+    writer.close();
+  }
+  
+  private void doIndexAndSearchTest(File file, boolean setBZCompress,
+      String bz2CompressVal) throws Exception {
+
+    Properties props = new Properties();
+    
+    // LineDocSource specific settings.
+    props.setProperty("docs.file", file.getAbsolutePath());
+    if (setBZCompress) {
+      props.setProperty("bzip.compression", bz2CompressVal);
+    }
+    
+    // Indexing configuration.
+    props.setProperty("analyzer", MockAnalyzer.class.getName());
+    props.setProperty("content.source", LineDocSource.class.getName());
+    props.setProperty("directory", "RAMDirectory");
+    
+    // Create PerfRunData
+    Config config = new Config(props);
+    PerfRunData runData = new PerfRunData(config);
+
+    TaskSequence tasks = new TaskSequence(runData, "testBzip2", null, false);
+    tasks.addTask(new CreateIndexTask(runData));
+    tasks.addTask(new AddDocTask(runData));
+    tasks.addTask(new CloseIndexTask(runData));
+    tasks.doLogic();
+    
+    IndexSearcher searcher = new IndexSearcher(runData.getDirectory(), true);
+    TopDocs td = searcher.search(new TermQuery(new Term("body", "body")), 10);
+    assertEquals(1, td.totalHits);
+    assertNotNull(td.scoreDocs[0]);
+    searcher.close();
+  }
+  
+  /* Tests LineDocSource with a bzip2 input stream. */
+  public void testBZip2() throws Exception {
+    File file = new File(getWorkDir(), "one-line.bz2");
+    createBZ2LineFile(file);
+    doIndexAndSearchTest(file, true, "true");
+  }
+  
+  public void testBZip2AutoDetect() throws Exception {
+    File file = new File(getWorkDir(), "one-line.bz2");
+    createBZ2LineFile(file);
+    doIndexAndSearchTest(file, false, null);
+  }
+  
+  public void testRegularFile() throws Exception {
+    File file = new File(getWorkDir(), "one-line");
+    createRegularLineFile(file);
+    doIndexAndSearchTest(file, false, null);
+  }
+
+  public void testInvalidFormat() throws Exception {
+    String[] testCases = new String[] {
+      "", // empty line
+      "title", // just title
+      "title" + WriteLineDocTask.SEP, // title + SEP
+      "title" + WriteLineDocTask.SEP + "body", // title + SEP + body
+      // note that title + SEP + body + SEP is a valid line, which results in an
+      // empty body
+    };
+    
+    for (int i = 0; i < testCases.length; i++) {
+      File file = new File(getWorkDir(), "one-line");
+      BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(file), "utf-8"));
+      writer.write(testCases[i]);
+      writer.newLine();
+      writer.close();
+      try {
+        doIndexAndSearchTest(file, false, null);
+        fail("Some exception should have been thrown for: [" + testCases[i] + "]");
+      } catch (Exception e) {
+        // expected.
+      }
+    }
+  }
+  
+}
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/TrecContentSourceTest.java b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/TrecContentSourceTest.java
new file mode 100644
index 0000000..a178c6a
--- /dev/null
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/TrecContentSourceTest.java
@@ -0,0 +1,333 @@
+package org.apache.lucene.benchmark.byTask.feeds;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.StringReader;
+import java.text.ParseException;
+import java.util.Date;
+
+import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.document.DateTools;
+import org.apache.lucene.util.LuceneTestCase;
+
+public class TrecContentSourceTest extends LuceneTestCase {
+
+  /** A TrecDocMaker which works on a String and not files. */
+  private static class StringableTrecSource extends TrecContentSource {
+  
+    private String docs = null;
+    
+    public StringableTrecSource(String docs, boolean forever) {
+      this.docs = docs;
+      this.forever = forever;
+    }
+    
+    @Override
+    void openNextFile() throws NoMoreDataException, IOException {
+      if (reader != null) {
+        if (!forever) {
+          throw new NoMoreDataException();
+        }
+        ++iteration;
+      }
+      
+      reader = new BufferedReader(new StringReader(docs));
+    }
+    
+    @Override
+    public void setConfig(Config config) {
+      htmlParser = new DemoHTMLParser();
+    }
+  }
+  
+  private void assertDocData(DocData dd, String expName, String expTitle,
+                             String expBody, Date expDate)
+      throws ParseException {
+    assertNotNull(dd);
+    assertEquals(expName, dd.getName());
+    assertEquals(expTitle, dd.getTitle());
+    assertTrue(dd.getBody().indexOf(expBody) != -1);
+    Date date = dd.getDate() != null ? DateTools.stringToDate(dd.getDate()) : null;
+    assertEquals(expDate, date);
+  }
+  
+  private void assertNoMoreDataException(StringableTrecSource stdm) throws Exception {
+    boolean thrown = false;
+    try {
+      stdm.getNextDocData(null);
+    } catch (NoMoreDataException e) {
+      thrown = true;
+    }
+    assertTrue("Expecting NoMoreDataException", thrown);
+  }
+  
+  public void testOneDocument() throws Exception {
+    String docs = "<DOC>\r\n" + 
+                  "<DOCNO>TEST-000</DOCNO>\r\n" + 
+                  "<DOCHDR>\r\n" + 
+                  "http://lucene.apache.org.trecdocmaker.test\r\n" + 
+                  "HTTP/1.1 200 OK\r\n" + 
+                  "Date: Sun, 11 Jan 2009 08:00:00 GMT\r\n" + 
+                  "Server: Apache/1.3.27 (Unix)\r\n" + 
+                  "Last-Modified: Sun, 11 Jan 2009 08:00:00 GMT\r\n" + 
+                  "Content-Length: 614\r\n" + 
+                  "Connection: close\r\n" + 
+                  "Content-Type: text/html\r\n" + 
+                  "</DOCHDR>\r\n" + 
+                  "<html>\r\n" + 
+                  "\r\n" + 
+                  "<head>\r\n" + 
+                  "<title>\r\n" + 
+                  "TEST-000 title\r\n" + 
+                  "</title>\r\n" + 
+                  "</head>\r\n" + 
+                  "\r\n" + 
+                  "<body>\r\n" + 
+                  "TEST-000 text\r\n" + 
+                  "\r\n" + 
+                  "</body>\r\n" + 
+                  "\r\n" + 
+                  "</DOC>";
+    StringableTrecSource source = new StringableTrecSource(docs, false);
+    source.setConfig(null);
+
+    DocData dd = source.getNextDocData(new DocData());
+    assertDocData(dd, "TEST-000_0", "TEST-000 title", "TEST-000 text", source
+        .parseDate("Sun, 11 Jan 2009 08:00:00 GMT"));
+    
+    assertNoMoreDataException(source);
+  }
+  
+  public void testTwoDocuments() throws Exception {
+    String docs = "<DOC>\r\n" + 
+                  "<DOCNO>TEST-000</DOCNO>\r\n" + 
+                  "<DOCHDR>\r\n" + 
+                  "http://lucene.apache.org.trecdocmaker.test\r\n" + 
+                  "HTTP/1.1 200 OK\r\n" + 
+                  "Date: Sun, 11 Jan 2009 08:00:00 GMT\r\n" + 
+                  "Server: Apache/1.3.27 (Unix)\r\n" + 
+                  "Last-Modified: Sun, 11 Jan 2009 08:00:00 GMT\r\n" + 
+                  "Content-Length: 614\r\n" + 
+                  "Connection: close\r\n" + 
+                  "Content-Type: text/html\r\n" + 
+                  "</DOCHDR>\r\n" + 
+                  "<html>\r\n" + 
+                  "\r\n" + 
+                  "<head>\r\n" + 
+                  "<title>\r\n" + 
+                  "TEST-000 title\r\n" + 
+                  "</title>\r\n" + 
+                  "</head>\r\n" + 
+                  "\r\n" + 
+                  "<body>\r\n" + 
+                  "TEST-000 text\r\n" + 
+                  "\r\n" + 
+                  "</body>\r\n" + 
+                  "\r\n" + 
+                  "</DOC>\r\n" +
+                  "<DOC>\r\n" + 
+                  "<DOCNO>TEST-001</DOCNO>\r\n" + 
+                  "<DOCHDR>\r\n" + 
+                  "http://lucene.apache.org.trecdocmaker.test\r\n" + 
+                  "HTTP/1.1 200 OK\r\n" + 
+                  "Date: Sun, 11 Jan 2009 08:01:00 GMT\r\n" + 
+                  "Server: Apache/1.3.27 (Unix)\r\n" + 
+                  "Last-Modified: Sun, 11 Jan 2008 08:01:00 GMT\r\n" + 
+                  "Content-Length: 614\r\n" + 
+                  "Connection: close\r\n" + 
+                  "Content-Type: text/html\r\n" + 
+                  "</DOCHDR>\r\n" + 
+                  "<html>\r\n" + 
+                  "\r\n" + 
+                  "<head>\r\n" + 
+                  "<title>\r\n" + 
+                  "TEST-001 title\r\n" + 
+                  "</title>\r\n" + 
+                  "</head>\r\n" + 
+                  "\r\n" + 
+                  "<body>\r\n" + 
+                  "TEST-001 text\r\n" + 
+                  "\r\n" + 
+                  "</body>\r\n" + 
+                  "\r\n" + 
+                  "</DOC>";
+    StringableTrecSource source = new StringableTrecSource(docs, false);
+    source.setConfig(null);
+
+    DocData dd = source.getNextDocData(new DocData());
+    assertDocData(dd, "TEST-000_0", "TEST-000 title", "TEST-000 text", source
+        .parseDate("Sun, 11 Jan 2009 08:00:00 GMT"));
+    
+    dd = source.getNextDocData(dd);
+    assertDocData(dd, "TEST-001_0", "TEST-001 title", "TEST-001 text", source
+        .parseDate("Sun, 11 Jan 2009 08:01:00 GMT"));
+    
+    assertNoMoreDataException(source);
+  }
+
+  // If a Date: attribute is missing, make sure the document is not skipped, but
+  // rather that null Data is assigned.
+  public void testMissingDate() throws Exception {
+    String docs = "<DOC>\r\n" + 
+                  "<DOCNO>TEST-000</DOCNO>\r\n" + 
+                  "<DOCHDR>\r\n" + 
+                  "http://lucene.apache.org.trecdocmaker.test\r\n" + 
+                  "HTTP/1.1 200 OK\r\n" + 
+                  "Server: Apache/1.3.27 (Unix)\r\n" + 
+                  "Last-Modified: Sun, 11 Jan 2009 08:00:00 GMT\r\n" + 
+                  "Content-Length: 614\r\n" + 
+                  "Connection: close\r\n" + 
+                  "Content-Type: text/html\r\n" + 
+                  "</DOCHDR>\r\n" + 
+                  "<html>\r\n" + 
+                  "\r\n" + 
+                  "<head>\r\n" + 
+                  "<title>\r\n" + 
+                  "TEST-000 title\r\n" + 
+                  "</title>\r\n" + 
+                  "</head>\r\n" + 
+                  "\r\n" + 
+                  "<body>\r\n" + 
+                  "TEST-000 text\r\n" + 
+                  "\r\n" + 
+                  "</body>\r\n" + 
+                  "\r\n" + 
+                  "</DOC>\r\n" +
+                  "<DOC>\r\n" + 
+                  "<DOCNO>TEST-001</DOCNO>\r\n" + 
+                  "<DOCHDR>\r\n" + 
+                  "http://lucene.apache.org.trecdocmaker.test\r\n" + 
+                  "HTTP/1.1 200 OK\r\n" + 
+                  "Date: Sun, 11 Jan 2009 08:01:00 GMT\r\n" + 
+                  "Server: Apache/1.3.27 (Unix)\r\n" + 
+                  "Last-Modified: Sun, 11 Jan 2009 08:01:00 GMT\r\n" + 
+                  "Content-Length: 614\r\n" + 
+                  "Connection: close\r\n" + 
+                  "Content-Type: text/html\r\n" + 
+                  "</DOCHDR>\r\n" + 
+                  "<html>\r\n" + 
+                  "\r\n" + 
+                  "<head>\r\n" + 
+                  "<title>\r\n" + 
+                  "TEST-001 title\r\n" + 
+                  "</title>\r\n" + 
+                  "</head>\r\n" + 
+                  "\r\n" + 
+                  "<body>\r\n" + 
+                  "TEST-001 text\r\n" + 
+                  "\r\n" + 
+                  "</body>\r\n" + 
+                  "\r\n" + 
+                  "</DOC>";
+    StringableTrecSource source = new StringableTrecSource(docs, false);
+    source.setConfig(null);
+
+    DocData dd = source.getNextDocData(new DocData());
+    assertDocData(dd, "TEST-000_0", "TEST-000 title", "TEST-000 text", null);
+    
+    dd = source.getNextDocData(dd);
+    assertDocData(dd, "TEST-001_0", "TEST-001 title", "TEST-001 text", source
+        .parseDate("Sun, 11 Jan 2009 08:01:00 GMT"));
+    
+    assertNoMoreDataException(source);
+  }
+
+  // When a 'bad date' is input (unparsable date), make sure the DocData date is
+  // assigned null.
+  public void testBadDate() throws Exception {
+    String docs = "<DOC>\r\n" + 
+                  "<DOCNO>TEST-000</DOCNO>\r\n" + 
+                  "<DOCHDR>\r\n" + 
+                  "http://lucene.apache.org.trecdocmaker.test\r\n" + 
+                  "HTTP/1.1 200 OK\r\n" + 
+                  "Date: Bad Date\r\n" + 
+                  "Server: Apache/1.3.27 (Unix)\r\n" + 
+                  "Last-Modified: Sun, 11 Jan 2009 08:00:00 GMT\r\n" + 
+                  "Content-Length: 614\r\n" + 
+                  "Connection: close\r\n" + 
+                  "Content-Type: text/html\r\n" + 
+                  "</DOCHDR>\r\n" + 
+                  "<html>\r\n" + 
+                  "\r\n" + 
+                  "<head>\r\n" + 
+                  "<title>\r\n" + 
+                  "TEST-000 title\r\n" + 
+                  "</title>\r\n" + 
+                  "</head>\r\n" + 
+                  "\r\n" + 
+                  "<body>\r\n" + 
+                  "TEST-000 text\r\n" + 
+                  "\r\n" + 
+                  "</body>\r\n" + 
+                  "\r\n" + 
+                  "</DOC>";
+    StringableTrecSource source = new StringableTrecSource(docs, false);
+    source.setConfig(null);
+
+    DocData dd = source.getNextDocData(new DocData());
+    assertDocData(dd, "TEST-000_0", "TEST-000 title", "TEST-000 text", null);
+    
+    assertNoMoreDataException(source);
+  }
+
+  public void testForever() throws Exception {
+    String docs = "<DOC>\r\n" + 
+                  "<DOCNO>TEST-000</DOCNO>\r\n" + 
+                  "<DOCHDR>\r\n" + 
+                  "http://lucene.apache.org.trecdocmaker.test\r\n" + 
+                  "HTTP/1.1 200 OK\r\n" + 
+                  "Date: Sun, 11 Jan 2009 08:00:00 GMT\r\n" + 
+                  "Server: Apache/1.3.27 (Unix)\r\n" + 
+                  "Last-Modified: Sun, 11 Jan 2009 08:00:00 GMT\r\n" + 
+                  "Content-Length: 614\r\n" + 
+                  "Connection: close\r\n" + 
+                  "Content-Type: text/html\r\n" + 
+                  "</DOCHDR>\r\n" + 
+                  "<html>\r\n" + 
+                  "\r\n" + 
+                  "<head>\r\n" + 
+                  "<title>\r\n" + 
+                  "TEST-000 title\r\n" + 
+                  "</title>\r\n" + 
+                  "</head>\r\n" + 
+                  "\r\n" + 
+                  "<body>\r\n" + 
+                  "TEST-000 text\r\n" + 
+                  "\r\n" + 
+                  "</body>\r\n" + 
+                  "\r\n" + 
+                  "</DOC>";
+    StringableTrecSource source = new StringableTrecSource(docs, true);
+    source.setConfig(null);
+
+    DocData dd = source.getNextDocData(new DocData());
+    assertDocData(dd, "TEST-000_0", "TEST-000 title", "TEST-000 text", source
+        .parseDate("Sun, 11 Jan 2009 08:00:00 GMT"));
+    
+    // same document, but the second iteration changes the name.
+    dd = source.getNextDocData(dd);
+    assertDocData(dd, "TEST-000_1", "TEST-000 title", "TEST-000 text", source
+        .parseDate("Sun, 11 Jan 2009 08:00:00 GMT"));
+
+    // Don't test that NoMoreDataException is thrown, since the forever flag is
+    // turned on.
+  }
+
+}
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/reuters.first20.lines.txt b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/reuters.first20.lines.txt
new file mode 100644
index 0000000..41b04b3
--- /dev/null
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/reuters.first20.lines.txt
@@ -0,0 +1,20 @@
+BAHIA COCOA REVIEW	19870226200101	Showers continued throughout the week in the Bahia cocoa zone, alleviating the drought since early January and improving prospects for the coming temporao, although normal humidity levels have not been restored, Comissaria Smith said in its weekly review.     The dry period means the temporao will be late this year.     Arrivals for the week ended February 22 were 155,221 bags of 60 kilos making a cumulative total for the season of 5.93 mln against 5.81 at the same stage last year. Again it seems that cocoa delivered earlier on consignment was included in the arrivals figures.     Comissaria Smith said there is still some doubt as to how much old crop cocoa is still available as harvesting has practically come to an end. With total Bahia crop estimates around 6.4 mln bags and sales standing at almost 6.2 mln there are a few hundred thousand bags still in the hands of farmers, middlemen, exporters and processors.     There are doubts as to how much of this cocoa would be fit for export as shippers are now experiencing dificulties in obtaining +Bahia superior+ certificates.     In view of the lower quality over recent weeks farmers have sold a good part of their cocoa held on consignment.     Comissaria Smith said spot bean prices rose to 340 to 350 cruzados per arroba of 15 kilos.     Bean shippers were reluctant to offer nearby shipment and only limited sales were booked for March shipment at 1,750 to 1,780 dlrs per tonne to ports to be named.     New crop sales were also light and all to open ports with June/July going at 1,850 and 1,880 dlrs and at 35 and 45 dlrs under New York july, Aug/Sept at 1,870, 1,875 and 1,880 dlrs per tonne FOB.     Routine sales of butter were made. March/April sold at 4,340, 4,345 and 4,350 dlrs.     April/May butter went at 2.27 times New York May, June/July at 4,400 and 4,415 dlrs, Aug/Sept at 4,351 to 4,450 dlrs and at 2.27 and 2.28 times New York Sept and Oct/Dec at 4,480 dlrs and 2.27 times New York Dec, Comissaria Smith said.     Destinations were the U.S., Covertible currency areas, Uruguay and open ports.     Cake sales were registered at 785 to 995 dlrs for March/April, 785 dlrs for May, 753 dlrs for Aug and 0.39 times New York Dec for Oct/Dec.     Buyers were the U.S., Argentina, Uruguay and convertible currency areas.     Liquor sales were limited with March/April selling at 2,325 and 2,380 dlrs, June/July at 2,375 dlrs and at 1.25 times New York July, Aug/Sept at 2,400 dlrs and at 1.25 times New York Sept and Oct/Dec at 1.25 times New York Dec, Comissaria Smith said.     Total Bahia sales are currently estimated at 6.13 mln bags against the 1986/87 crop and 1.06 mln bags against the 1987/88 crop.     Final figures for the period to February 28 are expected to be published by the Brazilian Cocoa Trade Commission after carnival which ends midday on February 27.  Reuter &#3;  
+STANDARD OIL <SRD> TO FORM FINANCIAL UNIT	19870226200220	Standard Oil Co and BP North America Inc said they plan to form a venture to manage the money market borrowing and investment activities of both companies.     BP North America is a subsidiary of British Petroleum Co Plc <BP>, which also owns a 55 pct interest in Standard Oil.     The venture will be called BP/Standard Financial Trading and will be operated by Standard Oil under the oversight of a joint management committee.   Reuter &#3;  
+COBANCO INC <CBCO> YEAR NET	19870226201859	Shr 34 cts vs 1.19 dlrs     Net 807,000 vs 2,858,000     Assets 510.2 mln vs 479.7 mln     Deposits 472.3 mln vs 440.3 mln     Loans 299.2 mln vs 327.2 mln     Note: 4th qtr not available. Year includes 1985 extraordinary gain from tax carry forward of 132,000 dlrs, or five cts per shr.  Reuter &#3;  
+WORLD MARKET PRICE FOR UPLAND COTTON - USDA	19870226213846	The U.S. Agriculture Department announced the prevailing world market price, adjusted to U.S. quality and location, for Strict Low Middling, 1-1/16 inch upland cotton at 52.69 cts per lb, to be in effect through midnight March 5.     The adjusted world price is at average U.S. producing locations (near Lubbock, Texas) and will be further adjusted for other qualities and locations. The price will be used in determining First Handler Cotton Certificate payment rates.     Based on data for the week ended February 26, the adjusted world price for upland cotton is determined as follows, in cts per lb --  Northern European Price               66.32       Adjustments --  Average U.S. spot mkt location 10.42   SLM 1-1/16 inch cotton          1.80   Average U.S. location           0.53  Sum of adjustments              12.75  Adjusted world price            53.57  Reuter &#3;  
+SUGAR QUOTA IMPORTS DETAILED -- USDA	19870226213854	The U.S. Agriculture Department said cumulative sugar imports from individual countries during the 1987 quota year, which began January 1, 1987 and ends December 31, 1987 were as follows, with quota allocations for the quota year in short tons, raw value --             CUMULATIVE     QUOTA 1987               IMPORTS     ALLOCATIONS  ARGENTINA        nil          39,130  AUSTRALIA        nil          75,530  BARBADOS         nil           7,500  BELIZE           nil          10,010  BOLIVIA          nil           7,500  BRAZIL           nil         131,950  CANADA           nil          18,876                            QUOTA 1987               IMPORTS     ALLOCATIONS  COLOMBIA         103          21,840  CONGO            nil           7,599  COSTA RICA       nil          17,583  IVORY COAST      nil           7,500  DOM REP        5,848         160,160  ECUADOR          nil          10,010  EL SALVADOR      nil          26,019.8  FIJI             nil          25,190  GABON            nil           7,500                            QUOTA 1987               IMPORTS     ALLOCATIONS  GUATEMALA        nil          43,680  GUYANA           nil          10,920  HAITI            nil           7,500  HONDURAS         nil          15,917.2  INDIA            nil           7,500  JAMAICA          nil          10,010  MADAGASCAR       nil           7,500  MALAWI           nil           9,,100                            QUOTA 1987                IMPORTS    ALLOCATIONS  MAURITIUS         nil         10,920  MEXICO             37          7,500  MOZAMBIQUE        nil         11,830  PANAMA            nil         26,390  PAPUA NEW GUINEA  nil          7,500  PARAGUAY          nil          7,500  PERU              nil         37,310  PHILIPPINES       nil        143,780  ST.CHRISTOPHER-  NEVIS             nil          7,500                           QUOTA 1987                 IMPORTS  ALLOCATIONS  SWAZILAND          nil         14,560  TAIWAN             nil         10,920  THAILAND           nil         12,740  TRINIDAD-TOBAGO    nil          7,500  URUGUAY            nil          7,500  ZIMBABWE           nil         10,920   Reuter &#3;  
+GRAIN SHIPS LOADING AT PORTLAND	19870226213903	There were seven grain ships loading and six ships were waiting to load at Portland, according to the Portland Merchants Exchange.  Reuter &#3;  
+IRAN ANNOUNCES END OF MAJOR OFFENSIVE IN GULF WAR	19870226214000	Iran announced tonight that its major offensive against Iraq in the Gulf war had ended after dealing savage blows against the Baghdad government.     The Iranian news agency IRNA, in a report received in London, said the operation code-named Karbala-5 launched into Iraq on January 9 was now over.     It quoted a joint statewment by the Iranian Army and Revolutionary Guards Corps as saying that their forces had "dealt one of the severest blows on the Iraqi war machine in the history of the Iraq-imposed war."     The statement by the Iranian High Command appeared to herald the close of an assault on the port city of Basra in southern Iraq.     "The operation was launched at a time when the Baghdad government was spreading extensive propaganda on the resistance power of its army...," said the statement quoted by IRNA.     It claimed massive victories in the seven-week offensive and called on supporters of Baghdad to "come to their senses" and discontinue support for what it called the tottering regime in Iraq.     Iran said its forces had "liberated" 155 square kilometers of enemy-occupied territory during the 1987 offensive and taken over islands, townships, rivers and part of a road leading into Basra.     The Iranian forces "are in full control of these areas," the statement said.     It said 81 Iraqi brigades and battalions were totally destroyed, along with 700 tanks and 1,500 other vehicles. The victory list also included 80 warplanes downed, 250 anti- aircraft guns and 400 pieces of military hardware destroyed and the seizure of 220 tanks and armoured personnel carriers.  Reuter &#3;  
+MERIDIAN BANCORP INC <MRDN> SETS REGULAR PAYOUT	19870226214034	Qtly div 25 cts vs 25 cts prior     Pay April one     Record March 15  Reuter &#3;  
+U.S. BANK DISCOUNT BORROWINGS 310 MLN DLRS	19870226214134	U.S. bank discount window borrowings less extended credits averaged 310 mln dlrs in the week to Wednesday February 25, the Federal Reserve said.     The Fed said that overall borrowings in the week fell 131 mln dlrs to 614 mln dlrs, with extended credits up 10 mln dlrs at 304 mln dlrs. The week was the second half of a two-week statement period. Net borrowings in the prior week averaged 451 mln dlrs.     Commenting on the two-week statement period ended February 25, the Fed said that banks had average net free reserves of 644 mln dlrs a day, down from 1.34 billion two weeks earlier.     A Federal Reserve spokesman told a press briefing that there were no large single day net misses in the Fed's reserve projections in the week to Wednesday.     He said that natural float had been "acting a bit strangely" for this time of year, noting that there had been poor weather during the latest week.     The spokesman said that natural float ranged from under 500 mln dlrs on Friday, for which he could give no reason, to nearly one billion dlrs on both Thursday and Wednesday.     The Fed spokeman could give no reason for Thursday's high float, but he said that about 750 mln dlrs of Wednesday's float figure was due to holdover and transportation float at two widely separated Fed districts.     For the week as a whole, he said that float related as of adjustments were "small," adding that they fell to a negative 750 mln dlrs on Tuesday due to a number of corrections for unrelated cash letter errors in six districts around the country.     The spokesman said that on both Tuesday and Wednesday, two different clearing banks had system problems and the securities and Federal funds wires had to be held open until about 2000 or 2100 EST on both days.     However, he said that both problems were cleared up during both afternoons and there was no evidence of any reserve impact.     During the week ended Wednesday, 45 pct of net discount window borrowings were made by the smallest banks, with 30 pct by the 14 large money center banks and 25 pct by large regional institutions.     On Wednesday, 55 pct of the borrowing was accounted for by the money center banks, with 30 pct by the large regionals and 15 pct by the smallest banks.     The Fed spokesman said the banking system had excess reserves on Thursday, Monday and Tuesday and a deficit on Friday and Wedndsday. That produced a small daily average deficit for the week as a whole.     For the two-week period, he said there were relatively high excess reserves on a daily avearge, almost all of which were at the smallest banks.  Reuter &#3;  
+AMERICAN EXPRESS <AXP> SEEN IN POSSIBLE SPINNOFF	19870226214313	American Express Co remained silent on market rumors it would spinoff all or part of its Shearson Lehman Brothers Inc, but some analysts said the company may be considering such a move because it is unhappy with the market value of its stock.     American Express stock got a lift from the rumor, as the market calculated a partially public Shearson may command a good market value, thereby boosting the total value of American Express. The rumor also was accompanied by talk the financial services firm would split its stock and boost its dividend.     American Express closed on the New York Stock Exchange at 72-5/8, up 4-1/8 on heavy volume.     American Express would not comment on the rumors or its stock activity.     Analysts said comments by the company at an analysts' meeting Tuesday helped fuel the rumors as did an announcement yesterday of management changes.     At the meeting, company officials said American Express stock is undervalued and does not fully reflect the performance of Shearson, according to analysts.     Yesterday, Shearson said it was elevating its chief operating officer, Jeffery Lane, to the added position of president, which had been vacant. It also created four new positions for chairmen of its operating divisions.     Analysts speculated a partial spinoff would make most sense, contrary to one variation on market rumors of a total spinoff.     Some analysts, however, disagreed that any spinoff of Shearson would be good since it is a strong profit center for American Express, contributing about 20 pct of earnings last year.     "I think it is highly unlikely that American Express is going to sell shearson," said Perrin Long of Lipper Analytical. He questioned what would be a better investment than "a very profitable securities firm."     Several analysts said American Express is not in need of cash, which might be the only reason to sell a part of a strong asset.     But others believe the company could very well of considered the option of spinning out part of Shearson, and one rumor suggests selling about 20 pct of it in the market.     Larry Eckenfelder of Prudential-Bache Securities said he believes American Express could have considered a partial spinoff in the past.     "Shearson being as profitable as it is would have fetched a big premium in the market place. Shearson's book value is in the 1.4 mln dlr range. Shearson in the market place would probably be worth three to 3.5 bilion dlrs in terms of market capitalization," said Eckenfelder.     Some analysts said American Express could use capital since it plans to expand globally.     "They have enormous internal growth plans that takes capital. You want your stock to reflect realistic valuations to enhance your ability to make all kinds of endeavors down the road," said E.F. Hutton Group analyst Michael Lewis.     "They've outlined the fact that they're investing heavily in the future, which goes heavily into the international arena," said Lewis. "...That does not preclude acquisitions and divestitures along the way," he said.     Lewis said if American Express reduced its exposure to the brokerage business by selling part of shearson, its stock might better reflect other assets, such as the travel related services business.     "It could find its true water mark with a lesser exposure to brokerage. The value of the other components could command a higher multiple because they constitute a higher percentage of the total operating earnings of the company," he said.      Lewis said Shearson contributed 316 mln in after-tax operating earnings, up from about 200 mln dlrs in 1985.      Reuter &#3;  
+OHIO MATTRESS <OMT> MAY HAVE LOWER 1ST QTR NET	19870226201915	Ohio Mattress Co said its first quarter, ending February 28, profits may be below the 2.4 mln dlrs, or 15 cts a share, earned in the first quarter of fiscal 1986.     The company said any decline would be due to expenses related to the acquisitions in the middle of the current quarter of seven licensees of Sealy Inc, as well as 82 pct of the outstanding capital stock of Sealy.     Because of these acquisitions, it said, first quarter sales will be substantially higher than last year's 67.1 mln dlrs.     Noting that it typically reports first quarter results in late march, said the report is likely to be issued in early April this year.     It said the delay is due to administrative considerations, including conducting appraisals, in connection with the acquisitions.  Reuter &#3;  
+U.S. M-1 MONEY SUPPLY ROSE 2.1 BILLION DLRS	19870226214435	U.S. M-1 money supply rose 2.1 billion dlrs to a seasonally adjusted 736.7 billion dlrs in the February 16 week, the Federal Reserve said.     The previous week's M-1 level was revised to 734.6 billion dlrs from 734.2 billion dlrs, while the four-week moving average of M-1 rose to 735.0 billion dlrs from 733.5 billion.     Economists polled by Reuters said that M-1 should be anywhere from down four billion dlrs to up 2.3 billion dlrs. The average forecast called for a 300 mln dlr M-1 rise.  Reuter &#3;  
+GENERAL BINDING <GBND> IN MARKETING AGREEMENT	19870226214508	General Binding Corp said it reached a marketing agreement with Varitronic Systems Inc, a manufacturer and marketer of electronic lettering systems.     Under terms of the agreement, General Binding will carry Varitronics' Merlin Express Presentation Lettering System, a portable, battery-operated lettering system which produces type on adhesive-backed tape.  Reuter &#3;  
+LIBERTY ALL-STAR <USA> SETS INITIAL PAYOUT	19870226214544	Liberty All-Star Equity Fund said it declared an initial dividend of five cts per share, payable April two to shareholders of record March 20.     It said the dividend includes a quarterly dividend of three cts a share and a special payout of two cts a share, which covers the period from November three, 1986, when the fund began operations, to December 31, 1986.     The fund said its quarterly dividend rate may fluctuate in the future.  Reuter &#3;  
+COCA COLA <KO> UNIT AND WORLD FILM IN VENTURE	19870226214745	Coca-Cola Co's Entertainment Business Sector Inc unit said it formed a joint venture with an affiliate of World Film Services to acquire, produce and distribute television programming around the world.     World Film Services was formed by chairman John Heyman in 1963 to produce films.      Reuter &#3;  
+FORD MOTOR CREDIT <F> TO REDEEM DEBENTURES	19870226214753	Ford Motor Co said its Ford Motor Credit Co on April One will redeem 4.0 mln dlrs of its 8.70 pct debentures due April 1, 1999.     It said the debentures are redeemable at a price of 100 pct of the principal. Because April 1, 1987 is an interest payment date on the debentures, no accrued interest will be payable on the redemption date as part of the redemption proceeds.     Debentures will be selected for redemption on a pro rata basis, Ford said.   Reuter &#3;  
+STERLING SOFTWARE <SSW> NOTE HOLDERS OK BUY	19870226214802	Sterling Software Inc said it received consent of a majority of the holders of its eight pct convertible sernior subordinated debentures required to purchase shares of its common.     The company said it may now buy its stock at its discretion depending on market conditions.  Reuter &#3;  
+<SCHULT HOMES CORP> MAKES INITIAL STOCK OFFER	19870226214818	Schult Homes Corp announced an initial public offering of 833,334 units at five dlrs per unit, said Janney Montgomery Scott Inc and Woolcott and Co, managing underwriters of the offering.     They said each unit consists of one common share and one warrant to buy one-half share of common.     The warrant will entitle holders to buy one-half common share at 5.50 dlrs per full share from March one, 1988, to September one, 1989, and thereafter at 6.50 dlrs per full share until March 1991, they said.  Reuter &#3;  
+FLUOR <FLR> UNIT GETS CONSTRUCTION CONTRACT	19870226214826	Fluor Corp said its Fluor Daniel unit received a contract from Union Carbide Corp <UK> covering design, procurement and construction of a 108 megawatt combined cycle cogeneration facility in Seadrift, Texas.     The value of the contract was not disclosed.  Reuter &#3;  
+SUFFIELD FINANCIAL CORP <SFCP> SELLS STOCK	19870226214835	Suffield Financial Corp said   Jon Googel and Benjamin Sisti of Colonial Realty, West Hartford, Conn., purchased 175,900 shares of its stock for 3,416,624.     The company said the purchase equals 5.2 pct of its outstanding shares.  Reuter &#3;  
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CountingHighlighterTestTask.java b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CountingHighlighterTestTask.java
new file mode 100644
index 0000000..357af06
--- /dev/null
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CountingHighlighterTestTask.java
@@ -0,0 +1,68 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.benchmark.byTask.tasks;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.search.highlight.SimpleHTMLFormatter;
+import org.apache.lucene.search.highlight.Highlighter;
+import org.apache.lucene.search.highlight.TextFragment;
+import org.apache.lucene.search.highlight.QueryScorer;
+import org.apache.lucene.search.highlight.TokenSources;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.IndexReader;
+
+import java.io.IOException;
+
+/**
+ * Test Search task which counts number of searches.
+ */
+public class CountingHighlighterTestTask extends SearchTravRetHighlightTask {
+
+  public static int numHighlightedResults = 0;
+  public static int numDocsRetrieved = 0;
+
+  public CountingHighlighterTestTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  protected Document retrieveDoc(IndexReader ir, int id) throws IOException {
+    Document document = ir.document(id);
+    if (document != null) {
+      numDocsRetrieved++;
+    }
+    return document;
+  }
+
+  @Override
+  public BenchmarkHighlighter getBenchmarkHighlighter(Query q) {
+    highlighter = new Highlighter(new SimpleHTMLFormatter(), new QueryScorer(q));
+    return new BenchmarkHighlighter() {
+      @Override
+      public int doHighlight(IndexReader reader, int doc, String field, Document document, Analyzer analyzer, String text) throws Exception {
+        TokenStream ts = TokenSources.getAnyTokenStream(reader, doc, field, document, analyzer);
+        TextFragment[] frag = highlighter.getBestTextFragments(ts, text, mergeContiguous, maxFrags);
+        numHighlightedResults += frag != null ? frag.length : 0;
+        return frag != null ? frag.length : 0;
+      }
+    };
+  }
+}
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CountingSearchTestTask.java b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CountingSearchTestTask.java
new file mode 100755
index 0000000..7125723
--- /dev/null
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CountingSearchTestTask.java
@@ -0,0 +1,55 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.benchmark.byTask.tasks;
+
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+
+/**
+ * Test Search task which counts number of searches.
+ */
+public class CountingSearchTestTask extends SearchTask {
+
+  public static int numSearches = 0; 
+  public static long startMillis;
+  public static long lastMillis;
+  public static long prevLastMillis;
+
+  public CountingSearchTestTask(PerfRunData runData) {
+    super(runData);
+  }
+
+  @Override
+  public int doLogic() throws Exception {
+    int res = super.doLogic();
+    incrNumSearches();
+    return res;
+  }
+
+  private static synchronized void incrNumSearches() {
+    prevLastMillis = lastMillis;
+    lastMillis = System.currentTimeMillis();
+    if (0 == numSearches) {
+      startMillis = prevLastMillis = lastMillis;
+    }
+    numSearches++;
+  }
+
+  public long getElapsedMillis() {
+    return lastMillis - startMillis;
+  }
+}
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTaskTest.java b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTaskTest.java
new file mode 100644
index 0000000..6bab788
--- /dev/null
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTaskTest.java
@@ -0,0 +1,108 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.ByteArrayOutputStream;
+import java.io.File;
+import java.io.PrintStream;
+import java.util.Properties;
+
+import org.apache.lucene.benchmark.BenchmarkTestCase;
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.index.NoDeletionPolicy;
+import org.apache.lucene.index.NoMergePolicy;
+import org.apache.lucene.index.NoMergeScheduler;
+import org.apache.lucene.util.Version;
+
+
+/** Tests the functionality of {@link CreateIndexTask}. */
+public class CreateIndexTaskTest extends BenchmarkTestCase {
+
+  private PerfRunData createPerfRunData(String infoStreamValue) throws Exception {
+    Properties props = new Properties();
+    props.setProperty("writer.version", Version.LUCENE_40.toString());
+    props.setProperty("print.props", "false"); // don't print anything
+    props.setProperty("directory", "RAMDirectory");
+    if (infoStreamValue != null) {
+      props.setProperty("writer.info.stream", infoStreamValue);
+    }
+    Config config = new Config(props);
+    return new PerfRunData(config);
+  }
+
+  public void testInfoStream_SystemOutErr() throws Exception {
+ 
+    PrintStream curOut = System.out;
+    ByteArrayOutputStream baos = new ByteArrayOutputStream();
+    System.setOut(new PrintStream(baos));
+    try {
+      PerfRunData runData = createPerfRunData("SystemOut");
+      CreateIndexTask cit = new CreateIndexTask(runData);
+      cit.doLogic();
+      new CloseIndexTask(runData).doLogic();
+      assertTrue(baos.size() > 0);
+    } finally {
+      System.setOut(curOut);
+    }
+    
+    PrintStream curErr = System.err;
+    baos.reset();
+    System.setErr(new PrintStream(baos));
+    try {
+      PerfRunData runData = createPerfRunData("SystemErr");
+      CreateIndexTask cit = new CreateIndexTask(runData);
+      cit.doLogic();
+      new CloseIndexTask(runData).doLogic();
+      assertTrue(baos.size() > 0);
+    } finally {
+      System.setErr(curErr);
+    }
+
+  }
+
+  public void testInfoStream_File() throws Exception {
+    
+    File outFile = new File(getWorkDir(), "infoStreamTest");
+    PerfRunData runData = createPerfRunData(outFile.getAbsolutePath());
+    new CreateIndexTask(runData).doLogic();
+    new CloseIndexTask(runData).doLogic();
+    assertTrue(outFile.length() > 0);
+  }
+
+  public void testNoMergePolicy() throws Exception {
+    PerfRunData runData = createPerfRunData(null);
+    runData.getConfig().set("merge.policy", NoMergePolicy.class.getName());
+    new CreateIndexTask(runData).doLogic();
+    new CloseIndexTask(runData).doLogic();
+  }
+  
+  public void testNoMergeScheduler() throws Exception {
+    PerfRunData runData = createPerfRunData(null);
+    runData.getConfig().set("merge.scheduler", NoMergeScheduler.class.getName());
+    new CreateIndexTask(runData).doLogic();
+    new CloseIndexTask(runData).doLogic();
+  }
+
+  public void testNoDeletionPolicy() throws Exception {
+    PerfRunData runData = createPerfRunData(null);
+    runData.getConfig().set("deletion.policy", NoDeletionPolicy.class.getName());
+    new CreateIndexTask(runData).doLogic();
+    new CloseIndexTask(runData).doLogic();
+  }
+}
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/PerfTaskTest.java b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/PerfTaskTest.java
new file mode 100644
index 0000000..2133faa
--- /dev/null
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/PerfTaskTest.java
@@ -0,0 +1,73 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Properties;
+
+import org.apache.lucene.benchmark.BenchmarkTestCase;
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+
+/** Tests the functionality of the abstract {@link PerfTask}. */
+public class PerfTaskTest extends BenchmarkTestCase {
+
+  private static final class MyPerfTask extends PerfTask {
+
+    public MyPerfTask(PerfRunData runData) {
+      super(runData);
+    }
+
+    @Override
+    public int doLogic() throws Exception {
+      return 0;
+    }
+
+    public int getLogStep() { return logStep; }
+    
+  }
+  
+  private PerfRunData createPerfRunData(boolean setLogStep, int logStepVal,
+      boolean setTaskLogStep, int taskLogStepVal) throws Exception {
+    Properties props = new Properties();
+    if (setLogStep) {
+      props.setProperty("log.step", Integer.toString(logStepVal));
+    }
+    if (setTaskLogStep) {
+      props.setProperty("log.step.MyPerf", Integer.toString(taskLogStepVal));
+    }
+    props.setProperty("directory", "RAMDirectory"); // no accidental FS dir.
+    Config config = new Config(props);
+    return new PerfRunData(config);
+  }
+  
+  private void doLogStepTest(boolean setLogStep, int logStepVal,
+      boolean setTaskLogStep, int taskLogStepVal, int expLogStepValue) throws Exception {
+    PerfRunData runData = createPerfRunData(setLogStep, logStepVal, setTaskLogStep, taskLogStepVal);
+    MyPerfTask mpt = new MyPerfTask(runData);
+    assertEquals(expLogStepValue, mpt.getLogStep());
+  }
+  
+  public void testLogStep() throws Exception {
+    doLogStepTest(false, -1, false, -1, PerfTask.DEFAULT_LOG_STEP);
+    doLogStepTest(true, -1, false, -1, Integer.MAX_VALUE);
+    doLogStepTest(true, 100, false, -1, 100);
+    doLogStepTest(false, -1, true, -1, Integer.MAX_VALUE);
+    doLogStepTest(false, -1, true, 100, 100);
+  }
+  
+}
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTaskTest.java b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTaskTest.java
new file mode 100644
index 0000000..f212116
--- /dev/null
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTaskTest.java
@@ -0,0 +1,290 @@
+package org.apache.lucene.benchmark.byTask.tasks;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.util.HashSet;
+import java.util.Properties;
+import java.util.Set;
+
+import org.apache.commons.compress.compressors.CompressorStreamFactory;
+import org.apache.lucene.benchmark.BenchmarkTestCase;
+import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
+import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Field.Index;
+import org.apache.lucene.document.Field.Store;
+
+/** Tests the functionality of {@link WriteLineDocTask}. */
+public class WriteLineDocTaskTest extends BenchmarkTestCase {
+
+  // class has to be public so that Class.forName.newInstance() will work
+  public static final class WriteLineDocMaker extends DocMaker {
+  
+    @Override
+    public Document makeDocument() throws Exception {
+      Document doc = new Document();
+      doc.add(new Field(BODY_FIELD, "body", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
+      doc.add(new Field(TITLE_FIELD, "title", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
+      doc.add(new Field(DATE_FIELD, "date", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
+      return doc;
+    }
+    
+  }
+  
+  // class has to be public so that Class.forName.newInstance() will work
+  public static final class NewLinesDocMaker extends DocMaker {
+  
+    @Override
+    public Document makeDocument() throws Exception {
+      Document doc = new Document();
+      doc.add(new Field(BODY_FIELD, "body\r\ntext\ttwo", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
+      doc.add(new Field(TITLE_FIELD, "title\r\ntext", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
+      doc.add(new Field(DATE_FIELD, "date\r\ntext", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
+      return doc;
+    }
+    
+  }
+  
+  // class has to be public so that Class.forName.newInstance() will work
+  public static final class NoBodyDocMaker extends DocMaker {
+    @Override
+    public Document makeDocument() throws Exception {
+      Document doc = new Document();
+      doc.add(new Field(TITLE_FIELD, "title", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
+      doc.add(new Field(DATE_FIELD, "date", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
+      return doc;
+    }
+  }
+  
+  // class has to be public so that Class.forName.newInstance() will work
+  public static final class NoTitleDocMaker extends DocMaker {
+    @Override
+    public Document makeDocument() throws Exception {
+      Document doc = new Document();
+      doc.add(new Field(BODY_FIELD, "body", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
+      doc.add(new Field(DATE_FIELD, "date", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
+      return doc;
+    }
+  }
+  
+  // class has to be public so that Class.forName.newInstance() will work
+  public static final class JustDateDocMaker extends DocMaker {
+    @Override
+    public Document makeDocument() throws Exception {
+      Document doc = new Document();
+      doc.add(new Field(DATE_FIELD, "date", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
+      return doc;
+    }
+  }
+  
+  // class has to be public so that Class.forName.newInstance() will work
+  public static final class ThreadingDocMaker extends DocMaker {
+  
+    @Override
+    public Document makeDocument() throws Exception {
+      Document doc = new Document();
+      String name = Thread.currentThread().getName();
+      doc.add(new Field(BODY_FIELD, "body_" + name, Store.NO, Index.NOT_ANALYZED_NO_NORMS));
+      doc.add(new Field(TITLE_FIELD, "title_" + name, Store.NO, Index.NOT_ANALYZED_NO_NORMS));
+      doc.add(new Field(DATE_FIELD, "date_" + name, Store.NO, Index.NOT_ANALYZED_NO_NORMS));
+      return doc;
+    }
+    
+  }
+
+  private static final CompressorStreamFactory csFactory = new CompressorStreamFactory();
+
+  private PerfRunData createPerfRunData(File file, boolean setBZCompress,
+                                        String bz2CompressVal,
+                                        String docMakerName) throws Exception {
+    Properties props = new Properties();
+    props.setProperty("doc.maker", docMakerName);
+    props.setProperty("line.file.out", file.getAbsolutePath());
+    if (setBZCompress) {
+      props.setProperty("bzip.compression", bz2CompressVal);
+    }
+    props.setProperty("directory", "RAMDirectory"); // no accidental FS dir.
+    Config config = new Config(props);
+    return new PerfRunData(config);
+  }
+  
+  private void doReadTest(File file, boolean bz2File, String expTitle,
+                          String expDate, String expBody) throws Exception {
+    InputStream in = new FileInputStream(file);
+    if (bz2File) {
+      in = csFactory.createCompressorInputStream("bzip2", in);
+    }
+    BufferedReader br = new BufferedReader(new InputStreamReader(in, "utf-8"));
+    try {
+      String line = br.readLine();
+      assertNotNull(line);
+      String[] parts = line.split(Character.toString(WriteLineDocTask.SEP));
+      int numExpParts = expBody == null ? 2 : 3;
+      assertEquals(numExpParts, parts.length);
+      assertEquals(expTitle, parts[0]);
+      assertEquals(expDate, parts[1]);
+      if (expBody != null) {
+        assertEquals(expBody, parts[2]);
+      }
+      assertNull(br.readLine());
+    } finally {
+      br.close();
+    }
+  }
+  
+  /* Tests WriteLineDocTask with a bzip2 format. */
+  public void testBZip2() throws Exception {
+    
+    // Create a document in bz2 format.
+    File file = new File(getWorkDir(), "one-line.bz2");
+    PerfRunData runData = createPerfRunData(file, true, "true", WriteLineDocMaker.class.getName());
+    WriteLineDocTask wldt = new WriteLineDocTask(runData);
+    wldt.doLogic();
+    wldt.close();
+    
+    doReadTest(file, true, "title", "date", "body");
+  }
+  
+  public void testBZip2AutoDetect() throws Exception {
+    
+    // Create a document in bz2 format.
+    File file = new File(getWorkDir(), "one-line.bz2");
+    PerfRunData runData = createPerfRunData(file, false, null, WriteLineDocMaker.class.getName());
+    WriteLineDocTask wldt = new WriteLineDocTask(runData);
+    wldt.doLogic();
+    wldt.close();
+    
+    doReadTest(file, true, "title", "date", "body");
+  }
+  
+  public void testRegularFile() throws Exception {
+    
+    // Create a document in regular format.
+    File file = new File(getWorkDir(), "one-line");
+    PerfRunData runData = createPerfRunData(file, true, "false", WriteLineDocMaker.class.getName());
+    WriteLineDocTask wldt = new WriteLineDocTask(runData);
+    wldt.doLogic();
+    wldt.close();
+    
+    doReadTest(file, false, "title", "date", "body");
+  }
+
+  public void testCharsReplace() throws Exception {
+    // WriteLineDocTask replaced only \t characters w/ a space, since that's its
+    // separator char. However, it didn't replace newline characters, which
+    // resulted in errors in LineDocSource.
+    File file = new File(getWorkDir(), "one-line");
+    PerfRunData runData = createPerfRunData(file, false, null, NewLinesDocMaker.class.getName());
+    WriteLineDocTask wldt = new WriteLineDocTask(runData);
+    wldt.doLogic();
+    wldt.close();
+    
+    doReadTest(file, false, "title text", "date text", "body text two");
+  }
+  
+  public void testEmptyBody() throws Exception {
+    // WriteLineDocTask threw away documents w/ no BODY element, even if they
+    // had a TITLE element (LUCENE-1755). It should throw away documents if they
+    // don't have BODY nor TITLE
+    File file = new File(getWorkDir(), "one-line");
+    PerfRunData runData = createPerfRunData(file, false, null, NoBodyDocMaker.class.getName());
+    WriteLineDocTask wldt = new WriteLineDocTask(runData);
+    wldt.doLogic();
+    wldt.close();
+    
+    doReadTest(file, false, "title", "date", null);
+  }
+  
+  public void testEmptyTitle() throws Exception {
+    File file = new File(getWorkDir(), "one-line");
+    PerfRunData runData = createPerfRunData(file, false, null, NoTitleDocMaker.class.getName());
+    WriteLineDocTask wldt = new WriteLineDocTask(runData);
+    wldt.doLogic();
+    wldt.close();
+    
+    doReadTest(file, false, "", "date", "body");
+  }
+  
+  public void testJustDate() throws Exception {
+    File file = new File(getWorkDir(), "one-line");
+    PerfRunData runData = createPerfRunData(file, false, null, JustDateDocMaker.class.getName());
+    WriteLineDocTask wldt = new WriteLineDocTask(runData);
+    wldt.doLogic();
+    wldt.close();
+    
+    BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(file), "utf-8"));
+    try {
+      String line = br.readLine();
+      assertNull(line);
+    } finally {
+      br.close();
+    }
+  }
+
+  public void testMultiThreaded() throws Exception {
+    File file = new File(getWorkDir(), "one-line");
+    PerfRunData runData = createPerfRunData(file, false, null, ThreadingDocMaker.class.getName());
+    final WriteLineDocTask wldt = new WriteLineDocTask(runData);
+    Thread[] threads = new Thread[10];
+    for (int i = 0; i < threads.length; i++) {
+      threads[i] = new Thread("t" + i) {
+        @Override
+        public void run() {
+          try {
+            wldt.doLogic();
+          } catch (Exception e) {
+            throw new RuntimeException(e);
+          }
+        }
+      };
+    }
+    
+    for (Thread t : threads) t.start();
+    for (Thread t : threads) t.join();
+    
+    wldt.close();
+    
+    Set<String> ids = new HashSet<String>();
+    BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(file), "utf-8"));
+    try {
+      for (int i = 0; i < threads.length; i++) {
+        String line = br.readLine();
+        String[] parts = line.split(Character.toString(WriteLineDocTask.SEP));
+        assertEquals(3, parts.length);
+        // check that all thread names written are the same in the same line
+        String tname = parts[0].substring(parts[0].indexOf('_'));
+        ids.add(tname);
+        assertEquals(tname, parts[1].substring(parts[1].indexOf('_')));
+        assertEquals(tname, parts[2].substring(parts[2].indexOf('_')));
+      }
+      // only threads.length lines should exist
+      assertNull(br.readLine());
+      assertEquals(threads.length, ids.size());
+    } finally {
+      br.close();
+    }
+  }
+
+}
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/utils/TestConfig.java b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/utils/TestConfig.java
new file mode 100644
index 0000000..939f74d
--- /dev/null
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/utils/TestConfig.java
@@ -0,0 +1,37 @@
+package org.apache.lucene.benchmark.byTask.utils;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Properties;
+
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+
+public class TestConfig extends LuceneTestCase {
+
+  @Test
+  public void testAbsolutePathNamesWindows() throws Exception {
+    Properties props = new Properties();
+    props.setProperty("work.dir1", "c:\\temp");
+    props.setProperty("work.dir2", "c:/temp");
+    Config conf = new Config(props);
+    assertEquals("c:\\temp", conf.get("work.dir1", ""));
+    assertEquals("c:/temp", conf.get("work.dir2", ""));
+  }
+
+}
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/quality/TestQualityRun.java b/modules/benchmark/src/test/org/apache/lucene/benchmark/quality/TestQualityRun.java
new file mode 100644
index 0000000..4cee276
--- /dev/null
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/quality/TestQualityRun.java
@@ -0,0 +1,195 @@
+package org.apache.lucene.benchmark.quality;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.benchmark.BenchmarkTestCase;
+import org.apache.lucene.benchmark.quality.trec.TrecJudge;
+import org.apache.lucene.benchmark.quality.trec.TrecTopicsReader;
+import org.apache.lucene.benchmark.quality.utils.SimpleQQParser;
+import org.apache.lucene.benchmark.quality.utils.SubmissionReport;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.store.Directory;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.io.PrintWriter;
+
+/**
+ * Test that quality run does its job.
+ * <p>
+ * NOTE: if the default scoring or StandardAnalyzer is changed, then
+ * this test will not work correctly, as it does not dynamically
+ * generate its test trec topics/qrels!
+ */
+public class TestQualityRun extends BenchmarkTestCase {
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    copyToWorkDir("reuters.578.lines.txt.bz2");
+  }
+
+  public void testTrecQuality() throws Exception {
+    // first create the partial reuters index
+    createReutersIndex();
+    
+    int maxResults = 1000;
+    String docNameField = "doctitle"; // orig docID is in the linedoc format title 
+    
+    PrintWriter logger = VERBOSE ? new PrintWriter(System.out,true) : null;
+   
+    // prepare topics
+    InputStream topics = getClass().getResourceAsStream("trecTopics.txt");
+    TrecTopicsReader qReader = new TrecTopicsReader();
+    QualityQuery qqs[] = qReader.readQueries(new BufferedReader(new InputStreamReader(topics, "UTF-8")));
+    
+    // prepare judge
+    InputStream qrels = getClass().getResourceAsStream("trecQRels.txt");
+    Judge judge = new TrecJudge(new BufferedReader(new InputStreamReader(qrels, "UTF-8")));
+    
+    // validate topics & judgments match each other
+    judge.validateData(qqs, logger);
+    
+    Directory dir = newFSDirectory(new File(getWorkDir(),"index"));
+    IndexSearcher searcher = new IndexSearcher(dir, true);
+
+    QualityQueryParser qqParser = new SimpleQQParser("title","body");
+    QualityBenchmark qrun = new QualityBenchmark(qqs, qqParser, searcher, docNameField);
+    
+    SubmissionReport submitLog = VERBOSE ? new SubmissionReport(logger, "TestRun") : null;
+    qrun.setMaxResults(maxResults);
+    QualityStats stats[] = qrun.execute(judge, submitLog, logger);
+    
+    // --------- verify by the way judgments were altered for this test:
+    // for some queries, depending on m = qnum % 8
+    // m==0: avg_precision and recall are hurt, by marking fake docs as relevant
+    // m==1: precision_at_n and avg_precision are hurt, by unmarking relevant docs
+    // m==2: all precision, precision_at_n and recall are hurt.
+    // m>=3: these queries remain perfect
+    for (int i = 0; i < stats.length; i++) {
+      QualityStats s = stats[i];
+      switch (i%8) {
+
+      case 0:
+        assertTrue("avg-p should be hurt: "+s.getAvp(), 1.0 > s.getAvp());
+        assertTrue("recall should be hurt: "+s.getRecall(), 1.0 > s.getRecall());
+        for (int j = 1; j <= QualityStats.MAX_POINTS; j++) {
+          assertEquals("p_at_"+j+" should be perfect: "+s.getPrecisionAt(j), 1.0, s.getPrecisionAt(j), 1E-2);
+        }
+        break;
+      
+      case 1:
+        assertTrue("avg-p should be hurt", 1.0 > s.getAvp());
+        assertEquals("recall should be perfect: "+s.getRecall(), 1.0, s.getRecall(), 1E-2);
+        for (int j = 1; j <= QualityStats.MAX_POINTS; j++) {
+          assertTrue("p_at_"+j+" should be hurt: "+s.getPrecisionAt(j), 1.0 > s.getPrecisionAt(j));
+        }
+        break;
+
+      case 2:
+        assertTrue("avg-p should be hurt: "+s.getAvp(), 1.0 > s.getAvp());
+        assertTrue("recall should be hurt: "+s.getRecall(), 1.0 > s.getRecall());
+        for (int j = 1; j <= QualityStats.MAX_POINTS; j++) {
+          assertTrue("p_at_"+j+" should be hurt: "+s.getPrecisionAt(j), 1.0 > s.getPrecisionAt(j));
+        }
+        break;
+
+      default: {
+        assertEquals("avg-p should be perfect: "+s.getAvp(), 1.0, s.getAvp(), 1E-2);
+        assertEquals("recall should be perfect: "+s.getRecall(), 1.0, s.getRecall(), 1E-2);
+        for (int j = 1; j <= QualityStats.MAX_POINTS; j++) {
+          assertEquals("p_at_"+j+" should be perfect: "+s.getPrecisionAt(j), 1.0, s.getPrecisionAt(j), 1E-2);
+        }
+      }
+      
+      }
+    }
+    
+    QualityStats avg = QualityStats.average(stats);
+    if (logger!=null) {
+      avg.log("Average statistis:",1,logger,"  ");
+    }
+    
+    assertTrue("mean avg-p should be hurt: "+avg.getAvp(), 1.0 > avg.getAvp());
+    assertTrue("avg recall should be hurt: "+avg.getRecall(), 1.0 > avg.getRecall());
+    for (int j = 1; j <= QualityStats.MAX_POINTS; j++) {
+      assertTrue("avg p_at_"+j+" should be hurt: "+avg.getPrecisionAt(j), 1.0 > avg.getPrecisionAt(j));
+    }
+    
+    searcher.close();
+    dir.close();
+  }
+  
+  public void testTrecTopicsReader() throws Exception {    
+    // prepare topics
+    InputStream topicsFile = getClass().getResourceAsStream("trecTopics.txt");
+    TrecTopicsReader qReader = new TrecTopicsReader();
+    QualityQuery qqs[] = qReader.readQueries(
+        new BufferedReader(new InputStreamReader(topicsFile, "UTF-8")));
+    
+    assertEquals(20, qqs.length);
+    
+    QualityQuery qq = qqs[0];
+    assertEquals("statement months  total 1987", qq.getValue("title"));
+    assertEquals("Topic 0 Description Line 1 Topic 0 Description Line 2", 
+        qq.getValue("description"));
+    assertEquals("Topic 0 Narrative Line 1 Topic 0 Narrative Line 2", 
+        qq.getValue("narrative"));
+    
+    qq = qqs[1];
+    assertEquals("agreed 15  against five", qq.getValue("title"));
+    assertEquals("Topic 1 Description Line 1 Topic 1 Description Line 2", 
+        qq.getValue("description"));
+    assertEquals("Topic 1 Narrative Line 1 Topic 1 Narrative Line 2", 
+        qq.getValue("narrative"));
+    
+    qq = qqs[19];
+    assertEquals("20 while  common week", qq.getValue("title"));
+    assertEquals("Topic 19 Description Line 1 Topic 19 Description Line 2", 
+        qq.getValue("description"));
+    assertEquals("Topic 19 Narrative Line 1 Topic 19 Narrative Line 2", 
+        qq.getValue("narrative"));
+  }
+
+  // use benchmark logic to create the mini Reuters index
+  private void createReutersIndex() throws Exception {
+    // 1. alg definition
+    String algLines[] = {
+        "# ----- properties ",
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "analyzer=org.apache.lucene.analysis.standard.ClassicAnalyzer",
+        "docs.file=" + getWorkDirResourcePath("reuters.578.lines.txt.bz2"),
+        "content.source.log.step=2500",
+        "doc.term.vector=false",
+        "content.source.forever=false",
+        "directory=FSDirectory",
+        "doc.stored=true",
+        "doc.tokenized=true",
+        "# ----- alg ",
+        "ResetSystemErase",
+        "CreateIndex",
+        "{ AddDoc } : *",
+        "CloseIndex",
+    };
+    
+    // 2. execute the algorithm  (required in every "logic" test)
+    execBenchmark(algLines);
+  }
+}
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/quality/reuters.578.lines.txt.bz2 b/modules/benchmark/src/test/org/apache/lucene/benchmark/quality/reuters.578.lines.txt.bz2
new file mode 100644
index 0000000..1fd8d54
Binary files /dev/null and b/modules/benchmark/src/test/org/apache/lucene/benchmark/quality/reuters.578.lines.txt.bz2 differ
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/quality/trecQRels.txt b/modules/benchmark/src/test/org/apache/lucene/benchmark/quality/trecQRels.txt
new file mode 100755
index 0000000..13c2d77
--- /dev/null
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/quality/trecQRels.txt
@@ -0,0 +1,723 @@
+# -----------------------------------------------------------------------
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+# 
+#     http://www.apache.org/licenses/LICENSE-2.0
+# 
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# -----------------------------------------------------------------------
+
+# ------------------------------------------------------------
+# Format:
+#
+#       qnum   0   doc-name     is-relevant
+#
+#
+# The origin of this file was created using 
+# utils.QualityQueriesFinder, so all queries 
+# would have perfect 1.0 for all meassures.
+#
+# To make it suitable for testing it was modified
+# for some queries, depending on m = qnum % 8
+# m==0: avg_precision and recall are hurt, by marking fake docs as relevant
+# m==1: precision_at_n and avg_precision are hurt, by unmarking relevant docs
+# m==2: all precision, precision_at_n and recall are hurt.
+# m>=3: these queries remain perfect
+# ------------------------------------------------------------
+
+# --- m==0: avg_precision and recall are hurt, by marking fake docs as relevant
+
+0 	 0 	 fakedoc1             	 1
+0 	 0 	 fakedoc2             	 1
+0 	 0 	 fakedoc3             	 1
+0 	 0 	 fakedoc4             	 1
+
+0 	 0 	 doc18211             	 1
+0 	 0 	 doc20192             	 1
+0 	 0 	 doc7401              	 1
+0 	 0 	 doc11285             	 1
+0 	 0 	 doc20647             	 1
+0 	 0 	 doc3057              	 1
+0 	 0 	 doc12431             	 1
+0 	 0 	 doc4989              	 1
+0 	 0 	 doc17324             	 1
+0 	 0 	 doc4030              	 1
+0 	 0 	 doc4290              	 1
+0 	 0 	 doc3462              	 1
+0 	 0 	 doc15313             	 1
+0 	 0 	 doc10303             	 1
+0 	 0 	 doc1893              	 1
+0 	 0 	 doc5008              	 1
+0 	 0 	 doc14634             	 1
+0 	 0 	 doc5471              	 1
+0 	 0 	 doc17904             	 1
+0 	 0 	 doc7168              	 1
+0 	 0 	 doc21275             	 1
+0 	 0 	 doc9011              	 1
+0 	 0 	 doc17546             	 1
+0 	 0 	 doc9102              	 1
+0 	 0 	 doc13199             	 1
+
+# --- m==1: precision_at_n and avg_precision are hurt, by unmarking relevant docs
+
+1 	 0 	 doc9857              	 0
+1 	 0 	 doc16846             	 1
+1 	 0 	 doc4320              	 1
+1 	 0 	 doc9501              	 0
+1 	 0 	 doc10159             	 1
+1 	 0 	 doc16642             	 1
+1 	 0 	 doc17536             	 0
+1 	 0 	 doc17571             	 1
+1 	 0 	 doc18728             	 1
+1 	 0 	 doc18828             	 1
+1 	 0 	 doc19108             	 0
+1 	 0 	 doc9940              	 1
+1 	 0 	 doc11852             	 1
+1 	 0 	 doc7430              	 0
+1 	 0 	 doc19162             	 1
+1 	 0 	 doc1743              	 1
+1 	 0 	 doc2137              	 1
+1 	 0 	 doc7611              	 1
+1 	 0 	 doc8072              	 1
+1 	 0 	 doc12764             	 1
+1 	 0 	 doc2593              	 1
+1 	 0 	 doc11088             	 1
+1 	 0 	 doc931               	 1
+1 	 0 	 doc7673              	 1
+1 	 0 	 doc12941             	 1
+1 	 0 	 doc11797             	 1
+1 	 0 	 doc11831             	 1
+1 	 0 	 doc13162             	 1
+1 	 0 	 doc4423              	 1
+1 	 0 	 doc5217              	 1
+
+# ---- m==2: all precision, precision_at_n and recall are hurt.
+
+2 	 0 	 fakedoc1             	 1
+2 	 0 	 fakedoc2             	 1
+2 	 0 	 fakedoc3             	 1
+2 	 0 	 fakedoc4             	 1
+
+2 	 0 	 doc3137              	 0
+2 	 0 	 doc7142              	 0
+2 	 0 	 doc13667             	 0
+2 	 0 	 doc13171             	 0
+2 	 0 	 doc13372             	 1
+2 	 0 	 doc21415             	 1
+2 	 0 	 doc16298             	 1
+2 	 0 	 doc14957             	 1
+2 	 0 	 doc153               	 1
+2 	 0 	 doc16092             	 1
+2 	 0 	 doc16096             	 1
+2 	 0 	 doc21303             	 1
+2 	 0 	 doc18681             	 1
+2 	 0 	 doc20756             	 1
+2 	 0 	 doc355               	 1
+2 	 0 	 doc13395             	 1
+2 	 0 	 doc5009              	 1
+2 	 0 	 doc17164             	 1
+2 	 0 	 doc13162             	 1
+2 	 0 	 doc11757             	 1
+2 	 0 	 doc9637              	 1
+2 	 0 	 doc18087             	 1
+2 	 0 	 doc4593              	 1
+2 	 0 	 doc4677              	 1
+2 	 0 	 doc20865             	 1
+2 	 0 	 doc8556              	 1
+2 	 0 	 doc2578              	 1
+2 	 0 	 doc1163              	 1
+2 	 0 	 doc3797              	 1
+2 	 0 	 doc11094             	 1
+
+
+3 	 0 	 doc19578             	 1
+3 	 0 	 doc14860             	 1
+3 	 0 	 doc7235              	 1
+3 	 0 	 doc20590             	 1
+3 	 0 	 doc17933             	 1
+3 	 0 	 doc9384              	 1
+3 	 0 	 doc10783             	 1
+3 	 0 	 doc1963              	 1
+3 	 0 	 doc18356             	 1
+3 	 0 	 doc13254             	 1
+3 	 0 	 doc18402             	 1
+3 	 0 	 doc15241             	 1
+3 	 0 	 doc3303              	 1
+3 	 0 	 doc8868              	 1
+3 	 0 	 doc18520             	 1
+3 	 0 	 doc4650              	 1
+3 	 0 	 doc4727              	 1
+3 	 0 	 doc21518             	 1
+3 	 0 	 doc5060              	 1
+3 	 0 	 doc7587              	 1
+3 	 0 	 doc2990              	 1
+3 	 0 	 doc8042              	 1
+3 	 0 	 doc6304              	 1
+3 	 0 	 doc13223             	 1
+3 	 0 	 doc1964              	 1
+3 	 0 	 doc10597             	 1
+3 	 0 	 doc21023             	 1
+3 	 0 	 doc19057             	 1
+3 	 0 	 doc14948             	 1
+3 	 0 	 doc9692              	 1
+
+
+4 	 0 	 doc2534              	 1
+4 	 0 	 doc21388             	 1
+4 	 0 	 doc20923             	 1
+4 	 0 	 doc11547             	 1
+4 	 0 	 doc19755             	 1
+4 	 0 	 doc3793              	 1
+4 	 0 	 doc6714              	 1
+4 	 0 	 doc12722             	 1
+4 	 0 	 doc5552              	 1
+4 	 0 	 doc6810              	 1
+4 	 0 	 doc16953             	 1
+4 	 0 	 doc2527              	 1
+4 	 0 	 doc5361              	 1
+4 	 0 	 doc12353             	 1
+4 	 0 	 doc7308              	 1
+4 	 0 	 doc3836              	 1
+4 	 0 	 doc2293              	 1
+4 	 0 	 doc7348              	 1
+4 	 0 	 doc17119             	 1
+4 	 0 	 doc19331             	 1
+4 	 0 	 doc3411              	 1
+4 	 0 	 doc14643             	 1
+4 	 0 	 doc9058              	 1
+4 	 0 	 doc11099             	 1
+4 	 0 	 doc12485             	 1
+4 	 0 	 doc16432             	 1
+4 	 0 	 doc10047             	 1
+4 	 0 	 doc13788             	 1
+4 	 0 	 doc117               	 1
+4 	 0 	 doc638               	 1
+
+
+
+5 	 0 	 doc169               	 1
+5 	 0 	 doc13181             	 1
+5 	 0 	 doc4350              	 1
+5 	 0 	 doc10242             	 1
+5 	 0 	 doc955               	 1
+5 	 0 	 doc5389              	 1
+5 	 0 	 doc17122             	 1
+5 	 0 	 doc17417             	 1
+5 	 0 	 doc12199             	 1
+5 	 0 	 doc6918              	 1
+5 	 0 	 doc3857              	 1
+5 	 0 	 doc2981              	 1
+5 	 0 	 doc10639             	 1
+5 	 0 	 doc10478             	 1
+5 	 0 	 doc8573              	 1
+5 	 0 	 doc9197              	 1
+5 	 0 	 doc9298              	 1
+5 	 0 	 doc2492              	 1
+5 	 0 	 doc10262             	 1
+5 	 0 	 doc5180              	 1
+5 	 0 	 doc11758             	 1
+5 	 0 	 doc4065              	 1
+5 	 0 	 doc9124              	 1
+5 	 0 	 doc11528             	 1
+5 	 0 	 doc18879             	 1
+5 	 0 	 doc17864             	 1
+5 	 0 	 doc3204              	 1
+5 	 0 	 doc12157             	 1
+5 	 0 	 doc4496              	 1
+5 	 0 	 doc20190             	 1
+
+
+
+6 	 0 	 doc9507              	 1
+6 	 0 	 doc15630             	 1
+6 	 0 	 doc8469              	 1
+6 	 0 	 doc11918             	 1
+6 	 0 	 doc20482             	 1
+6 	 0 	 doc20158             	 1
+6 	 0 	 doc19831             	 1
+6 	 0 	 doc8296              	 1
+6 	 0 	 doc8930              	 1
+6 	 0 	 doc16460             	 1
+6 	 0 	 doc2577              	 1
+6 	 0 	 doc15476             	 1
+6 	 0 	 doc1767              	 1
+6 	 0 	 doc689               	 1
+6 	 0 	 doc16606             	 1
+6 	 0 	 doc6149              	 1
+6 	 0 	 doc18691             	 1
+6 	 0 	 doc2208              	 1
+6 	 0 	 doc3592              	 1
+6 	 0 	 doc11199             	 1
+6 	 0 	 doc16329             	 1
+6 	 0 	 doc6007              	 1
+6 	 0 	 doc15231             	 1
+6 	 0 	 doc20622             	 1
+6 	 0 	 doc21468             	 1
+6 	 0 	 doc12230             	 1
+6 	 0 	 doc5723              	 1
+6 	 0 	 doc8120              	 1
+6 	 0 	 doc8668              	 1
+6 	 0 	 doc303               	 1
+
+
+
+
+7 	 0 	 doc7728              	 1
+7 	 0 	 doc7693              	 1
+7 	 0 	 doc21088             	 1
+7 	 0 	 doc5017              	 1
+7 	 0 	 doc10807             	 1
+7 	 0 	 doc16204             	 1
+7 	 0 	 doc2233              	 1
+7 	 0 	 doc3632              	 1
+7 	 0 	 doc4719              	 1
+7 	 0 	 doc6477              	 1
+7 	 0 	 doc6502              	 1
+7 	 0 	 doc6709              	 1
+7 	 0 	 doc7710              	 1
+7 	 0 	 doc9193              	 1
+7 	 0 	 doc9309              	 1
+7 	 0 	 doc9789              	 1
+7 	 0 	 doc10971             	 1
+7 	 0 	 doc18059             	 1
+7 	 0 	 doc19906             	 1
+7 	 0 	 doc20089             	 1
+7 	 0 	 doc20102             	 1
+7 	 0 	 doc21040             	 1
+7 	 0 	 doc21153             	 1
+7 	 0 	 doc9147              	 1
+7 	 0 	 doc9930              	 1
+7 	 0 	 doc19763             	 1
+7 	 0 	 doc1559              	 1
+7 	 0 	 doc21248             	 1
+7 	 0 	 doc17945             	 1
+7 	 0 	 doc526               	 1
+
+
+# --- m==0: avg_precision and recall are hurt, by marking fake docs as relevant
+
+8 	 0 	 fakedoc1             	 1
+8 	 0 	 fakedoc2             	 1
+8 	 0 	 fakedoc3             	 1
+8 	 0 	 fakedoc4             	 1
+
+8 	 0 	 doc16299             	 1
+8 	 0 	 doc1662              	 1
+8 	 0 	 doc4585              	 1
+8 	 0 	 doc12315             	 1
+8 	 0 	 doc16266             	 1
+8 	 0 	 doc13136             	 1
+8 	 0 	 doc19212             	 1
+8 	 0 	 doc7086              	 1
+8 	 0 	 doc7062              	 1
+8 	 0 	 doc6134              	 1
+8 	 0 	 doc13953             	 1
+8 	 0 	 doc16264             	 1
+8 	 0 	 doc2494              	 1
+8 	 0 	 doc10636             	 1
+8 	 0 	 doc10894             	 1
+8 	 0 	 doc6844              	 1
+8 	 0 	 doc674               	 1
+8 	 0 	 doc13520             	 1
+8 	 0 	 doc344               	 1
+8 	 0 	 doc2896              	 1
+8 	 0 	 doc11871             	 1
+8 	 0 	 doc1862              	 1
+8 	 0 	 doc16728             	 1
+8 	 0 	 doc10308             	 1
+8 	 0 	 doc2227              	 1
+8 	 0 	 doc13167             	 1
+8 	 0 	 doc20607             	 1
+8 	 0 	 doc9670              	 1
+8 	 0 	 doc1566              	 1
+8 	 0 	 doc17885             	 1
+
+
+# ---- m==1: precision_at_n and avg_precision are hurt, by unmarking relevant docs
+
+
+9 	 0 	 doc1990              	 0
+9 	 0 	 doc9342              	 1
+9 	 0 	 doc19427             	 1
+9 	 0 	 doc12432             	 0
+9 	 0 	 doc13480             	 1
+9 	 0 	 doc3322              	 1
+9 	 0 	 doc16044             	 1
+9 	 0 	 doc266               	 0
+9 	 0 	 doc3437              	 1
+9 	 0 	 doc5370              	 1
+9 	 0 	 doc10314             	 1
+9 	 0 	 doc4892              	 1
+9 	 0 	 doc5763              	 0
+9 	 0 	 doc14045             	 1
+9 	 0 	 doc1090              	 1
+9 	 0 	 doc7437              	 1
+9 	 0 	 doc5822              	 1
+9 	 0 	 doc4285              	 1
+9 	 0 	 doc17119             	 1
+9 	 0 	 doc21001             	 1
+9 	 0 	 doc4337              	 1
+9 	 0 	 doc5967              	 1
+9 	 0 	 doc10214             	 1
+9 	 0 	 doc12001             	 1
+9 	 0 	 doc18553             	 1
+9 	 0 	 doc12116             	 1
+9 	 0 	 doc5064              	 1
+9 	 0 	 doc5018              	 1
+9 	 0 	 doc5037              	 1
+9 	 0 	 doc8025              	 1
+
+
+# ---- m==2: all precision, precision_at_n and recall are hurt.
+
+10 	 0 	 fakedoc1             	 1
+10 	 0 	 fakedoc2             	 1
+10 	 0 	 fakedoc3             	 1
+10 	 0 	 fakedoc4             	 1
+
+10 	 0 	 doc17218             	 0
+10 	 0 	 doc10270             	 0
+10 	 0 	 doc5958              	 0
+10 	 0 	 doc19943             	 0
+10 	 0 	 doc6510              	 1
+10 	 0 	 doc16087             	 1
+10 	 0 	 doc14893             	 1
+10 	 0 	 doc8933              	 1
+10 	 0 	 doc4354              	 1
+10 	 0 	 doc16729             	 1
+10 	 0 	 doc16761             	 1
+10 	 0 	 doc6964              	 1
+10 	 0 	 doc16743             	 1
+10 	 0 	 doc7357              	 1
+10 	 0 	 doc2534              	 1
+10 	 0 	 doc18321             	 1
+10 	 0 	 doc18497             	 1
+10 	 0 	 doc11214             	 1
+10 	 0 	 doc11819             	 1
+10 	 0 	 doc10818             	 1
+10 	 0 	 doc15769             	 1
+10 	 0 	 doc5348              	 1
+10 	 0 	 doc14948             	 1
+10 	 0 	 doc7891              	 1
+10 	 0 	 doc9897              	 1
+10 	 0 	 doc15559             	 1
+10 	 0 	 doc14935             	 1
+10 	 0 	 doc14954             	 1
+10 	 0 	 doc6621              	 1
+10 	 0 	 doc6930              	 1
+
+
+11 	 0 	 doc11943             	 1
+11 	 0 	 doc286               	 1
+11 	 0 	 doc1574              	 1
+11 	 0 	 doc17916             	 1
+11 	 0 	 doc17918             	 1
+11 	 0 	 doc19213             	 1
+11 	 0 	 doc9337              	 1
+11 	 0 	 doc8593              	 1
+11 	 0 	 doc8800              	 1
+11 	 0 	 doc18580             	 1
+11 	 0 	 doc209               	 1
+11 	 0 	 doc1893              	 1
+11 	 0 	 doc11189             	 1
+11 	 0 	 doc17702             	 1
+11 	 0 	 doc10180             	 1
+11 	 0 	 doc11869             	 1
+11 	 0 	 doc9705              	 1
+11 	 0 	 doc8715              	 1
+11 	 0 	 doc12753             	 1
+11 	 0 	 doc10195             	 1
+11 	 0 	 doc3552              	 1
+11 	 0 	 doc16030             	 1
+11 	 0 	 doc4623              	 1
+11 	 0 	 doc3188              	 1
+11 	 0 	 doc8735              	 1
+11 	 0 	 doc151               	 1
+11 	 0 	 doc5792              	 1
+11 	 0 	 doc5194              	 1
+11 	 0 	 doc3393              	 1
+11 	 0 	 doc19027             	 1
+
+
+
+12 	 0 	 doc18198             	 1
+12 	 0 	 doc2444              	 1
+12 	 0 	 doc4305              	 1
+12 	 0 	 doc6544              	 1
+12 	 0 	 doc11639             	 1
+12 	 0 	 doc10640             	 1
+12 	 0 	 doc12192             	 1
+12 	 0 	 doc128               	 1
+12 	 0 	 doc10760             	 1
+12 	 0 	 doc10881             	 1
+12 	 0 	 doc2698              	 1
+12 	 0 	 doc3552              	 1
+12 	 0 	 doc20524             	 1
+12 	 0 	 doc1884              	 1
+12 	 0 	 doc9187              	 1
+12 	 0 	 doc3131              	 1
+12 	 0 	 doc2911              	 1
+12 	 0 	 doc2589              	 1
+12 	 0 	 doc3747              	 1
+12 	 0 	 doc3813              	 1
+12 	 0 	 doc5222              	 1
+12 	 0 	 doc6023              	 1
+12 	 0 	 doc6624              	 1
+12 	 0 	 doc7655              	 1
+12 	 0 	 doc9205              	 1
+12 	 0 	 doc12062             	 1
+12 	 0 	 doc15504             	 1
+12 	 0 	 doc13625             	 1
+12 	 0 	 doc18704             	 1
+12 	 0 	 doc2277              	 1
+
+
+
+13 	 0 	 doc4948              	 1
+13 	 0 	 doc21565             	 1
+13 	 0 	 doc17135             	 1
+13 	 0 	 doc1866              	 1
+13 	 0 	 doc13989             	 1
+13 	 0 	 doc5605              	 1
+13 	 0 	 doc13431             	 1
+13 	 0 	 doc2100              	 1
+13 	 0 	 doc16347             	 1
+13 	 0 	 doc16894             	 1
+13 	 0 	 doc6764              	 1
+13 	 0 	 doc8554              	 1
+13 	 0 	 doc8695              	 1
+13 	 0 	 doc8977              	 1
+13 	 0 	 doc19478             	 1
+13 	 0 	 doc14595             	 1
+13 	 0 	 doc2408              	 1
+13 	 0 	 doc2592              	 1
+13 	 0 	 doc10947             	 1
+13 	 0 	 doc15794             	 1
+13 	 0 	 doc5236              	 1
+13 	 0 	 doc14847             	 1
+13 	 0 	 doc3980              	 1
+13 	 0 	 doc1844              	 1
+13 	 0 	 doc42                	 1
+13 	 0 	 doc7783              	 1
+13 	 0 	 doc4557              	 1
+13 	 0 	 doc16423             	 1
+13 	 0 	 doc17170             	 1
+13 	 0 	 doc5822              	 1
+
+
+
+14 	 0 	 doc17172             	 1
+14 	 0 	 doc17210             	 1
+14 	 0 	 doc5044              	 1
+14 	 0 	 doc4627              	 1
+14 	 0 	 doc4683              	 1
+14 	 0 	 doc15126             	 1
+14 	 0 	 doc4538              	 1
+14 	 0 	 doc273               	 1
+14 	 0 	 doc19585             	 1
+14 	 0 	 doc16078             	 1
+14 	 0 	 doc4529              	 1
+14 	 0 	 doc4186              	 1
+14 	 0 	 doc12961             	 1
+14 	 0 	 doc19217             	 1
+14 	 0 	 doc5670              	 1
+14 	 0 	 doc1699              	 1
+14 	 0 	 doc4716              	 1
+14 	 0 	 doc12644             	 1
+14 	 0 	 doc18387             	 1
+14 	 0 	 doc336               	 1
+14 	 0 	 doc16130             	 1
+14 	 0 	 doc18718             	 1
+14 	 0 	 doc12527             	 1
+14 	 0 	 doc11797             	 1
+14 	 0 	 doc11831             	 1
+14 	 0 	 doc7538              	 1
+14 	 0 	 doc17259             	 1
+14 	 0 	 doc18724             	 1
+14 	 0 	 doc19330             	 1
+14 	 0 	 doc19206             	 1
+
+
+
+15 	 0 	 doc12198             	 1
+15 	 0 	 doc20371             	 1
+15 	 0 	 doc2947              	 1
+15 	 0 	 doc10750             	 1
+15 	 0 	 doc7239              	 1
+15 	 0 	 doc14189             	 1
+15 	 0 	 doc19474             	 1
+15 	 0 	 doc14776             	 1
+15 	 0 	 doc21270             	 1
+15 	 0 	 doc6387              	 1
+15 	 0 	 doc12908             	 1
+15 	 0 	 doc9573              	 1
+15 	 0 	 doc17102             	 1
+15 	 0 	 doc21482             	 1
+15 	 0 	 doc6524              	 1
+15 	 0 	 doc18034             	 1
+15 	 0 	 doc1358              	 1
+15 	 0 	 doc13147             	 1
+15 	 0 	 doc17731             	 1
+15 	 0 	 doc12890             	 1
+15 	 0 	 doc20887             	 1
+15 	 0 	 doc19508             	 1
+15 	 0 	 doc18498             	 1
+15 	 0 	 doc20642             	 1
+15 	 0 	 doc19878             	 1
+15 	 0 	 doc6556              	 1
+15 	 0 	 doc10272             	 1
+15 	 0 	 doc5720              	 1
+15 	 0 	 doc17578             	 1
+15 	 0 	 doc17164             	 1
+
+
+# --- m==0: avg_precision and recall are hurt, by marking fake docs as relevant
+
+16 	 0 	 fakedoc1             	 1
+16 	 0 	 fakedoc2             	 1
+16 	 0 	 fakedoc3             	 1
+16 	 0 	 fakedoc4             	 1
+
+16 	 0 	 doc4043              	 1
+16 	 0 	 doc14985             	 1
+16 	 0 	 doc15370             	 1
+16 	 0 	 doc15426             	 1
+16 	 0 	 doc1702              	 1
+16 	 0 	 doc3062              	 1
+16 	 0 	 doc16134             	 1
+16 	 0 	 doc15037             	 1
+16 	 0 	 doc8224              	 1
+16 	 0 	 doc5044              	 1
+16 	 0 	 doc8545              	 1
+16 	 0 	 doc7228              	 1
+16 	 0 	 doc12686             	 1
+16 	 0 	 doc16609             	 1
+16 	 0 	 doc13161             	 1
+16 	 0 	 doc3446              	 1
+16 	 0 	 doc16493             	 1
+16 	 0 	 doc19297             	 1
+16 	 0 	 doc13619             	 1
+16 	 0 	 doc3281              	 1
+16 	 0 	 doc15499             	 1
+16 	 0 	 doc7373              	 1
+16 	 0 	 doc9064              	 1
+16 	 0 	 doc1710              	 1
+16 	 0 	 doc15411             	 1
+16 	 0 	 doc10890             	 1
+16 	 0 	 doc3166              	 1
+16 	 0 	 doc17894             	 1
+16 	 0 	 doc4560              	 1
+16 	 0 	 doc12766             	 1
+
+
+# --- m==1: precision_at_n and avg_precision are hurt, by unmarking relevant docs
+
+17 	 0 	 doc3117              	 0
+17 	 0 	 doc7477              	 0
+17 	 0 	 doc7569              	 0
+17 	 0 	 doc20667             	 0
+17 	 0 	 doc20260             	 1
+17 	 0 	 doc17355             	 1
+17 	 0 	 doc11021             	 1
+17 	 0 	 doc20934             	 1
+17 	 0 	 doc552               	 1
+17 	 0 	 doc20856             	 1
+17 	 0 	 doc3524              	 1
+17 	 0 	 doc17343             	 1
+17 	 0 	 doc21055             	 1
+17 	 0 	 doc19032             	 1
+17 	 0 	 doc19786             	 1
+17 	 0 	 doc9281              	 1
+17 	 0 	 doc1695              	 1
+17 	 0 	 doc15940             	 1
+17 	 0 	 doc9215              	 1
+17 	 0 	 doc8335              	 1
+17 	 0 	 doc20936             	 1
+17 	 0 	 doc6914              	 1
+17 	 0 	 doc12122             	 1
+17 	 0 	 doc6618              	 1
+17 	 0 	 doc5049              	 1
+17 	 0 	 doc450               	 1
+17 	 0 	 doc19206             	 1
+17 	 0 	 doc18823             	 1
+17 	 0 	 doc5307              	 1
+17 	 0 	 doc17295             	 1
+
+
+# ---- m==2: all precision, precision_at_n and recall are hurt.
+
+18 	 0 	 fakedoc1             	 1
+18 	 0 	 fakedoc2             	 1
+18 	 0 	 fakedoc3             	 1
+18 	 0 	 fakedoc4             	 1
+
+18 	 0 	 doc8064              	 0
+18 	 0 	 doc18142             	 0
+18 	 0 	 doc19383             	 0
+18 	 0 	 doc21151             	 0
+18 	 0 	 doc4665              	 1
+18 	 0 	 doc2897              	 1
+18 	 0 	 doc6878              	 1
+18 	 0 	 doc14507             	 1
+18 	 0 	 doc2976              	 1
+18 	 0 	 doc11757             	 1
+18 	 0 	 doc12625             	 1
+18 	 0 	 doc14908             	 1
+18 	 0 	 doc12790             	 1
+18 	 0 	 doc17915             	 1
+18 	 0 	 doc11804             	 1
+18 	 0 	 doc12935             	 1
+18 	 0 	 doc8225              	 1
+18 	 0 	 doc18011             	 1
+18 	 0 	 doc10493             	 1
+18 	 0 	 doc17922             	 1
+18 	 0 	 doc1902              	 1
+18 	 0 	 doc14049             	 1
+18 	 0 	 doc1334              	 1
+18 	 0 	 doc1168              	 1
+18 	 0 	 doc4859              	 1
+18 	 0 	 doc7124              	 1
+18 	 0 	 doc9692              	 1
+18 	 0 	 doc18402             	 1
+18 	 0 	 doc9089              	 1
+18 	 0 	 doc15375             	 1
+
+
+19 	 0 	 doc5267              	 1
+19 	 0 	 doc2310              	 1
+19 	 0 	 doc11435             	 1
+19 	 0 	 doc15666             	 1
+19 	 0 	 doc12733             	 1
+19 	 0 	 doc7925              	 1
+19 	 0 	 doc2444              	 1
+19 	 0 	 doc4900              	 1
+19 	 0 	 doc10803             	 1
+19 	 0 	 doc8869              	 1
+19 	 0 	 doc5051              	 1
+19 	 0 	 doc9163              	 1
+19 	 0 	 doc529               	 1
+19 	 0 	 doc19546             	 1
+19 	 0 	 doc18561             	 1
+19 	 0 	 doc10634             	 1
+19 	 0 	 doc3979              	 1
+19 	 0 	 doc8833              	 1
+19 	 0 	 doc7652              	 1
+19 	 0 	 doc4804              	 1
+19 	 0 	 doc12616             	 1
+19 	 0 	 doc8419              	 1
+19 	 0 	 doc9431              	 1
+19 	 0 	 doc16235             	 1
+19 	 0 	 doc732               	 1
+19 	 0 	 doc2515              	 1
+19 	 0 	 doc7194              	 1
+19 	 0 	 doc16301             	 1
+19 	 0 	 doc4494              	 1
+19 	 0 	 doc4496              	 1
diff --git a/modules/benchmark/src/test/org/apache/lucene/benchmark/quality/trecTopics.txt b/modules/benchmark/src/test/org/apache/lucene/benchmark/quality/trecTopics.txt
new file mode 100755
index 0000000..2f3ada2
--- /dev/null
+++ b/modules/benchmark/src/test/org/apache/lucene/benchmark/quality/trecTopics.txt
@@ -0,0 +1,287 @@
+# -----------------------------------------------------------------------
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+# 
+#     http://www.apache.org/licenses/LICENSE-2.0
+# 
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# -----------------------------------------------------------------------
+
+# ------------------------------------------------------------
+# This file was created using utils.QualityQueriesFinder.
+# See also TrecQRels.txt.
+# ------------------------------------------------------------
+
+<top>
+<num> Number: 0
+
+<title> statement months  total 1987
+
+<desc> Description:
+Topic 0 Description Line 1
+Topic 0 Description Line 2
+
+<narr> Narrative:
+Topic 0 Narrative Line 1
+Topic 0 Narrative Line 2
+
+</top>
+
+<top>
+<num> Number: 1
+
+<title> agreed 15  against five
+
+<desc> Description:
+Topic 1 Description Line 1
+Topic 1 Description Line 2
+
+<narr> Narrative:
+Topic 1 Narrative Line 1
+Topic 1 Narrative Line 2
+
+</top>
+
+<top>
+<num> Number: 2
+
+<title> nine only  month international
+
+<desc> Description:
+
+
+<narr> Narrative:
+
+
+</top>
+
+<top>
+<num> Number: 3
+
+<title> finance any  10 government
+
+<desc> Description:
+
+
+<narr> Narrative:
+
+
+</top>
+
+<top>
+<num> Number: 4
+
+<title> issue next  years all
+
+<desc> Description:
+
+
+<narr> Narrative:
+
+
+</top>
+
+<top>
+<num> Number: 5
+
+<title> who major  ltd today
+
+<desc> Description:
+
+
+<narr> Narrative:
+
+
+</top>
+
+<top>
+<num> Number: 6
+
+<title> business revs  securities per
+
+<desc> Description:
+
+
+<narr> Narrative:
+
+
+</top>
+
+<top>
+<num> Number: 7
+
+<title> quarter time  note sales
+
+<desc> Description:
+
+
+<narr> Narrative:
+
+
+</top>
+
+<top>
+<num> Number: 8
+
+<title> february earlier  loss group
+
+<desc> Description:
+
+
+<narr> Narrative:
+
+
+</top>
+
+<top>
+<num> Number: 9
+
+<title> out end  made some
+
+<desc> Description:
+
+
+<narr> Narrative:
+
+
+</top>
+
+<top>
+<num> Number: 10
+
+<title> spokesman financial  30 expected
+
+<desc> Description:
+
+
+<narr> Narrative:
+
+
+</top>
+
+<top>
+<num> Number: 11
+
+<title> 1985 now  prices due
+
+<desc> Description:
+
+
+<narr> Narrative:
+
+
+</top>
+
+<top>
+<num> Number: 12
+
+<title> before board  record could
+
+<desc> Description:
+
+
+<narr> Narrative:
+
+
+</top>
+
+<top>
+<num> Number: 13
+
+<title> pay debt  because trade
+
+<desc> Description:
+
+
+<narr> Narrative:
+
+
+</top>
+
+<top>
+<num> Number: 14
+
+<title> meeting increase  four price
+
+<desc> Description:
+
+
+<narr> Narrative:
+
+
+</top>
+
+<top>
+<num> Number: 15
+
+<title> chairman rate  six interest
+
+<desc> Description:
+
+
+<narr> Narrative:
+
+
+</top>
+
+<top>
+<num> Number: 16
+
+<title> since current  between agreement
+
+<desc> Description:
+
+
+<narr> Narrative:
+
+
+</top>
+
+<top>
+<num> Number: 17
+
+<title> oil we  when president
+
+<desc> Description:
+
+
+<narr> Narrative:
+
+
+</top>
+
+<top>
+<num> Number: 18
+
+<title> capital through  foreign added
+
+<desc> Description:
+
+
+<narr> Narrative:
+
+
+</top>
+
+<top>
+<num> Number: 19
+
+<title> 20 while  common week
+
+<desc> Description:
+Topic 19 Description Line 1
+Topic 19 Description Line 2
+
+<narr> Narrative:
+Topic 19 Narrative Line 1
+Topic 19 Narrative Line 2
+
+</top>
diff --git a/modules/build.xml b/modules/build.xml
index 179ef68..5a07003 100644
--- a/modules/build.xml
+++ b/modules/build.xml
@@ -22,6 +22,7 @@
     <sequential>
       <subant target="test" inheritall="false" failonerror="true">
         <fileset dir="analysis" includes="build.xml" />
+        <fileset dir="benchmark" includes="build.xml" />
       </subant>
     </sequential>
   </target>
@@ -30,6 +31,7 @@
     <sequential>
       <subant target="compile" inheritall="false" failonerror="true">
         <fileset dir="analysis" includes="build.xml" />
+        <fileset dir="benchmark" includes="build.xml" />
       </subant>
     </sequential>
   </target>
@@ -38,6 +40,7 @@
     <sequential>
       <subant target="compile-test" inheritall="false" failonerror="true">
         <fileset dir="analysis" includes="build.xml" />
+        <fileset dir="benchmark" includes="build.xml" />
       </subant>
     </sequential>
   </target>
@@ -46,6 +49,7 @@
     <sequential>
       <subant target="javadocs" inheritall="false" failonerror="true">
         <fileset dir="analysis" includes="build.xml" />
+        <fileset dir="benchmark" includes="build.xml" />
       </subant>
     </sequential>
   </target>
@@ -54,6 +58,7 @@
     <sequential>
       <subant target="dist-maven" inheritall="false" failonerror="true">
         <fileset dir="analysis" includes="build.xml" />
+        <fileset dir="benchmark" includes="build.xml" />
       </subant>
     </sequential>
   </target>
@@ -62,6 +67,7 @@
     <sequential>
       <subant target="clean" inheritall="false" failonerror="true">
         <fileset dir="analysis" includes="build.xml" />
+        <fileset dir="benchmark" includes="build.xml" />
       </subant>
     </sequential>
   </target>

