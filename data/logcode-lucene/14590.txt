GitDiffStart: 52ab4cb9a8dbeb8cefc5fba5150dbc34ef1b7638 | Thu Nov 3 06:26:22 2011 +0000
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateTaxonomyIndexTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateTaxonomyIndexTask.java
index 3752f49..0c469f6 100644
--- a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateTaxonomyIndexTask.java
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateTaxonomyIndexTask.java
@@ -18,7 +18,7 @@ package org.apache.lucene.benchmark.byTask.tasks;
  */
 
 import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 
 import java.io.IOException;
@@ -37,7 +37,7 @@ public class CreateTaxonomyIndexTask extends PerfTask {
   @Override
   public int doLogic() throws IOException {
     PerfRunData runData = getRunData();
-    runData.setTaxonomyWriter(new LuceneTaxonomyWriter(runData.getTaxonomyDir(), OpenMode.CREATE));
+    runData.setTaxonomyWriter(new DirectoryTaxonomyWriter(runData.getTaxonomyDir(), OpenMode.CREATE));
     return 1;
   }
 
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenTaxonomyIndexTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenTaxonomyIndexTask.java
index 613578d..b6aacd7 100644
--- a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenTaxonomyIndexTask.java
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenTaxonomyIndexTask.java
@@ -18,7 +18,8 @@ package org.apache.lucene.benchmark.byTask.tasks;
  */
 
 import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+
 import java.io.IOException;
 
 
@@ -35,7 +36,7 @@ public class OpenTaxonomyIndexTask extends PerfTask {
   @Override
   public int doLogic() throws IOException {
     PerfRunData runData = getRunData();
-    runData.setTaxonomyWriter(new LuceneTaxonomyWriter(runData.getTaxonomyDir()));
+    runData.setTaxonomyWriter(new DirectoryTaxonomyWriter(runData.getTaxonomyDir()));
     return 1;
   }
 
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenTaxonomyReaderTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenTaxonomyReaderTask.java
index 03268f0..7324675 100644
--- a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenTaxonomyReaderTask.java
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenTaxonomyReaderTask.java
@@ -20,7 +20,7 @@ package org.apache.lucene.benchmark.byTask.tasks;
 import java.io.IOException;
 
 import org.apache.lucene.benchmark.byTask.PerfRunData;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 
 /**
  * Open a taxonomy index reader.
@@ -35,7 +35,7 @@ public class OpenTaxonomyReaderTask extends PerfTask {
   @Override
   public int doLogic() throws IOException {
     PerfRunData runData = getRunData();
-    LuceneTaxonomyReader taxoReader = new LuceneTaxonomyReader(runData.getTaxonomyDir());
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(runData.getTaxonomyDir());
     runData.setTaxonomyReader(taxoReader);
     // We transfer reference to the run data
     taxoReader.decRef();
diff --git a/modules/facet/docs/userguide.html b/modules/facet/docs/userguide.html
index 5c97a99..684d498 100644
--- a/modules/facet/docs/userguide.html
+++ b/modules/facet/docs/userguide.html
@@ -264,7 +264,7 @@ Following is a code snippet for indexing categories. The complete example can be
 found in package <code>org.apache.lucene.facet.example.simple.SimpleIndexer</code>.
 <pre class="prettyprint lang-java linenums">
 IndexWriter writer = ...
-TaxonomyWriter taxo = new LuceneTaxonomyWriter(taxoDir, OpenMode.CREATE);
+TaxonomyWriter taxo = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
 ...
 Document doc = new Document();
 doc.add(new Field("title", titleText, Store.YES, Index.ANALYZED));
@@ -366,7 +366,7 @@ found under <code>org.apache.lucene.facet.example.simple.Searcher</code>:
 <pre class="prettyprint lang-java linenums">
 IndexReader indexReader = IndexReader.open(indexDir);
 Searcher searcher = new IndexSearcher(indexReader);
-TaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
 ...
 Query q = new TermQuery(new Term(SimpleUtils.TEXT, "white"));
 TopScoreDocCollector tdc = TopScoreDocCollector.create(10, true);
@@ -781,4 +781,4 @@ the only thing that can happen is that new categories are added. Since search th
 are added in the middle of a search, there is no reason to keep around the old object, and the new one suffices.
 
 </body>
-</html>
\ No newline at end of file
+</html>
diff --git a/modules/facet/src/examples/org/apache/lucene/facet/example/ExampleUtils.java b/modules/facet/src/examples/org/apache/lucene/facet/example/ExampleUtils.java
index 2e12c98..f5e211b 100644
--- a/modules/facet/src/examples/org/apache/lucene/facet/example/ExampleUtils.java
+++ b/modules/facet/src/examples/org/apache/lucene/facet/example/ExampleUtils.java
@@ -27,7 +27,7 @@ public class ExampleUtils {
   public static final boolean VERBOSE = Boolean.getBoolean("tests.verbose");
 
   /** The Lucene {@link Version} used by the example code. */
-  public static final Version EXAMPLE_VER = Version.LUCENE_31;
+  public static final Version EXAMPLE_VER = Version.LUCENE_40;
   
   public static void log(Object msg) {
     if (VERBOSE) {
diff --git a/modules/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher.java b/modules/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher.java
index 93326e5..6cf8083 100644
--- a/modules/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher.java
+++ b/modules/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher.java
@@ -20,7 +20,7 @@ import org.apache.lucene.facet.search.params.FacetSearchParams;
 import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -55,7 +55,7 @@ public class AdaptiveSearcher {
    */
   public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {
     // prepare index reader and taxonomy.
-    TaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     IndexReader indexReader = IndexReader.open(indexDir);
     
     // prepare searcher to search against
diff --git a/modules/facet/src/examples/org/apache/lucene/facet/example/association/AssociationIndexer.java b/modules/facet/src/examples/org/apache/lucene/facet/example/association/AssociationIndexer.java
index 1382837..6c91040 100644
--- a/modules/facet/src/examples/org/apache/lucene/facet/example/association/AssociationIndexer.java
+++ b/modules/facet/src/examples/org/apache/lucene/facet/example/association/AssociationIndexer.java
@@ -16,7 +16,7 @@ import org.apache.lucene.facet.index.CategoryContainer;
 import org.apache.lucene.facet.index.CategoryDocumentBuilder;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -60,7 +60,7 @@ public class AssociationIndexer {
     IndexWriter iw = new IndexWriter(indexDir, new IndexWriterConfig(ExampleUtils.EXAMPLE_VER, SimpleUtils.analyzer));
 
     // create and open a taxonomy writer
-    TaxonomyWriter taxo = new LuceneTaxonomyWriter(taxoDir, OpenMode.CREATE);
+    TaxonomyWriter taxo = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
 
     // loop over sample documents
     int nDocsAdded = 0;
diff --git a/modules/facet/src/examples/org/apache/lucene/facet/example/association/AssociationSearcher.java b/modules/facet/src/examples/org/apache/lucene/facet/example/association/AssociationSearcher.java
index 08a8e71..f429c43 100644
--- a/modules/facet/src/examples/org/apache/lucene/facet/example/association/AssociationSearcher.java
+++ b/modules/facet/src/examples/org/apache/lucene/facet/example/association/AssociationSearcher.java
@@ -11,7 +11,7 @@ import org.apache.lucene.facet.search.params.association.AssociationIntSumFacetR
 import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -43,7 +43,7 @@ public class AssociationSearcher {
       Directory taxoDir) throws Exception {
     // prepare index reader 
     IndexReader indexReader = IndexReader.open(indexDir);
-    TaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     
     AssociationIntSumFacetRequest facetRequest = new AssociationIntSumFacetRequest(
         new CategoryPath("tags"), 10);
@@ -63,7 +63,7 @@ public class AssociationSearcher {
       Directory taxoDir) throws Exception {
     // prepare index reader 
     IndexReader indexReader = IndexReader.open(indexDir);
-    TaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     
     AssociationFloatSumFacetRequest facetRequest = new AssociationFloatSumFacetRequest(
         new CategoryPath("genre"), 10);
diff --git a/modules/facet/src/examples/org/apache/lucene/facet/example/merge/TaxonomyMergeUtils.java b/modules/facet/src/examples/org/apache/lucene/facet/example/merge/TaxonomyMergeUtils.java
index f310f9f..40dfac5 100644
--- a/modules/facet/src/examples/org/apache/lucene/facet/example/merge/TaxonomyMergeUtils.java
+++ b/modules/facet/src/examples/org/apache/lucene/facet/example/merge/TaxonomyMergeUtils.java
@@ -11,10 +11,10 @@ import org.apache.lucene.store.Directory;
 import org.apache.lucene.facet.example.ExampleUtils;
 import org.apache.lucene.facet.index.FacetsPayloadProcessorProvider;
 import org.apache.lucene.facet.index.params.DefaultFacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter.DiskOrdinalMap;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter.MemoryOrdinalMap;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter.OrdinalMap;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.DiskOrdinalMap;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.MemoryOrdinalMap;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -40,19 +40,19 @@ public class TaxonomyMergeUtils {
 
   /**
    * Merges the given taxonomy and index directories. Note that this method
-   * opens {@link LuceneTaxonomyWriter} and {@link IndexWriter} on the
+   * opens {@link DirectoryTaxonomyWriter} and {@link IndexWriter} on the
    * respective destination indexes. Therefore if you have a writer open on any
    * of them, it should be closed, or you should use
-   * {@link #merge(Directory, Directory, IndexWriter, LuceneTaxonomyWriter)}
+   * {@link #merge(Directory, Directory, IndexWriter, DirectoryTaxonomyWriter)}
    * instead.
    * 
-   * @see #merge(Directory, Directory, IndexWriter, LuceneTaxonomyWriter)
+   * @see #merge(Directory, Directory, IndexWriter, DirectoryTaxonomyWriter)
    */
   public static void merge(Directory srcIndexDir, Directory srcTaxDir,
                             Directory destIndexDir, Directory destTaxDir) throws IOException {
     IndexWriter destIndexWriter = new IndexWriter(destIndexDir,
         new IndexWriterConfig(ExampleUtils.EXAMPLE_VER, null));
-    LuceneTaxonomyWriter destTaxWriter = new LuceneTaxonomyWriter(destTaxDir);
+    DirectoryTaxonomyWriter destTaxWriter = new DirectoryTaxonomyWriter(destTaxDir);
     merge(srcIndexDir, srcTaxDir, new MemoryOrdinalMap(), destIndexWriter, destTaxWriter);
     destTaxWriter.close();
     destIndexWriter.close();
@@ -62,14 +62,14 @@ public class TaxonomyMergeUtils {
    * Merges the given taxonomy and index directories and commits the changes to
    * the given writers. This method uses {@link MemoryOrdinalMap} to store the
    * mapped ordinals. If you cannot afford the memory, you can use
-   * {@link #merge(Directory, Directory, LuceneTaxonomyWriter.OrdinalMap, IndexWriter, LuceneTaxonomyWriter)}
+   * {@link #merge(Directory, Directory, DirectoryTaxonomyWriter.OrdinalMap, IndexWriter, DirectoryTaxonomyWriter)}
    * by passing {@link DiskOrdinalMap}.
    * 
-   * @see #merge(Directory, Directory, LuceneTaxonomyWriter.OrdinalMap, IndexWriter, LuceneTaxonomyWriter)
+   * @see #merge(Directory, Directory, DirectoryTaxonomyWriter.OrdinalMap, IndexWriter, DirectoryTaxonomyWriter)
    */
   public static void merge(Directory srcIndexDir, Directory srcTaxDir,
                             IndexWriter destIndexWriter, 
-                            LuceneTaxonomyWriter destTaxWriter) throws IOException {
+                            DirectoryTaxonomyWriter destTaxWriter) throws IOException {
     merge(srcIndexDir, srcTaxDir, new MemoryOrdinalMap(), destIndexWriter, destTaxWriter);
   }
   
@@ -79,7 +79,7 @@ public class TaxonomyMergeUtils {
    */
   public static void merge(Directory srcIndexDir, Directory srcTaxDir,
                             OrdinalMap map, IndexWriter destIndexWriter,
-                            LuceneTaxonomyWriter destTaxWriter) throws IOException {
+                            DirectoryTaxonomyWriter destTaxWriter) throws IOException {
     // merge the taxonomies
     destTaxWriter.addTaxonomies(new Directory[] { srcTaxDir }, new OrdinalMap[] { map });
 
diff --git a/modules/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLIndexer.java b/modules/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLIndexer.java
index a17454e..9778386 100644
--- a/modules/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLIndexer.java
+++ b/modules/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLIndexer.java
@@ -21,7 +21,7 @@ import org.apache.lucene.facet.index.params.CategoryListParams;
 import org.apache.lucene.facet.index.params.FacetIndexingParams;
 import org.apache.lucene.facet.index.params.PerDimensionIndexingParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -140,7 +140,7 @@ public class MultiCLIndexer {
     IndexWriter iw = new IndexWriter(indexDir, new IndexWriterConfig(
         ExampleUtils.EXAMPLE_VER, SimpleUtils.analyzer).setOpenMode(OpenMode.CREATE));
     // create and open a taxonomy writer
-    LuceneTaxonomyWriter taxo = new LuceneTaxonomyWriter(taxoDir, OpenMode.CREATE);
+    DirectoryTaxonomyWriter taxo = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
     index(iw, taxo, iParams, docTitles, docTexts, cPaths);
   }
   
@@ -153,7 +153,7 @@ public class MultiCLIndexer {
    *             on error (no detailed exception handling here for sample
    *             simplicity
    */
-  public static void index(IndexWriter iw, LuceneTaxonomyWriter taxo,
+  public static void index(IndexWriter iw, DirectoryTaxonomyWriter taxo,
       FacetIndexingParams iParams, String[] docTitles,
       String[] docTexts, CategoryPath[][] cPaths) throws Exception {
 
diff --git a/modules/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLSearcher.java b/modules/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLSearcher.java
index 8be59ca..650b420 100644
--- a/modules/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLSearcher.java
+++ b/modules/facet/src/examples/org/apache/lucene/facet/example/multiCL/MultiCLSearcher.java
@@ -20,7 +20,7 @@ import org.apache.lucene.facet.search.params.FacetSearchParams;
 import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -64,7 +64,7 @@ public class MultiCLSearcher {
     
     // prepare index reader and taxonomy.
     IndexReader indexReader = IndexReader.open(indexDir);
-    TaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     
     // Get results
     List<FacetResult> results = searchWithFacets(indexReader, taxo, iParams);
diff --git a/modules/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleIndexer.java b/modules/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleIndexer.java
index 644ce35..cef9ff7 100644
--- a/modules/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleIndexer.java
+++ b/modules/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleIndexer.java
@@ -15,7 +15,7 @@ import org.apache.lucene.facet.example.ExampleUtils;
 import org.apache.lucene.facet.index.CategoryDocumentBuilder;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -53,7 +53,7 @@ public class SimpleIndexer {
     IndexWriter iw = new IndexWriter(indexDir, new IndexWriterConfig(ExampleUtils.EXAMPLE_VER, SimpleUtils.analyzer));
 
     // create and open a taxonomy writer
-    TaxonomyWriter taxo = new LuceneTaxonomyWriter(taxoDir, OpenMode.CREATE);
+    TaxonomyWriter taxo = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
 
     // loop over  sample documents 
     int nDocsAdded = 0;
diff --git a/modules/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleMain.java b/modules/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleMain.java
index da3a5d3..f3449ce 100644
--- a/modules/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleMain.java
+++ b/modules/facet/src/examples/org/apache/lucene/facet/example/simple/SimpleMain.java
@@ -10,7 +10,7 @@ import org.apache.lucene.facet.example.ExampleResult;
 import org.apache.lucene.facet.example.ExampleUtils;
 import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -56,7 +56,7 @@ public class SimpleMain {
     SimpleIndexer.index(indexDir, taxoDir);
 
     // open readers
-    TaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     IndexReader indexReader = IndexReader.open(indexDir, true);
 
     ExampleUtils.log("search the sample documents...");
@@ -81,7 +81,7 @@ public class SimpleMain {
     SimpleIndexer.index(indexDir, taxoDir);
 
     // open readers
-    TaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     IndexReader indexReader = IndexReader.open(indexDir, true);
 
     ExampleUtils.log("search the sample documents...");
diff --git a/modules/facet/src/java/org/apache/lucene/facet/index/FacetsPayloadProcessorProvider.java b/modules/facet/src/java/org/apache/lucene/facet/index/FacetsPayloadProcessorProvider.java
index 881ebf0..4fab2a8 100644
--- a/modules/facet/src/java/org/apache/lucene/facet/index/FacetsPayloadProcessorProvider.java
+++ b/modules/facet/src/java/org/apache/lucene/facet/index/FacetsPayloadProcessorProvider.java
@@ -13,7 +13,7 @@ import org.apache.lucene.store.Directory;
 
 import org.apache.lucene.facet.index.params.CategoryListParams;
 import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter.OrdinalMap;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.encoding.IntDecoder;
 import org.apache.lucene.util.encoding.IntEncoder;
diff --git a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyWriter.java b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyWriter.java
index 6cc9ca8..fb3df37 100644
--- a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyWriter.java
+++ b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyWriter.java
@@ -2,9 +2,8 @@ package org.apache.lucene.facet.taxonomy;
 
 import java.io.Closeable;
 import java.io.IOException;
-import java.util.Map;
 
-import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.util.TwoPhaseCommit;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -52,7 +51,7 @@ import org.apache.lucene.index.IndexWriter;
  * 
  * @lucene.experimental
  */
-public interface TaxonomyWriter extends Closeable {
+public interface TaxonomyWriter extends Closeable, TwoPhaseCommit {
   
   /**
    * addCategory() adds a category with a given path name to the taxonomy,
@@ -67,32 +66,6 @@ public interface TaxonomyWriter extends Closeable {
   public int addCategory(CategoryPath categoryPath) throws IOException;
   
   /**
-   * Calling commit() ensures that all the categories written so far are
-   * visible to a reader that is opened (or reopened) after that call.
-   * When the index is closed(), commit() is also implicitly done. 
-   */
-  public void commit() throws IOException;
-
-  /**
-   * Like commit(), but also store properties with the index. These properties
-   * are retrievable by {@link TaxonomyReader#getCommitUserData}.
-   * See {@link IndexWriter#commit(Map)}. 
-   */
-  public void commit(Map<String,String> commitUserData) throws IOException;
-  
-  /**
-   * prepare most of the work needed for a two-phase commit.
-   * See {@link IndexWriter#prepareCommit}.
-   */
-  public void prepareCommit() throws IOException;
-  
-  /**
-   * Like above, and also prepares to store user data with the index.
-   * See {@link IndexWriter#prepareCommit(Map)}
-   */
-  public void prepareCommit(Map<String,String> commitUserData) throws IOException;
-  
-  /**
    * getParent() returns the ordinal of the parent category of the category
    * with the given ordinal.
    * <P>
@@ -108,8 +81,8 @@ public interface TaxonomyWriter extends Closeable {
    * ordinal), an ArrayIndexOutOfBoundsException is thrown. However, it is
    * expected that getParent will only be called for ordinals which are
    * already known to be in the taxonomy.
-   * <P>
    * TODO (Facet): instead of a getParent(ordinal) method, consider having a
+   * <P>
    * getCategory(categorypath, prefixlen) which is similar to addCategory
    * except it doesn't add new categories; This method can be used to get
    * the ordinals of all prefixes of the given category, and it can use
diff --git a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/Consts.java b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/Consts.java
new file mode 100644
index 0000000..5ce1334
--- /dev/null
+++ b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/Consts.java
@@ -0,0 +1,70 @@
+package org.apache.lucene.facet.taxonomy.directory;
+
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.StoredFieldVisitor;
+import org.apache.lucene.store.IndexInput;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * @lucene.experimental
+ */
+abstract class Consts {
+
+  static final String FULL = "$full_path$";
+  static final String FIELD_PAYLOADS = "$payloads$";
+  static final String PAYLOAD_PARENT = "p";
+  static final char[] PAYLOAD_PARENT_CHARS = PAYLOAD_PARENT.toCharArray();
+
+  /**
+   * The following is a "stored field visitor", an object
+   * which tells Lucene to extract only a single field
+   * rather than a whole document.
+   */
+  public static final class LoadFullPathOnly extends StoredFieldVisitor {
+    private String fullPath;
+
+    public boolean stringField(FieldInfo fieldInfo, IndexInput in, int numUTF8Bytes) throws IOException {
+      final byte[] bytes = new byte[numUTF8Bytes];
+      in.readBytes(bytes, 0, bytes.length);
+      fullPath = new String(bytes, "UTF-8");
+
+      // Stop loading:
+      return true;
+    }
+
+    public String getFullPath() {
+      return fullPath;
+    }
+  }
+
+  /**
+   * Delimiter used for creating the full path of a category from the list of
+   * its labels from root. It is forbidden for labels to contain this
+   * character.
+   * <P>
+   * Originally, we used \uFFFE, officially a "unicode noncharacter" (invalid
+   * unicode character) for this purpose. Recently, we switched to the
+   * "private-use" character \uF749.
+   */
+  //static final char DEFAULT_DELIMITER = '\uFFFE';
+  static final char DEFAULT_DELIMITER = '\uF749';
+  
+}
diff --git a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java
new file mode 100644
index 0000000..b5c2184
--- /dev/null
+++ b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java
@@ -0,0 +1,574 @@
+package org.apache.lucene.facet.taxonomy.directory;
+
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.concurrent.locks.ReadWriteLock;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+import java.util.logging.Level;
+import java.util.logging.Logger;
+
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.Consts.LoadFullPathOnly;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.collections.LRUHashMap;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link TaxonomyReader} which retrieves stored taxonomy information from a
+ * {@link Directory}.
+ * <P>
+ * Reading from the on-disk index on every method call is too slow, so this
+ * implementation employs caching: Some methods cache recent requests and their
+ * results, while other methods prefetch all the data into memory and then
+ * provide answers directly from in-memory tables. See the documentation of
+ * individual methods for comments on their performance.
+ * 
+ * @lucene.experimental
+ */
+public class DirectoryTaxonomyReader implements TaxonomyReader {
+
+  private static final Logger logger = Logger.getLogger(DirectoryTaxonomyReader.class.getName());
+  
+  private IndexReader indexReader;
+
+  // The following lock is used to allow multiple threads to read from the
+  // index concurrently, while having them block during the very short
+  // critical moment of refresh() (see comments below). Note, however, that
+  // we only read from the index when we don't have the entry in our cache,
+  // and the caches are locked separately.
+  private ReadWriteLock indexReaderLock = new ReentrantReadWriteLock();
+
+  // The following are the limited-size LRU caches used to cache the latest
+  // results from getOrdinal() and getLabel().
+  // Because LRUHashMap is not thread-safe, we need to synchronize on this
+  // object when using it. Unfortunately, this is not optimal under heavy
+  // contention because it means that while one thread is using the cache
+  // (reading or modifying) others are blocked from using it - or even
+  // starting to do benign things like calculating the hash function. A more
+  // efficient approach would be to use a non-locking (as much as possible)
+  // concurrent solution, along the lines of java.util.concurrent.ConcurrentHashMap
+  // but with LRU semantics.
+  // However, even in the current sub-optimal implementation we do not make
+  // the mistake of locking out readers while waiting for disk in a cache
+  // miss - below, we do not hold cache lock while reading missing data from
+  // disk.
+  private final LRUHashMap<String, Integer> ordinalCache;
+  private final LRUHashMap<Integer, String> categoryCache;
+
+  // getParent() needs to be extremely efficient, to the point that we need
+  // to fetch all the data in advance into memory, and answer these calls
+  // from memory. Currently we use a large integer array, which is
+  // initialized when the taxonomy is opened, and potentially enlarged
+  // when it is refresh()ed.
+  // These arrays are not syncrhonized. Rather, the reference to the array
+  // is volatile, and the only writing operation (refreshPrefetchArrays)
+  // simply creates a new array and replaces the reference. The volatility
+  // of the reference ensures the correct atomic replacement and its
+  // visibility properties (the content of the array is visible when the
+  // new reference is visible).
+  private ParentArray parentArray;
+
+  private char delimiter = Consts.DEFAULT_DELIMITER;
+
+  private volatile boolean closed = false;
+  
+  /**
+   * Open for reading a taxonomy stored in a given {@link Directory}.
+   * @param directory
+   *    The {@link Directory} in which to the taxonomy lives. Note that
+   *    the taxonomy is read directly to that directory (not from a
+   *    subdirectory of it).
+   * @throws CorruptIndexException if the Taxonomy is corrupted.
+   * @throws IOException if another error occurred.
+   */
+  public DirectoryTaxonomyReader(Directory directory) throws IOException {
+    this.indexReader = openIndexReader(directory);
+
+    // These are the default cache sizes; they can be configured after
+    // construction with the cache's setMaxSize() method
+    ordinalCache = new LRUHashMap<String, Integer>(4000);
+    categoryCache = new LRUHashMap<Integer, String>(4000);
+
+    // TODO (Facet): consider lazily create parent array when asked, not in the constructor
+    parentArray = new ParentArray();
+    parentArray.refresh(indexReader);
+  }
+
+  protected IndexReader openIndexReader(Directory directory) throws CorruptIndexException, IOException {
+    return IndexReader.open(directory);
+  }
+
+  /**
+   * @throws AlreadyClosedException if this IndexReader is closed
+   */
+  protected final void ensureOpen() throws AlreadyClosedException {
+    if (indexReader.getRefCount() <= 0) {
+      throw new AlreadyClosedException("this TaxonomyReader is closed");
+    }
+  }
+  
+  /**
+   * setCacheSize controls the maximum allowed size of each of the caches
+   * used by {@link #getPath(int)} and {@link #getOrdinal(CategoryPath)}.
+   * <P>
+   * Currently, if the given size is smaller than the current size of
+   * a cache, it will not shrink, and rather we be limited to its current
+   * size.
+   * @param size the new maximum cache size, in number of entries.
+   */
+  public void setCacheSize(int size) {
+    ensureOpen();
+    synchronized(categoryCache) {
+      categoryCache.setMaxSize(size);
+    }
+    synchronized(ordinalCache) {
+      ordinalCache.setMaxSize(size);
+    }
+  }
+
+  /**
+   * setDelimiter changes the character that the taxonomy uses in its
+   * internal storage as a delimiter between category components. Do not
+   * use this method unless you really know what you are doing.
+   * <P>
+   * If you do use this method, make sure you call it before any other
+   * methods that actually queries the taxonomy. Moreover, make sure you
+   * always pass the same delimiter for all LuceneTaxonomyWriter and
+   * LuceneTaxonomyReader objects you create.
+   */
+  public void setDelimiter(char delimiter) {
+    ensureOpen();
+    this.delimiter = delimiter;
+  }
+
+  public int getOrdinal(CategoryPath categoryPath) throws IOException {
+    ensureOpen();
+    if (categoryPath.length()==0) {
+      return ROOT_ORDINAL;
+    }
+    String path = categoryPath.toString(delimiter);
+
+    // First try to find the answer in the LRU cache:
+    synchronized(ordinalCache) {
+      Integer res = ordinalCache.get(path);
+      if (res!=null) {
+        return res.intValue();
+      }
+    }
+
+    // If we're still here, we have a cache miss. We need to fetch the
+    // value from disk, and then also put it in the cache:
+    int ret = TaxonomyReader.INVALID_ORDINAL;
+    try {
+      indexReaderLock.readLock().lock();
+      // TODO (Facet): avoid Multi*?
+      Bits liveDocs = MultiFields.getLiveDocs(indexReader);
+      DocsEnum docs = MultiFields.getTermDocsEnum(indexReader, liveDocs, Consts.FULL, new BytesRef(path));
+      if (docs != null && docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
+        ret = docs.docID();
+      }
+    } finally {
+      indexReaderLock.readLock().unlock();
+    }
+
+    // Put the new value in the cache. Note that it is possible that while
+    // we were doing the above fetching (without the cache locked), some
+    // other thread already added the same category to the cache. We do
+    // not care about this possibilty, as LRUCache replaces previous values
+    // of the same keys (it doesn't store duplicates).
+    synchronized(ordinalCache) {
+      // GB: new Integer(int); creates a new object each and every time.
+      // Integer.valueOf(int) might not (See JavaDoc). 
+      ordinalCache.put(path, Integer.valueOf(ret));
+    }
+
+    return ret;
+  }
+
+  public CategoryPath getPath(int ordinal) throws CorruptIndexException, IOException {
+    ensureOpen();
+    // TODO (Facet): Currently, the LRU cache we use (getCategoryCache) holds
+    // strings with delimiters, not CategoryPath objects, so even if
+    // we have a cache hit, we need to process the string and build a new
+    // CategoryPath object every time. What is preventing us from putting
+    // the actual CategoryPath object in the cache is the fact that these
+    // objects are mutable. So we should create an immutable (read-only)
+    // interface that CategoryPath implements, and this method should
+    // return this interface, not the writable CategoryPath.
+    String label = getLabel(ordinal);
+    if (label==null) {
+      return null;  
+    }
+    return new CategoryPath(label, delimiter);
+  }
+
+  public boolean getPath(int ordinal, CategoryPath result) throws CorruptIndexException, IOException {
+    ensureOpen();
+    String label = getLabel(ordinal);
+    if (label==null) {
+      return false;
+    }
+    result.clear();
+    result.add(label, delimiter);
+    return true;
+  }
+
+  private String getLabel(int catID) throws CorruptIndexException, IOException {
+    ensureOpen();
+    // First try to find the answer in the LRU cache. It is very
+    // unfortunate that we need to allocate an Integer object here -
+    // it would have been better if we used a hash table specifically
+    // designed for int keys...
+    // GB: new Integer(int); creates a new object each and every time.
+    // Integer.valueOf(int) might not (See JavaDoc). 
+    Integer catIDInteger = Integer.valueOf(catID);
+
+    synchronized(categoryCache) {
+      String res = categoryCache.get(catIDInteger);
+      if (res!=null) {
+        return res;
+      }
+    }
+
+    // If we're still here, we have a cache miss. We need to fetch the
+    // value from disk, and then also put it in the cache:
+    String ret;
+    try {
+      indexReaderLock.readLock().lock();
+      // The taxonomy API dictates that if we get an invalid category
+      // ID, we should return null, If we don't check this here, we
+      // can some sort of an exception from the document() call below.
+      // NOTE: Currently, we *do not* cache this return value; There
+      // isn't much point to do so, because checking the validity of
+      // the docid doesn't require disk access - just comparing with
+      // the number indexReader.maxDoc().
+      if (catID<0 || catID>=indexReader.maxDoc()) {
+        return null;
+      }
+      final LoadFullPathOnly loader = new LoadFullPathOnly();
+      indexReader.document(catID, loader);
+      ret = loader.getFullPath();
+    } finally {
+      indexReaderLock.readLock().unlock();
+    }
+    // Put the new value in the cache. Note that it is possible that while
+    // we were doing the above fetching (without the cache locked), some
+    // other thread already added the same category to the cache. We do
+    // not care about this possibility, as LRUCache replaces previous
+    // values of the same keys (it doesn't store duplicates).
+    synchronized (categoryCache) {
+      categoryCache.put(catIDInteger, ret);
+    }
+
+    return ret;
+  }
+
+  public int getParent(int ordinal) {
+    ensureOpen();
+    // Note how we don't need to hold the read lock to do the following,
+    // because the array reference is volatile, ensuring the correct
+    // visibility and ordering: if we get the new reference, the new
+    // data is also visible to this thread.
+    return getParentArray()[ordinal];
+  }
+
+  /**
+   * getParentArray() returns an int array of size getSize() listing the
+   * ordinal of the parent category of each category in the taxonomy.
+   * <P>
+   * The caller can hold on to the array it got indefinitely - it is
+   * guaranteed that no-one else will modify it. The other side of the
+   * same coin is that the caller must treat the array it got as read-only
+   * and <B>not modify it</B>, because other callers might have gotten the
+   * same array too, and getParent() calls are also answered from the
+   * same array.
+   * <P>
+   * The getParentArray() call is extremely efficient, merely returning
+   * a reference to an array that already exists. For a caller that plans
+   * to call getParent() for many categories, using getParentArray() and
+   * the array it returns is a somewhat faster approach because it avoids
+   * the overhead of method calls and volatile dereferencing.
+   * <P>
+   * If you use getParentArray() instead of getParent(), remember that
+   * the array you got is (naturally) not modified after a refresh(),
+   * so you should always call getParentArray() again after a refresh().
+   */
+
+  public int[] getParentArray() {
+    ensureOpen();
+    // Note how we don't need to hold the read lock to do the following,
+    // because the array reference is volatile, ensuring the correct
+    // visibility and ordering: if we get the new reference, the new
+    // data is also visible to this thread.
+    return parentArray.getArray();
+  }
+
+  // Note that refresh() is synchronized (it is the only synchronized
+  // method in this class) to ensure that it never gets called concurrently
+  // with itself.
+  public synchronized void refresh() throws IOException {
+    ensureOpen();
+    /*
+     * Since refresh() can be a lengthy operation, it is very important that we
+     * avoid locking out all readers for its duration. This is why we don't hold
+     * the indexReaderLock write lock for the entire duration of this method. In
+     * fact, it is enough to hold it only during a single assignment! Other
+     * comments in this method will explain this.
+     */
+
+    // note that the lengthy operation indexReader.reopen() does not
+    // modify the reader, so we can do it without holding a lock. We can
+    // safely read indexReader without holding the write lock, because
+    // no other thread can be writing at this time (this method is the
+    // only possible writer, and it is "synchronized" to avoid this case).
+    IndexReader r2 = IndexReader.openIfChanged(indexReader);
+    if (r2 != null) {
+      IndexReader oldreader = indexReader;
+      // we can close the old searcher, but need to synchronize this
+      // so that we don't close it in the middle that another routine
+      // is reading from it.
+      indexReaderLock.writeLock().lock();
+      indexReader = r2;
+      indexReaderLock.writeLock().unlock();
+      // We can close the old reader, but need to be certain that we
+      // don't close it while another method is reading from it.
+      // Luckily, we can be certain of that even without putting the
+      // oldreader.close() in the locked section. The reason is that
+      // after lock() succeeded above, we know that all existing readers
+      // had finished (this is what a read-write lock ensures). New
+      // readers, starting after the unlock() we just did, already got
+      // the new indexReader we set above. So nobody can be possibly
+      // using the old indexReader, and we can close it:
+      oldreader.close();
+
+      // We prefetch some of the arrays to make requests much faster.
+      // Let's refresh these prefetched arrays; This refresh is much
+      // is made more efficient by assuming that it is enough to read
+      // the values for new categories (old categories could not have been
+      // changed or deleted)
+      // Note that this this done without the write lock being held,
+      // which means that it is possible that during a refresh(), a
+      // reader will have some methods (like getOrdinal and getCategory)
+      // return fresh information, while getParent()
+      // (only to be prefetched now) still return older information.
+      // We consider this to be acceptable. The important thing,
+      // however, is that refreshPrefetchArrays() itself writes to
+      // the arrays in a correct manner (see discussion there)
+      parentArray.refresh(indexReader);
+
+      // Remove any INVALID_ORDINAL values from the ordinal cache,
+      // because it is possible those are now answered by the new data!
+      Iterator<Entry<String, Integer>> i = ordinalCache.entrySet().iterator();
+      while (i.hasNext()) {
+        Entry<String, Integer> e = i.next();
+        if (e.getValue().intValue() == INVALID_ORDINAL) {
+          i.remove();
+        }
+      }
+    }
+  }
+
+  public void close() throws IOException {
+    if (!closed) {
+      decRef();
+      closed = true;
+    }
+  }
+  
+  /** Do the actual closing, free up resources */
+  private void doClose() throws IOException {
+    indexReader.close();
+    closed = true;
+
+    parentArray = null;
+    childrenArrays = null;
+    categoryCache.clear();
+    ordinalCache.clear();
+  }
+
+  public int getSize() {
+    ensureOpen();
+    indexReaderLock.readLock().lock();
+    try {
+      return indexReader.numDocs();
+    } finally {
+      indexReaderLock.readLock().unlock();
+    }
+  }
+
+  public Map<String, String> getCommitUserData() {
+    ensureOpen();
+    return indexReader.getCommitUserData();
+  }
+  
+  private ChildrenArrays childrenArrays;
+  Object childrenArraysRebuild = new Object();
+
+  public ChildrenArrays getChildrenArrays() {
+    ensureOpen();
+    // Check if the taxonomy grew since we built the array, and if it
+    // did, create new (and larger) arrays and fill them as required.
+    // We do all this under a lock, two prevent to concurrent calls to
+    // needlessly do the same array building at the same time.
+    synchronized(childrenArraysRebuild) {
+      int num = getSize();
+      int first;
+      if (childrenArrays==null) {
+        first = 0;
+      } else {
+        first = childrenArrays.getYoungestChildArray().length;
+      }
+      // If the taxonomy hasn't grown, we can return the existing object
+      // immediately
+      if (first == num) {
+        return childrenArrays;
+      }
+      // Otherwise, build new arrays for a new ChildrenArray object.
+      // These arrays start with an enlarged copy of the previous arrays,
+      // and then are modified to take into account the new categories:
+      int[] newYoungestChildArray = new int[num];
+      int[] newOlderSiblingArray = new int[num];
+      // In Java 6, we could just do Arrays.copyOf()...
+      if (childrenArrays!=null) {
+        System.arraycopy(childrenArrays.getYoungestChildArray(), 0,
+            newYoungestChildArray, 0, childrenArrays.getYoungestChildArray().length);
+        System.arraycopy(childrenArrays.getOlderSiblingArray(), 0,
+            newOlderSiblingArray, 0, childrenArrays.getOlderSiblingArray().length);
+      }
+      int[] parents = getParentArray();
+      for (int i=first; i<num; i++) {
+        newYoungestChildArray[i] = INVALID_ORDINAL;
+      }
+      // In the loop below we can ignore the root category (0) because
+      // it has no parent
+      if (first==0) {
+        first = 1;
+        newOlderSiblingArray[0] = INVALID_ORDINAL;
+      }
+      for (int i=first; i<num; i++) {
+        // Note that parents[i] is always < i, so the right-hand-side of
+        // the following line is already set when we get here.
+        newOlderSiblingArray[i] = newYoungestChildArray[parents[i]];
+        newYoungestChildArray[parents[i]] = i;
+      }
+      // Finally switch to the new arrays
+      childrenArrays = new ChildrenArraysImpl(newYoungestChildArray,
+          newOlderSiblingArray);
+      return childrenArrays;
+    }
+  }
+
+  public String toString(int max) {
+    ensureOpen();
+    StringBuilder sb = new StringBuilder();
+    int upperl = Math.min(max, this.indexReader.maxDoc());
+    for (int i = 0; i < upperl; i++) {
+      try {
+        CategoryPath category = this.getPath(i);
+        if (category == null) {
+          sb.append(i + ": NULL!! \n");
+          continue;
+        } 
+        if (category.length() == 0) {
+          sb.append(i + ": EMPTY STRING!! \n");
+          continue;
+        }
+        sb.append(i +": "+category.toString()+"\n");
+      } catch (IOException e) {
+        if (logger.isLoggable(Level.FINEST)) {
+          logger.log(Level.FINEST, e.getMessage(), e);
+        }
+      }
+    }
+    return sb.toString();
+  }
+
+  private static final class ChildrenArraysImpl implements ChildrenArrays {
+    private int[] youngestChildArray, olderSiblingArray;
+    public ChildrenArraysImpl(int[] youngestChildArray, int[] olderSiblingArray) {
+      this.youngestChildArray = youngestChildArray;
+      this.olderSiblingArray = olderSiblingArray;
+    }
+    public int[] getOlderSiblingArray() {
+      return olderSiblingArray;
+    }
+    public int[] getYoungestChildArray() {
+      return youngestChildArray;
+    }    
+  }
+
+  /**
+   * Expert:  This method is only for expert use.
+   * Note also that any call to refresh() will invalidate the returned reader,
+   * so the caller needs to take care of appropriate locking.
+   * 
+   * @return lucene indexReader
+   */
+  IndexReader getInternalIndexReader() {
+    ensureOpen();
+    return this.indexReader;
+  }
+
+  /**
+   * Expert: decreases the refCount of this TaxonomyReader instance. 
+   * If the refCount drops to 0, then pending changes (if any) are 
+   * committed to the taxonomy index and this reader is closed. 
+   * @throws IOException 
+   */
+  public void decRef() throws IOException {
+    ensureOpen();
+    if (indexReader.getRefCount() == 1) {
+      // Do not decRef the indexReader - doClose does it by calling reader.close()
+      doClose();
+    } else {
+      indexReader.decRef();
+    }
+  }
+  
+  /**
+   * Expert: returns the current refCount for this taxonomy reader
+   */
+  public int getRefCount() {
+    ensureOpen();
+    return this.indexReader.getRefCount();
+  }
+  
+  /**
+   * Expert: increments the refCount of this TaxonomyReader instance. 
+   * RefCounts are used to determine when a taxonomy reader can be closed 
+   * safely, i.e. as soon as there are no more references. 
+   * Be sure to always call a corresponding decRef(), in a finally clause; 
+   * otherwise the reader may never be closed. 
+   */
+  public void incRef() {
+    ensureOpen();
+    this.indexReader.incRef();
+  }
+}
diff --git a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
new file mode 100644
index 0000000..24b92d3
--- /dev/null
+++ b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
@@ -0,0 +1,1027 @@
+package org.apache.lucene.facet.taxonomy.directory;
+
+import java.io.BufferedInputStream;
+import java.io.BufferedOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.util.Map;
+
+import org.apache.lucene.analysis.core.KeywordAnalyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.index.LogByteSizeMergePolicy;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.LockObtainFailedException;
+import org.apache.lucene.store.NativeFSLockFactory;
+import org.apache.lucene.store.SimpleFSLockFactory;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Version;
+
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
+import org.apache.lucene.facet.taxonomy.writercache.cl2o.Cl2oTaxonomyWriterCache;
+import org.apache.lucene.facet.taxonomy.writercache.lru.LruTaxonomyWriterCache;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * {@link TaxonomyWriter} which uses a {@link Directory} to store the taxonomy
+ * information on disk, and keeps an additional in-memory cache of some or all
+ * categories.
+ * <p>
+ * In addition to the permanently-stored information in the {@link Directory},
+ * efficiency dictates that we also keep an in-memory cache of <B>recently
+ * seen</B> or <B>all</B> categories, so that we do not need to go back to disk
+ * for every category addition to see which ordinal this category already has,
+ * if any. A {@link TaxonomyWriterCache} object determines the specific caching
+ * algorithm used.
+ * <p>
+ * This class offers some hooks for extending classes to control the
+ * {@link IndexWriter} instance that is used. See {@link #openIndexWriter} and
+ * {@link #closeIndexWriter()} .
+ * 
+ * @lucene.experimental
+ */
+public class DirectoryTaxonomyWriter implements TaxonomyWriter {
+
+  protected IndexWriter indexWriter;
+  private int nextID;
+  private char delimiter = Consts.DEFAULT_DELIMITER;
+  private SinglePositionTokenStream parentStream = new SinglePositionTokenStream(Consts.PAYLOAD_PARENT);
+  private Field parentStreamField;
+  private Field fullPathField;
+
+  private TaxonomyWriterCache cache;
+  /**
+   * We call the cache "complete" if we know that every category in our
+   * taxonomy is in the cache. When the cache is <B>not</B> complete, and
+   * we can't find a category in the cache, we still need to look for it
+   * in the on-disk index; Therefore when the cache is not complete, we
+   * need to open a "reader" to the taxonomy index.
+   * The cache becomes incomplete if it was never filled with the existing
+   * categories, or if a put() to the cache ever returned true (meaning
+   * that some of the cached data was cleared).
+   */
+  private boolean cacheIsComplete;
+  private IndexReader reader;
+  private int cacheMisses;
+
+  /**
+   * setDelimiter changes the character that the taxonomy uses in its internal
+   * storage as a delimiter between category components. Do not use this
+   * method unless you really know what you are doing. It has nothing to do
+   * with whatever character the application may be using to represent
+   * categories for its own use.
+   * <P>
+   * If you do use this method, make sure you call it before any other methods
+   * that actually queries the taxonomy. Moreover, make sure you always pass
+   * the same delimiter for all LuceneTaxonomyWriter and LuceneTaxonomyReader
+   * objects you create for the same directory.
+   */
+  public void setDelimiter(char delimiter) {
+    this.delimiter = delimiter;
+  }
+
+  /**
+   * Forcibly unlocks the taxonomy in the named directory.
+   * <P>
+   * Caution: this should only be used by failure recovery code, when it is
+   * known that no other process nor thread is in fact currently accessing
+   * this taxonomy.
+   * <P>
+   * This method is unnecessary if your {@link Directory} uses a
+   * {@link NativeFSLockFactory} instead of the default
+   * {@link SimpleFSLockFactory}. When the "native" lock is used, a lock
+   * does not stay behind forever when the process using it dies. 
+   */
+  public static void unlock(Directory directory) throws IOException {
+    IndexWriter.unlock(directory);
+  }
+
+  /**
+   * Construct a Taxonomy writer.
+   * 
+   * @param directory
+   *    The {@link Directory} in which to store the taxonomy. Note that
+   *    the taxonomy is written directly to that directory (not to a
+   *    subdirectory of it).
+   * @param openMode
+   *    Specifies how to open a taxonomy for writing: <code>APPEND</code>
+   *    means open an existing index for append (failing if the index does
+   *    not yet exist). <code>CREATE</code> means create a new index (first
+   *    deleting the old one if it already existed).
+   *    <code>APPEND_OR_CREATE</code> appends to an existing index if there
+   *    is one, otherwise it creates a new index.
+   * @param cache
+   *    A {@link TaxonomyWriterCache} implementation which determines
+   *    the in-memory caching policy. See for example
+   *    {@link LruTaxonomyWriterCache} and {@link Cl2oTaxonomyWriterCache}.
+   *    If null or missing, {@link #defaultTaxonomyWriterCache()} is used.
+   * @throws CorruptIndexException
+   *     if the taxonomy is corrupted.
+   * @throws LockObtainFailedException
+   *     if the taxonomy is locked by another writer. If it is known
+   *     that no other concurrent writer is active, the lock might
+   *     have been left around by an old dead process, and should be
+   *     removed using {@link #unlock(Directory)}.
+   * @throws IOException
+   *     if another error occurred.
+   */
+  public DirectoryTaxonomyWriter(Directory directory, OpenMode openMode,
+                              TaxonomyWriterCache cache)
+  throws CorruptIndexException, LockObtainFailedException,
+  IOException {
+
+    openIndexWriter(directory, openMode);
+    reader = null;
+
+    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);
+    ft.setOmitNorms(true);
+    parentStreamField = new Field(Consts.FIELD_PAYLOADS, parentStream, ft);
+    fullPathField = new Field(Consts.FULL, "", StringField.TYPE_STORED);
+
+    this.nextID = indexWriter.maxDoc();
+
+    if (cache==null) {
+      cache = defaultTaxonomyWriterCache();
+    }
+    this.cache = cache;
+
+    if (nextID == 0) {
+      cacheIsComplete = true;
+      // Make sure that the taxonomy always contain the root category
+      // with category id 0.
+      addCategory(new CategoryPath());
+      refreshReader();
+    } else {
+      // There are some categories on the disk, which we have not yet
+      // read into the cache, and therefore the cache is incomplete.
+      // We chose not to read all the categories into the cache now,
+      // to avoid terrible performance when a taxonomy index is opened
+      // to add just a single category. We will do it later, after we
+      // notice a few cache misses.
+      cacheIsComplete = false;
+    }
+    cacheMisses = 0;
+  }
+
+  /**
+   * A hook for extensions of this class to provide their own
+   * {@link IndexWriter} implementation or instance. Extending classes can
+   * instantiate and configure the {@link IndexWriter} as they see fit,
+   * including setting a {@link org.apache.lucene.index.MergeScheduler}, or
+   * {@link org.apache.lucene.index.IndexDeletionPolicy}, different RAM size
+   * etc.<br>
+   * <b>NOTE:</b> the instance this method returns will be closed upon calling
+   * to {@link #close()}. If you wish to do something different, you should
+   * override {@link #closeIndexWriter()}.
+   * 
+   * @param directory
+   *          the {@link Directory} on top of which an {@link IndexWriter}
+   *          should be opened.
+   * @param openMode
+   *          see {@link OpenMode}
+   */
+  protected void openIndexWriter(Directory directory, OpenMode openMode)
+      throws IOException {
+    // Make sure we use a MergePolicy which merges segments in-order and thus
+    // keeps the doc IDs ordered as well (this is crucial for the taxonomy
+    // index).
+    IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_40,
+        new KeywordAnalyzer()).setOpenMode(openMode).setMergePolicy(
+        new LogByteSizeMergePolicy());
+    indexWriter = new IndexWriter(directory, config);
+  }
+
+  // Currently overridden by a unit test that verifies that every index we open
+  // is close()ed.
+  /**
+   * Open an {@link IndexReader} from the {@link #indexWriter} member, by
+   * calling {@link IndexWriter#getReader()}. Extending classes can override
+   * this method to return their own {@link IndexReader}.
+   */
+  protected IndexReader openReader() throws IOException {
+    return IndexReader.open(indexWriter, true); 
+  }
+
+  /**
+   * Creates a new instance with a default cached as defined by
+   * {@link #defaultTaxonomyWriterCache()}.
+   */
+  public DirectoryTaxonomyWriter(Directory directory, OpenMode openMode)
+  throws CorruptIndexException, LockObtainFailedException, IOException {
+    this(directory, openMode, defaultTaxonomyWriterCache());
+  }
+
+  /**
+   * Defines the default {@link TaxonomyWriterCache} to use in constructors
+   * which do not specify one.
+   * <P>  
+   * The current default is {@link Cl2oTaxonomyWriterCache} constructed
+   * with the parameters (1024, 0.15f, 3), i.e., the entire taxonomy is
+   * cached in memory while building it.
+   */
+  public static TaxonomyWriterCache defaultTaxonomyWriterCache() {
+    return new Cl2oTaxonomyWriterCache(1024, 0.15f, 3);
+  }
+
+  // convenience constructors:
+
+  public DirectoryTaxonomyWriter(Directory d)
+  throws CorruptIndexException, LockObtainFailedException,
+  IOException {
+    this(d, OpenMode.CREATE_OR_APPEND);
+  }
+
+  /**
+   * Frees used resources as well as closes the underlying {@link IndexWriter},
+   * which commits whatever changes made to it to the underlying
+   * {@link Directory}.
+   */
+  @Override
+  public synchronized void close() throws CorruptIndexException, IOException {
+    closeIndexWriter();
+    closeResources();
+  }
+
+  /**
+   * Returns the number of memory bytes used by the cache.
+   * @return Number of cache bytes in memory, for CL2O only; zero otherwise.
+   */
+  public int getCacheMemoryUsage() {
+    if (this.cache == null || !(this.cache instanceof Cl2oTaxonomyWriterCache)) {
+      return 0;
+    }
+    return ((Cl2oTaxonomyWriterCache)this.cache).getMemoryUsage();
+  }
+
+  /**
+   * A hook for extending classes to close additional resources that were used.
+   * The default implementation closes the {@link IndexReader} as well as the
+   * {@link TaxonomyWriterCache} instances that were used. <br>
+   * <b>NOTE:</b> if you override this method, you should include a
+   * <code>super.closeResources()</code> call in your implementation.
+   */
+  protected synchronized void closeResources() throws IOException {
+    if (reader != null) {
+      reader.close();
+      reader = null;
+    }
+    if (cache != null) {
+      cache.close();
+      cache = null;
+    }
+  }
+
+  /**
+   * A hook for extending classes to control closing the {@link IndexWriter}
+   * returned by {@link #openIndexWriter}.
+   */
+  protected void closeIndexWriter() throws CorruptIndexException, IOException {
+    if (indexWriter != null) {
+      indexWriter.close();
+      indexWriter = null;
+    }
+  }
+
+  /**
+   * Look up the given category in the cache and/or the on-disk storage,
+   * returning the category's ordinal, or a negative number in case the
+   * category does not yet exist in the taxonomy.
+   */
+  protected int findCategory(CategoryPath categoryPath) throws IOException {
+    // If we can find the category in our cache, we can return the
+    // response directly from it:
+    int res = cache.get(categoryPath);
+    if (res >= 0) {
+      return res;
+    }
+    // If we know that the cache is complete, i.e., contains every category
+    // which exists, we can return -1 immediately. However, if the cache is
+    // not complete, we need to check the disk.
+    if (cacheIsComplete) {
+      return -1;
+    }
+    cacheMisses++;
+    // After a few cache misses, it makes sense to read all the categories
+    // from disk and into the cache. The reason not to do this on the first
+    // cache miss (or even when opening the writer) is that it will
+    // significantly slow down the case when a taxonomy is opened just to
+    // add one category. The idea only spending a long time on reading
+    // after enough time was spent on cache misses is known as a "online
+    // algorithm".
+    if (perhapsFillCache()) {
+      return cache.get(categoryPath);
+    }
+
+    // We need to get an answer from the on-disk index. If a reader
+    // is not yet open, do it now:
+    if (reader == null) {
+      reader = openReader();
+    }
+
+    // TODO (Facet): avoid Multi*?
+    Bits liveDocs = MultiFields.getLiveDocs(reader);
+    DocsEnum docs = MultiFields.getTermDocsEnum(reader, liveDocs, Consts.FULL, 
+        new BytesRef(categoryPath.toString(delimiter)));
+    if (docs == null || docs.nextDoc() == DocIdSetIterator.NO_MORE_DOCS) {
+      return -1; // category does not exist in taxonomy
+    }
+    // Note: we do NOT add to the cache the fact that the category
+    // does not exist. The reason is that our only use for this
+    // method is just before we actually add this category. If
+    // in the future this usage changes, we should consider caching
+    // the fact that the category is not in the taxonomy.
+    addToCache(categoryPath, docs.docID());
+    return docs.docID();
+  }
+
+  /**
+   * Look up the given prefix of the given category in the cache and/or the
+   * on-disk storage, returning that prefix's ordinal, or a negative number in
+   * case the category does not yet exist in the taxonomy.
+   */
+  private int findCategory(CategoryPath categoryPath, int prefixLen)
+  throws IOException {
+    int res = cache.get(categoryPath, prefixLen);
+    if (res >= 0) {
+      return res;
+    }
+    if (cacheIsComplete) {
+      return -1;
+    }
+    cacheMisses++;
+    if (perhapsFillCache()) {
+      return cache.get(categoryPath, prefixLen);
+    }
+    if (reader == null) {
+      reader = openReader();
+    }
+    Bits liveDocs = MultiFields.getLiveDocs(reader);
+    DocsEnum docs = MultiFields.getTermDocsEnum(reader, liveDocs, Consts.FULL, 
+        new BytesRef(categoryPath.toString(delimiter, prefixLen)));
+    if (docs == null || docs.nextDoc() == DocIdSetIterator.NO_MORE_DOCS) {
+      return -1; // category does not exist in taxonomy
+    }
+    addToCache(categoryPath, prefixLen, docs.docID());
+    return docs.docID();
+  }
+
+  // TODO (Facet): addCategory() is synchronized. This means that if indexing is
+  // multi-threaded, a new category that needs to be written to disk (and
+  // potentially even trigger a lengthy merge) locks out other addCategory()
+  // calls - even those which could immediately return a cached value.
+  // We definitely need to fix this situation!
+  @Override
+  public synchronized int addCategory(CategoryPath categoryPath)
+  throws IOException {
+    // If the category is already in the cache and/or the taxonomy, we
+    // should return its existing ordinal:
+    int res = findCategory(categoryPath);
+    if (res < 0) {
+      // This is a new category, and we need to insert it into the index
+      // (and the cache). Actually, we might also need to add some of
+      // the category's ancestors before we can add the category itself
+      // (while keeping the invariant that a parent is always added to
+      // the taxonomy before its child). internalAddCategory() does all
+      // this recursively:
+      res = internalAddCategory(categoryPath, categoryPath.length());
+    }
+    return res;
+
+  }
+
+  /**
+   * Add a new category into the index (and the cache), and return its new
+   * ordinal.
+   * <P>
+   * Actually, we might also need to add some of the category's ancestors
+   * before we can add the category itself (while keeping the invariant that a
+   * parent is always added to the taxonomy before its child). We do this by
+   * recursion.
+   */
+  private int internalAddCategory(CategoryPath categoryPath, int length)
+  throws CorruptIndexException, IOException {
+
+    // Find our parent's ordinal (recursively adding the parent category
+    // to the taxonomy if it's not already there). Then add the parent
+    // ordinal as payloads (rather than a stored field; payloads can be
+    // more efficiently read into memory in bulk by LuceneTaxonomyReader)
+    int parent;
+    if (length > 1) {
+      parent = findCategory(categoryPath, length - 1);
+      if (parent < 0) {
+        parent = internalAddCategory(categoryPath, length - 1);
+      }
+    } else if (length == 1) {
+      parent = TaxonomyReader.ROOT_ORDINAL;
+    } else {
+      parent = TaxonomyReader.INVALID_ORDINAL;
+    }
+    int id = addCategoryDocument(categoryPath, length, parent);
+
+    return id;
+  }
+
+  // Note that the methods calling addCategoryDocument() are synchornized,
+  // so this method is effectively synchronized as well, but we'll add
+  // synchronized to be on the safe side, and we can reuse class-local objects
+  // instead of allocating them every time
+  protected synchronized int addCategoryDocument(CategoryPath categoryPath,
+                                                  int length, int parent)
+      throws CorruptIndexException, IOException {
+    // Before Lucene 2.9, position increments >=0 were supported, so we
+    // added 1 to parent to allow the parent -1 (the parent of the root).
+    // Unfortunately, starting with Lucene 2.9, after LUCENE-1542, this is
+    // no longer enough, since 0 is not encoded consistently either (see
+    // comment in SinglePositionTokenStream). But because we must be
+    // backward-compatible with existing indexes, we can't just fix what
+    // we write here (e.g., to write parent+2), and need to do a workaround
+    // in the reader (which knows that anyway only category 0 has a parent
+    // -1).    
+    parentStream.set(parent+1);
+    Document d = new Document();
+    d.add(parentStreamField);
+
+    fullPathField.setValue(categoryPath.toString(delimiter, length));
+    d.add(fullPathField);
+
+    // Note that we do no pass an Analyzer here because the fields that are
+    // added to the Document are untokenized or contains their own TokenStream.
+    // Therefore the IndexWriter's Analyzer has no effect.
+    indexWriter.addDocument(d);
+    int id = nextID++;
+
+    addToCache(categoryPath, length, id);
+
+    // also add to the parent array
+    getParentArray().add(id, parent);
+
+    return id;
+  }
+
+  private static class SinglePositionTokenStream extends TokenStream {
+    private CharTermAttribute termAtt;
+    private PositionIncrementAttribute posIncrAtt;
+    private boolean returned;
+    public SinglePositionTokenStream(String word) {
+      termAtt = addAttribute(CharTermAttribute.class);
+      posIncrAtt = addAttribute(PositionIncrementAttribute.class);
+      termAtt.setEmpty().append(word);
+      returned = true;
+    }
+    /**
+     * Set the value we want to keep, as the position increment.
+     * Note that when TermPositions.nextPosition() is later used to
+     * retrieve this value, val-1 will be returned, not val.
+     * <P>
+     * IMPORTANT NOTE: Before Lucene 2.9, val>=0 were safe (for val==0,
+     * the retrieved position would be -1). But starting with Lucene 2.9,
+     * this unfortunately changed, and only val>0 are safe. val=0 can
+     * still be used, but don't count on the value you retrieve later
+     * (it could be 0 or -1, depending on circumstances or versions).
+     * This change is described in Lucene's JIRA: LUCENE-1542. 
+     */
+    public void set(int val) {
+      posIncrAtt.setPositionIncrement(val);
+      returned = false;
+    }
+    @Override
+    public boolean incrementToken() throws IOException {
+      if (returned) {
+        return false;
+      }
+      returned = true;
+      return true;
+    }
+  }
+
+  private void addToCache(CategoryPath categoryPath, int id)
+  throws CorruptIndexException, IOException {
+    if (cache.put(categoryPath, id)) {
+      // If cache.put() returned true, it means the cache was limited in
+      // size, became full, so parts of it had to be cleared.
+      // Unfortunately we don't know which part was cleared - it is
+      // possible that a relatively-new category that hasn't yet been
+      // committed to disk (and therefore isn't yet visible in our
+      // "reader") was deleted from the cache, and therefore we must
+      // now refresh the reader.
+      // Because this is a slow operation, cache implementations are
+      // expected not to delete entries one-by-one but rather in bulk
+      // (LruTaxonomyWriterCache removes the 2/3rd oldest entries).
+      refreshReader();
+      cacheIsComplete = false;
+    }
+  }
+
+  private void addToCache(CategoryPath categoryPath, int prefixLen, int id)
+  throws CorruptIndexException, IOException {
+    if (cache.put(categoryPath, prefixLen, id)) {
+      refreshReader();
+      cacheIsComplete = false;
+    }
+  }
+
+  private synchronized void refreshReader() throws IOException {
+    if (reader != null) {
+      IndexReader r2 = IndexReader.openIfChanged(reader);
+      if (r2 != null) {
+        reader.close();
+        reader = r2;
+      }
+    }
+  }
+  
+  /**
+   * Calling commit() ensures that all the categories written so far are
+   * visible to a reader that is opened (or reopened) after that call.
+   * When the index is closed(), commit() is also implicitly done.
+   * See {@link TaxonomyWriter#commit()}
+   */ 
+  @Override
+  public synchronized void commit() throws CorruptIndexException, IOException {
+    indexWriter.commit();
+    refreshReader();
+  }
+
+  /**
+   * Like commit(), but also store properties with the index. These properties
+   * are retrievable by {@link DirectoryTaxonomyReader#getCommitUserData}.
+   * See {@link TaxonomyWriter#commit(Map)}. 
+   */
+  @Override
+  public synchronized void commit(Map<String,String> commitUserData) throws CorruptIndexException, IOException {
+    indexWriter.commit(commitUserData);
+    refreshReader();
+  }
+  
+  /**
+   * prepare most of the work needed for a two-phase commit.
+   * See {@link IndexWriter#prepareCommit}.
+   */
+  @Override
+  public synchronized void prepareCommit() throws CorruptIndexException, IOException {
+    indexWriter.prepareCommit();
+  }
+
+  /**
+   * Like above, and also prepares to store user data with the index.
+   * See {@link IndexWriter#prepareCommit(Map)}
+   */
+  @Override
+  public synchronized void prepareCommit(Map<String,String> commitUserData) throws CorruptIndexException, IOException {
+    indexWriter.prepareCommit(commitUserData);
+  }
+  
+  /**
+   * getSize() returns the number of categories in the taxonomy.
+   * <P>
+   * Because categories are numbered consecutively starting with 0, it means
+   * the taxonomy contains ordinals 0 through getSize()-1.
+   * <P>
+   * Note that the number returned by getSize() is often slightly higher than
+   * the number of categories inserted into the taxonomy; This is because when
+   * a category is added to the taxonomy, its ancestors are also added
+   * automatically (including the root, which always get ordinal 0).
+   */
+  @Override
+  synchronized public int getSize() {
+    return indexWriter.maxDoc();
+  }
+
+  private boolean alreadyCalledFillCache = false;
+
+  /**
+   * Set the number of cache misses before an attempt is made to read the
+   * entire taxonomy into the in-memory cache.
+   * <P> 
+   * LuceneTaxonomyWriter holds an in-memory cache of recently seen
+   * categories to speed up operation. On each cache-miss, the on-disk index
+   * needs to be consulted. When an existing taxonomy is opened, a lot of
+   * slow disk reads like that are needed until the cache is filled, so it
+   * is more efficient to read the entire taxonomy into memory at once.
+   * We do this complete read after a certain number (defined by this method)
+   * of cache misses.
+   * <P>
+   * If the number is set to <CODE>0</CODE>, the entire taxonomy is read
+   * into the cache on first use, without fetching individual categories
+   * first.
+   * <P>
+   * Note that if the memory cache of choice is limited in size, and cannot
+   * hold the entire content of the on-disk taxonomy, then it is never
+   * read in its entirety into the cache, regardless of the setting of this
+   * method. 
+   */
+  public void setCacheMissesUntilFill(int i) {
+    cacheMissesUntilFill = i;
+  }
+  private int cacheMissesUntilFill = 11;
+
+  private boolean perhapsFillCache() throws IOException {
+    // Note: we assume that we're only called when cacheIsComplete==false.
+    // TODO (Facet): parametrize this criterion:
+    if (cacheMisses < cacheMissesUntilFill) {
+      return false;
+    }
+    // If the cache was already filled (or we decided not to fill it because
+    // there was no room), there is no sense in trying it again.
+    if (alreadyCalledFillCache) {
+      return false;
+    }
+    alreadyCalledFillCache = true;
+    // TODO (Facet): we should probably completely clear the cache before starting
+    // to read it?
+    if (reader == null) {
+      reader = openReader();
+    }
+
+    if (!cache.hasRoom(reader.numDocs())) {
+      return false;
+    }
+
+    CategoryPath cp = new CategoryPath();
+    Terms terms = MultiFields.getTerms(reader, Consts.FULL);
+    // The check is done here to avoid checking it on every iteration of the
+    // below loop. A null term wlil be returned if there are no terms in the
+    // lexicon, or after the Consts.FULL term. However while the loop is
+    // executed we're safe, because we only iterate as long as there are next()
+    // terms.
+    if (terms != null) {
+      TermsEnum termsEnum = terms.iterator();
+      Bits liveDocs = MultiFields.getLiveDocs(reader);
+      DocsEnum docsEnum = null;
+      while (termsEnum.next() != null) {
+        BytesRef t = termsEnum.term();
+        // Since we guarantee uniqueness of categories, each term has exactly
+        // one document. Also, since we do not allow removing categories (and
+        // hence documents), there are no deletions in the index. Therefore, it
+        // is sufficient to call next(), and then doc(), exactly once with no
+        // 'validation' checks.
+        docsEnum = termsEnum.docs(liveDocs, docsEnum);
+        docsEnum.nextDoc();
+        cp.clear();
+        // TODO (Facet): avoid String creation/use bytes?
+        cp.add(t.utf8ToString(), delimiter);
+        cache.put(cp, docsEnum.docID());
+      }
+    }
+
+    cacheIsComplete = true;
+    // No sense to keep the reader open - we will not need to read from it
+    // if everything is in the cache.
+    reader.close();
+    reader = null;
+    return true;
+  }
+
+  private ParentArray parentArray;
+  private synchronized ParentArray getParentArray() throws IOException {
+    if (parentArray==null) {
+      if (reader == null) {
+        reader = openReader();
+      }
+      parentArray = new ParentArray();
+      parentArray.refresh(reader);
+    }
+    return parentArray;
+  }
+  @Override
+  public int getParent(int ordinal) throws IOException {
+    // Note: the following if() just enforces that a user can never ask
+    // for the parent of a nonexistant category - even if the parent array
+    // was allocated bigger than it really needs to be.
+    if (ordinal >= getSize()) {
+      throw new ArrayIndexOutOfBoundsException();
+    }
+    return getParentArray().getArray()[ordinal];
+  }
+
+  /**
+   * Take all the categories of one or more given taxonomies, and add them to
+   * the main taxonomy (this), if they are not already there.
+   * <P>
+   * Additionally, fill a <I>mapping</I> for each of the added taxonomies,
+   * mapping its ordinals to the ordinals in the enlarged main taxonomy.
+   * These mapping are saved into an array of OrdinalMap objects given by the
+   * user, one for each of the given taxonomies (not including "this", the main
+   * taxonomy). Often the first of these will be a MemoryOrdinalMap and the
+   * others will be a DiskOrdinalMap - see discussion in {OrdinalMap}. 
+   * <P> 
+   * Note that the taxonomies to be added are given as Directory objects,
+   * not opened TaxonomyReader/TaxonomyWriter objects, so if any of them are
+   * currently managed by an open TaxonomyWriter, make sure to commit() (or
+   * close()) it first. The main taxonomy (this) is an open TaxonomyWriter,
+   * and does not need to be commit()ed before this call. 
+   */
+  public void addTaxonomies(Directory[] taxonomies, OrdinalMap[] ordinalMaps) throws IOException {
+    // To prevent us stepping on the rest of this class's decisions on when
+    // to open a reader, and when not, we'll be opening a new reader instead
+    // of using the existing "reader" object:
+    IndexReader mainreader = openReader();
+    // TODO (Facet): can this then go segment-by-segment and avoid MultiDocsEnum etc?
+    Terms terms = MultiFields.getTerms(mainreader, Consts.FULL);
+    assert terms != null; // TODO (Facet): explicit check / throw exception?
+    TermsEnum mainte = terms.iterator();
+    DocsEnum mainde = null;
+
+    IndexReader[] otherreaders = new IndexReader[taxonomies.length];
+    TermsEnum[] othertes = new TermsEnum[taxonomies.length];
+    DocsEnum[] otherdocsEnum = new DocsEnum[taxonomies.length]; // just for reuse
+    for (int i=0; i<taxonomies.length; i++) {
+      otherreaders[i] = IndexReader.open(taxonomies[i]);
+      terms = MultiFields.getTerms(otherreaders[i], Consts.FULL);
+      assert terms != null; // TODO (Facet): explicit check / throw exception?
+      othertes[i] = terms.iterator();
+      // Also tell the ordinal maps their expected sizes:
+      ordinalMaps[i].setSize(otherreaders[i].numDocs());
+    }
+
+    CategoryPath cp = new CategoryPath();
+
+    // We keep a "current" cursor over the alphabetically-ordered list of
+    // categories in each taxonomy. We start the cursor on the first
+    // (alphabetically) category of each taxonomy:
+
+    String currentMain;
+    String[] currentOthers = new String[taxonomies.length];
+    currentMain = nextTE(mainte);
+    int otherTaxonomiesLeft = 0;
+    for (int i=0; i<taxonomies.length; i++) {
+      currentOthers[i] = nextTE(othertes[i]);
+      if (currentOthers[i]!=null) {
+        otherTaxonomiesLeft++;
+      }
+    }
+
+    // And then, at each step look at the first (alphabetically) of the
+    // current taxonomies.
+    // NOTE: The most efficient way we could have done this is using a
+    // PriorityQueue. But for simplicity, and assuming that usually we'll
+    // have a very small number of other taxonomies (often just 1), we use
+    // a more naive algorithm (o(ntaxonomies) instead of o(ln ntaxonomies)
+    // per step)
+
+    while (otherTaxonomiesLeft>0) {
+      // TODO: use a pq here
+      String first=null;
+      for (int i=0; i<taxonomies.length; i++) {
+        if (currentOthers[i]==null) continue;
+        if (first==null || first.compareTo(currentOthers[i])>0) {
+          first = currentOthers[i];
+        }
+      }
+      int comp = 0;
+      if (currentMain==null || (comp = currentMain.compareTo(first))>0) {
+        // If 'first' is before currentMain, or currentMain is null,
+        // then 'first' is a new category and we need to add it to the
+        // main taxonomy. Then for all taxonomies with this 'first'
+        // category, we need to add the new category number to their
+        // map, and move to the next category in all of them.
+        cp.clear();
+        cp.add(first, delimiter);
+        // We can call internalAddCategory() instead of addCategory()
+        // because we know the category hasn't been seen yet.
+        int newordinal = internalAddCategory(cp, cp.length());
+        // TODO (Facet): we already had this term in our hands before, in nextTE...
+        // // TODO (Facet): no need to make this term?
+        for (int i=0; i<taxonomies.length; i++) {
+          if (first.equals(currentOthers[i])) {
+            // remember the remapping of this ordinal. Note how
+            // this requires reading a posting list from the index -
+            // but since we do this in lexical order of terms, just
+            // like Lucene's merge works, we hope there are few seeks.
+            // TODO (Facet): is there a quicker way? E.g., not specifying the
+            // next term by name every time?
+            otherdocsEnum[i] = othertes[i].docs(MultiFields.getLiveDocs(otherreaders[i]), otherdocsEnum[i]);
+            otherdocsEnum[i].nextDoc(); // TODO (Facet): check?
+            int origordinal = otherdocsEnum[i].docID();
+            ordinalMaps[i].addMapping(origordinal, newordinal);
+            // and move to the next category in the i'th taxonomy 
+            currentOthers[i] = nextTE(othertes[i]);
+            if (currentOthers[i]==null) {
+              otherTaxonomiesLeft--;
+            }
+          }
+        }
+      } else if (comp==0) {
+        // 'first' and currentMain are the same, so both the main and some
+        // other taxonomies need to be moved, but a category doesn't need
+        // to be added because it already existed in the main taxonomy.
+
+        // TODO (Facet): Again, is there a quicker way?
+        mainde = mainte.docs(MultiFields.getLiveDocs(mainreader), mainde);
+        mainde.nextDoc(); // TODO (Facet): check?
+        int newordinal = mainde.docID();
+
+        currentMain = nextTE(mainte);
+        for (int i=0; i<taxonomies.length; i++) {
+          if (first.equals(currentOthers[i])) {
+            // TODO (Facet): again, is there a quicker way?
+            otherdocsEnum[i] = othertes[i].docs(MultiFields.getLiveDocs(otherreaders[i]), otherdocsEnum[i]);
+            otherdocsEnum[i].nextDoc(); // TODO (Facet): check?
+            int origordinal = otherdocsEnum[i].docID();
+            ordinalMaps[i].addMapping(origordinal, newordinal);
+
+            // and move to the next category 
+            currentOthers[i] = nextTE(othertes[i]);
+            if (currentOthers[i]==null) {
+              otherTaxonomiesLeft--;
+            }
+          }
+        }
+      } else /* comp > 0 */ {
+        // The currentMain doesn't appear in any of the other taxonomies -
+        // we don't need to do anything, just continue to the next one
+        currentMain = nextTE(mainte);
+      }
+    }
+
+    // Close all the readers we've opened, and also tell the ordinal maps
+    // we're done adding to them
+    mainreader.close();
+    for (int i=0; i<taxonomies.length; i++) {
+      otherreaders[i].close();
+      // We never actually added a mapping for the root ordinal - let's do
+      // it now, just so that the map is complete (every ordinal between 0
+      // and size-1 is remapped)
+      ordinalMaps[i].addMapping(0, 0);
+      ordinalMaps[i].addDone();
+    }
+  }
+
+  /**
+   * Mapping from old ordinal to new ordinals, used when merging indexes 
+   * wit separate taxonomies.
+   * <p> 
+   * addToTaxonomies() merges one or more taxonomies into the given taxonomy
+   * (this). An OrdinalMap is filled for each of the added taxonomies,
+   * containing the new ordinal (in the merged taxonomy) of each of the
+   * categories in the old taxonomy.
+   * <P>  
+   * There exist two implementations of OrdinalMap: MemoryOrdinalMap and
+   * DiskOrdinalMap. As their names suggest, the former keeps the map in
+   * memory and the latter in a temporary disk file. Because these maps will
+   * later be needed one by one (to remap the counting lists), not all at the
+   * same time, it is recommended to put the first taxonomy's map in memory,
+   * and all the rest on disk (later to be automatically read into memory one
+   * by one, when needed).
+   */
+  public static interface OrdinalMap {
+    /**
+     * Set the size of the map. This MUST be called before addMapping().
+     * It is assumed (but not verified) that addMapping() will then be
+     * called exactly 'size' times, with different origOrdinals between 0
+     * and size-1.  
+     */
+    public void setSize(int size) throws IOException;
+    public void addMapping(int origOrdinal, int newOrdinal) throws IOException;
+    /**
+     * Call addDone() to say that all addMapping() have been done.
+     * In some implementations this might free some resources. 
+     */
+    public void addDone() throws IOException;
+    /**
+     * Return the map from the taxonomy's original (consecutive) ordinals
+     * to the new taxonomy's ordinals. If the map has to be read from disk
+     * and ordered appropriately, it is done when getMap() is called.
+     * getMap() should only be called once, and only when the map is actually
+     * needed. Calling it will also free all resources that the map might
+     * be holding (such as temporary disk space), other than the returned int[].
+     */
+    public int[] getMap() throws IOException;
+  }
+
+  /**
+   * {@link OrdinalMap} maintained in memory
+   */
+  public static final class MemoryOrdinalMap implements OrdinalMap {
+    int[] map;
+    @Override
+    public void setSize(int taxonomySize) {
+      map = new int[taxonomySize];
+    }
+    @Override
+    public void addMapping(int origOrdinal, int newOrdinal) {
+      map[origOrdinal] = newOrdinal;
+    }
+    @Override
+    public void addDone() { /* nothing to do */ }
+    @Override
+    public int[] getMap() {
+      return map;
+    }
+  }
+
+  /**
+   * {@link OrdinalMap} maintained on file system
+   */
+  public static final class DiskOrdinalMap implements OrdinalMap {
+    File tmpfile;
+    DataOutputStream out;
+
+    public DiskOrdinalMap(File tmpfile) throws FileNotFoundException {
+      this.tmpfile = tmpfile;
+      out = new DataOutputStream(new BufferedOutputStream(
+          new FileOutputStream(tmpfile)));
+    }
+
+    @Override
+    public void addMapping(int origOrdinal, int newOrdinal) throws IOException {
+      out.writeInt(origOrdinal);
+      out.writeInt(newOrdinal);
+    }
+
+    @Override
+    public void setSize(int taxonomySize) throws IOException {
+      out.writeInt(taxonomySize);
+    }
+
+    @Override
+    public void addDone() throws IOException {
+      if (out!=null) {
+        out.close();
+        out = null;
+      }
+    }
+
+    int[] map = null;
+
+    @Override
+    public int[] getMap() throws IOException {
+      if (map!=null) {
+        return map;
+      }
+      addDone(); // in case this wasn't previously called
+      DataInputStream in = new DataInputStream(new BufferedInputStream(
+          new FileInputStream(tmpfile)));
+      map = new int[in.readInt()];
+      // NOTE: The current code assumes here that the map is complete,
+      // i.e., every ordinal gets one and exactly one value. Otherwise,
+      // we may run into an EOF here, or vice versa, not read everything.
+      for (int i=0; i<map.length; i++) {
+        int origordinal = in.readInt();
+        int newordinal = in.readInt();
+        map[origordinal] = newordinal;
+      }
+      in.close();
+      // Delete the temporary file, which is no longer needed.
+      if (!tmpfile.delete()) {
+        tmpfile.deleteOnExit();
+      }
+      return map;
+    }
+  }
+
+  private static final String nextTE(TermsEnum te) throws IOException {
+    if (te.next() != null) {
+      return te.term().utf8ToString(); // TODO (Facet): avoid String creation/use Bytes?
+    } 
+    return null;
+  }
+
+  @Override
+  public void rollback() throws IOException {
+    indexWriter.rollback();
+    refreshReader();
+  }
+  
+}
diff --git a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/ParentArray.java b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/ParentArray.java
new file mode 100644
index 0000000..2e56dc8
--- /dev/null
+++ b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/ParentArray.java
@@ -0,0 +1,160 @@
+package org.apache.lucene.facet.taxonomy.directory;
+
+import java.io.IOException;
+
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// getParent() needs to be extremely efficient, to the point that we need
+// to fetch all the data in advance into memory, and answer these calls
+// from memory. Currently we use a large integer array, which is
+// initialized when the taxonomy is opened, and potentially enlarged
+// when it is refresh()ed.
+/**
+ * @lucene.experimental
+ */
+class ParentArray {
+
+  // These arrays are not syncrhonized. Rather, the reference to the array
+  // is volatile, and the only writing operation (refreshPrefetchArrays)
+  // simply creates a new array and replaces the reference. The volatility
+  // of the reference ensures the correct atomic replacement and its
+  // visibility properties (the content of the array is visible when the
+  // new reference is visible).
+  private volatile int prefetchParentOrdinal[] = null;
+
+  public int[] getArray() {
+    return prefetchParentOrdinal;
+  }
+
+  /**
+   * refreshPrefetch() refreshes the parent array. Initially, it fills the
+   * array from the positions of an appropriate posting list. If called during
+   * a refresh(), when the arrays already exist, only values for new documents
+   * (those beyond the last one in the array) are read from the positions and
+   * added to the arrays (that are appropriately enlarged). We assume (and
+   * this is indeed a correct assumption in our case) that existing categories
+   * are never modified or deleted.
+   */
+  void refresh(IndexReader indexReader) throws IOException {
+    // Note that it is not necessary for us to obtain the read lock.
+    // The reason is that we are only called from refresh() (precluding
+    // another concurrent writer) or from the constructor (when no method
+    // could be running).
+    // The write lock is also not held during the following code, meaning
+    // that reads *can* happen while this code is running. The "volatile"
+    // property of the prefetchParentOrdinal and prefetchDepth array
+    // references ensure the correct visibility property of the assignment
+    // but other than that, we do *not* guarantee that a reader will not
+    // use an old version of one of these arrays (or both) while a refresh
+    // is going on. But we find this acceptable - until a refresh has
+    // finished, the reader should not expect to see new information
+    // (and the old information is the same in the old and new versions).
+    int first;
+    int num = indexReader.maxDoc();
+    if (prefetchParentOrdinal==null) {
+      prefetchParentOrdinal = new int[num];
+      // Starting Lucene 2.9, following the change LUCENE-1542, we can
+      // no longer reliably read the parent "-1" (see comment in
+      // LuceneTaxonomyWriter.SinglePositionTokenStream). We have no way
+      // to fix this in indexing without breaking backward-compatibility
+      // with existing indexes, so what we'll do instead is just
+      // hard-code the parent of ordinal 0 to be -1, and assume (as is
+      // indeed the case) that no other parent can be -1.
+      if (num>0) {
+        prefetchParentOrdinal[0] = TaxonomyReader.INVALID_ORDINAL;
+      }
+      first = 1;
+    } else {
+      first = prefetchParentOrdinal.length;
+      if (first==num) {
+        return; // nothing to do - no category was added
+      }
+      // In Java 6, we could just do Arrays.copyOf()...
+      int[] newarray = new int[num];
+      System.arraycopy(prefetchParentOrdinal, 0, newarray, 0,
+          prefetchParentOrdinal.length);
+      prefetchParentOrdinal = newarray;
+    }
+
+    // Read the new part of the parents array from the positions:
+    // TODO (Facet): avoid Multi*?
+    Bits liveDocs = MultiFields.getLiveDocs(indexReader);
+    DocsAndPositionsEnum positions = MultiFields.getTermPositionsEnum(indexReader, liveDocs,
+        Consts.FIELD_PAYLOADS, new BytesRef(Consts.PAYLOAD_PARENT));
+      if ((positions == null || positions.advance(first) == DocsAndPositionsEnum.NO_MORE_DOCS) && first < num) {
+        throw new CorruptIndexException("Missing parent data for category " + first);
+      }
+      for (int i=first; i<num; i++) {
+        // Note that we know positions.doc() >= i (this is an
+        // invariant kept throughout this loop)
+        if (positions.docID()==i) {
+          if (positions.freq() == 0) { // shouldn't happen
+            throw new CorruptIndexException(
+                "Missing parent data for category "+i);
+          }
+
+          // TODO (Facet): keep a local (non-volatile) copy of the prefetchParentOrdinal
+          // reference, because access to volatile reference is slower (?).
+          // Note: The positions we get here are one less than the position
+          // increment we added originally, so we get here the right numbers:
+          prefetchParentOrdinal[i] = positions.nextPosition();
+
+          if (positions.nextDoc() == DocsAndPositionsEnum.NO_MORE_DOCS) {
+            if ( i+1 < num ) {
+              throw new CorruptIndexException(
+                  "Missing parent data for category "+(i+1));
+            }
+            break;
+          }
+        } else { // this shouldn't happen
+        throw new CorruptIndexException(
+            "Missing parent data for category "+i);
+      }
+    }
+  }
+
+  /**
+   * add() is used in LuceneTaxonomyWriter, not in LuceneTaxonomyReader.
+   * It is only called from a synchronized method, so it is not reentrant,
+   * and also doesn't need to worry about reads happening at the same time.
+   * 
+   * NOTE: add() and refresh() CANNOT be used together. If you call add(),
+   * this changes the arrays and refresh() can no longer be used.
+   */
+  void add(int ordinal, int parentOrdinal) throws IOException {
+    if (ordinal >= prefetchParentOrdinal.length) {
+      // grow the array, if necessary.
+      // In Java 6, we could just do Arrays.copyOf()...
+      int[] newarray = new int[ordinal*2+1];
+      System.arraycopy(prefetchParentOrdinal, 0, newarray, 0,
+          prefetchParentOrdinal.length);
+      prefetchParentOrdinal = newarray;
+    }
+    prefetchParentOrdinal[ordinal] = parentOrdinal;
+  }
+
+}
diff --git a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/package.html b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/package.html
new file mode 100644
index 0000000..e1ee330
--- /dev/null
+++ b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/package.html
@@ -0,0 +1,9 @@
+<html>
+<head>
+<title>Taxonomy implemented using a Lucene-Index</title>
+</head>
+<body>
+	<h1>Taxonomy implemented using a Lucene-Index</h1>
+	
+</body>
+</html>
\ No newline at end of file
diff --git a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/lucene/Consts.java b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/lucene/Consts.java
deleted file mode 100644
index 2a91eb3..0000000
--- a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/lucene/Consts.java
+++ /dev/null
@@ -1,70 +0,0 @@
-package org.apache.lucene.facet.taxonomy.lucene;
-
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.StoredFieldVisitor;
-import org.apache.lucene.store.IndexInput;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * @lucene.experimental
- */
-abstract class Consts {
-
-  static final String FULL = "$full_path$";
-  static final String FIELD_PAYLOADS = "$payloads$";
-  static final String PAYLOAD_PARENT = "p";
-  static final char[] PAYLOAD_PARENT_CHARS = PAYLOAD_PARENT.toCharArray();
-
-  /**
-   * The following is a "stored field visitor", an object
-   * which tells Lucene to extract only a single field
-   * rather than a whole document.
-   */
-  public static final class LoadFullPathOnly extends StoredFieldVisitor {
-    private String fullPath;
-
-    public boolean stringField(FieldInfo fieldInfo, IndexInput in, int numUTF8Bytes) throws IOException {
-      final byte[] bytes = new byte[numUTF8Bytes];
-      in.readBytes(bytes, 0, bytes.length);
-      fullPath = new String(bytes, "UTF-8");
-
-      // Stop loading:
-      return true;
-    }
-
-    public String getFullPath() {
-      return fullPath;
-    }
-  }
-
-  /**
-   * Delimiter used for creating the full path of a category from the list of
-   * its labels from root. It is forbidden for labels to contain this
-   * character.
-   * <P>
-   * Originally, we used \uFFFE, officially a "unicode noncharacter" (invalid
-   * unicode character) for this purpose. Recently, we switched to the
-   * "private-use" character \uF749.
-   */
-  //static final char DEFAULT_DELIMITER = '\uFFFE';
-  static final char DEFAULT_DELIMITER = '\uF749';
-  
-}
diff --git a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/lucene/LuceneTaxonomyReader.java b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/lucene/LuceneTaxonomyReader.java
deleted file mode 100644
index fa30448..0000000
--- a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/lucene/LuceneTaxonomyReader.java
+++ /dev/null
@@ -1,578 +0,0 @@
-package org.apache.lucene.facet.taxonomy.lucene;
-
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.Map.Entry;
-import java.util.concurrent.locks.ReadWriteLock;
-import java.util.concurrent.locks.ReentrantReadWriteLock;
-import java.util.logging.Level;
-import java.util.logging.Logger;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.Consts.LoadFullPathOnly;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.collections.LRUHashMap;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** 
- * LuceneTaxonomyReader is a {@link TaxonomyReader} which retrieves stored
- * taxonomy information from a separate Lucene index. By using a Lucene index,
- * rather than some specialized file format, we get for "free" its correctness
- * (especially regarding concurrency), and the ability to save it on any
- * implementation of Directory (and not just the file system).
- * <P>
- * Reading from the on-disk index on every method call is too slow, so this
- * implementation employs caching: Some methods cache recent requests and
- * their results, while other methods prefetch all the data into memory
- * and then provide answers directly from in-memory tables. See the
- * documentation of individual methods for comments on their performance.
- * 
- * @lucene.experimental
- */
-public class LuceneTaxonomyReader implements TaxonomyReader {
-
-  private static final Logger logger = Logger.getLogger(LuceneTaxonomyReader.class.getName());
-  
-  private IndexReader indexReader;
-
-  // The following lock is used to allow multiple threads to read from the
-  // index concurrently, while having them block during the very short
-  // critical moment of refresh() (see comments below). Note, however, that
-  // we only read from the index when we don't have the entry in our cache,
-  // and the caches are locked separately.
-  private ReadWriteLock indexReaderLock = new ReentrantReadWriteLock();
-
-  // The following are the limited-size LRU caches used to cache the latest
-  // results from getOrdinal() and getLabel().
-  // Because LRUHashMap is not thread-safe, we need to synchronize on this
-  // object when using it. Unfortunately, this is not optimal under heavy
-  // contention because it means that while one thread is using the cache
-  // (reading or modifying) others are blocked from using it - or even
-  // starting to do benign things like calculating the hash function. A more
-  // efficient approach would be to use a non-locking (as much as possible)
-  // concurrent solution, along the lines of java.util.concurrent.ConcurrentHashMap
-  // but with LRU semantics.
-  // However, even in the current sub-optimal implementation we do not make
-  // the mistake of locking out readers while waiting for disk in a cache
-  // miss - below, we do not hold cache lock while reading missing data from
-  // disk.
-  private final LRUHashMap<String, Integer> ordinalCache;
-  private final LRUHashMap<Integer, String> categoryCache;
-
-  // getParent() needs to be extremely efficient, to the point that we need
-  // to fetch all the data in advance into memory, and answer these calls
-  // from memory. Currently we use a large integer array, which is
-  // initialized when the taxonomy is opened, and potentially enlarged
-  // when it is refresh()ed.
-  // These arrays are not syncrhonized. Rather, the reference to the array
-  // is volatile, and the only writing operation (refreshPrefetchArrays)
-  // simply creates a new array and replaces the reference. The volatility
-  // of the reference ensures the correct atomic replacement and its
-  // visibility properties (the content of the array is visible when the
-  // new reference is visible).
-  private ParentArray parentArray;
-
-  private char delimiter = Consts.DEFAULT_DELIMITER;
-
-  private volatile boolean closed = false;
-  
-  /**
-   * Open for reading a taxonomy stored in a given {@link Directory}.
-   * @param directory
-   *    The {@link Directory} in which to the taxonomy lives. Note that
-   *    the taxonomy is read directly to that directory (not from a
-   *    subdirectory of it).
-   * @throws CorruptIndexException if the Taxonomy is corrupted.
-   * @throws IOException if another error occurred.
-   */
-  public LuceneTaxonomyReader(Directory directory)
-  throws CorruptIndexException, IOException {
-    this.indexReader = openIndexReader(directory);
-
-    // These are the default cache sizes; they can be configured after
-    // construction with the cache's setMaxSize() method
-    ordinalCache = new LRUHashMap<String, Integer>(4000);
-    categoryCache = new LRUHashMap<Integer, String>(4000);
-
-    // TODO (Facet): consider lazily create parent array when asked, not in the constructor
-    parentArray = new ParentArray();
-    parentArray.refresh(indexReader);
-  }
-
-  protected IndexReader openIndexReader(Directory directory) throws CorruptIndexException, IOException {
-    return IndexReader.open(directory);
-  }
-
-  /**
-   * @throws AlreadyClosedException if this IndexReader is closed
-   */
-  protected final void ensureOpen() throws AlreadyClosedException {
-    if (indexReader.getRefCount() <= 0) {
-      throw new AlreadyClosedException("this TaxonomyReader is closed");
-    }
-  }
-  
-  /**
-   * setCacheSize controls the maximum allowed size of each of the caches
-   * used by {@link #getPath(int)} and {@link #getOrdinal(CategoryPath)}.
-   * <P>
-   * Currently, if the given size is smaller than the current size of
-   * a cache, it will not shrink, and rather we be limited to its current
-   * size.
-   * @param size the new maximum cache size, in number of entries.
-   */
-  public void setCacheSize(int size) {
-    ensureOpen();
-    synchronized(categoryCache) {
-      categoryCache.setMaxSize(size);
-    }
-    synchronized(ordinalCache) {
-      ordinalCache.setMaxSize(size);
-    }
-  }
-
-  /**
-   * setDelimiter changes the character that the taxonomy uses in its
-   * internal storage as a delimiter between category components. Do not
-   * use this method unless you really know what you are doing.
-   * <P>
-   * If you do use this method, make sure you call it before any other
-   * methods that actually queries the taxonomy. Moreover, make sure you
-   * always pass the same delimiter for all LuceneTaxonomyWriter and
-   * LuceneTaxonomyReader objects you create.
-   */
-  public void setDelimiter(char delimiter) {
-    ensureOpen();
-    this.delimiter = delimiter;
-  }
-
-  public int getOrdinal(CategoryPath categoryPath) throws IOException {
-    ensureOpen();
-    if (categoryPath.length()==0) {
-      return ROOT_ORDINAL;
-    }
-    String path = categoryPath.toString(delimiter);
-
-    // First try to find the answer in the LRU cache:
-    synchronized(ordinalCache) {
-      Integer res = ordinalCache.get(path);
-      if (res!=null) {
-        return res.intValue();
-      }
-    }
-
-    // If we're still here, we have a cache miss. We need to fetch the
-    // value from disk, and then also put it in the cache:
-    int ret = TaxonomyReader.INVALID_ORDINAL;
-    try {
-      indexReaderLock.readLock().lock();
-      // TODO (Facet): avoid Multi*?
-      Bits liveDocs = MultiFields.getLiveDocs(indexReader);
-      DocsEnum docs = MultiFields.getTermDocsEnum(indexReader, liveDocs, Consts.FULL, new BytesRef(path));
-      if (docs != null && docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
-        ret = docs.docID();
-      }
-    } finally {
-      indexReaderLock.readLock().unlock();
-    }
-
-    // Put the new value in the cache. Note that it is possible that while
-    // we were doing the above fetching (without the cache locked), some
-    // other thread already added the same category to the cache. We do
-    // not care about this possibilty, as LRUCache replaces previous values
-    // of the same keys (it doesn't store duplicates).
-    synchronized(ordinalCache) {
-      // GB: new Integer(int); creates a new object each and every time.
-      // Integer.valueOf(int) might not (See JavaDoc). 
-      ordinalCache.put(path, Integer.valueOf(ret));
-    }
-
-    return ret;
-  }
-
-  public CategoryPath getPath(int ordinal) throws CorruptIndexException, IOException {
-    ensureOpen();
-    // TODO (Facet): Currently, the LRU cache we use (getCategoryCache) holds
-    // strings with delimiters, not CategoryPath objects, so even if
-    // we have a cache hit, we need to process the string and build a new
-    // CategoryPath object every time. What is preventing us from putting
-    // the actual CategoryPath object in the cache is the fact that these
-    // objects are mutable. So we should create an immutable (read-only)
-    // interface that CategoryPath implements, and this method should
-    // return this interface, not the writable CategoryPath.
-    String label = getLabel(ordinal);
-    if (label==null) {
-      return null;  
-    }
-    return new CategoryPath(label, delimiter);
-  }
-
-  public boolean getPath(int ordinal, CategoryPath result) throws CorruptIndexException, IOException {
-    ensureOpen();
-    String label = getLabel(ordinal);
-    if (label==null) {
-      return false;
-    }
-    result.clear();
-    result.add(label, delimiter);
-    return true;
-  }
-
-  private String getLabel(int catID) throws CorruptIndexException, IOException {
-    ensureOpen();
-    // First try to find the answer in the LRU cache. It is very
-    // unfortunate that we need to allocate an Integer object here -
-    // it would have been better if we used a hash table specifically
-    // designed for int keys...
-    // GB: new Integer(int); creates a new object each and every time.
-    // Integer.valueOf(int) might not (See JavaDoc). 
-    Integer catIDInteger = Integer.valueOf(catID);
-
-    synchronized(categoryCache) {
-      String res = categoryCache.get(catIDInteger);
-      if (res!=null) {
-        return res;
-      }
-    }
-
-    // If we're still here, we have a cache miss. We need to fetch the
-    // value from disk, and then also put it in the cache:
-    String ret;
-    try {
-      indexReaderLock.readLock().lock();
-      // The taxonomy API dictates that if we get an invalid category
-      // ID, we should return null, If we don't check this here, we
-      // can some sort of an exception from the document() call below.
-      // NOTE: Currently, we *do not* cache this return value; There
-      // isn't much point to do so, because checking the validity of
-      // the docid doesn't require disk access - just comparing with
-      // the number indexReader.maxDoc().
-      if (catID<0 || catID>=indexReader.maxDoc()) {
-        return null;
-      }
-      final LoadFullPathOnly loader = new LoadFullPathOnly();
-      indexReader.document(catID, loader);
-      ret = loader.getFullPath();
-    } finally {
-      indexReaderLock.readLock().unlock();
-    }
-    // Put the new value in the cache. Note that it is possible that while
-    // we were doing the above fetching (without the cache locked), some
-    // other thread already added the same category to the cache. We do
-    // not care about this possibility, as LRUCache replaces previous
-    // values of the same keys (it doesn't store duplicates).
-    synchronized (categoryCache) {
-      categoryCache.put(catIDInteger, ret);
-    }
-
-    return ret;
-  }
-
-  public int getParent(int ordinal) {
-    ensureOpen();
-    // Note how we don't need to hold the read lock to do the following,
-    // because the array reference is volatile, ensuring the correct
-    // visibility and ordering: if we get the new reference, the new
-    // data is also visible to this thread.
-    return getParentArray()[ordinal];
-  }
-
-  /**
-   * getParentArray() returns an int array of size getSize() listing the
-   * ordinal of the parent category of each category in the taxonomy.
-   * <P>
-   * The caller can hold on to the array it got indefinitely - it is
-   * guaranteed that no-one else will modify it. The other side of the
-   * same coin is that the caller must treat the array it got as read-only
-   * and <B>not modify it</B>, because other callers might have gotten the
-   * same array too, and getParent() calls are also answered from the
-   * same array.
-   * <P>
-   * The getParentArray() call is extremely efficient, merely returning
-   * a reference to an array that already exists. For a caller that plans
-   * to call getParent() for many categories, using getParentArray() and
-   * the array it returns is a somewhat faster approach because it avoids
-   * the overhead of method calls and volatile dereferencing.
-   * <P>
-   * If you use getParentArray() instead of getParent(), remember that
-   * the array you got is (naturally) not modified after a refresh(),
-   * so you should always call getParentArray() again after a refresh().
-   */
-
-  public int[] getParentArray() {
-    ensureOpen();
-    // Note how we don't need to hold the read lock to do the following,
-    // because the array reference is volatile, ensuring the correct
-    // visibility and ordering: if we get the new reference, the new
-    // data is also visible to this thread.
-    return parentArray.getArray();
-  }
-
-  // Note that refresh() is synchronized (it is the only synchronized
-  // method in this class) to ensure that it never gets called concurrently
-  // with itself.
-  public synchronized void refresh() throws IOException {
-    ensureOpen();
-    /*
-     * Since refresh() can be a lengthy operation, it is very important that we
-     * avoid locking out all readers for its duration. This is why we don't hold
-     * the indexReaderLock write lock for the entire duration of this method. In
-     * fact, it is enough to hold it only during a single assignment! Other
-     * comments in this method will explain this.
-     */
-
-    // note that the lengthy operation indexReader.reopen() does not
-    // modify the reader, so we can do it without holding a lock. We can
-    // safely read indexReader without holding the write lock, because
-    // no other thread can be writing at this time (this method is the
-    // only possible writer, and it is "synchronized" to avoid this case).
-    IndexReader r2 = IndexReader.openIfChanged(indexReader);
-    if (r2 != null) {
-      IndexReader oldreader = indexReader;
-      // we can close the old searcher, but need to synchronize this
-      // so that we don't close it in the middle that another routine
-      // is reading from it.
-      indexReaderLock.writeLock().lock();
-      indexReader = r2;
-      indexReaderLock.writeLock().unlock();
-      // We can close the old reader, but need to be certain that we
-      // don't close it while another method is reading from it.
-      // Luckily, we can be certain of that even without putting the
-      // oldreader.close() in the locked section. The reason is that
-      // after lock() succeeded above, we know that all existing readers
-      // had finished (this is what a read-write lock ensures). New
-      // readers, starting after the unlock() we just did, already got
-      // the new indexReader we set above. So nobody can be possibly
-      // using the old indexReader, and we can close it:
-      oldreader.close();
-
-      // We prefetch some of the arrays to make requests much faster.
-      // Let's refresh these prefetched arrays; This refresh is much
-      // is made more efficient by assuming that it is enough to read
-      // the values for new categories (old categories could not have been
-      // changed or deleted)
-      // Note that this this done without the write lock being held,
-      // which means that it is possible that during a refresh(), a
-      // reader will have some methods (like getOrdinal and getCategory)
-      // return fresh information, while getParent()
-      // (only to be prefetched now) still return older information.
-      // We consider this to be acceptable. The important thing,
-      // however, is that refreshPrefetchArrays() itself writes to
-      // the arrays in a correct manner (see discussion there)
-      parentArray.refresh(indexReader);
-
-      // Remove any INVALID_ORDINAL values from the ordinal cache,
-      // because it is possible those are now answered by the new data!
-      Iterator<Entry<String, Integer>> i = ordinalCache.entrySet().iterator();
-      while (i.hasNext()) {
-        Entry<String, Integer> e = i.next();
-        if (e.getValue().intValue() == INVALID_ORDINAL) {
-          i.remove();
-        }
-      }
-    }
-  }
-
-  public void close() throws IOException {
-    if (!closed) {
-      decRef();
-      closed = true;
-    }
-  }
-  
-  /** Do the actual closing, free up resources */
-  private void doClose() throws IOException {
-    indexReader.close();
-    closed = true;
-
-    parentArray = null;
-    childrenArrays = null;
-    categoryCache.clear();
-    ordinalCache.clear();
-  }
-
-  public int getSize() {
-    ensureOpen();
-    indexReaderLock.readLock().lock();
-    try {
-      return indexReader.numDocs();
-    } finally {
-      indexReaderLock.readLock().unlock();
-    }
-  }
-
-  public Map<String, String> getCommitUserData() {
-    ensureOpen();
-    return indexReader.getCommitUserData();
-  }
-  
-  private ChildrenArrays childrenArrays;
-  Object childrenArraysRebuild = new Object();
-
-  public ChildrenArrays getChildrenArrays() {
-    ensureOpen();
-    // Check if the taxonomy grew since we built the array, and if it
-    // did, create new (and larger) arrays and fill them as required.
-    // We do all this under a lock, two prevent to concurrent calls to
-    // needlessly do the same array building at the same time.
-    synchronized(childrenArraysRebuild) {
-      int num = getSize();
-      int first;
-      if (childrenArrays==null) {
-        first = 0;
-      } else {
-        first = childrenArrays.getYoungestChildArray().length;
-      }
-      // If the taxonomy hasn't grown, we can return the existing object
-      // immediately
-      if (first == num) {
-        return childrenArrays;
-      }
-      // Otherwise, build new arrays for a new ChildrenArray object.
-      // These arrays start with an enlarged copy of the previous arrays,
-      // and then are modified to take into account the new categories:
-      int[] newYoungestChildArray = new int[num];
-      int[] newOlderSiblingArray = new int[num];
-      // In Java 6, we could just do Arrays.copyOf()...
-      if (childrenArrays!=null) {
-        System.arraycopy(childrenArrays.getYoungestChildArray(), 0,
-            newYoungestChildArray, 0, childrenArrays.getYoungestChildArray().length);
-        System.arraycopy(childrenArrays.getOlderSiblingArray(), 0,
-            newOlderSiblingArray, 0, childrenArrays.getOlderSiblingArray().length);
-      }
-      int[] parents = getParentArray();
-      for (int i=first; i<num; i++) {
-        newYoungestChildArray[i] = INVALID_ORDINAL;
-      }
-      // In the loop below we can ignore the root category (0) because
-      // it has no parent
-      if (first==0) {
-        first = 1;
-        newOlderSiblingArray[0] = INVALID_ORDINAL;
-      }
-      for (int i=first; i<num; i++) {
-        // Note that parents[i] is always < i, so the right-hand-side of
-        // the following line is already set when we get here.
-        newOlderSiblingArray[i] = newYoungestChildArray[parents[i]];
-        newYoungestChildArray[parents[i]] = i;
-      }
-      // Finally switch to the new arrays
-      childrenArrays = new ChildrenArraysImpl(newYoungestChildArray,
-          newOlderSiblingArray);
-      return childrenArrays;
-    }
-  }
-
-  public String toString(int max) {
-    ensureOpen();
-    StringBuilder sb = new StringBuilder();
-    int upperl = Math.min(max, this.indexReader.maxDoc());
-    for (int i = 0; i < upperl; i++) {
-      try {
-        CategoryPath category = this.getPath(i);
-        if (category == null) {
-          sb.append(i + ": NULL!! \n");
-          continue;
-        } 
-        if (category.length() == 0) {
-          sb.append(i + ": EMPTY STRING!! \n");
-          continue;
-        }
-        sb.append(i +": "+category.toString()+"\n");
-      } catch (IOException e) {
-        if (logger.isLoggable(Level.FINEST)) {
-          logger.log(Level.FINEST, e.getMessage(), e);
-        }
-      }
-    }
-    return sb.toString();
-  }
-
-  private static final class ChildrenArraysImpl implements ChildrenArrays {
-    private int[] youngestChildArray, olderSiblingArray;
-    public ChildrenArraysImpl(int[] youngestChildArray, int[] olderSiblingArray) {
-      this.youngestChildArray = youngestChildArray;
-      this.olderSiblingArray = olderSiblingArray;
-    }
-    public int[] getOlderSiblingArray() {
-      return olderSiblingArray;
-    }
-    public int[] getYoungestChildArray() {
-      return youngestChildArray;
-    }    
-  }
-
-  /**
-   * Expert:  This method is only for expert use.
-   * Note also that any call to refresh() will invalidate the returned reader,
-   * so the caller needs to take care of appropriate locking.
-   * 
-   * @return lucene indexReader
-   */
-  IndexReader getInternalIndexReader() {
-    ensureOpen();
-    return this.indexReader;
-  }
-
-  /**
-   * Expert: decreases the refCount of this TaxonomyReader instance. 
-   * If the refCount drops to 0, then pending changes (if any) are 
-   * committed to the taxonomy index and this reader is closed. 
-   * @throws IOException 
-   */
-  public void decRef() throws IOException {
-    ensureOpen();
-    if (indexReader.getRefCount() == 1) {
-      // Do not decRef the indexReader - doClose does it by calling reader.close()
-      doClose();
-    } else {
-      indexReader.decRef();
-    }
-  }
-  
-  /**
-   * Expert: returns the current refCount for this taxonomy reader
-   */
-  public int getRefCount() {
-    ensureOpen();
-    return this.indexReader.getRefCount();
-  }
-  
-  /**
-   * Expert: increments the refCount of this TaxonomyReader instance. 
-   * RefCounts are used to determine when a taxonomy reader can be closed 
-   * safely, i.e. as soon as there are no more references. 
-   * Be sure to always call a corresponding decRef(), in a finally clause; 
-   * otherwise the reader may never be closed. 
-   */
-  public void incRef() {
-    ensureOpen();
-    this.indexReader.incRef();
-  }
-}
diff --git a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/lucene/LuceneTaxonomyWriter.java b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/lucene/LuceneTaxonomyWriter.java
deleted file mode 100644
index 3a7c899..0000000
--- a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/lucene/LuceneTaxonomyWriter.java
+++ /dev/null
@@ -1,1008 +0,0 @@
-package org.apache.lucene.facet.taxonomy.lucene;
-
-import java.io.BufferedInputStream;
-import java.io.BufferedOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.FileNotFoundException;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.util.Map;
-
-import org.apache.lucene.analysis.core.KeywordAnalyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.LogByteSizeMergePolicy;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.LockObtainFailedException;
-import org.apache.lucene.store.NativeFSLockFactory;
-import org.apache.lucene.store.SimpleFSLockFactory;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Version;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
-import org.apache.lucene.facet.taxonomy.writercache.cl2o.Cl2oTaxonomyWriterCache;
-import org.apache.lucene.facet.taxonomy.writercache.lru.LruTaxonomyWriterCache;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * {@link TaxonomyWriter} which uses a Lucene index to store the taxonomy
- * information on disk, and keeps an additional in-memory cache of some or all
- * categories.
- * <P>
- * By using a Lucene index to store the information on disk, rather than some
- * specialized file format, we get for "free" Lucene's correctness (especially
- * regarding multi-process concurrency), and the ability to write to any
- * implementation of Directory (and not just the file system).
- * <P>
- * In addition to the permanently-stored Lucene index, efficiency dictates that
- * we also keep an in-memory cache of <B>recently seen</B> or <B>all</B>
- * categories, so that we do not need to go back to disk for every category
- * addition to see which ordinal this category already has, if any. A
- * {@link TaxonomyWriterCache} object determines the specific caching algorithm
- * used.
- * <p>
- * This class offers some hooks for extending classes to control the
- * {@link IndexWriter} instance that is used. See {@link #openLuceneIndex} and
- * {@link #closeLuceneIndex()} .
- * 
- * @lucene.experimental
- */
-public class LuceneTaxonomyWriter implements TaxonomyWriter {
-
-  protected IndexWriter indexWriter;
-  private int nextID;
-  private char delimiter = Consts.DEFAULT_DELIMITER;
-  private SinglePositionTokenStream parentStream = new SinglePositionTokenStream(Consts.PAYLOAD_PARENT);
-  private Field parentStreamField;
-  private Field fullPathField;
-
-  private TaxonomyWriterCache cache;
-  /**
-   * We call the cache "complete" if we know that every category in our
-   * taxonomy is in the cache. When the cache is <B>not</B> complete, and
-   * we can't find a category in the cache, we still need to look for it
-   * in the on-disk index; Therefore when the cache is not complete, we
-   * need to open a "reader" to the taxonomy index.
-   * The cache becomes incomplete if it was never filled with the existing
-   * categories, or if a put() to the cache ever returned true (meaning
-   * that some of the cached data was cleared).
-   */
-  private boolean cacheIsComplete;
-  private IndexReader reader;
-  private int cacheMisses;
-
-  /**
-   * setDelimiter changes the character that the taxonomy uses in its internal
-   * storage as a delimiter between category components. Do not use this
-   * method unless you really know what you are doing. It has nothing to do
-   * with whatever character the application may be using to represent
-   * categories for its own use.
-   * <P>
-   * If you do use this method, make sure you call it before any other methods
-   * that actually queries the taxonomy. Moreover, make sure you always pass
-   * the same delimiter for all LuceneTaxonomyWriter and LuceneTaxonomyReader
-   * objects you create for the same directory.
-   */
-  public void setDelimiter(char delimiter) {
-    this.delimiter = delimiter;
-  }
-
-  /**
-   * Forcibly unlocks the taxonomy in the named directory.
-   * <P>
-   * Caution: this should only be used by failure recovery code, when it is
-   * known that no other process nor thread is in fact currently accessing
-   * this taxonomy.
-   * <P>
-   * This method is unnecessary if your {@link Directory} uses a
-   * {@link NativeFSLockFactory} instead of the default
-   * {@link SimpleFSLockFactory}. When the "native" lock is used, a lock
-   * does not stay behind forever when the process using it dies. 
-   */
-  public static void unlock(Directory directory) throws IOException {
-    IndexWriter.unlock(directory);
-  }
-
-  /**
-   * Construct a Taxonomy writer.
-   * 
-   * @param directory
-   *    The {@link Directory} in which to store the taxonomy. Note that
-   *    the taxonomy is written directly to that directory (not to a
-   *    subdirectory of it).
-   * @param openMode
-   *    Specifies how to open a taxonomy for writing: <code>APPEND</code>
-   *    means open an existing index for append (failing if the index does
-   *    not yet exist). <code>CREATE</code> means create a new index (first
-   *    deleting the old one if it already existed).
-   *    <code>APPEND_OR_CREATE</code> appends to an existing index if there
-   *    is one, otherwise it creates a new index.
-   * @param cache
-   *    A {@link TaxonomyWriterCache} implementation which determines
-   *    the in-memory caching policy. See for example
-   *    {@link LruTaxonomyWriterCache} and {@link Cl2oTaxonomyWriterCache}.
-   *    If null or missing, {@link #defaultTaxonomyWriterCache()} is used.
-   * @throws CorruptIndexException
-   *     if the taxonomy is corrupted.
-   * @throws LockObtainFailedException
-   *     if the taxonomy is locked by another writer. If it is known
-   *     that no other concurrent writer is active, the lock might
-   *     have been left around by an old dead process, and should be
-   *     removed using {@link #unlock(Directory)}.
-   * @throws IOException
-   *     if another error occurred.
-   */
-  public LuceneTaxonomyWriter(Directory directory, OpenMode openMode,
-                              TaxonomyWriterCache cache)
-  throws CorruptIndexException, LockObtainFailedException,
-  IOException {
-
-    openLuceneIndex(directory, openMode);
-    reader = null;
-
-    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);
-    ft.setOmitNorms(true);
-    parentStreamField = new Field(Consts.FIELD_PAYLOADS, parentStream, ft);
-    fullPathField = new Field(Consts.FULL, "", StringField.TYPE_STORED);
-
-    this.nextID = indexWriter.maxDoc();
-
-    if (cache==null) {
-      cache = defaultTaxonomyWriterCache();
-    }
-    this.cache = cache;
-
-    if (nextID == 0) {
-      cacheIsComplete = true;
-      // Make sure that the taxonomy always contain the root category
-      // with category id 0.
-      addCategory(new CategoryPath());
-      refreshReader();
-    } else {
-      // There are some categories on the disk, which we have not yet
-      // read into the cache, and therefore the cache is incomplete.
-      // We chose not to read all the categories into the cache now,
-      // to avoid terrible performance when a taxonomy index is opened
-      // to add just a single category. We will do it later, after we
-      // notice a few cache misses.
-      cacheIsComplete = false;
-    }
-    cacheMisses = 0;
-  }
-
-  /**
-   * A hook for extensions of this class to provide their own
-   * {@link IndexWriter} implementation or instance. Extending classes can
-   * instantiate and configure the {@link IndexWriter} as they see fit,
-   * including setting a {@link org.apache.lucene.index.MergeScheduler}, or
-   * {@link org.apache.lucene.index.IndexDeletionPolicy}, different RAM size
-   * etc.<br>
-   * <b>NOTE:</b> the instance this method returns will be closed upon calling
-   * to {@link #close()}. If you wish to do something different, you should
-   * override {@link #closeLuceneIndex()}.
-   * 
-   * @param directory the {@link Directory} on top of wich an
-   *        {@link IndexWriter} should be opened.
-   * @param openMode see {@link OpenMode}
-   */
-  protected void openLuceneIndex (Directory directory, OpenMode openMode) 
-  throws CorruptIndexException, LockObtainFailedException, IOException {
-    // Make sure we use a MergePolicy which merges segments in-order and thus
-    // keeps the doc IDs ordered as well (this is crucial for the taxonomy
-    // index).
-    IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_30,
-        new KeywordAnalyzer()).setOpenMode(openMode).setMergePolicy(
-        new LogByteSizeMergePolicy());
-    indexWriter = new IndexWriter(directory, config);
-  }
-
-  // Currently overridden by a unit test that verifies that every index we open
-  // is close()ed.
-  /**
-   * Open an {@link IndexReader} from the {@link #indexWriter} member, by
-   * calling {@link IndexWriter#getReader()}. Extending classes can override
-   * this method to return their own {@link IndexReader}.
-   */
-  protected IndexReader openReader() throws IOException {
-    return IndexReader.open(indexWriter, true); 
-  }
-
-  /**
-   * Creates a new instance with a default cached as defined by
-   * {@link #defaultTaxonomyWriterCache()}.
-   */
-  public LuceneTaxonomyWriter(Directory directory, OpenMode openMode)
-  throws CorruptIndexException, LockObtainFailedException, IOException {
-    this(directory, openMode, defaultTaxonomyWriterCache());
-  }
-
-  /**
-   * Defines the default {@link TaxonomyWriterCache} to use in constructors
-   * which do not specify one.
-   * <P>  
-   * The current default is {@link Cl2oTaxonomyWriterCache} constructed
-   * with the parameters (1024, 0.15f, 3), i.e., the entire taxonomy is
-   * cached in memory while building it.
-   */
-  public static TaxonomyWriterCache defaultTaxonomyWriterCache() {
-    return new Cl2oTaxonomyWriterCache(1024, 0.15f, 3);
-  }
-
-  // convenience constructors:
-
-  public LuceneTaxonomyWriter(Directory d)
-  throws CorruptIndexException, LockObtainFailedException,
-  IOException {
-    this(d, OpenMode.CREATE_OR_APPEND);
-  }
-
-  /**
-   * Frees used resources as well as closes the underlying {@link IndexWriter},
-   * which commits whatever changes made to it to the underlying
-   * {@link Directory}.
-   */
-  public synchronized void close() throws CorruptIndexException, IOException {
-    closeLuceneIndex();
-    closeResources();
-  }
-
-  /**
-   * Returns the number of memory bytes used by the cache.
-   * @return Number of cache bytes in memory, for CL2O only; zero otherwise.
-   */
-  public int getCacheMemoryUsage() {
-    if (this.cache == null || !(this.cache instanceof Cl2oTaxonomyWriterCache)) {
-      return 0;
-    }
-    return ((Cl2oTaxonomyWriterCache)this.cache).getMemoryUsage();
-  }
-
-  /**
-   * A hook for extending classes to close additional resources that were used.
-   * The default implementation closes the {@link IndexReader} as well as the
-   * {@link TaxonomyWriterCache} instances that were used. <br>
-   * <b>NOTE:</b> if you override this method, you should include a
-   * <code>super.closeResources()</code> call in your implementation.
-   */
-  protected synchronized void closeResources() throws IOException {
-    if (reader != null) {
-      reader.close();
-      reader = null;
-    }
-    if (cache != null) {
-      cache.close();
-      cache = null;
-    }
-  }
-
-  /**
-   * A hook for extending classes to control closing the {@link IndexWriter}
-   * returned by {@link #openLuceneIndex}.
-   */
-  protected void closeLuceneIndex() throws CorruptIndexException, IOException {
-    if (indexWriter != null) {
-      indexWriter.close();
-      indexWriter = null;
-    }
-  }
-
-  /**
-   * Look up the given category in the cache and/or the on-disk storage,
-   * returning the category's ordinal, or a negative number in case the
-   * category does not yet exist in the taxonomy.
-   */
-  protected int findCategory(CategoryPath categoryPath) throws IOException {
-    // If we can find the category in our cache, we can return the
-    // response directly from it:
-    int res = cache.get(categoryPath);
-    if (res >= 0) {
-      return res;
-    }
-    // If we know that the cache is complete, i.e., contains every category
-    // which exists, we can return -1 immediately. However, if the cache is
-    // not complete, we need to check the disk.
-    if (cacheIsComplete) {
-      return -1;
-    }
-    cacheMisses++;
-    // After a few cache misses, it makes sense to read all the categories
-    // from disk and into the cache. The reason not to do this on the first
-    // cache miss (or even when opening the writer) is that it will
-    // significantly slow down the case when a taxonomy is opened just to
-    // add one category. The idea only spending a long time on reading
-    // after enough time was spent on cache misses is known as a "online
-    // algorithm".
-    if (perhapsFillCache()) {
-      return cache.get(categoryPath);
-    }
-
-    // We need to get an answer from the on-disk index. If a reader
-    // is not yet open, do it now:
-    if (reader == null) {
-      reader = openReader();
-    }
-
-    // TODO (Facet): avoid Multi*?
-    Bits liveDocs = MultiFields.getLiveDocs(reader);
-    DocsEnum docs = MultiFields.getTermDocsEnum(reader, liveDocs, Consts.FULL, 
-        new BytesRef(categoryPath.toString(delimiter)));
-    if (docs == null || docs.nextDoc() == DocIdSetIterator.NO_MORE_DOCS) {
-      return -1; // category does not exist in taxonomy
-    }
-    // Note: we do NOT add to the cache the fact that the category
-    // does not exist. The reason is that our only use for this
-    // method is just before we actually add this category. If
-    // in the future this usage changes, we should consider caching
-    // the fact that the category is not in the taxonomy.
-    addToCache(categoryPath, docs.docID());
-    return docs.docID();
-  }
-
-  /**
-   * Look up the given prefix of the given category in the cache and/or the
-   * on-disk storage, returning that prefix's ordinal, or a negative number in
-   * case the category does not yet exist in the taxonomy.
-   */
-  private int findCategory(CategoryPath categoryPath, int prefixLen)
-  throws IOException {
-    int res = cache.get(categoryPath, prefixLen);
-    if (res >= 0) {
-      return res;
-    }
-    if (cacheIsComplete) {
-      return -1;
-    }
-    cacheMisses++;
-    if (perhapsFillCache()) {
-      return cache.get(categoryPath, prefixLen);
-    }
-    if (reader == null) {
-      reader = openReader();
-    }
-    Bits liveDocs = MultiFields.getLiveDocs(reader);
-    DocsEnum docs = MultiFields.getTermDocsEnum(reader, liveDocs, Consts.FULL, 
-        new BytesRef(categoryPath.toString(delimiter, prefixLen)));
-    if (docs == null || docs.nextDoc() == DocIdSetIterator.NO_MORE_DOCS) {
-      return -1; // category does not exist in taxonomy
-    }
-    addToCache(categoryPath, prefixLen, docs.docID());
-    return docs.docID();
-  }
-
-  // TODO (Facet): addCategory() is synchronized. This means that if indexing is
-  // multi-threaded, a new category that needs to be written to disk (and
-  // potentially even trigger a lengthy merge) locks out other addCategory()
-  // calls - even those which could immediately return a cached value.
-  // We definitely need to fix this situation!
-  public synchronized int addCategory(CategoryPath categoryPath)
-  throws IOException {
-    // If the category is already in the cache and/or the taxonomy, we
-    // should return its existing ordinal:
-    int res = findCategory(categoryPath);
-    if (res < 0) {
-      // This is a new category, and we need to insert it into the index
-      // (and the cache). Actually, we might also need to add some of
-      // the category's ancestors before we can add the category itself
-      // (while keeping the invariant that a parent is always added to
-      // the taxonomy before its child). internalAddCategory() does all
-      // this recursively:
-      res = internalAddCategory(categoryPath, categoryPath.length());
-    }
-    return res;
-
-  }
-
-  /**
-   * Add a new category into the index (and the cache), and return its new
-   * ordinal.
-   * <P>
-   * Actually, we might also need to add some of the category's ancestors
-   * before we can add the category itself (while keeping the invariant that a
-   * parent is always added to the taxonomy before its child). We do this by
-   * recursion.
-   */
-  private int internalAddCategory(CategoryPath categoryPath, int length)
-  throws CorruptIndexException, IOException {
-
-    // Find our parent's ordinal (recursively adding the parent category
-    // to the taxonomy if it's not already there). Then add the parent
-    // ordinal as payloads (rather than a stored field; payloads can be
-    // more efficiently read into memory in bulk by LuceneTaxonomyReader)
-    int parent;
-    if (length > 1) {
-      parent = findCategory(categoryPath, length - 1);
-      if (parent < 0) {
-        parent = internalAddCategory(categoryPath, length - 1);
-      }
-    } else if (length == 1) {
-      parent = TaxonomyReader.ROOT_ORDINAL;
-    } else {
-      parent = TaxonomyReader.INVALID_ORDINAL;
-    }
-    int id = addCategoryDocument(categoryPath, length, parent);
-
-    return id;
-  }
-
-  // Note that the methods calling addCategoryDocument() are synchornized,
-  // so this method is effectively synchronized as well, but we'll add
-  // synchronized to be on the safe side, and we can reuse class-local objects
-  // instead of allocating them every time
-  protected synchronized int addCategoryDocument(CategoryPath categoryPath,
-                                                  int length, int parent)
-      throws CorruptIndexException, IOException {
-    // Before Lucene 2.9, position increments >=0 were supported, so we
-    // added 1 to parent to allow the parent -1 (the parent of the root).
-    // Unfortunately, starting with Lucene 2.9, after LUCENE-1542, this is
-    // no longer enough, since 0 is not encoded consistently either (see
-    // comment in SinglePositionTokenStream). But because we must be
-    // backward-compatible with existing indexes, we can't just fix what
-    // we write here (e.g., to write parent+2), and need to do a workaround
-    // in the reader (which knows that anyway only category 0 has a parent
-    // -1).    
-    parentStream.set(parent+1);
-    Document d = new Document();
-    d.add(parentStreamField);
-
-    fullPathField.setValue(categoryPath.toString(delimiter, length));
-    d.add(fullPathField);
-
-    // Note that we do no pass an Analyzer here because the fields that are
-    // added to the Document are untokenized or contains their own TokenStream.
-    // Therefore the IndexWriter's Analyzer has no effect.
-    indexWriter.addDocument(d);
-    int id = nextID++;
-
-    addToCache(categoryPath, length, id);
-
-    // also add to the parent array
-    getParentArray().add(id, parent);
-
-    return id;
-  }
-
-  private static class SinglePositionTokenStream extends TokenStream {
-    private CharTermAttribute termAtt;
-    private PositionIncrementAttribute posIncrAtt;
-    private boolean returned;
-    public SinglePositionTokenStream(String word) {
-      termAtt = addAttribute(CharTermAttribute.class);
-      posIncrAtt = addAttribute(PositionIncrementAttribute.class);
-      termAtt.setEmpty().append(word);
-      returned = true;
-    }
-    /**
-     * Set the value we want to keep, as the position increment.
-     * Note that when TermPositions.nextPosition() is later used to
-     * retrieve this value, val-1 will be returned, not val.
-     * <P>
-     * IMPORTANT NOTE: Before Lucene 2.9, val>=0 were safe (for val==0,
-     * the retrieved position would be -1). But starting with Lucene 2.9,
-     * this unfortunately changed, and only val>0 are safe. val=0 can
-     * still be used, but don't count on the value you retrieve later
-     * (it could be 0 or -1, depending on circumstances or versions).
-     * This change is described in Lucene's JIRA: LUCENE-1542. 
-     */
-    public void set(int val) {
-      posIncrAtt.setPositionIncrement(val);
-      returned = false;
-    }
-    @Override
-    public boolean incrementToken() throws IOException {
-      if (returned) {
-        return false;
-      }
-      returned = true;
-      return true;
-    }
-  }
-
-  private void addToCache(CategoryPath categoryPath, int id)
-  throws CorruptIndexException, IOException {
-    if (cache.put(categoryPath, id)) {
-      // If cache.put() returned true, it means the cache was limited in
-      // size, became full, so parts of it had to be cleared.
-      // Unfortunately we don't know which part was cleared - it is
-      // possible that a relatively-new category that hasn't yet been
-      // committed to disk (and therefore isn't yet visible in our
-      // "reader") was deleted from the cache, and therefore we must
-      // now refresh the reader.
-      // Because this is a slow operation, cache implementations are
-      // expected not to delete entries one-by-one but rather in bulk
-      // (LruTaxonomyWriterCache removes the 2/3rd oldest entries).
-      refreshReader();
-      cacheIsComplete = false;
-    }
-  }
-
-  private void addToCache(CategoryPath categoryPath, int prefixLen, int id)
-  throws CorruptIndexException, IOException {
-    if (cache.put(categoryPath, prefixLen, id)) {
-      refreshReader();
-      cacheIsComplete = false;
-    }
-  }
-
-  private synchronized void refreshReader() throws IOException {
-    if (reader != null) {
-      IndexReader r2 = IndexReader.openIfChanged(reader);
-      if (r2 != null) {
-        reader.close();
-        reader = r2;
-      }
-    }
-  }
-  
-  /**
-   * Calling commit() ensures that all the categories written so far are
-   * visible to a reader that is opened (or reopened) after that call.
-   * When the index is closed(), commit() is also implicitly done.
-   * See {@link TaxonomyWriter#commit()}
-   */ 
-  public synchronized void commit() throws CorruptIndexException, IOException {
-    indexWriter.commit();
-    refreshReader();
-  }
-
-  /**
-   * Like commit(), but also store properties with the index. These properties
-   * are retrievable by {@link LuceneTaxonomyReader#getCommitUserData}.
-   * See {@link TaxonomyWriter#commit(Map)}. 
-   */
-  public synchronized void commit(Map<String,String> commitUserData) throws CorruptIndexException, IOException {
-    indexWriter.commit(commitUserData);
-    refreshReader();
-  }
-  
-  /**
-   * prepare most of the work needed for a two-phase commit.
-   * See {@link IndexWriter#prepareCommit}.
-   */
-  public synchronized void prepareCommit() throws CorruptIndexException, IOException {
-    indexWriter.prepareCommit();
-  }
-
-  /**
-   * Like above, and also prepares to store user data with the index.
-   * See {@link IndexWriter#prepareCommit(Map)}
-   */
-  public synchronized void prepareCommit(Map<String,String> commitUserData) throws CorruptIndexException, IOException {
-    indexWriter.prepareCommit(commitUserData);
-  }
-  
-  /**
-   * getSize() returns the number of categories in the taxonomy.
-   * <P>
-   * Because categories are numbered consecutively starting with 0, it means
-   * the taxonomy contains ordinals 0 through getSize()-1.
-   * <P>
-   * Note that the number returned by getSize() is often slightly higher than
-   * the number of categories inserted into the taxonomy; This is because when
-   * a category is added to the taxonomy, its ancestors are also added
-   * automatically (including the root, which always get ordinal 0).
-   */
-  synchronized public int getSize() {
-    return indexWriter.maxDoc();
-  }
-
-  private boolean alreadyCalledFillCache = false;
-
-  /**
-   * Set the number of cache misses before an attempt is made to read the
-   * entire taxonomy into the in-memory cache.
-   * <P> 
-   * LuceneTaxonomyWriter holds an in-memory cache of recently seen
-   * categories to speed up operation. On each cache-miss, the on-disk index
-   * needs to be consulted. When an existing taxonomy is opened, a lot of
-   * slow disk reads like that are needed until the cache is filled, so it
-   * is more efficient to read the entire taxonomy into memory at once.
-   * We do this complete read after a certain number (defined by this method)
-   * of cache misses.
-   * <P>
-   * If the number is set to <CODE>0</CODE>, the entire taxonomy is read
-   * into the cache on first use, without fetching individual categories
-   * first.
-   * <P>
-   * Note that if the memory cache of choice is limited in size, and cannot
-   * hold the entire content of the on-disk taxonomy, then it is never
-   * read in its entirety into the cache, regardless of the setting of this
-   * method. 
-   */
-  public void setCacheMissesUntilFill(int i) {
-    cacheMissesUntilFill = i;
-  }
-  private int cacheMissesUntilFill = 11;
-
-  private boolean perhapsFillCache() throws IOException {
-    // Note: we assume that we're only called when cacheIsComplete==false.
-    // TODO (Facet): parametrize this criterion:
-    if (cacheMisses < cacheMissesUntilFill) {
-      return false;
-    }
-    // If the cache was already filled (or we decided not to fill it because
-    // there was no room), there is no sense in trying it again.
-    if (alreadyCalledFillCache) {
-      return false;
-    }
-    alreadyCalledFillCache = true;
-    // TODO (Facet): we should probably completely clear the cache before starting
-    // to read it?
-    if (reader == null) {
-      reader = openReader();
-    }
-
-    if (!cache.hasRoom(reader.numDocs())) {
-      return false;
-    }
-
-    CategoryPath cp = new CategoryPath();
-    Terms terms = MultiFields.getTerms(reader, Consts.FULL);
-    // The check is done here to avoid checking it on every iteration of the
-    // below loop. A null term wlil be returned if there are no terms in the
-    // lexicon, or after the Consts.FULL term. However while the loop is
-    // executed we're safe, because we only iterate as long as there are next()
-    // terms.
-    if (terms != null) {
-      TermsEnum termsEnum = terms.iterator();
-      Bits liveDocs = MultiFields.getLiveDocs(reader);
-      DocsEnum docsEnum = null;
-      while (termsEnum.next() != null) {
-        BytesRef t = termsEnum.term();
-        // Since we guarantee uniqueness of categories, each term has exactly
-        // one document. Also, since we do not allow removing categories (and
-        // hence documents), there are no deletions in the index. Therefore, it
-        // is sufficient to call next(), and then doc(), exactly once with no
-        // 'validation' checks.
-        docsEnum = termsEnum.docs(liveDocs, docsEnum);
-        docsEnum.nextDoc();
-        cp.clear();
-        // TODO (Facet): avoid String creation/use bytes?
-        cp.add(t.utf8ToString(), delimiter);
-        cache.put(cp, docsEnum.docID());
-      }
-    }
-
-    cacheIsComplete = true;
-    // No sense to keep the reader open - we will not need to read from it
-    // if everything is in the cache.
-    reader.close();
-    reader = null;
-    return true;
-  }
-
-  private ParentArray parentArray;
-  private synchronized ParentArray getParentArray() throws IOException {
-    if (parentArray==null) {
-      if (reader == null) {
-        reader = openReader();
-      }
-      parentArray = new ParentArray();
-      parentArray.refresh(reader);
-    }
-    return parentArray;
-  }
-  public int getParent(int ordinal) throws IOException {
-    // Note: the following if() just enforces that a user can never ask
-    // for the parent of a nonexistant category - even if the parent array
-    // was allocated bigger than it really needs to be.
-    if (ordinal >= getSize()) {
-      throw new ArrayIndexOutOfBoundsException();
-    }
-    return getParentArray().getArray()[ordinal];
-  }
-
-  /**
-   * Take all the categories of one or more given taxonomies, and add them to
-   * the main taxonomy (this), if they are not already there.
-   * <P>
-   * Additionally, fill a <I>mapping</I> for each of the added taxonomies,
-   * mapping its ordinals to the ordinals in the enlarged main taxonomy.
-   * These mapping are saved into an array of OrdinalMap objects given by the
-   * user, one for each of the given taxonomies (not including "this", the main
-   * taxonomy). Often the first of these will be a MemoryOrdinalMap and the
-   * others will be a DiskOrdinalMap - see discussion in {OrdinalMap}. 
-   * <P> 
-   * Note that the taxonomies to be added are given as Directory objects,
-   * not opened TaxonomyReader/TaxonomyWriter objects, so if any of them are
-   * currently managed by an open TaxonomyWriter, make sure to commit() (or
-   * close()) it first. The main taxonomy (this) is an open TaxonomyWriter,
-   * and does not need to be commit()ed before this call. 
-   */
-  public void addTaxonomies(Directory[] taxonomies, OrdinalMap[] ordinalMaps) throws IOException {
-    // To prevent us stepping on the rest of this class's decisions on when
-    // to open a reader, and when not, we'll be opening a new reader instead
-    // of using the existing "reader" object:
-    IndexReader mainreader = openReader();
-    // TODO (Facet): can this then go segment-by-segment and avoid MultiDocsEnum etc?
-    Terms terms = MultiFields.getTerms(mainreader, Consts.FULL);
-    assert terms != null; // TODO (Facet): explicit check / throw exception?
-    TermsEnum mainte = terms.iterator();
-    DocsEnum mainde = null;
-
-    IndexReader[] otherreaders = new IndexReader[taxonomies.length];
-    TermsEnum[] othertes = new TermsEnum[taxonomies.length];
-    DocsEnum[] otherdocsEnum = new DocsEnum[taxonomies.length]; // just for reuse
-    for (int i=0; i<taxonomies.length; i++) {
-      otherreaders[i] = IndexReader.open(taxonomies[i]);
-      terms = MultiFields.getTerms(otherreaders[i], Consts.FULL);
-      assert terms != null; // TODO (Facet): explicit check / throw exception?
-      othertes[i] = terms.iterator();
-      // Also tell the ordinal maps their expected sizes:
-      ordinalMaps[i].setSize(otherreaders[i].numDocs());
-    }
-
-    CategoryPath cp = new CategoryPath();
-
-    // We keep a "current" cursor over the alphabetically-ordered list of
-    // categories in each taxonomy. We start the cursor on the first
-    // (alphabetically) category of each taxonomy:
-
-    String currentMain;
-    String[] currentOthers = new String[taxonomies.length];
-    currentMain = nextTE(mainte);
-    int otherTaxonomiesLeft = 0;
-    for (int i=0; i<taxonomies.length; i++) {
-      currentOthers[i] = nextTE(othertes[i]);
-      if (currentOthers[i]!=null) {
-        otherTaxonomiesLeft++;
-      }
-    }
-
-    // And then, at each step look at the first (alphabetically) of the
-    // current taxonomies.
-    // NOTE: The most efficient way we could have done this is using a
-    // PriorityQueue. But for simplicity, and assuming that usually we'll
-    // have a very small number of other taxonomies (often just 1), we use
-    // a more naive algorithm (o(ntaxonomies) instead of o(ln ntaxonomies)
-    // per step)
-
-    while (otherTaxonomiesLeft>0) {
-      // TODO: use a pq here
-      String first=null;
-      for (int i=0; i<taxonomies.length; i++) {
-        if (currentOthers[i]==null) continue;
-        if (first==null || first.compareTo(currentOthers[i])>0) {
-          first = currentOthers[i];
-        }
-      }
-      int comp = 0;
-      if (currentMain==null || (comp = currentMain.compareTo(first))>0) {
-        // If 'first' is before currentMain, or currentMain is null,
-        // then 'first' is a new category and we need to add it to the
-        // main taxonomy. Then for all taxonomies with this 'first'
-        // category, we need to add the new category number to their
-        // map, and move to the next category in all of them.
-        cp.clear();
-        cp.add(first, delimiter);
-        // We can call internalAddCategory() instead of addCategory()
-        // because we know the category hasn't been seen yet.
-        int newordinal = internalAddCategory(cp, cp.length());
-        // TODO (Facet): we already had this term in our hands before, in nextTE...
-        // // TODO (Facet): no need to make this term?
-        for (int i=0; i<taxonomies.length; i++) {
-          if (first.equals(currentOthers[i])) {
-            // remember the remapping of this ordinal. Note how
-            // this requires reading a posting list from the index -
-            // but since we do this in lexical order of terms, just
-            // like Lucene's merge works, we hope there are few seeks.
-            // TODO (Facet): is there a quicker way? E.g., not specifying the
-            // next term by name every time?
-            otherdocsEnum[i] = othertes[i].docs(MultiFields.getLiveDocs(otherreaders[i]), otherdocsEnum[i]);
-            otherdocsEnum[i].nextDoc(); // TODO (Facet): check?
-            int origordinal = otherdocsEnum[i].docID();
-            ordinalMaps[i].addMapping(origordinal, newordinal);
-            // and move to the next category in the i'th taxonomy 
-            currentOthers[i] = nextTE(othertes[i]);
-            if (currentOthers[i]==null) {
-              otherTaxonomiesLeft--;
-            }
-          }
-        }
-      } else if (comp==0) {
-        // 'first' and currentMain are the same, so both the main and some
-        // other taxonomies need to be moved, but a category doesn't need
-        // to be added because it already existed in the main taxonomy.
-
-        // TODO (Facet): Again, is there a quicker way?
-        mainde = mainte.docs(MultiFields.getLiveDocs(mainreader), mainde);
-        mainde.nextDoc(); // TODO (Facet): check?
-        int newordinal = mainde.docID();
-
-        currentMain = nextTE(mainte);
-        for (int i=0; i<taxonomies.length; i++) {
-          if (first.equals(currentOthers[i])) {
-            // TODO (Facet): again, is there a quicker way?
-            otherdocsEnum[i] = othertes[i].docs(MultiFields.getLiveDocs(otherreaders[i]), otherdocsEnum[i]);
-            otherdocsEnum[i].nextDoc(); // TODO (Facet): check?
-            int origordinal = otherdocsEnum[i].docID();
-            ordinalMaps[i].addMapping(origordinal, newordinal);
-
-            // and move to the next category 
-            currentOthers[i] = nextTE(othertes[i]);
-            if (currentOthers[i]==null) {
-              otherTaxonomiesLeft--;
-            }
-          }
-        }
-      } else /* comp > 0 */ {
-        // The currentMain doesn't appear in any of the other taxonomies -
-        // we don't need to do anything, just continue to the next one
-        currentMain = nextTE(mainte);
-      }
-    }
-
-    // Close all the readers we've opened, and also tell the ordinal maps
-    // we're done adding to them
-    mainreader.close();
-    for (int i=0; i<taxonomies.length; i++) {
-      otherreaders[i].close();
-      // We never actually added a mapping for the root ordinal - let's do
-      // it now, just so that the map is complete (every ordinal between 0
-      // and size-1 is remapped)
-      ordinalMaps[i].addMapping(0, 0);
-      ordinalMaps[i].addDone();
-    }
-  }
-
-  /**
-   * Mapping from old ordinal to new ordinals, used when merging indexes 
-   * wit separate taxonomies.
-   * <p> 
-   * addToTaxonomies() merges one or more taxonomies into the given taxonomy
-   * (this). An OrdinalMap is filled for each of the added taxonomies,
-   * containing the new ordinal (in the merged taxonomy) of each of the
-   * categories in the old taxonomy.
-   * <P>  
-   * There exist two implementations of OrdinalMap: MemoryOrdinalMap and
-   * DiskOrdinalMap. As their names suggest, the former keeps the map in
-   * memory and the latter in a temporary disk file. Because these maps will
-   * later be needed one by one (to remap the counting lists), not all at the
-   * same time, it is recommended to put the first taxonomy's map in memory,
-   * and all the rest on disk (later to be automatically read into memory one
-   * by one, when needed).
-   */
-  public static interface OrdinalMap {
-    /**
-     * Set the size of the map. This MUST be called before addMapping().
-     * It is assumed (but not verified) that addMapping() will then be
-     * called exactly 'size' times, with different origOrdinals between 0
-     * and size-1.  
-     */
-    public void setSize(int size) throws IOException;
-    public void addMapping(int origOrdinal, int newOrdinal) throws IOException;
-    /**
-     * Call addDone() to say that all addMapping() have been done.
-     * In some implementations this might free some resources. 
-     */
-    public void addDone() throws IOException;
-    /**
-     * Return the map from the taxonomy's original (consecutive) ordinals
-     * to the new taxonomy's ordinals. If the map has to be read from disk
-     * and ordered appropriately, it is done when getMap() is called.
-     * getMap() should only be called once, and only when the map is actually
-     * needed. Calling it will also free all resources that the map might
-     * be holding (such as temporary disk space), other than the returned int[].
-     */
-    public int[] getMap() throws IOException;
-  }
-
-  /**
-   * {@link OrdinalMap} maintained in memory
-   */
-  public static final class MemoryOrdinalMap implements OrdinalMap {
-    int[] map;
-    public void setSize(int taxonomySize) {
-      map = new int[taxonomySize];
-    }
-    public void addMapping(int origOrdinal, int newOrdinal) {
-      map[origOrdinal] = newOrdinal;
-    }
-    public void addDone() { /* nothing to do */ }
-    public int[] getMap() {
-      return map;
-    }
-  }
-
-  /**
-   * {@link OrdinalMap} maintained on file system
-   */
-  public static final class DiskOrdinalMap implements OrdinalMap {
-    File tmpfile;
-    DataOutputStream out;
-
-    public DiskOrdinalMap(File tmpfile) throws FileNotFoundException {
-      this.tmpfile = tmpfile;
-      out = new DataOutputStream(new BufferedOutputStream(
-          new FileOutputStream(tmpfile)));
-    }
-
-    public void addMapping(int origOrdinal, int newOrdinal) throws IOException {
-      out.writeInt(origOrdinal);
-      out.writeInt(newOrdinal);
-    }
-
-    public void setSize(int taxonomySize) throws IOException {
-      out.writeInt(taxonomySize);
-    }
-
-    public void addDone() throws IOException {
-      if (out!=null) {
-        out.close();
-        out = null;
-      }
-    }
-
-    int[] map = null;
-
-    public int[] getMap() throws IOException {
-      if (map!=null) {
-        return map;
-      }
-      addDone(); // in case this wasn't previously called
-      DataInputStream in = new DataInputStream(new BufferedInputStream(
-          new FileInputStream(tmpfile)));
-      map = new int[in.readInt()];
-      // NOTE: The current code assumes here that the map is complete,
-      // i.e., every ordinal gets one and exactly one value. Otherwise,
-      // we may run into an EOF here, or vice versa, not read everything.
-      for (int i=0; i<map.length; i++) {
-        int origordinal = in.readInt();
-        int newordinal = in.readInt();
-        map[origordinal] = newordinal;
-      }
-      in.close();
-      // Delete the temporary file, which is no longer needed.
-      if (!tmpfile.delete()) {
-        tmpfile.deleteOnExit();
-      }
-      return map;
-    }
-  }
-
-  private static final String nextTE(TermsEnum te) throws IOException {
-    if (te.next() != null) {
-      return te.term().utf8ToString(); // TODO (Facet): avoid String creation/use Bytes?
-    } 
-    return null;
-  }
-
-}
diff --git a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/lucene/ParentArray.java b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/lucene/ParentArray.java
deleted file mode 100644
index 617d59e..0000000
--- a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/lucene/ParentArray.java
+++ /dev/null
@@ -1,160 +0,0 @@
-package org.apache.lucene.facet.taxonomy.lucene;
-
-import java.io.IOException;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-// getParent() needs to be extremely efficient, to the point that we need
-// to fetch all the data in advance into memory, and answer these calls
-// from memory. Currently we use a large integer array, which is
-// initialized when the taxonomy is opened, and potentially enlarged
-// when it is refresh()ed.
-/**
- * @lucene.experimental
- */
-class ParentArray {
-
-  // These arrays are not syncrhonized. Rather, the reference to the array
-  // is volatile, and the only writing operation (refreshPrefetchArrays)
-  // simply creates a new array and replaces the reference. The volatility
-  // of the reference ensures the correct atomic replacement and its
-  // visibility properties (the content of the array is visible when the
-  // new reference is visible).
-  private volatile int prefetchParentOrdinal[] = null;
-
-  public int[] getArray() {
-    return prefetchParentOrdinal;
-  }
-
-  /**
-   * refreshPrefetch() refreshes the parent array. Initially, it fills the
-   * array from the positions of an appropriate posting list. If called during
-   * a refresh(), when the arrays already exist, only values for new documents
-   * (those beyond the last one in the array) are read from the positions and
-   * added to the arrays (that are appropriately enlarged). We assume (and
-   * this is indeed a correct assumption in our case) that existing categories
-   * are never modified or deleted.
-   */
-  void refresh(IndexReader indexReader) throws IOException {
-    // Note that it is not necessary for us to obtain the read lock.
-    // The reason is that we are only called from refresh() (precluding
-    // another concurrent writer) or from the constructor (when no method
-    // could be running).
-    // The write lock is also not held during the following code, meaning
-    // that reads *can* happen while this code is running. The "volatile"
-    // property of the prefetchParentOrdinal and prefetchDepth array
-    // references ensure the correct visibility property of the assignment
-    // but other than that, we do *not* guarantee that a reader will not
-    // use an old version of one of these arrays (or both) while a refresh
-    // is going on. But we find this acceptable - until a refresh has
-    // finished, the reader should not expect to see new information
-    // (and the old information is the same in the old and new versions).
-    int first;
-    int num = indexReader.maxDoc();
-    if (prefetchParentOrdinal==null) {
-      prefetchParentOrdinal = new int[num];
-      // Starting Lucene 2.9, following the change LUCENE-1542, we can
-      // no longer reliably read the parent "-1" (see comment in
-      // LuceneTaxonomyWriter.SinglePositionTokenStream). We have no way
-      // to fix this in indexing without breaking backward-compatibility
-      // with existing indexes, so what we'll do instead is just
-      // hard-code the parent of ordinal 0 to be -1, and assume (as is
-      // indeed the case) that no other parent can be -1.
-      if (num>0) {
-        prefetchParentOrdinal[0] = TaxonomyReader.INVALID_ORDINAL;
-      }
-      first = 1;
-    } else {
-      first = prefetchParentOrdinal.length;
-      if (first==num) {
-        return; // nothing to do - no category was added
-      }
-      // In Java 6, we could just do Arrays.copyOf()...
-      int[] newarray = new int[num];
-      System.arraycopy(prefetchParentOrdinal, 0, newarray, 0,
-          prefetchParentOrdinal.length);
-      prefetchParentOrdinal = newarray;
-    }
-
-    // Read the new part of the parents array from the positions:
-    // TODO (Facet): avoid Multi*?
-    Bits liveDocs = MultiFields.getLiveDocs(indexReader);
-    DocsAndPositionsEnum positions = MultiFields.getTermPositionsEnum(indexReader, liveDocs,
-        Consts.FIELD_PAYLOADS, new BytesRef(Consts.PAYLOAD_PARENT));
-      if ((positions == null || positions.advance(first) == DocsAndPositionsEnum.NO_MORE_DOCS) && first < num) {
-        throw new CorruptIndexException("Missing parent data for category " + first);
-      }
-      for (int i=first; i<num; i++) {
-        // Note that we know positions.doc() >= i (this is an
-        // invariant kept throughout this loop)
-        if (positions.docID()==i) {
-          if (positions.freq() == 0) { // shouldn't happen
-            throw new CorruptIndexException(
-                "Missing parent data for category "+i);
-          }
-
-          // TODO (Facet): keep a local (non-volatile) copy of the prefetchParentOrdinal
-          // reference, because access to volatile reference is slower (?).
-          // Note: The positions we get here are one less than the position
-          // increment we added originally, so we get here the right numbers:
-          prefetchParentOrdinal[i] = positions.nextPosition();
-
-          if (positions.nextDoc() == DocsAndPositionsEnum.NO_MORE_DOCS) {
-            if ( i+1 < num ) {
-              throw new CorruptIndexException(
-                  "Missing parent data for category "+(i+1));
-            }
-            break;
-          }
-        } else { // this shouldn't happen
-        throw new CorruptIndexException(
-            "Missing parent data for category "+i);
-      }
-    }
-  }
-
-  /**
-   * add() is used in LuceneTaxonomyWriter, not in LuceneTaxonomyReader.
-   * It is only called from a synchronized method, so it is not reentrant,
-   * and also doesn't need to worry about reads happening at the same time.
-   * 
-   * NOTE: add() and refresh() CANNOT be used together. If you call add(),
-   * this changes the arrays and refresh() can no longer be used.
-   */
-  void add(int ordinal, int parentOrdinal) throws IOException {
-    if (ordinal >= prefetchParentOrdinal.length) {
-      // grow the array, if necessary.
-      // In Java 6, we could just do Arrays.copyOf()...
-      int[] newarray = new int[ordinal*2+1];
-      System.arraycopy(prefetchParentOrdinal, 0, newarray, 0,
-          prefetchParentOrdinal.length);
-      prefetchParentOrdinal = newarray;
-    }
-    prefetchParentOrdinal[ordinal] = parentOrdinal;
-  }
-
-}
diff --git a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/lucene/package.html b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/lucene/package.html
deleted file mode 100644
index e1ee330..0000000
--- a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/lucene/package.html
+++ /dev/null
@@ -1,9 +0,0 @@
-<html>
-<head>
-<title>Taxonomy implemented using a Lucene-Index</title>
-</head>
-<body>
-	<h1>Taxonomy implemented using a Lucene-Index</h1>
-	
-</body>
-</html>
\ No newline at end of file
diff --git a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/TaxonomyWriterCache.java b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/TaxonomyWriterCache.java
index 8d80ce1..646bdc0 100644
--- a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/TaxonomyWriterCache.java
+++ b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/TaxonomyWriterCache.java
@@ -1,7 +1,7 @@
 package org.apache.lucene.facet.taxonomy.writercache;
 
 import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -23,7 +23,7 @@ import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
 /**
  * TaxonomyWriterCache is a relatively simple interface for a cache of
  * category->ordinal mappings, used in TaxonomyWriter implementations
- * (such as {@link LuceneTaxonomyWriter}).
+ * (such as {@link DirectoryTaxonomyWriter}).
  * <P>
  * It basically has put() methods for adding a mapping, and get() for looking
  * a mapping up the cache. The cache does <B>not</B> guarantee to hold
diff --git a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameIntCacheLRU.java b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameIntCacheLRU.java
index baecdbb..917becd 100644
--- a/modules/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameIntCacheLRU.java
+++ b/modules/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameIntCacheLRU.java
@@ -119,7 +119,7 @@ class NameIntCacheLRU {
    * If cache is full remove least recently used entries from cache.
    * Return true if anything was removed, false otherwise.
    * 
-   * See comment in {@link LuceneTaxonomyWriter#addToCache(String, Integer)}
+   * See comment in {@link DirectoryTaxonomyWriter#addToCache(String, Integer)}
    * for an explanation why we clean 2/3rds of the cache, and not just one
    * entry.
    */ 
diff --git a/modules/facet/src/test/org/apache/lucene/facet/FacetTestBase.java b/modules/facet/src/test/org/apache/lucene/facet/FacetTestBase.java
index 1c18056..30e0322 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/FacetTestBase.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/FacetTestBase.java
@@ -43,8 +43,8 @@ import org.apache.lucene.facet.search.results.FacetResultNode;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -137,7 +137,7 @@ public abstract class FacetTestBase extends LuceneTestCase {
     }
     
     RandomIndexWriter iw = new RandomIndexWriter(random, indexDir, getIndexWriterConfig(getAnalyzer()));
-    TaxonomyWriter taxo = new LuceneTaxonomyWriter(taxoDir, OpenMode.CREATE);
+    TaxonomyWriter taxo = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
     
     populateIndex(iw, taxo, getFacetIndexingParams(partitionSize));
     
@@ -148,7 +148,7 @@ public abstract class FacetTestBase extends LuceneTestCase {
     iw.close();
     
     // prepare for searching
-    taxoReader = new LuceneTaxonomyReader(taxoDir);
+    taxoReader = new DirectoryTaxonomyReader(taxoDir);
     indexReader = IndexReader.open(indexDir);
     searcher = newSearcher(indexReader);
   }
diff --git a/modules/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java b/modules/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java
index 66fc7ba..1ef82ee 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java
@@ -31,8 +31,8 @@ import org.apache.lucene.facet.search.params.FacetSearchParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -69,7 +69,7 @@ public class FacetTestUtils {
       IndexTaxonomyReaderPair pair = new IndexTaxonomyReaderPair();
       pair.indexReader = IndexReader.open(dirs[i][0]);
       pair.indexSearcher = new IndexSearcher(pair.indexReader);
-      pair.taxReader = new LuceneTaxonomyReader(dirs[i][1]);
+      pair.taxReader = new DirectoryTaxonomyReader(dirs[i][1]);
       pairs[i] = pair;
     }
     return pairs;
@@ -83,7 +83,7 @@ public class FacetTestUtils {
       pair.indexWriter = new IndexWriter(dirs[i][0], new IndexWriterConfig(
           LuceneTestCase.TEST_VERSION_CURRENT, new StandardAnalyzer(
               LuceneTestCase.TEST_VERSION_CURRENT)));
-      pair.taxWriter = new LuceneTaxonomyWriter(dirs[i][1]);
+      pair.taxWriter = new DirectoryTaxonomyWriter(dirs[i][1]);
       pair.indexWriter.commit();
       pair.taxWriter.commit();
       pairs[i] = pair;
diff --git a/modules/facet/src/test/org/apache/lucene/facet/enhancements/TwoEnhancementsTest.java b/modules/facet/src/test/org/apache/lucene/facet/enhancements/TwoEnhancementsTest.java
index 5a38ba2..866bb11 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/enhancements/TwoEnhancementsTest.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/enhancements/TwoEnhancementsTest.java
@@ -21,7 +21,7 @@ import org.apache.lucene.facet.enhancements.params.EnhancementsIndexingParams;
 import org.apache.lucene.facet.search.DrillDown;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -58,7 +58,7 @@ public class TwoEnhancementsTest extends LuceneTestCase {
 
     RandomIndexWriter indexWriter = new RandomIndexWriter(random, indexDir, newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
-    TaxonomyWriter taxo = new LuceneTaxonomyWriter(taxoDir);
+    TaxonomyWriter taxo = new DirectoryTaxonomyWriter(taxoDir);
 
     // a category document builder will add the categories to a document
     // once build() is called
@@ -103,7 +103,7 @@ public class TwoEnhancementsTest extends LuceneTestCase {
 
     RandomIndexWriter indexWriter = new RandomIndexWriter(random, indexDir, newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
-    TaxonomyWriter taxo = new LuceneTaxonomyWriter(taxoDir);
+    TaxonomyWriter taxo = new DirectoryTaxonomyWriter(taxoDir);
 
     // a category document builder will add the categories to a document
     // once build() is called
diff --git a/modules/facet/src/test/org/apache/lucene/facet/enhancements/association/CustomAssociationPropertyTest.java b/modules/facet/src/test/org/apache/lucene/facet/enhancements/association/CustomAssociationPropertyTest.java
index 5acace1..09bcab4 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/enhancements/association/CustomAssociationPropertyTest.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/enhancements/association/CustomAssociationPropertyTest.java
@@ -16,8 +16,8 @@ import org.apache.lucene.facet.index.CategoryContainer;
 import org.apache.lucene.facet.index.attributes.CategoryAttributeImpl;
 import org.apache.lucene.facet.index.attributes.CategoryProperty;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -59,7 +59,7 @@ public class CustomAssociationPropertyTest extends LuceneTestCase {
     
     RandomIndexWriter w = new RandomIndexWriter(random, iDir, 
         newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false)));
-    LuceneTaxonomyWriter taxoW = new LuceneTaxonomyWriter(tDir);
+    DirectoryTaxonomyWriter taxoW = new DirectoryTaxonomyWriter(tDir);
     
     CategoryContainer cc = new CategoryContainer();
     EnhancementsDocumentBuilder builder = new EnhancementsDocumentBuilder(taxoW, iParams);
@@ -75,7 +75,7 @@ public class CustomAssociationPropertyTest extends LuceneTestCase {
     IndexReader reader = w.getReader();
     w.close();
     
-    LuceneTaxonomyReader taxo = new LuceneTaxonomyReader(tDir);
+    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(tDir);
     String field = iParams.getCategoryListParams(new CategoryPath("0")).getTerm().field();
     AssociationsPayloadIterator api = new AssociationsPayloadIterator(reader, field);
 
diff --git a/modules/facet/src/test/org/apache/lucene/facet/index/FacetsPayloadProcessorProviderTest.java b/modules/facet/src/test/org/apache/lucene/facet/index/FacetsPayloadProcessorProviderTest.java
index 86fef2c..5073050 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/index/FacetsPayloadProcessorProviderTest.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/index/FacetsPayloadProcessorProviderTest.java
@@ -23,8 +23,8 @@ import org.apache.lucene.facet.search.params.FacetSearchParams;
 import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.search.results.FacetResultNode;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -68,7 +68,7 @@ public class FacetsPayloadProcessorProviderTest extends LuceneTestCase {
 
   private void verifyResults(Directory dir, Directory taxDir) throws IOException {
     IndexReader reader1 = IndexReader.open(dir);
-    LuceneTaxonomyReader taxReader = new LuceneTaxonomyReader(taxDir);
+    DirectoryTaxonomyReader taxReader = new DirectoryTaxonomyReader(taxDir);
     IndexSearcher searcher = newSearcher(reader1);
     FacetSearchParams fsp = new FacetSearchParams();
     fsp.addFacetRequest(new CountFacetRequest(new CategoryPath("tag"), NUM_DOCS));
@@ -94,7 +94,7 @@ public class FacetsPayloadProcessorProviderTest extends LuceneTestCase {
         new MockAnalyzer(random, MockTokenizer.WHITESPACE, false));
     RandomIndexWriter writer = new RandomIndexWriter(random, dir, config);
     
-    LuceneTaxonomyWriter taxonomyWriter = new LuceneTaxonomyWriter(taxDir);
+    DirectoryTaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxDir);
     for (int i = 1; i <= NUM_DOCS; i++) {
       Document doc = new Document();
       List<CategoryPath> categoryPaths = new ArrayList<CategoryPath>(i + 1);
diff --git a/modules/facet/src/test/org/apache/lucene/facet/index/categorypolicy/OrdinalPolicyTest.java b/modules/facet/src/test/org/apache/lucene/facet/index/categorypolicy/OrdinalPolicyTest.java
index 00b5c27..7514143 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/index/categorypolicy/OrdinalPolicyTest.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/index/categorypolicy/OrdinalPolicyTest.java
@@ -10,7 +10,7 @@ import org.apache.lucene.facet.index.categorypolicy.OrdinalPolicy;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -48,7 +48,7 @@ public class OrdinalPolicyTest extends LuceneTestCase {
   public void testNonTopLevelOrdinalPolicy() throws Exception {
     Directory dir = newDirectory();
     TaxonomyWriter taxonomy = null;
-    taxonomy = new LuceneTaxonomyWriter(dir);
+    taxonomy = new DirectoryTaxonomyWriter(dir);
 
     int[] topLevelOrdinals = new int[10];
     String[] topLevelStrings = new String[10];
diff --git a/modules/facet/src/test/org/apache/lucene/facet/index/categorypolicy/PathPolicyTest.java b/modules/facet/src/test/org/apache/lucene/facet/index/categorypolicy/PathPolicyTest.java
index 7f7b651..df7f9f3 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/index/categorypolicy/PathPolicyTest.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/index/categorypolicy/PathPolicyTest.java
@@ -9,7 +9,7 @@ import org.apache.lucene.facet.index.categorypolicy.NonTopLevelPathPolicy;
 import org.apache.lucene.facet.index.categorypolicy.PathPolicy;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -54,7 +54,7 @@ public class PathPolicyTest extends LuceneTestCase {
   public void testNonTopLevelPathPolicy() throws Exception {
     Directory dir = newDirectory();
     TaxonomyWriter taxonomy = null;
-    taxonomy = new LuceneTaxonomyWriter(dir);
+    taxonomy = new DirectoryTaxonomyWriter(dir);
 
     CategoryPath[] topLevelPaths = new CategoryPath[10];
     String[] topLevelStrings = new String[10];
diff --git a/modules/facet/src/test/org/apache/lucene/facet/index/streaming/CategoryParentsStreamTest.java b/modules/facet/src/test/org/apache/lucene/facet/index/streaming/CategoryParentsStreamTest.java
index c59f1c6..eb57984 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/index/streaming/CategoryParentsStreamTest.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/index/streaming/CategoryParentsStreamTest.java
@@ -19,7 +19,7 @@ import org.apache.lucene.facet.index.streaming.CategoryAttributesStream;
 import org.apache.lucene.facet.index.streaming.CategoryListTokenizer;
 import org.apache.lucene.facet.index.streaming.CategoryParentsStream;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -50,7 +50,7 @@ public class CategoryParentsStreamTest extends CategoryContainerTestBase {
   @Test
   public void testStreamDefaultParams() throws IOException {
     Directory directory = newDirectory();
-    TaxonomyWriter taxonomyWriter = new LuceneTaxonomyWriter(
+    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(
         directory);
     CategoryParentsStream stream = new CategoryParentsStream(
         new CategoryAttributesStream(categoryContainer),
@@ -77,7 +77,7 @@ public class CategoryParentsStreamTest extends CategoryContainerTestBase {
   @Test
   public void testStreamNonTopLevelParams() throws IOException {
     Directory directory = newDirectory();
-    final TaxonomyWriter taxonomyWriter = new LuceneTaxonomyWriter(
+    final TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(
         directory);
     FacetIndexingParams indexingParams = new DefaultFacetIndexingParams() {
       @Override
@@ -118,7 +118,7 @@ public class CategoryParentsStreamTest extends CategoryContainerTestBase {
   @Test
   public void testNoRetainableAttributes() throws IOException, FacetException {
     Directory directory = newDirectory();
-    TaxonomyWriter taxonomyWriter = new LuceneTaxonomyWriter(directory);
+    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(directory);
 
     new CategoryParentsStream(new CategoryAttributesStream(categoryContainer),
         taxonomyWriter, new DefaultFacetIndexingParams());
@@ -152,7 +152,7 @@ public class CategoryParentsStreamTest extends CategoryContainerTestBase {
   @Test
   public void testRetainableAttributes() throws IOException, FacetException {
     Directory directory = newDirectory();
-    TaxonomyWriter taxonomyWriter = new LuceneTaxonomyWriter(
+    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(
         directory);
 
     FacetIndexingParams indexingParams = new DefaultFacetIndexingParams();
diff --git a/modules/facet/src/test/org/apache/lucene/facet/index/streaming/CategoryTokenizerTest.java b/modules/facet/src/test/org/apache/lucene/facet/index/streaming/CategoryTokenizerTest.java
index 14804e6..8d52ab1 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/index/streaming/CategoryTokenizerTest.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/index/streaming/CategoryTokenizerTest.java
@@ -17,7 +17,7 @@ import org.apache.lucene.facet.index.streaming.CategoryAttributesStream;
 import org.apache.lucene.facet.index.streaming.CategoryTokenizer;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -47,7 +47,7 @@ public class CategoryTokenizerTest extends CategoryContainerTestBase {
   @Test
   public void testTokensDefaultParams() throws IOException {
     Directory directory = newDirectory();
-    TaxonomyWriter taxonomyWriter = new LuceneTaxonomyWriter(
+    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(
         directory);
     DefaultFacetIndexingParams indexingParams = new DefaultFacetIndexingParams();
     CategoryTokenizer tokenizer = new CategoryTokenizer(
@@ -86,7 +86,7 @@ public class CategoryTokenizerTest extends CategoryContainerTestBase {
   @Test
   public void testLongCategoryPath() throws IOException {
     Directory directory = newDirectory();
-    TaxonomyWriter taxonomyWriter = new LuceneTaxonomyWriter(
+    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(
         directory);
 
     List<CategoryPath> longCategory = new ArrayList<CategoryPath>();
diff --git a/modules/facet/src/test/org/apache/lucene/facet/search/DrillDownTest.java b/modules/facet/src/test/org/apache/lucene/facet/search/DrillDownTest.java
index 20fdc92..2cfa438 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/search/DrillDownTest.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/search/DrillDownTest.java
@@ -28,8 +28,8 @@ import org.apache.lucene.facet.index.params.PerDimensionIndexingParams;
 import org.apache.lucene.facet.search.params.FacetSearchParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -53,7 +53,7 @@ public class DrillDownTest extends LuceneTestCase {
   private FacetSearchParams defaultParams = new FacetSearchParams();
   private FacetSearchParams nonDefaultParams;
   private static IndexReader reader;
-  private static LuceneTaxonomyReader taxo;
+  private static DirectoryTaxonomyReader taxo;
   private static Directory dir;
   private static Directory taxoDir;
   
@@ -74,7 +74,7 @@ public class DrillDownTest extends LuceneTestCase {
         newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false)));
     
     taxoDir = newDirectory();
-    TaxonomyWriter taxoWriter = new LuceneTaxonomyWriter(taxoDir);
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
     
     for (int i = 0; i < 100; i++) {
       ArrayList<CategoryPath> paths = new ArrayList<CategoryPath>();
@@ -100,7 +100,7 @@ public class DrillDownTest extends LuceneTestCase {
     reader = writer.getReader();
     writer.close();
     
-    taxo = new LuceneTaxonomyReader(taxoDir);
+    taxo = new DirectoryTaxonomyReader(taxoDir);
   }
   
   @Test
@@ -149,6 +149,8 @@ public class DrillDownTest extends LuceneTestCase {
     Query q4 = DrillDown.query(defaultParams, fooQuery, new CategoryPath("b"));
     docs = searcher.search(q4, 100);
     assertEquals(10, docs.totalHits);
+    
+    searcher.close();
   }
   
   @Test
@@ -170,6 +172,8 @@ public class DrillDownTest extends LuceneTestCase {
     Query q4 = DrillDown.query(fooQuery, new CategoryPath("b"));
     docs = searcher.search(q4, 100);
     assertEquals(10, docs.totalHits);
+    
+    searcher.close();
   }
   
   @AfterClass
diff --git a/modules/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java b/modules/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java
index d68b9dc..898f7a0 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java
@@ -38,8 +38,8 @@ import org.apache.lucene.facet.search.results.FacetResultNode;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -67,7 +67,7 @@ public class TestMultipleCategoryLists extends LuceneTestCase {
     RandomIndexWriter iw = new RandomIndexWriter(random, dirs[0][0], newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
     // create and open a taxonomy writer
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(dirs[0][1], OpenMode.CREATE);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(dirs[0][1], OpenMode.CREATE);
 
     /**
      * Configure with no custom counting lists
@@ -80,7 +80,7 @@ public class TestMultipleCategoryLists extends LuceneTestCase {
     tw.commit();
 
     // prepare index reader and taxonomy.
-    TaxonomyReader tr = new LuceneTaxonomyReader(dirs[0][1]);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dirs[0][1]);
 
     // prepare searcher to search against
     IndexSearcher searcher = newSearcher(ir);
@@ -109,7 +109,7 @@ public class TestMultipleCategoryLists extends LuceneTestCase {
     RandomIndexWriter iw = new RandomIndexWriter(random, dirs[0][0], newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
     // create and open a taxonomy writer
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(dirs[0][1],
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(dirs[0][1],
         OpenMode.CREATE);
 
     PerDimensionIndexingParams iParams = new PerDimensionIndexingParams();
@@ -121,7 +121,7 @@ public class TestMultipleCategoryLists extends LuceneTestCase {
     tw.commit();
 
     // prepare index reader and taxonomy.
-    TaxonomyReader tr = new LuceneTaxonomyReader(dirs[0][1]);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dirs[0][1]);
 
     // prepare searcher to search against
     IndexSearcher searcher = newSearcher(ir);
@@ -150,7 +150,7 @@ public class TestMultipleCategoryLists extends LuceneTestCase {
     RandomIndexWriter iw = new RandomIndexWriter(random, dirs[0][0], newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
     // create and open a taxonomy writer
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(dirs[0][1],
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(dirs[0][1],
         OpenMode.CREATE);
 
     PerDimensionIndexingParams iParams = new PerDimensionIndexingParams();
@@ -164,7 +164,7 @@ public class TestMultipleCategoryLists extends LuceneTestCase {
     tw.commit();
 
     // prepare index reader and taxonomy.
-    TaxonomyReader tr = new LuceneTaxonomyReader(dirs[0][1]);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dirs[0][1]);
 
     // prepare searcher to search against
     IndexSearcher searcher = newSearcher(ir);
@@ -199,7 +199,7 @@ public class TestMultipleCategoryLists extends LuceneTestCase {
     RandomIndexWriter iw = new RandomIndexWriter(random, dirs[0][0], newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
     // create and open a taxonomy writer
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(dirs[0][1], OpenMode.CREATE);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(dirs[0][1], OpenMode.CREATE);
 
     PerDimensionIndexingParams iParams = new PerDimensionIndexingParams();
     iParams.addCategoryListParams(new CategoryPath("Band"),
@@ -212,7 +212,7 @@ public class TestMultipleCategoryLists extends LuceneTestCase {
     tw.commit();
 
     // prepare index reader and taxonomy.
-    TaxonomyReader tr = new LuceneTaxonomyReader(dirs[0][1]);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dirs[0][1]);
 
     // prepare searcher to search against
     IndexSearcher searcher = newSearcher(ir);
@@ -240,7 +240,7 @@ public class TestMultipleCategoryLists extends LuceneTestCase {
     RandomIndexWriter iw = new RandomIndexWriter(random, dirs[0][0], newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
     // create and open a taxonomy writer
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(dirs[0][1],
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(dirs[0][1],
         OpenMode.CREATE);
 
     PerDimensionIndexingParams iParams = new PerDimensionIndexingParams();
@@ -257,7 +257,7 @@ public class TestMultipleCategoryLists extends LuceneTestCase {
     tw.commit();
 
     // prepare index reader and taxonomy.
-    TaxonomyReader tr = new LuceneTaxonomyReader(dirs[0][1]);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dirs[0][1]);
 
     // prepare searcher to search against
     IndexSearcher searcher = newSearcher(ir);
diff --git a/modules/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java b/modules/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java
index d70d0ed..0583bc0 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java
@@ -29,8 +29,8 @@ import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.search.results.FacetResultNode;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.facet.util.PartitionsUtils;
 
 /**
@@ -80,7 +80,7 @@ public class TestTopKInEachNodeResultHandler extends LuceneTestCase {
       RandomIndexWriter iw = new RandomIndexWriter(random, iDir,
           newIndexWriterConfig(TEST_VERSION_CURRENT,
               new MockAnalyzer(random)).setOpenMode(OpenMode.CREATE));
-      TaxonomyWriter tw = new LuceneTaxonomyWriter(tDir);
+      TaxonomyWriter tw = new DirectoryTaxonomyWriter(tDir);
       prvt_add(iParams, iw, tw, "a", "b");
       prvt_add(iParams, iw, tw, "a", "b", "1");
       prvt_add(iParams, iw, tw, "a", "b", "1");
@@ -104,7 +104,7 @@ public class TestTopKInEachNodeResultHandler extends LuceneTestCase {
       tw.close();
 
       IndexSearcher is = newSearcher(ir);
-      LuceneTaxonomyReader tr = new LuceneTaxonomyReader(tDir);
+      DirectoryTaxonomyReader tr = new DirectoryTaxonomyReader(tDir);
 
       // Get all of the documents and run the query, then do different
       // facet counts and compare to control
diff --git a/modules/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java b/modules/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java
index 8b54182..ec1e4b3 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java
@@ -34,8 +34,8 @@ import org.apache.lucene.facet.search.results.FacetResultNode;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.SlowRAMDirectory;
 import org.apache.lucene.util._TestUtil;
@@ -67,12 +67,12 @@ public class TestTotalFacetCountsCache extends LuceneTestCase {
    */
   private static class TFCThread extends Thread {
     private final IndexReader r;
-    private final LuceneTaxonomyReader tr;
+    private final DirectoryTaxonomyReader tr;
     private final FacetIndexingParams iParams;
     
     TotalFacetCounts tfc;
 
-    public TFCThread(IndexReader r, LuceneTaxonomyReader tr, FacetIndexingParams iParams) {
+    public TFCThread(IndexReader r, DirectoryTaxonomyReader tr, FacetIndexingParams iParams) {
       this.r = r;
       this.tr = tr;
       this.iParams = iParams;
@@ -156,7 +156,7 @@ public class TestTotalFacetCountsCache extends LuceneTestCase {
     
     // Open the slow readers
     IndexReader slowIndexReader = IndexReader.open(indexDir);
-    TaxonomyReader slowTaxoReader = new LuceneTaxonomyReader(taxoDir);
+    TaxonomyReader slowTaxoReader = new DirectoryTaxonomyReader(taxoDir);
 
     // Class to perform search and return results as threads
     class Multi extends Thread {
@@ -421,7 +421,7 @@ public class TestTotalFacetCountsCache extends LuceneTestCase {
     // Write index using 'normal' directories
     IndexWriter w = new IndexWriter(indexDir, new IndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
-    LuceneTaxonomyWriter tw = new LuceneTaxonomyWriter(taxoDir);
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
     DefaultFacetIndexingParams iParams = new DefaultFacetIndexingParams();
     // Add documents and facets
     for (int i = 0; i < 1000; i++) {
@@ -434,7 +434,7 @@ public class TestTotalFacetCountsCache extends LuceneTestCase {
     taxoDir.setSleepMillis(1);
 
     IndexReader r = IndexReader.open(indexDir);
-    LuceneTaxonomyReader tr = new LuceneTaxonomyReader(taxoDir);
+    DirectoryTaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
 
     // Create and start threads. Thread1 should lock the cache and calculate
     // the TFC array. The second thread should block until the first is
diff --git a/modules/facet/src/test/org/apache/lucene/facet/search/association/AssociationsFacetRequestTest.java b/modules/facet/src/test/org/apache/lucene/facet/search/association/AssociationsFacetRequestTest.java
index 89dee04..11cdeb1 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/search/association/AssociationsFacetRequestTest.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/search/association/AssociationsFacetRequestTest.java
@@ -29,8 +29,8 @@ import org.apache.lucene.facet.search.params.association.AssociationIntSumFacetR
 import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -69,7 +69,7 @@ public class AssociationsFacetRequestTest extends LuceneTestCase {
     RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, 
         new MockAnalyzer(random, MockTokenizer.KEYWORD, false)));
     
-    TaxonomyWriter taxoWriter = new LuceneTaxonomyWriter(taxoDir);
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
     
     EnhancementsDocumentBuilder builder = new EnhancementsDocumentBuilder(
         taxoWriter, new DefaultEnhancementsIndexingParams(
@@ -106,7 +106,7 @@ public class AssociationsFacetRequestTest extends LuceneTestCase {
   
   @Test
   public void testIntSumAssociation() throws Exception {
-    LuceneTaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
 
     // facet requests for two facets
     FacetSearchParams fsp = new FacetSearchParams();
@@ -132,7 +132,7 @@ public class AssociationsFacetRequestTest extends LuceneTestCase {
   
   @Test
   public void testFloatSumAssociation() throws Exception {
-    LuceneTaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
 
     // facet requests for two facets
     FacetSearchParams fsp = new FacetSearchParams();
@@ -161,7 +161,7 @@ public class AssociationsFacetRequestTest extends LuceneTestCase {
     // Same category list cannot be aggregated by two different aggregators. If
     // you want to do that, you need to separate the categories into two
     // category list (you'll still have one association list).
-    LuceneTaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
 
     // facet requests for two facets
     FacetSearchParams fsp = new FacetSearchParams();
diff --git a/modules/facet/src/test/org/apache/lucene/facet/search/params/FacetRequestTest.java b/modules/facet/src/test/org/apache/lucene/facet/search/params/FacetRequestTest.java
index ff2e060..f86d36b 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/search/params/FacetRequestTest.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/search/params/FacetRequestTest.java
@@ -9,7 +9,7 @@ import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.facet.search.FacetResultsHandler;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -58,8 +58,8 @@ public class FacetRequestTest extends LuceneTestCase {
     // create empty indexes, so that LTR ctor won't complain about a missing index.
     new IndexWriter(dir1, new IndexWriterConfig(TEST_VERSION_CURRENT, null)).close();
     new IndexWriter(dir2, new IndexWriterConfig(TEST_VERSION_CURRENT, null)).close();
-    TaxonomyReader tr1 = new LuceneTaxonomyReader(dir1);
-    TaxonomyReader tr2 = new LuceneTaxonomyReader(dir2);
+    TaxonomyReader tr1 = new DirectoryTaxonomyReader(dir1);
+    TaxonomyReader tr2 = new DirectoryTaxonomyReader(dir2);
     FacetResultsHandler frh1 = fr.createFacetResultsHandler(tr1);
     FacetResultsHandler frh2 = fr.createFacetResultsHandler(tr2);
     assertTrue("should not return the same FacetResultHandler instance for different TaxonomyReader instances", frh1 != frh2);
@@ -77,7 +77,7 @@ public class FacetRequestTest extends LuceneTestCase {
     Directory dir = newDirectory();
     // create empty indexes, so that LTR ctor won't complain about a missing index.
     new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, null)).close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(dir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dir);
     FacetResultsHandler frh = fr.createFacetResultsHandler(tr);
     fr.setDepth(10);
     assertEquals(FacetRequest.DEFAULT_DEPTH, frh.getFacetRequest().getDepth());
diff --git a/modules/facet/src/test/org/apache/lucene/facet/search/params/FacetSearchParamsTest.java b/modules/facet/src/test/org/apache/lucene/facet/search/params/FacetSearchParamsTest.java
index 5916366..40df7ba 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/search/params/FacetSearchParamsTest.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/search/params/FacetSearchParamsTest.java
@@ -8,8 +8,8 @@ import org.apache.lucene.facet.index.params.DefaultFacetIndexingParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.facet.util.PartitionsUtils;
 
 /**
@@ -37,8 +37,8 @@ public class FacetSearchParamsTest extends LuceneTestCase {
     assertEquals("unexpected default facet indexing params class", DefaultFacetIndexingParams.class.getName(), fsp.getFacetIndexingParams().getClass().getName());
     assertEquals("no facet requests should be added by default", 0, fsp.getFacetRequests().size());
     Directory dir = newDirectory();
-    new LuceneTaxonomyWriter(dir).close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(dir);
+    new DirectoryTaxonomyWriter(dir).close();
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dir);
     assertEquals("unexpected partition offset for 0 categories", 1, PartitionsUtils.partitionOffset(fsp, 1, tr));
     assertEquals("unexpected partition size for 0 categories", 1, PartitionsUtils.partitionSize(fsp,tr));
     tr.close();
@@ -56,11 +56,11 @@ public class FacetSearchParamsTest extends LuceneTestCase {
   public void testPartitionSizeWithCategories() throws Exception {
     FacetSearchParams fsp = new FacetSearchParams();
     Directory dir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(dir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(dir);
     tw.addCategory(new CategoryPath("a"));
     tw.commit();
     tw.close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(dir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dir);
     assertEquals("unexpected partition offset for 1 categories", 2, PartitionsUtils.partitionOffset(fsp, 1, tr));
     assertEquals("unexpected partition size for 1 categories", 2, PartitionsUtils.partitionSize(fsp,tr));
     tr.close();
diff --git a/modules/facet/src/test/org/apache/lucene/facet/search/params/MultiIteratorsPerCLParamsTest.java b/modules/facet/src/test/org/apache/lucene/facet/search/params/MultiIteratorsPerCLParamsTest.java
index 3fcfccc..3689d04 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/search/params/MultiIteratorsPerCLParamsTest.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/search/params/MultiIteratorsPerCLParamsTest.java
@@ -32,8 +32,8 @@ import org.apache.lucene.facet.search.results.IntermediateFacetResult;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.facet.util.ScoredDocIdsUtils;
 
 /**
@@ -93,7 +93,7 @@ public class MultiIteratorsPerCLParamsTest extends LuceneTestCase {
     Directory taxoDir = newDirectory();
     populateIndex(iParams, indexDir, taxoDir);
 
-    TaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
+    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     IndexReader reader = IndexReader.open(indexDir);
 
     CategoryListCache clCache = null;
@@ -168,7 +168,7 @@ public class MultiIteratorsPerCLParamsTest extends LuceneTestCase {
       Directory taxoDir) throws Exception {
     RandomIndexWriter writer = new RandomIndexWriter(random, indexDir, 
         newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false)));
-    TaxonomyWriter taxoWriter = new LuceneTaxonomyWriter(taxoDir);
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
 
     for (CategoryPath[] categories : perDocCategories) {
       writer.addDocument(new CategoryDocumentBuilder(taxoWriter, iParams)
diff --git a/modules/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java b/modules/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java
index a92dbad..52f0f88 100644
--- a/modules/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java
+++ b/modules/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java
@@ -14,8 +14,8 @@ import org.junit.Test;
 
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader.ChildrenArrays;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.util.SlowRAMDirectory;
 
 /**
@@ -159,7 +159,7 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testWriter() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     // Also check TaxonomyWriter.getSize() - see that the taxonomy's size
     // is what we expect it to be.
@@ -175,7 +175,7 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testWriterTwice() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     // run fillTaxonomy again - this will try to add the same categories
     // again, and check that we see the same ordinal paths again, not
@@ -197,10 +197,10 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testWriterTwice2() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.close();
-    tw = new LuceneTaxonomyWriter(indexDir);
+    tw = new DirectoryTaxonomyWriter(indexDir);
     // run fillTaxonomy again - this will try to add the same categories
     // again, and check that we see the same ordinals again, not different
     // ones, and that the number of categories hasn't grown by the new
@@ -222,7 +222,7 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   public void testWriterTwice3() throws Exception {
     Directory indexDir = newDirectory();
     // First, create and fill the taxonomy
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.close();
     // Now, open the same taxonomy and add the same categories again.
@@ -231,7 +231,7 @@ public class TestTaxonomyCombined extends LuceneTestCase {
     // all into memory and close it's reader. The bug was that it closed
     // the reader, but forgot that it did (because it didn't set the reader
     // reference to null).
-    tw = new LuceneTaxonomyWriter(indexDir);
+    tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     // Add one new category, just to make commit() do something:
     tw.addCategory(new CategoryPath("hi"));
@@ -253,7 +253,7 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testWriterSimpler() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     assertEquals(1, tw.getSize()); // the root only
     // Test that adding a new top-level category works
     assertEquals(1, tw.addCategory(new CategoryPath("a")));
@@ -297,12 +297,12 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testRootOnly() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     // right after opening the index, it should already contain the
     // root, so have size 1:
     assertEquals(1, tw.getSize());
     tw.close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     assertEquals(1, tr.getSize());
     assertEquals(0, tr.getPath(0).length());
     assertEquals(TaxonomyReader.INVALID_ORDINAL, tr.getParent(0));
@@ -319,9 +319,9 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testRootOnly2() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     tw.commit();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     assertEquals(1, tr.getSize());
     assertEquals(0, tr.getPath(0).length());
     assertEquals(TaxonomyReader.INVALID_ORDINAL, tr.getParent(0));
@@ -339,10 +339,10 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testReaderBasic() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
 
     // test TaxonomyReader.getSize():
     assertEquals(expectedCategories.length, tr.getSize());
@@ -398,10 +398,10 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testReaderParent() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
 
     // check that the parent of the root ordinal is the invalid ordinal:
     assertEquals(TaxonomyReader.INVALID_ORDINAL, tr.getParent(0));
@@ -463,11 +463,11 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testWriterParent1() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.close();
-    tw = new LuceneTaxonomyWriter(indexDir);
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    tw = new DirectoryTaxonomyWriter(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     
     checkWriterParent(tr, tw);
     
@@ -479,10 +479,10 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testWriterParent2() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.commit();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     
     checkWriterParent(tr, tw);
     
@@ -542,10 +542,10 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testReaderParentArray() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     int[] parents = tr.getParentArray();
     assertEquals(tr.getSize(), parents.length);
     for (int i=0; i<tr.getSize(); i++) {
@@ -563,10 +563,10 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testChildrenArrays() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     ChildrenArrays ca = tr.getChildrenArrays();
     int[] youngestChildArray = ca.getYoungestChildArray();
     assertEquals(tr.getSize(), youngestChildArray.length);
@@ -627,10 +627,10 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testChildrenArraysInvariants() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     tw.close();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     ChildrenArrays ca = tr.getChildrenArrays();
     int[] youngestChildArray = ca.getYoungestChildArray();
     assertEquals(tr.getSize(), youngestChildArray.length);
@@ -707,10 +707,10 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testChildrenArraysGrowth() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     tw.addCategory(new CategoryPath("hi", "there"));
     tw.commit();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     ChildrenArrays ca = tr.getChildrenArrays();
     assertEquals(3, tr.getSize());
     assertEquals(3, ca.getOlderSiblingArray().length);
@@ -747,12 +747,12 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   public void testTaxonomyReaderRefreshRaces() throws Exception {
     // compute base child arrays - after first chunk, and after the other
     Directory indexDirBase =  newDirectory();
-    TaxonomyWriter twBase = new LuceneTaxonomyWriter(indexDirBase);
+    TaxonomyWriter twBase = new DirectoryTaxonomyWriter(indexDirBase);
     twBase.addCategory(new CategoryPath("a", "0"));
     final CategoryPath abPath = new CategoryPath("a", "b");
     twBase.addCategory(abPath);
     twBase.commit();
-    TaxonomyReader trBase = new LuceneTaxonomyReader(indexDirBase);
+    TaxonomyReader trBase = new DirectoryTaxonomyReader(indexDirBase);
 
     final ChildrenArrays ca1 = trBase.getChildrenArrays();
     
@@ -779,12 +779,12 @@ public class TestTaxonomyCombined extends LuceneTestCase {
       final int abOrd, final int abYoungChildBase1, final int abYoungChildBase2, final int retry)
       throws Exception {
     SlowRAMDirectory indexDir =  new SlowRAMDirectory(-1,null); // no slowness for intialization
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     tw.addCategory(new CategoryPath("a", "0"));
     tw.addCategory(abPath);
     tw.commit();
     
-    final TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    final TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     for (int i=0; i < 1<<10; i++) { //1024 facets
       final CategoryPath cp = new CategoryPath("a", "b", Integer.toString(i));
       tw.addCategory(cp);
@@ -865,9 +865,9 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testSeparateReaderAndWriter() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     tw.commit();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
 
     int author = 1;
 
@@ -932,9 +932,9 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testSeparateReaderAndWriter2() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     tw.commit();
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
 
     // Test getOrdinal():
     CategoryPath author = new CategoryPath("Author");
@@ -968,26 +968,26 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   public void testWriterLock() throws Exception {
     // native fslock impl gets angry if we use it, so use RAMDirectory explicitly.
     Directory indexDir = new RAMDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     tw.addCategory(new CategoryPath("hi", "there"));
     tw.commit();
     // we deliberately not close the write now, and keep it open and
     // locked.
     // Verify that the writer worked:
-    TaxonomyReader tr = new LuceneTaxonomyReader(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     assertEquals(2, tr.getOrdinal(new CategoryPath("hi", "there")));
     // Try to open a second writer, with the first one locking the directory.
     // We expect to get a LockObtainFailedException.
     try {
-      new LuceneTaxonomyWriter(indexDir);
+      new DirectoryTaxonomyWriter(indexDir);
       fail("should have failed to write in locked directory");
     } catch (LockObtainFailedException e) {
       // this is what we expect to happen.
     }
     // Remove the lock, and now the open should succeed, and we can
     // write to the new writer.
-    LuceneTaxonomyWriter.unlock(indexDir);
-    TaxonomyWriter tw2 = new LuceneTaxonomyWriter(indexDir);
+    DirectoryTaxonomyWriter.unlock(indexDir);
+    TaxonomyWriter tw2 = new DirectoryTaxonomyWriter(indexDir);
     tw2.addCategory(new CategoryPath("hey"));
     tw2.close();
     // See that the writer indeed wrote:
@@ -1054,7 +1054,7 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testWriterCheckPaths() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomyCheckPaths(tw);
     // Also check TaxonomyWriter.getSize() - see that the taxonomy's size
     // is what we expect it to be.
@@ -1073,14 +1073,14 @@ public class TestTaxonomyCombined extends LuceneTestCase {
   @Test
   public void testWriterCheckPaths2() throws Exception {
     Directory indexDir = newDirectory();
-    TaxonomyWriter tw = new LuceneTaxonomyWriter(indexDir);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     checkPaths(tw);
     fillTaxonomy(tw);
     checkPaths(tw);
     tw.close();
 
-    tw = new LuceneTaxonomyWriter(indexDir);
+    tw = new DirectoryTaxonomyWriter(indexDir);
     checkPaths(tw);
     fillTaxonomy(tw);
     checkPaths(tw);
diff --git a/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestAddTaxonomies.java b/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestAddTaxonomies.java
new file mode 100644
index 0000000..a25cd4c
--- /dev/null
+++ b/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestAddTaxonomies.java
@@ -0,0 +1,254 @@
+package org.apache.lucene.facet.taxonomy.directory;
+
+import java.io.File;
+
+import org.apache.lucene.store.Directory;
+import org.junit.Test;
+
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.DiskOrdinalMap;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.MemoryOrdinalMap;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestAddTaxonomies extends LuceneTestCase {
+
+  @Test
+  public void test1() throws Exception {
+    Directory dir1 = newDirectory();
+    DirectoryTaxonomyWriter tw1 = new DirectoryTaxonomyWriter(dir1);
+    tw1.addCategory(new CategoryPath("Author", "Mark Twain"));
+    tw1.addCategory(new CategoryPath("Animals", "Dog"));
+    Directory dir2 = newDirectory();
+    DirectoryTaxonomyWriter tw2 = new DirectoryTaxonomyWriter(dir2);
+    tw2.addCategory(new CategoryPath("Author", "Rob Pike"));
+    tw2.addCategory(new CategoryPath("Aardvarks", "Bob"));
+    tw2.close();
+    Directory dir3 = newDirectory();
+    DirectoryTaxonomyWriter tw3 = new DirectoryTaxonomyWriter(dir3);
+    tw3.addCategory(new CategoryPath("Author", "Zebra Smith"));
+    tw3.addCategory(new CategoryPath("Aardvarks", "Bob"));
+    tw3.addCategory(new CategoryPath("Aardvarks", "Aaron"));
+    tw3.close();
+
+    MemoryOrdinalMap[] maps = new MemoryOrdinalMap[2];
+    maps[0] = new MemoryOrdinalMap();
+    maps[1] = new MemoryOrdinalMap();
+
+    tw1.addTaxonomies(new Directory[] { dir2, dir3 }, maps);
+    tw1.close();
+
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dir1);
+
+    // Test that the merged taxonomy now contains what we expect:
+    // First all the categories of the original taxonomy, in their original order:
+    assertEquals(tr.getPath(0).toString(), "");
+    assertEquals(tr.getPath(1).toString(), "Author");
+    assertEquals(tr.getPath(2).toString(), "Author/Mark Twain");
+    assertEquals(tr.getPath(3).toString(), "Animals");
+    assertEquals(tr.getPath(4).toString(), "Animals/Dog");
+    // Then the categories new in the new taxonomy, in alphabetical order: 
+    assertEquals(tr.getPath(5).toString(), "Aardvarks");
+    assertEquals(tr.getPath(6).toString(), "Aardvarks/Aaron");
+    assertEquals(tr.getPath(7).toString(), "Aardvarks/Bob");
+    assertEquals(tr.getPath(8).toString(), "Author/Rob Pike");
+    assertEquals(tr.getPath(9).toString(), "Author/Zebra Smith");
+    assertEquals(tr.getSize(), 10);
+
+    // Test that the maps contain what we expect
+    int[] map0 = maps[0].getMap();
+    assertEquals(5, map0.length);
+    assertEquals(0, map0[0]);
+    assertEquals(1, map0[1]);
+    assertEquals(8, map0[2]);
+    assertEquals(5, map0[3]);
+    assertEquals(7, map0[4]);
+
+    int[] map1 = maps[1].getMap();
+    assertEquals(6, map1.length);
+    assertEquals(0, map1[0]);
+    assertEquals(1, map1[1]);
+    assertEquals(9, map1[2]);
+    assertEquals(5, map1[3]);
+    assertEquals(7, map1[4]);
+    assertEquals(6, map1[5]);
+    
+    tr.close();
+    dir1.close();
+    dir2.close();
+    dir3.close();
+  }
+
+  // a reasonable random test
+  public void testmedium() throws Exception {
+    int numTests = atLeast(3);
+    for (int i = 0; i < numTests; i++) {
+      dotest(_TestUtil.nextInt(random, 1, 10), 
+             _TestUtil.nextInt(random, 1, 100), 
+             _TestUtil.nextInt(random, 100, 1000),
+             random.nextBoolean());
+    }
+  }
+
+  // A more comprehensive and big random test.
+  @Test @Nightly
+  public void testbig() throws Exception {
+    dotest(2, 1000, 5000, false);
+    dotest(10, 10000, 100, false);
+    dotest(50, 20, 100, false);
+    dotest(10, 1000, 10000, false);
+    dotest(50, 20, 10000, false);
+    dotest(1, 20, 10000, false);
+    dotest(10, 1, 10000, false);
+    dotest(10, 1000, 20000, true);
+  }
+
+  private void dotest(int ntaxonomies, int ncats, int range, boolean disk) throws Exception {
+    Directory dirs[] = new Directory[ntaxonomies];
+    Directory copydirs[] = new Directory[ntaxonomies];
+
+    for (int i=0; i<ntaxonomies; i++) {
+      dirs[i] = newDirectory();
+      copydirs[i] = newDirectory();
+      DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(dirs[i]);
+      DirectoryTaxonomyWriter copytw = new DirectoryTaxonomyWriter(copydirs[i]);
+      for (int j=0; j<ncats; j++) {
+        String cat = Integer.toString(random.nextInt(range));
+        tw.addCategory(new CategoryPath("a",cat));
+        copytw.addCategory(new CategoryPath("a",cat));
+      }
+      // System.err.println("Taxonomy "+i+": "+tw.getSize());
+      tw.close();
+      copytw.close();
+    }
+
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(dirs[0]);
+    Directory otherdirs[] = new Directory[ntaxonomies-1];
+    System.arraycopy(dirs, 1, otherdirs, 0, ntaxonomies-1);
+
+    OrdinalMap[] maps = new OrdinalMap[ntaxonomies-1];
+    if (ntaxonomies>1) {
+      for (int i=0; i<ntaxonomies-1; i++) {
+        if (disk) {
+          // TODO: use a LTC tempfile
+          maps[i] = new DiskOrdinalMap(new File(System.getProperty("java.io.tmpdir"),
+              "tmpmap"+i));
+        } else {
+          maps[i] = new MemoryOrdinalMap();
+        }
+      }
+    }
+
+    tw.addTaxonomies(otherdirs, maps);
+    // System.err.println("Merged axonomy: "+tw.getSize());
+    tw.close();
+
+    // Check that all original categories in the main taxonomy remain in
+    // unchanged, and the rest of the taxonomies are completely unchanged.
+    for (int i=0; i<ntaxonomies; i++) {
+      TaxonomyReader tr = new DirectoryTaxonomyReader(dirs[i]);
+      TaxonomyReader copytr = new DirectoryTaxonomyReader(copydirs[i]);
+      if (i==0) {
+        assertTrue(tr.getSize() >= copytr.getSize());
+      } else {
+        assertEquals(copytr.getSize(), tr.getSize());
+      }
+      for (int j=0; j<copytr.getSize(); j++) {
+        String expected = copytr.getPath(j).toString();
+        String got = tr.getPath(j).toString();
+        assertTrue("Comparing category "+j+" of taxonomy "+i+": expected "+expected+", got "+got,
+            expected.equals(got));
+      }
+      tr.close();
+      copytr.close();
+    }
+
+    // Check that all the new categories in the main taxonomy are in
+    // lexicographic order. This isn't a requirement of our API, but happens
+    // this way in our current implementation.
+    TaxonomyReader tr = new DirectoryTaxonomyReader(dirs[0]);
+    TaxonomyReader copytr = new DirectoryTaxonomyReader(copydirs[0]);
+    if (tr.getSize() > copytr.getSize()) {
+      String prev = tr.getPath(copytr.getSize()).toString();
+      for (int j=copytr.getSize()+1; j<tr.getSize(); j++) {
+        String n = tr.getPath(j).toString();
+        assertTrue(prev.compareTo(n)<0);
+        prev=n;
+      }
+    }
+    int oldsize = copytr.getSize(); // remember for later
+    tr.close();
+    copytr.close();
+
+    // Check that all the categories from other taxonomies exist in the new
+    // taxonomy.
+    TaxonomyReader main = new DirectoryTaxonomyReader(dirs[0]);
+    for (int i=1; i<ntaxonomies; i++) {
+      TaxonomyReader other = new DirectoryTaxonomyReader(dirs[i]);
+      for (int j=0; j<other.getSize(); j++) {
+        int otherord = main.getOrdinal(other.getPath(j));
+        assertTrue(otherord != TaxonomyReader.INVALID_ORDINAL);
+      }
+      other.close();
+    }
+
+    // Check that all the new categories in the merged taxonomy exist in
+    // one of the added taxonomies.
+    TaxonomyReader[] others = new TaxonomyReader[ntaxonomies-1]; 
+    for (int i=1; i<ntaxonomies; i++) {
+      others[i-1] = new DirectoryTaxonomyReader(dirs[i]);
+    }
+    for (int j=oldsize; j<main.getSize(); j++) {
+      boolean found=false;
+      CategoryPath path = main.getPath(j);
+      for (int i=1; i<ntaxonomies; i++) {
+        if (others[i-1].getOrdinal(path) != TaxonomyReader.INVALID_ORDINAL) {
+          found=true;
+          break;
+        }
+      }
+      if (!found) {
+        fail("Found category "+j+" ("+path+") in merged taxonomy not in any of the separate ones");
+      }
+    }
+
+    // Check that all the maps are correct
+    for (int i=0; i<ntaxonomies-1; i++) {
+      int[] map = maps[i].getMap();
+      for (int j=0; j<map.length; j++) {
+        assertEquals(map[j], main.getOrdinal(others[i].getPath(j)));
+      }
+    }
+
+    for (int i=1; i<ntaxonomies; i++) {
+      others[i-1].close();
+    }
+
+    main.close();
+    IOUtils.close(dirs);
+    IOUtils.close(copydirs);
+  }
+
+}
diff --git a/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyReader.java b/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyReader.java
new file mode 100644
index 0000000..2af37fc
--- /dev/null
+++ b/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyReader.java
@@ -0,0 +1,80 @@
+package org.apache.lucene.facet.taxonomy.directory;
+
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestDirectoryTaxonomyReader extends LuceneTestCase {
+
+  @Test
+  public void testCloseAfterIncRef() throws Exception {
+    Directory dir = newDirectory();
+    DirectoryTaxonomyWriter ltw = new DirectoryTaxonomyWriter(dir);
+    ltw.addCategory(new CategoryPath("a"));
+    ltw.close();
+    
+    DirectoryTaxonomyReader ltr = new DirectoryTaxonomyReader(dir);
+    ltr.incRef();
+    ltr.close();
+    
+    // should not fail as we incRef() before close
+    ltr.getSize();
+    ltr.decRef();
+    
+    dir.close();
+  }
+  
+  @Test
+  public void testCloseTwice() throws Exception {
+    Directory dir = newDirectory();
+    DirectoryTaxonomyWriter ltw = new DirectoryTaxonomyWriter(dir);
+    ltw.addCategory(new CategoryPath("a"));
+    ltw.close();
+    
+    DirectoryTaxonomyReader ltr = new DirectoryTaxonomyReader(dir);
+    ltr.close();
+    ltr.close(); // no exception should be thrown
+    
+    dir.close();
+  }
+  
+  @Test
+  public void testAlreadyClosed() throws Exception {
+    Directory dir = newDirectory();
+    DirectoryTaxonomyWriter ltw = new DirectoryTaxonomyWriter(dir);
+    ltw.addCategory(new CategoryPath("a"));
+    ltw.close();
+    
+    DirectoryTaxonomyReader ltr = new DirectoryTaxonomyReader(dir);
+    ltr.close();
+    try {
+      ltr.getSize();
+      fail("An AlreadyClosedException should have been thrown here");
+    } catch (AlreadyClosedException ace) {
+      // good!
+    }
+    dir.close();
+  }
+  
+}
diff --git a/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java b/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java
new file mode 100644
index 0000000..73bafde
--- /dev/null
+++ b/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java
@@ -0,0 +1,89 @@
+package org.apache.lucene.facet.taxonomy.directory;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.store.Directory;
+import org.junit.Test;
+
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestDirectoryTaxonomyWriter extends LuceneTestCase {
+
+  // A No-Op TaxonomyWriterCache which always discards all given categories, and
+  // always returns true in put(), to indicate some cache entries were cleared.
+  private static class NoOpCache implements TaxonomyWriterCache {
+
+    NoOpCache() { }
+    
+    public void close() {}
+    public int get(CategoryPath categoryPath) { return -1; }
+    public int get(CategoryPath categoryPath, int length) { return get(categoryPath); }
+    public boolean put(CategoryPath categoryPath, int ordinal) { return true; }
+    public boolean put(CategoryPath categoryPath, int prefixLen, int ordinal) { return true; }
+    public boolean hasRoom(int numberOfEntries) { return false; }
+    
+  }
+  
+  @Test
+  public void testCommit() throws Exception {
+    // Verifies that nothing is committed to the underlying Directory, if
+    // commit() wasn't called.
+    Directory dir = newDirectory();
+    DirectoryTaxonomyWriter ltw = new DirectoryTaxonomyWriter(dir, OpenMode.CREATE_OR_APPEND, new NoOpCache());
+    assertFalse(IndexReader.indexExists(dir));
+    ltw.commit(); // first commit, so that an index will be created
+    ltw.addCategory(new CategoryPath("a"));
+    
+    IndexReader r = IndexReader.open(dir);
+    assertEquals("No categories should have been committed to the underlying directory", 1, r.numDocs());
+    r.close();
+    ltw.close();
+    dir.close();
+  }
+  
+  @Test
+  public void testCommitUserData() throws Exception {
+    // Verifies that committed data is retrievable
+    Directory dir = newDirectory();
+    DirectoryTaxonomyWriter ltw = new DirectoryTaxonomyWriter(dir, OpenMode.CREATE_OR_APPEND, new NoOpCache());
+    assertFalse(IndexReader.indexExists(dir));
+    ltw.commit(); // first commit, so that an index will be created
+    ltw.addCategory(new CategoryPath("a"));
+    ltw.addCategory(new CategoryPath("b"));
+    Map <String, String> userCommitData = new HashMap<String, String>();
+    userCommitData.put("testing", "1 2 3");
+    ltw.commit(userCommitData);
+    ltw.close();
+    IndexReader r = IndexReader.open(dir);
+    assertEquals("2 categories plus root should have been committed to the underlying directory", 3, r.numDocs());
+    Map <String, String> readUserCommitData = r.getCommitUserData();
+    assertTrue("wrong value extracted from commit data", 
+        "1 2 3".equals(readUserCommitData.get("testing")));
+    r.close();
+    dir.close();
+  }
+  
+}
diff --git a/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestIndexClose.java b/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestIndexClose.java
new file mode 100644
index 0000000..77c0086
--- /dev/null
+++ b/modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestIndexClose.java
@@ -0,0 +1,204 @@
+package org.apache.lucene.facet.taxonomy.directory;
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FilterIndexReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.LockObtainFailedException;
+import org.junit.Test;
+
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * This test case attempts to catch index "leaks" in LuceneTaxonomyReader/Writer,
+ * i.e., cases where an index has been opened, but never closed; In that case,
+ * Java would eventually collect this object and close the index, but leaving
+ * the index open might nevertheless cause problems - e.g., on Windows it prevents
+ * deleting it.
+ */
+public class TestIndexClose extends LuceneTestCase {
+
+  @Test
+  public void testLeaks() throws Exception {
+    LeakChecker checker = new LeakChecker();
+    Directory dir = newDirectory();
+    DirectoryTaxonomyWriter tw = checker.openWriter(dir);
+    tw.close();
+    assertEquals(0, checker.nopen());
+
+    tw = checker.openWriter(dir);
+    tw.addCategory(new CategoryPath("animal", "dog"));
+    tw.close();
+    assertEquals(0, checker.nopen());
+
+    DirectoryTaxonomyReader tr = checker.openReader(dir);
+    tr.getPath(1);
+    tr.refresh();
+    tr.close();
+    assertEquals(0, checker.nopen());
+
+    tr = checker.openReader(dir);
+    tw = checker.openWriter(dir);
+    tw.addCategory(new CategoryPath("animal", "cat"));
+    tr.refresh();
+    tw.commit();
+    tw.close();
+    tr.refresh();
+    tr.close();
+    assertEquals(0, checker.nopen());
+
+    tw = checker.openWriter(dir);
+    for (int i=0; i<10000; i++) {
+      tw.addCategory(new CategoryPath("number", Integer.toString(i)));
+    }
+    tw.close();
+    assertEquals(0, checker.nopen());
+    tw = checker.openWriter(dir);
+    for (int i=0; i<10000; i++) {
+      tw.addCategory(new CategoryPath("number", Integer.toString(i*2)));
+    }
+    tw.close();
+    assertEquals(0, checker.nopen());
+    dir.close();
+  }
+
+  private static class LeakChecker {
+    int ireader=0;
+    Set<Integer> openReaders = new HashSet<Integer>();
+
+    int iwriter=0;
+    Set<Integer> openWriters = new HashSet<Integer>();
+
+    LeakChecker() { }
+    
+    public DirectoryTaxonomyWriter openWriter(Directory dir) throws CorruptIndexException, LockObtainFailedException, IOException {
+      return new InstrumentedTaxonomyWriter(dir);
+    }
+
+    public DirectoryTaxonomyReader openReader(Directory dir) throws CorruptIndexException, LockObtainFailedException, IOException {
+      return new InstrumentedTaxonomyReader(dir);
+    }
+
+    public int nopen() {
+      int ret=0;
+      for (int i: openReaders) {
+        System.err.println("reader "+i+" still open");
+        ret++;
+      }
+      for (int i: openWriters) {
+        System.err.println("writer "+i+" still open");
+        ret++;
+      }
+      return ret;
+    }
+
+    private class InstrumentedTaxonomyWriter extends DirectoryTaxonomyWriter {
+      public InstrumentedTaxonomyWriter(Directory dir) throws CorruptIndexException, LockObtainFailedException, IOException {
+        super(dir);
+      }    
+      @Override
+      protected IndexReader openReader() throws IOException {
+        return new InstrumentedIndexReader(super.openReader()); 
+      }
+      @Override
+      protected void openIndexWriter (Directory directory, OpenMode openMode) throws IOException {
+        indexWriter = new InstrumentedIndexWriter(directory,
+            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))
+                .setOpenMode(openMode));
+      }
+
+    }
+
+    private class InstrumentedTaxonomyReader extends DirectoryTaxonomyReader {
+      public InstrumentedTaxonomyReader(Directory dir) throws CorruptIndexException, LockObtainFailedException, IOException {
+        super(dir);
+      }  
+      @Override
+      protected IndexReader openIndexReader(Directory dir) throws CorruptIndexException, IOException {
+        return new InstrumentedIndexReader(IndexReader.open(dir,true)); 
+      }
+
+    }
+
+    private class InstrumentedIndexReader extends FilterIndexReader {
+      int mynum;
+      public InstrumentedIndexReader(IndexReader in) {
+        super(in);
+        this.in = in;
+        mynum = ireader++;
+        openReaders.add(mynum);
+        //        System.err.println("opened "+mynum);
+      }
+      @Override
+      protected synchronized IndexReader doOpenIfChanged() throws CorruptIndexException, IOException {
+        IndexReader n = IndexReader.openIfChanged(in);
+        if (n == null) {
+          return null;
+        }
+        return new InstrumentedIndexReader(n);
+      }
+
+      // Unfortunately, IndexReader.close() is marked final so we can't
+      // change it! Fortunately, close() calls (if the object wasn't
+      // already closed) doClose() so we can override it to do our thing -
+      // just like FilterIndexReader does.
+      @Override
+      public void doClose() throws IOException {
+        in.close();
+        if (!openReaders.contains(mynum)) { // probably can't happen...
+          fail("Reader #"+mynum+" was closed twice!");
+        }
+        openReaders.remove(mynum);
+        //        System.err.println("closed "+mynum);
+      }
+    }
+    private class InstrumentedIndexWriter extends IndexWriter {
+      int mynum;
+      public InstrumentedIndexWriter(Directory d, IndexWriterConfig conf) throws CorruptIndexException, LockObtainFailedException, IOException {
+        super(d, conf);
+        mynum = iwriter++;
+        openWriters.add(mynum);
+        //        System.err.println("openedw "+mynum);
+      }
+
+      @Override
+      public void close() throws IOException {
+        super.close();
+        if (!openWriters.contains(mynum)) { // probably can't happen...
+          fail("Writer #"+mynum+" was closed twice!");
+        }
+        openWriters.remove(mynum);
+        //        System.err.println("closedw "+mynum);
+      }
+    }
+  }
+}
diff --git a/modules/facet/src/test/org/apache/lucene/facet/taxonomy/lucene/TestAddTaxonomies.java b/modules/facet/src/test/org/apache/lucene/facet/taxonomy/lucene/TestAddTaxonomies.java
deleted file mode 100644
index 2dfb5ec..0000000
--- a/modules/facet/src/test/org/apache/lucene/facet/taxonomy/lucene/TestAddTaxonomies.java
+++ /dev/null
@@ -1,254 +0,0 @@
-package org.apache.lucene.facet.taxonomy.lucene;
-
-import java.io.File;
-
-import org.apache.lucene.store.Directory;
-import org.junit.Test;
-
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter.DiskOrdinalMap;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter.MemoryOrdinalMap;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter.OrdinalMap;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestAddTaxonomies extends LuceneTestCase {
-
-  @Test
-  public void test1() throws Exception {
-    Directory dir1 = newDirectory();
-    LuceneTaxonomyWriter tw1 = new LuceneTaxonomyWriter(dir1);
-    tw1.addCategory(new CategoryPath("Author", "Mark Twain"));
-    tw1.addCategory(new CategoryPath("Animals", "Dog"));
-    Directory dir2 = newDirectory();
-    LuceneTaxonomyWriter tw2 = new LuceneTaxonomyWriter(dir2);
-    tw2.addCategory(new CategoryPath("Author", "Rob Pike"));
-    tw2.addCategory(new CategoryPath("Aardvarks", "Bob"));
-    tw2.close();
-    Directory dir3 = newDirectory();
-    LuceneTaxonomyWriter tw3 = new LuceneTaxonomyWriter(dir3);
-    tw3.addCategory(new CategoryPath("Author", "Zebra Smith"));
-    tw3.addCategory(new CategoryPath("Aardvarks", "Bob"));
-    tw3.addCategory(new CategoryPath("Aardvarks", "Aaron"));
-    tw3.close();
-
-    MemoryOrdinalMap[] maps = new MemoryOrdinalMap[2];
-    maps[0] = new MemoryOrdinalMap();
-    maps[1] = new MemoryOrdinalMap();
-
-    tw1.addTaxonomies(new Directory[] { dir2, dir3 }, maps);
-    tw1.close();
-
-    TaxonomyReader tr = new LuceneTaxonomyReader(dir1);
-
-    // Test that the merged taxonomy now contains what we expect:
-    // First all the categories of the original taxonomy, in their original order:
-    assertEquals(tr.getPath(0).toString(), "");
-    assertEquals(tr.getPath(1).toString(), "Author");
-    assertEquals(tr.getPath(2).toString(), "Author/Mark Twain");
-    assertEquals(tr.getPath(3).toString(), "Animals");
-    assertEquals(tr.getPath(4).toString(), "Animals/Dog");
-    // Then the categories new in the new taxonomy, in alphabetical order: 
-    assertEquals(tr.getPath(5).toString(), "Aardvarks");
-    assertEquals(tr.getPath(6).toString(), "Aardvarks/Aaron");
-    assertEquals(tr.getPath(7).toString(), "Aardvarks/Bob");
-    assertEquals(tr.getPath(8).toString(), "Author/Rob Pike");
-    assertEquals(tr.getPath(9).toString(), "Author/Zebra Smith");
-    assertEquals(tr.getSize(), 10);
-
-    // Test that the maps contain what we expect
-    int[] map0 = maps[0].getMap();
-    assertEquals(5, map0.length);
-    assertEquals(0, map0[0]);
-    assertEquals(1, map0[1]);
-    assertEquals(8, map0[2]);
-    assertEquals(5, map0[3]);
-    assertEquals(7, map0[4]);
-
-    int[] map1 = maps[1].getMap();
-    assertEquals(6, map1.length);
-    assertEquals(0, map1[0]);
-    assertEquals(1, map1[1]);
-    assertEquals(9, map1[2]);
-    assertEquals(5, map1[3]);
-    assertEquals(7, map1[4]);
-    assertEquals(6, map1[5]);
-    
-    tr.close();
-    dir1.close();
-    dir2.close();
-    dir3.close();
-  }
-
-  // a reasonable random test
-  public void testmedium() throws Exception {
-    int numTests = atLeast(3);
-    for (int i = 0; i < numTests; i++) {
-      dotest(_TestUtil.nextInt(random, 1, 10), 
-             _TestUtil.nextInt(random, 1, 100), 
-             _TestUtil.nextInt(random, 100, 1000),
-             random.nextBoolean());
-    }
-  }
-
-  // A more comprehensive and big random test.
-  @Test @Nightly
-  public void testbig() throws Exception {
-    dotest(2, 1000, 5000, false);
-    dotest(10, 10000, 100, false);
-    dotest(50, 20, 100, false);
-    dotest(10, 1000, 10000, false);
-    dotest(50, 20, 10000, false);
-    dotest(1, 20, 10000, false);
-    dotest(10, 1, 10000, false);
-    dotest(10, 1000, 20000, true);
-  }
-
-  private void dotest(int ntaxonomies, int ncats, int range, boolean disk) throws Exception {
-    Directory dirs[] = new Directory[ntaxonomies];
-    Directory copydirs[] = new Directory[ntaxonomies];
-
-    for (int i=0; i<ntaxonomies; i++) {
-      dirs[i] = newDirectory();
-      copydirs[i] = newDirectory();
-      LuceneTaxonomyWriter tw = new LuceneTaxonomyWriter(dirs[i]);
-      LuceneTaxonomyWriter copytw = new LuceneTaxonomyWriter(copydirs[i]);
-      for (int j=0; j<ncats; j++) {
-        String cat = Integer.toString(random.nextInt(range));
-        tw.addCategory(new CategoryPath("a",cat));
-        copytw.addCategory(new CategoryPath("a",cat));
-      }
-      // System.err.println("Taxonomy "+i+": "+tw.getSize());
-      tw.close();
-      copytw.close();
-    }
-
-    LuceneTaxonomyWriter tw = new LuceneTaxonomyWriter(dirs[0]);
-    Directory otherdirs[] = new Directory[ntaxonomies-1];
-    System.arraycopy(dirs, 1, otherdirs, 0, ntaxonomies-1);
-
-    OrdinalMap[] maps = new OrdinalMap[ntaxonomies-1];
-    if (ntaxonomies>1) {
-      for (int i=0; i<ntaxonomies-1; i++) {
-        if (disk) {
-          // TODO: use a LTC tempfile
-          maps[i] = new DiskOrdinalMap(new File(System.getProperty("java.io.tmpdir"),
-              "tmpmap"+i));
-        } else {
-          maps[i] = new MemoryOrdinalMap();
-        }
-      }
-    }
-
-    tw.addTaxonomies(otherdirs, maps);
-    // System.err.println("Merged axonomy: "+tw.getSize());
-    tw.close();
-
-    // Check that all original categories in the main taxonomy remain in
-    // unchanged, and the rest of the taxonomies are completely unchanged.
-    for (int i=0; i<ntaxonomies; i++) {
-      TaxonomyReader tr = new LuceneTaxonomyReader(dirs[i]);
-      TaxonomyReader copytr = new LuceneTaxonomyReader(copydirs[i]);
-      if (i==0) {
-        assertTrue(tr.getSize() >= copytr.getSize());
-      } else {
-        assertEquals(copytr.getSize(), tr.getSize());
-      }
-      for (int j=0; j<copytr.getSize(); j++) {
-        String expected = copytr.getPath(j).toString();
-        String got = tr.getPath(j).toString();
-        assertTrue("Comparing category "+j+" of taxonomy "+i+": expected "+expected+", got "+got,
-            expected.equals(got));
-      }
-      tr.close();
-      copytr.close();
-    }
-
-    // Check that all the new categories in the main taxonomy are in
-    // lexicographic order. This isn't a requirement of our API, but happens
-    // this way in our current implementation.
-    TaxonomyReader tr = new LuceneTaxonomyReader(dirs[0]);
-    TaxonomyReader copytr = new LuceneTaxonomyReader(copydirs[0]);
-    if (tr.getSize() > copytr.getSize()) {
-      String prev = tr.getPath(copytr.getSize()).toString();
-      for (int j=copytr.getSize()+1; j<tr.getSize(); j++) {
-        String n = tr.getPath(j).toString();
-        assertTrue(prev.compareTo(n)<0);
-        prev=n;
-      }
-    }
-    int oldsize = copytr.getSize(); // remember for later
-    tr.close();
-    copytr.close();
-
-    // Check that all the categories from other taxonomies exist in the new
-    // taxonomy.
-    TaxonomyReader main = new LuceneTaxonomyReader(dirs[0]);
-    for (int i=1; i<ntaxonomies; i++) {
-      TaxonomyReader other = new LuceneTaxonomyReader(dirs[i]);
-      for (int j=0; j<other.getSize(); j++) {
-        int otherord = main.getOrdinal(other.getPath(j));
-        assertTrue(otherord != TaxonomyReader.INVALID_ORDINAL);
-      }
-      other.close();
-    }
-
-    // Check that all the new categories in the merged taxonomy exist in
-    // one of the added taxonomies.
-    TaxonomyReader[] others = new TaxonomyReader[ntaxonomies-1]; 
-    for (int i=1; i<ntaxonomies; i++) {
-      others[i-1] = new LuceneTaxonomyReader(dirs[i]);
-    }
-    for (int j=oldsize; j<main.getSize(); j++) {
-      boolean found=false;
-      CategoryPath path = main.getPath(j);
-      for (int i=1; i<ntaxonomies; i++) {
-        if (others[i-1].getOrdinal(path) != TaxonomyReader.INVALID_ORDINAL) {
-          found=true;
-          break;
-        }
-      }
-      if (!found) {
-        fail("Found category "+j+" ("+path+") in merged taxonomy not in any of the separate ones");
-      }
-    }
-
-    // Check that all the maps are correct
-    for (int i=0; i<ntaxonomies-1; i++) {
-      int[] map = maps[i].getMap();
-      for (int j=0; j<map.length; j++) {
-        assertEquals(map[j], main.getOrdinal(others[i].getPath(j)));
-      }
-    }
-
-    for (int i=1; i<ntaxonomies; i++) {
-      others[i-1].close();
-    }
-
-    main.close();
-    IOUtils.close(dirs);
-    IOUtils.close(copydirs);
-  }
-
-}
diff --git a/modules/facet/src/test/org/apache/lucene/facet/taxonomy/lucene/TestIndexClose.java b/modules/facet/src/test/org/apache/lucene/facet/taxonomy/lucene/TestIndexClose.java
deleted file mode 100644
index ef1276b..0000000
--- a/modules/facet/src/test/org/apache/lucene/facet/taxonomy/lucene/TestIndexClose.java
+++ /dev/null
@@ -1,205 +0,0 @@
-package org.apache.lucene.facet.taxonomy.lucene;
-
-import java.io.IOException;
-import java.util.HashSet;
-import java.util.Set;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FilterIndexReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.LockObtainFailedException;
-import org.junit.Test;
-
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * This test case attempts to catch index "leaks" in LuceneTaxonomyReader/Writer,
- * i.e., cases where an index has been opened, but never closed; In that case,
- * Java would eventually collect this object and close the index, but leaving
- * the index open might nevertheless cause problems - e.g., on Windows it prevents
- * deleting it.
- */
-public class TestIndexClose extends LuceneTestCase {
-
-  @Test
-  public void testLeaks() throws Exception {
-    LeakChecker checker = new LeakChecker();
-    Directory dir = newDirectory();
-    LuceneTaxonomyWriter tw = checker.openWriter(dir);
-    tw.close();
-    assertEquals(0, checker.nopen());
-
-    tw = checker.openWriter(dir);
-    tw.addCategory(new CategoryPath("animal", "dog"));
-    tw.close();
-    assertEquals(0, checker.nopen());
-
-    LuceneTaxonomyReader tr = checker.openReader(dir);
-    tr.getPath(1);
-    tr.refresh();
-    tr.close();
-    assertEquals(0, checker.nopen());
-
-    tr = checker.openReader(dir);
-    tw = checker.openWriter(dir);
-    tw.addCategory(new CategoryPath("animal", "cat"));
-    tr.refresh();
-    tw.commit();
-    tw.close();
-    tr.refresh();
-    tr.close();
-    assertEquals(0, checker.nopen());
-
-    tw = checker.openWriter(dir);
-    for (int i=0; i<10000; i++) {
-      tw.addCategory(new CategoryPath("number", Integer.toString(i)));
-    }
-    tw.close();
-    assertEquals(0, checker.nopen());
-    tw = checker.openWriter(dir);
-    for (int i=0; i<10000; i++) {
-      tw.addCategory(new CategoryPath("number", Integer.toString(i*2)));
-    }
-    tw.close();
-    assertEquals(0, checker.nopen());
-    dir.close();
-  }
-
-  private static class LeakChecker {
-    int ireader=0;
-    Set<Integer> openReaders = new HashSet<Integer>();
-
-    int iwriter=0;
-    Set<Integer> openWriters = new HashSet<Integer>();
-
-    LeakChecker() { }
-    
-    public LuceneTaxonomyWriter openWriter(Directory dir) throws CorruptIndexException, LockObtainFailedException, IOException {
-      return new InstrumentedTaxonomyWriter(dir);
-    }
-
-    public LuceneTaxonomyReader openReader(Directory dir) throws CorruptIndexException, LockObtainFailedException, IOException {
-      return new InstrumentedTaxonomyReader(dir);
-    }
-
-    public int nopen() {
-      int ret=0;
-      for (int i: openReaders) {
-        System.err.println("reader "+i+" still open");
-        ret++;
-      }
-      for (int i: openWriters) {
-        System.err.println("writer "+i+" still open");
-        ret++;
-      }
-      return ret;
-    }
-
-    private class InstrumentedTaxonomyWriter extends LuceneTaxonomyWriter {
-      public InstrumentedTaxonomyWriter(Directory dir) throws CorruptIndexException, LockObtainFailedException, IOException {
-        super(dir);
-      }    
-      @Override
-      protected IndexReader openReader() throws IOException {
-        return new InstrumentedIndexReader(super.openReader()); 
-      }
-      @Override
-      protected void openLuceneIndex (Directory directory, OpenMode openMode)
-      throws CorruptIndexException, LockObtainFailedException, IOException {
-        indexWriter = new InstrumentedIndexWriter(directory,
-            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))
-                .setOpenMode(openMode));
-      }
-
-    }
-
-    private class InstrumentedTaxonomyReader extends LuceneTaxonomyReader {
-      public InstrumentedTaxonomyReader(Directory dir) throws CorruptIndexException, LockObtainFailedException, IOException {
-        super(dir);
-      }  
-      @Override
-      protected IndexReader openIndexReader(Directory dir) throws CorruptIndexException, IOException {
-        return new InstrumentedIndexReader(IndexReader.open(dir,true)); 
-      }
-
-    }
-
-    private class InstrumentedIndexReader extends FilterIndexReader {
-      int mynum;
-      public InstrumentedIndexReader(IndexReader in) {
-        super(in);
-        this.in = in;
-        mynum = ireader++;
-        openReaders.add(mynum);
-        //        System.err.println("opened "+mynum);
-      }
-      @Override
-      protected synchronized IndexReader doOpenIfChanged() throws CorruptIndexException, IOException {
-        IndexReader n = IndexReader.openIfChanged(in);
-        if (n == null) {
-          return null;
-        }
-        return new InstrumentedIndexReader(n);
-      }
-
-      // Unfortunately, IndexReader.close() is marked final so we can't
-      // change it! Fortunately, close() calls (if the object wasn't
-      // already closed) doClose() so we can override it to do our thing -
-      // just like FilterIndexReader does.
-      @Override
-      public void doClose() throws IOException {
-        in.close();
-        if (!openReaders.contains(mynum)) { // probably can't happen...
-          fail("Reader #"+mynum+" was closed twice!");
-        }
-        openReaders.remove(mynum);
-        //        System.err.println("closed "+mynum);
-      }
-    }
-    private class InstrumentedIndexWriter extends IndexWriter {
-      int mynum;
-      public InstrumentedIndexWriter(Directory d, IndexWriterConfig conf) throws CorruptIndexException, LockObtainFailedException, IOException {
-        super(d, conf);
-        mynum = iwriter++;
-        openWriters.add(mynum);
-        //        System.err.println("openedw "+mynum);
-      }
-
-      @Override
-      public void close() throws IOException {
-        super.close();
-        if (!openWriters.contains(mynum)) { // probably can't happen...
-          fail("Writer #"+mynum+" was closed twice!");
-        }
-        openWriters.remove(mynum);
-        //        System.err.println("closedw "+mynum);
-      }
-    }
-  }
-}
diff --git a/modules/facet/src/test/org/apache/lucene/facet/taxonomy/lucene/TestLuceneTaxonomyReader.java b/modules/facet/src/test/org/apache/lucene/facet/taxonomy/lucene/TestLuceneTaxonomyReader.java
deleted file mode 100644
index 5071b76..0000000
--- a/modules/facet/src/test/org/apache/lucene/facet/taxonomy/lucene/TestLuceneTaxonomyReader.java
+++ /dev/null
@@ -1,78 +0,0 @@
-package org.apache.lucene.facet.taxonomy.lucene;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.junit.Test;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestLuceneTaxonomyReader extends LuceneTestCase {
-
-  @Test
-  public void testCloseAfterIncRef() throws Exception {
-    Directory dir = newDirectory();
-    LuceneTaxonomyWriter ltw = new LuceneTaxonomyWriter(dir);
-    ltw.addCategory(new CategoryPath("a"));
-    ltw.close();
-    
-    LuceneTaxonomyReader ltr = new LuceneTaxonomyReader(dir);
-    ltr.incRef();
-    ltr.close();
-    
-    // should not fail as we incRef() before close
-    ltr.getSize();
-    ltr.decRef();
-    
-    dir.close();
-  }
-  
-  @Test
-  public void testCloseTwice() throws Exception {
-    Directory dir = newDirectory();
-    LuceneTaxonomyWriter ltw = new LuceneTaxonomyWriter(dir);
-    ltw.addCategory(new CategoryPath("a"));
-    ltw.close();
-    
-    LuceneTaxonomyReader ltr = new LuceneTaxonomyReader(dir);
-    ltr.close();
-    ltr.close(); // no exception should be thrown
-    
-    dir.close();
-  }
-  
-  @Test
-  public void testAlreadyClosed() throws Exception {
-    Directory dir = newDirectory();
-    LuceneTaxonomyWriter ltw = new LuceneTaxonomyWriter(dir);
-    ltw.addCategory(new CategoryPath("a"));
-    ltw.close();
-    
-    LuceneTaxonomyReader ltr = new LuceneTaxonomyReader(dir);
-    ltr.close();
-    try {
-      ltr.getSize();
-      fail("An AlreadyClosedException should have been thrown here");
-    } catch (AlreadyClosedException ace) {
-      // good!
-    }
-    dir.close();
-  }
-  
-}
diff --git a/modules/facet/src/test/org/apache/lucene/facet/taxonomy/lucene/TestLuceneTaxonomyWriter.java b/modules/facet/src/test/org/apache/lucene/facet/taxonomy/lucene/TestLuceneTaxonomyWriter.java
deleted file mode 100644
index 37bd968..0000000
--- a/modules/facet/src/test/org/apache/lucene/facet/taxonomy/lucene/TestLuceneTaxonomyWriter.java
+++ /dev/null
@@ -1,89 +0,0 @@
-package org.apache.lucene.facet.taxonomy.lucene;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.store.Directory;
-import org.junit.Test;
-
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.lucene.LuceneTaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestLuceneTaxonomyWriter extends LuceneTestCase {
-
-  // A No-Op TaxonomyWriterCache which always discards all given categories, and
-  // always returns true in put(), to indicate some cache entries were cleared.
-  private static class NoOpCache implements TaxonomyWriterCache {
-
-    NoOpCache() { }
-    
-    public void close() {}
-    public int get(CategoryPath categoryPath) { return -1; }
-    public int get(CategoryPath categoryPath, int length) { return get(categoryPath); }
-    public boolean put(CategoryPath categoryPath, int ordinal) { return true; }
-    public boolean put(CategoryPath categoryPath, int prefixLen, int ordinal) { return true; }
-    public boolean hasRoom(int numberOfEntries) { return false; }
-    
-  }
-  
-  @Test
-  public void testCommit() throws Exception {
-    // Verifies that nothing is committed to the underlying Directory, if
-    // commit() wasn't called.
-    Directory dir = newDirectory();
-    LuceneTaxonomyWriter ltw = new LuceneTaxonomyWriter(dir, OpenMode.CREATE_OR_APPEND, new NoOpCache());
-    assertFalse(IndexReader.indexExists(dir));
-    ltw.commit(); // first commit, so that an index will be created
-    ltw.addCategory(new CategoryPath("a"));
-    
-    IndexReader r = IndexReader.open(dir);
-    assertEquals("No categories should have been committed to the underlying directory", 1, r.numDocs());
-    r.close();
-    ltw.close();
-    dir.close();
-  }
-  
-  @Test
-  public void testCommitUserData() throws Exception {
-    // Verifies that committed data is retrievable
-    Directory dir = newDirectory();
-    LuceneTaxonomyWriter ltw = new LuceneTaxonomyWriter(dir, OpenMode.CREATE_OR_APPEND, new NoOpCache());
-    assertFalse(IndexReader.indexExists(dir));
-    ltw.commit(); // first commit, so that an index will be created
-    ltw.addCategory(new CategoryPath("a"));
-    ltw.addCategory(new CategoryPath("b"));
-    Map <String, String> userCommitData = new HashMap<String, String>();
-    userCommitData.put("testing", "1 2 3");
-    ltw.commit(userCommitData);
-    ltw.close();
-    IndexReader r = IndexReader.open(dir);
-    assertEquals("2 categories plus root should have been committed to the underlying directory", 3, r.numDocs());
-    Map <String, String> readUserCommitData = r.getCommitUserData();
-    assertTrue("wrong value extracted from commit data", 
-        "1 2 3".equals(readUserCommitData.get("testing")));
-    r.close();
-    dir.close();
-  }
-  
-}

